[
  {
    "title": "Model conversion overview  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/lite/models/convert",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nRunning ML models\nBuild TensorFlow Lite\nHardware acceleration\nDelegates\nGPU delegates\nImplementing a delegate\nAndroid\nOverview\nGoogle Play services runtime\nDevelopment tools\nHardware acceleration\nModels with metadata\niOS\nOverview\nCore ML delegate\nGPU delegate\nMicrocontrollers\nOverview\nGet started\nLinux-based devices with Python\nUnderstand the C library\nBuild and convert models\nModels\nSupport multiple frameworks\nUse trained models\nDesign and build models\nConvert TF models\nOverview\nConvert TensorFlow models\nAdd Signatures\nConversion tools\nModel compatibility\nOptimize models\nAdd model metadata\nConvert PyTorch models\nOverview\nPyTorch to TFLite quickstart\nConvert generative PyTorch models\nConvert JAX models\nOverview\nJAX to TFLite quickstart\nOn this page\nConversion workflow\nInput model formats\nConversion evaluation\nModel conversion\nAdvanced conversion\nNext steps\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nTensorFlow Lite\nWas this helpful?\nSend feedback\nModel conversion overview \nbookmark_border\n\nThe machine learning (ML) models you use with TensorFlow Lite are originally built and trained using TensorFlow core libraries and tools. Once you've built a model with TensorFlow core, you can convert it to a smaller, more efficient ML model format called a TensorFlow Lite model. This section provides guidance for converting your TensorFlow models to the TensorFlow Lite model format.\n\nNote: If you don't have a model to convert yet, see the Models overview page for guidance on choosing or building models.\nConversion workflow\n\nConverting TensorFlow models to TensorFlow Lite format can take a few paths depending on the content of your ML model. As the first step of that process, you should evaluate your model to determine if it can be directly converted. This evaluation determines if the content of the model is supported by the standard TensorFlow Lite runtime environments based on the TensorFlow operations it uses. If your model uses operations outside of the supported set, you have the option to refactor your model or use advanced conversion techniques.\n\nThe diagram below shows the high level steps in converting a model.\n\nFigure 1. TensorFlow Lite conversion workflow.\n\nThe following sections outline the process of evaluating and converting models for use with TensorFlow Lite.\n\nInput model formats\n\nYou can use the converter with the following input model formats:\n\nSavedModel (recommended): A TensorFlow model saved as a set of files on disk.\nKeras model: A model created using the high level Keras API.\nKeras H5 format: A light-weight alternative to SavedModel format supported by Keras API.\nModels built from concrete functions: A model created using the low level TensorFlow API.\n\nYou can save both the Keras and concrete function models as a SavedModel and convert using the recommeded path.\n\nNote: To avoid errors during inference, include signatures when exporting to the SavedModel format. The TensorFlow converter supports converting TensorFlow model's input/output specifications to TensorFlow Lite models. See the topic on adding signatures.\n\nIf you have a Jax model, you can use the TFLiteConverter.experimental_from_jax API to convert it to the TensorFlow Lite format. Note that this API is subject to change while in experimental mode.\n\nConversion evaluation\n\nEvaluating your model is an important step before attempting to convert it. When evaluating, you want to determine if the contents of your model is compatible with the TensorFlow Lite format. You should also determine if your model is a good fit for use on mobile and edge devices in terms of the size of data the model uses, its hardware processing requirements, and the model's overall size and complexity.\n\nFor many models, the converter should work out of the box. However, TensorFlow Lite builtin operator library supports a subset of TensorFlow core operators, which means some models may need additional steps before converting to TensorFlow Lite. Additionally some operations that are supported by TensorFlow Lite have restricted usage requirements for performance reasons. See the operator compatibility guide to determine if your model needs to be refactored for conversion.\n\nKey Point: Most models can be directly converted to TensorFlow Lite format. Some models may require refactoring or use of advanced conversion techniques to make them compatible.\nModel conversion\n\nThe TensorFlow Lite converter takes a TensorFlow model and generates a TensorFlow Lite model (an optimized FlatBuffer format identified by the .tflite file extension). You can load a SavedModel or directly convert a model you create in code.\n\nThe converter takes 3 main flags (or options) that customize the conversion for your model:\n\nCompatibility flags allow you to specify whether the conversion should allow custom operators.\nOptimization flags allow you to specify the type of optimization to apply during conversion. The most commonly used optimization technique is post-training quanitization.\nMetadata flags allow you to add metadata to the converted model which makes it easier to create platform specific wrapper code when deploying models on devices.\n\nYou can convert your model using the Python API or the Command line tool. See the Convert TF model guide for step by step instructions on running the converter on your model.\n\nTypically you would convert your model for the standard TensorFlow Lite runtime environment or the Google Play services runtime environment for TensorFlow Lite (Beta). Some advanced use cases require customization of model runtime environment, which require additional steps in the conversion proceess. See the advanced runtime environment section of the Android overview for more guidance.\n\nAdvanced conversion\n\nIf you run into errors while running the converter on your model, it's most likely that you have an operator compatibility issue. Not all TensorFlow operations are supported by TensorFlow Lite. You can work around these issues by refactoring your model, or by using advanced conversion options that allow you to create a modified TensorFlow Lite format model and a custom runtime environment for that model.\n\nSee the Model compatibility overview for more information on TensorFlow and TensorFlow Lite model compatibility considerations.\nTopics under the Model compatibility overview cover advanced techniques for refactoring your model, such as the Select operators guide.\nFor full list of operations and limitations see TensorFlow Lite Ops page.\nNext steps\nSee the convert TF models guide to quickly get started on converting your model.\nSee the optimization overview for guidance on how to optimize your converted model using techniques like post-training quanitization.\nSee the Adding metadata overview to learn how to add metadata to your models. Metadata provides other uses a description of your model as well as information that can be leveraged by code generators.\nWas this helpful?\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-03 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Guess the word  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/prompts/guess-the-word",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nJoin the Gemini API Developer Competition! Learn more\nPrompt gallery\nGuess the word\n\nPlay a word game with AI\n\nPrompt type:\nchat_bubble_outline\nChat\nUse case:\nAnswer\nConverse\nCreate\nRelated:\nCreative party ideas\nScience fiction writer\nOpen in Google AI Studio\nContext\nWe're playing a game. I'm thinking of a word, and I need to get you to guess that word. But I can't say the word itself. I'll give you clues, and you'll respond with a guess. Your guess should be a single word only.\nPrompt examples\nUser\n\nThis is a thing into which feathers go, and it makes it more comfortable to sit.\n\nModel\n\nA pillow?\n\nUser\n\nYou might find this on a couch.\n\nModel\n\nA cushion?\n\nUser\n\nYes. Okay next. This is something you might use when drops are falling from the sky\n\nModel\n\nAn umbrella?\n\nUser\n\nNext: Oops\n\nModel\n\nA mistake?\n\nUser\n\nNext: booboo\n\nModel\n\nA boo-boo?\n\nUser\n\nNo, another word for that\n\nModel\n\nA wound?\n\nSample responses\nUser\n\nThis is a type of creature that lived a very long time ago\n\nModel\n\nA dinosaur?\n\nUser\n\nNext: This is a food that has leaves\n\nModel\n\nLettuce?\n\nUser\n\nYou might use that to make what I'm thinking of\n\nModel\n\nA salad?\n\nGet code\nEnable API key to develop with the Gemini API\n\nYou can call this prompt from our Gemini API by integrating the following code into your project.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.holistic_landmarker.holistic_landmarker_graph_options_pb2.mediapipe_dot_tasks_dot_cc_dot_vision_dotpose_detector_dot_proto_dotposedetectorgraphoptions_pb2.PoseDetectorGraphOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/holistic_landmarker_graph_options_pb2/mediapipe_dot_tasks_dot_cc_dot_vision_dot_pose__detector_dot_proto_dot_pose__detector__graph__options__pb2/PoseDetectorGraphOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\ncategory_module\nholistic_landmarker_graph_options_pb2\nOverview\nHolisticLandmarkerGraphOptions\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_face__detector_dot_proto_dot_face__detector__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_face__landmarker_dot_proto_dot_face__landmarks__detector__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_hand__landmarker_dot_proto_dot_hand__landmarks__detector__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_hand__landmarker_dot_proto_dot_hand__roi__refinement__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_pose__detector_dot_proto_dot_pose__detector__graph__options__pb2\nOverview\nPoseDetectorGraphOptions\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_pose__landmarker_dot_proto_dot_pose__landmarks__detector__graph__options__pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.holistic_landmarker.holistic_landmarker_graph_options_pb2.mediapipe_dot_tasks_dot_cc_dot_vision_dotpose_detector_dot_proto_dotposedetectorgraphoptions_pb2.PoseDetectorGraphOptions \nbookmark_border\n\nView source on GitHub\n\nA ProtocolMessage\n\nAttributes\n\nbase_options\tBaseOptions base_options\nmin_detection_confidence\tfloat min_detection_confidence\nmin_suppression_threshold\tfloat min_suppression_threshold\nnum_poses\tint32 num_poses\n\nClass Variables\n\next\tInstance of google.protobuf.pyext._message.FieldDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/swift/vision/Classes/ImageSegmenter",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nOverview\nBaseOptions\nClassificationResult\nClassifications\nClassifierOptions\nConnection\nDetection\nEmbedding\nEmbeddingResult\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceStylizer\nFaceStylizerOptions\nFaceStylizerResult\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageClassifierResult\nImageEmbedder\nImageEmbedderOptions\nImageEmbedderResult\nImageSegmenter\nImageSegmenterOptions\nImageSegmenterResult\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterResult\nLandmark\nMPImage\nMask\nNormalizedKeypoint\nNormalizedLandmark\nObjectDetector\nObjectDetectorOptions\nObjectDetectorResult\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nRegionOfInterest\nResultCategory\nTaskOptions\nTaskResult\nTransformMatrix\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nlabels\ninit(modelPath:)\ninit(options:)\nsegment(image:)\nsegment(image:)\nsegment(videoFrame:timestampInMilliseconds:)\nsegment(videoFrame:timestampInMilliseconds:)\nsegmentAsync(image:timestampInMilliseconds:)\n-init\n+new\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nImageSegmenter\nclass ImageSegmenter : NSObject\n\n@brief Class that performs segmentation on images.\n\nThe API expects a TFLite model with mandatory TFLite Model Metadata.\n\nlabels\n\nGet the category label list of the ImageSegmenter can recognize. For CATEGORY_MASK type, the index in the category mask corresponds to the category in the label list. For CONFIDENCE_MASK type, the output mask list at index corresponds to the category in the label list. If there is no labelmap provided in the model file, empty array is returned.\n\nDeclaration\n\nSWIFT\n\nvar labels: [String] { get }\ninit(modelPath:)\n\nCreates a new instance of ImageSegmenter from an absolute path to a TensorFlow Lite model file stored locally on the device and the default ImageSegmenterOptions.\n\nDeclaration\n\nSWIFT\n\nconvenience init(modelPath: String) throws\nParameters\nmodelPath\t\n\nAn absolute path to a TensorFlow Lite model file stored locally on the device.\n\nReturn Value\n\nA new instance of ImageSegmenter with the given model path. nil if there is an error in initializing the image segmenter.\n\ninit(options:)\n\nCreates a new instance of ImageSegmenter from the given ImageSegmenterOptions.\n\nDeclaration\n\nSWIFT\n\ninit(options: ImageSegmenterOptions) throws\nParameters\noptions\t\n\nThe options of type ImageSegmenterOptions to use for configuring the ImageSegmenter.\n\nReturn Value\n\nA new instance of ImageSegmenter with the given options. nil if there is an error in initializing the image segmenter.\n\nsegment(image:)\n\nPerforms segmentation on the provided MPPImage using the whole image as region of interest. Rotation will be applied according to the orientation property of the provided MPImage. Only use this method when the ImageSegmenter is created with running mode, image.\n\nThis method supports segmentation of RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf your MPImage has a source type of .image ensure that the color space is RGB with an Alpha channel.\n\nDeclaration\n\nSWIFT\n\nfunc segment(image: MPImage) throws -> ImageSegmenterResult\nParameters\nimage\t\n\nThe MPImage on which segmentation is to be performed.\n\nReturn Value\n\nAn ImageSegmenterResult that contains the segmented masks.\n\nsegment(image:)\n\nPerforms segmentation on the provided MPPImage using the whole image as region of interest and invokes the given completion handler block with the response. The method returns synchronously once the completion handler returns.\n\nRotation will be applied according to the orientation property of the provided MPImage. Only use this method when the ImageSegmenter is created with running mode, image.\n\nThis method supports segmentation of RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf your MPImage has a source type of image ensure that the color space is RGB with an Alpha channel.\n\nDeclaration\n\nSWIFT\n\nfunc segment(image: MPImage) async throws -> ImageSegmenterResult\nParameters\nimage\t\n\nThe MPImage on which segmentation is to be performed.\n\n\ncompletionHandler\t\n\nA block to be invoked with the results of performing segmentation on the image. The block takes two arguments, the optional ImageSegmenterResult that contains the segmented masks if the segmentation was successful and an optional error populated upon failure. The lifetime of the returned masks is only guaranteed for the duration of the block.\n\nsegment(videoFrame:timestampInMilliseconds:)\n\nPerforms segmentation on the provided video frame of type MPImage using the whole image as region of interest.\n\nRotation will be applied according to the orientation property of the provided MPImage. Only use this method when the ImageSegmenter is created with video.\n\nThis method supports segmentation of RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf your MPImage has a source type of image ensure that the color space is RGB with an Alpha channel.\n\nDeclaration\n\nSWIFT\n\nfunc segment(videoFrame image: MPImage, timestampInMilliseconds: Int) throws -> ImageSegmenterResult\nParameters\nimage\t\n\nThe MPImage on which segmentation is to be performed.\n\n\ntimestampInMilliseconds\t\n\nThe video frame’s timestamp (in milliseconds). The input timestamps must be monotonically increasing.\n\nReturn Value\n\nAn ImageSegmenterResult that contains a the segmented masks.\n\nsegment(videoFrame:timestampInMilliseconds:)\n\nPerforms segmentation on the provided video frame of type MPImage using the whole image as region of interest invokes the given completion handler block with the response. The method returns synchronously once the completion handler returns.\n\nRotation will be applied according to the orientation property of the provided MPImage. Only use this method when the ImageSegmenter is created with running mode, video.\n\nThis method supports segmentation of RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf your MPImage has a source type of image ensure that the color space is RGB with an Alpha channel.\n\nDeclaration\n\nSWIFT\n\nfunc segment(videoFrame image: MPImage, timestampInMilliseconds: Int) async throws -> ImageSegmenterResult\nParameters\nimage\t\n\nThe MPImage on which segmentation is to be performed.\n\n\ntimestampInMilliseconds\t\n\nThe video frame’s timestamp (in milliseconds). The input timestamps must be monotonically increasing.\n\n\ncompletionHandler\t\n\nA block to be invoked with the results of performing segmentation on the image. The block takes two arguments, the optional ImageSegmenterResult that contains the segmented masks if the segmentation was successful and an optional error only populated upon failure. The lifetime of the returned masks is only guaranteed for the duration of the block.\n\nsegmentAsync(image:timestampInMilliseconds:)\n\nSends live stream image data of type MPImage to perform segmentation using the whole image as region of interest.\n\nRotation will be applied according to the orientation property of the provided MPImage. Only use this method when the ImageSegmenter is created with running mode, liveStream.\n\nThe object which needs to be continuously notified of the available results of image segmentation must confirm to ImageSegmenterLiveStreamDelegate protocol and implement the imageSegmenter(_:didFinishSegmentationWithResult:timestampInMilliseconds:error:) delegate method.\n\nIt’s required to provide a timestamp (in milliseconds) to indicate when the input image is sent to the segmenter. The input timestamps must be monotonically increasing.\n\nThis method supports segmentation of RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf the input MPImage has a source type of image ensure that the color space is RGB with an Alpha channel.\n\nIf this method is used for classifying live camera frames using AVFoundation, ensure that you request AVCaptureVideoDataOutput to output frames in kCMPixelFormat_32BGRA using its videoSettings property.\n\nDeclaration\n\nSWIFT\n\nfunc segmentAsync(image: MPImage, timestampInMilliseconds: Int) throws\nParameters\nimage\t\n\nA live stream image data of type MPImage on which segmentation is to be performed.\n\n\ntimestampInMilliseconds\t\n\nThe timestamp (in milliseconds) which indicates when the input image is sent to the segmenter. The input timestamps must be monotonically increasing.\n\nReturn Value\n\nYES if the image was sent to the task successfully, otherwise NO.\n\n-init\n\nUndocumented\n\n+new\n\nUndocumented\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Module: mp.calculators.tflite.ssd_anchors_calculator_pb2  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/tflite/ssd_anchors_calculator_pb2",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nimage\ntensor\ntflite\nOverview\nssd_anchors_calculator_pb2\nOverview\nSsdAnchorsCalculatorOptions\nmediapipe_dot_framework_dot_formats_dot_object__detection_dot_anchor__pb2\nutil\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nModules\nClasses\nOther Members\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nModule: mp.calculators.tflite.ssd_anchors_calculator_pb2 \nbookmark_border\n\nView source on GitHub\n\nGenerated protocol buffer code.\n\nModules\n\nmediapipe_dot_framework_dot_calculator__options__pb2 module: Generated protocol buffer code.\n\nmediapipe_dot_framework_dot_calculator__pb2 module: Generated protocol buffer code.\n\nmediapipe_dot_framework_dot_formats_dot_object__detection_dot_anchor__pb2 module: Generated protocol buffer code.\n\nClasses\n\nclass SsdAnchorsCalculatorOptions: A ProtocolMessage\n\nOther Members\n\nDESCRIPTOR\tInstance of google.protobuf.pyext._message.FileDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.Model  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/Model",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.Model \nbookmark_border\n\nView source on GitHub\n\nInformation about a Generative Language Model.\n\nAttributes\n\nname\tstr\n\nRequired. The resource name of the Model.\n\nFormat: models/{model} with a {model} naming convention of:\n\n\"{base_model_id}-{version}\"\n\nExamples:\n\nmodels/chat-bison-001\n\nbase_model_id\tstr\n\nRequired. The name of the base model, pass this to the generation request.\n\nExamples:\n\nchat-bison\n\nversion\tstr\n\nRequired. The version number of the model.\n\nThis represents the major version\n\n\ndisplay_name\tstr\n\nThe human-readable name of the model. E.g. \"Chat Bison\". The name can be up to 128 characters long and can consist of any UTF-8 characters.\n\n\ndescription\tstr\n\nA short description of the model.\n\n\ninput_token_limit\tint\n\nMaximum number of input tokens allowed for this model.\n\n\noutput_token_limit\tint\n\nMaximum number of output tokens available for this model.\n\n\nsupported_generation_methods\tMutableSequence[str]\n\nThe model's supported generation methods.\n\nThe method names are defined as Pascal case strings, such as generateMessage which correspond to API methods.\n\n\ntemperature\tfloat\n\nControls the randomness of the output.\n\nValues can range over [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied, while a value closer to 0.0 will typically result in less surprising responses from the model. This value specifies default to be used by the backend while making the call to the model.\n\n\ntop_p\tfloat\n\nFor Nucleus sampling.\n\nNucleus sampling considers the smallest set of tokens whose probability sum is at least top_p. This value specifies default to be used by the backend while making the call to the model.\n\n\ntop_k\tint\n\nFor Top-k sampling.\n\nTop-k sampling considers the set of top_k most probable tokens. This value specifies default to be used by the backend while making the call to the model. If empty, indicates the model doesn't use top-k sampling, and top_k isn't allowed as a generation parameter.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.PoseLandmarksConnections  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/PoseLandmarksConnections",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nChild Classes\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.PoseLandmarksConnections \nbookmark_border\n\nView source on GitHub\n\nThe connections between pose landmarks.\n\nChild Classes\n\nclass Connection\n\nClass Variables\n\nPOSE_LANDMARKS\t[PoseLandmarksConnections.Connection(start=0, end=1),PoseLandmarksConnections.Connection(start=1, end=2),PoseLandmarksConnections.Connection(start=2, end=3),PoseLandmarksConnections.Connection(start=3, end=7),PoseLandmarksConnections.Connection(start=0, end=4),PoseLandmarksConnections.Connection(start=4, end=5),PoseLandmarksConnections.Connection(start=5, end=6),PoseLandmarksConnections.Connection(start=6, end=8),PoseLandmarksConnections.Connection(start=9, end=10),PoseLandmarksConnections.Connection(start=11, end=12),PoseLandmarksConnections.Connection(start=11, end=13),PoseLandmarksConnections.Connection(start=13, end=15),PoseLandmarksConnections.Connection(start=15, end=17),PoseLandmarksConnections.Connection(start=15, end=19),PoseLandmarksConnections.Connection(start=15, end=21),PoseLandmarksConnections.Connection(start=17, end=19),PoseLandmarksConnections.Connection(start=12, end=14),PoseLandmarksConnections.Connection(start=14, end=16),PoseLandmarksConnections.Connection(start=16, end=18),PoseLandmarksConnections.Connection(start=16, end=20),PoseLandmarksConnections.Connection(start=16, end=22),PoseLandmarksConnections.Connection(start=18, end=20),PoseLandmarksConnections.Connection(start=11, end=23),PoseLandmarksConnections.Connection(start=12, end=24),PoseLandmarksConnections.Connection(start=23, end=24),PoseLandmarksConnections.Connection(start=23, end=25),PoseLandmarksConnections.Connection(start=24, end=26),PoseLandmarksConnections.Connection(start=25, end=27),PoseLandmarksConnections.Connection(start=26, end=28),PoseLandmarksConnections.Connection(start=27, end=29),PoseLandmarksConnections.Connection(start=28, end=30),PoseLandmarksConnections.Connection(start=29, end=31),PoseLandmarksConnections.Connection(start=30, end=32),PoseLandmarksConnections.Connection(start=27, end=31),PoseLandmarksConnections.Connection(start=28, end=32)]\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Method: corpora.documents.delete  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/rest/v1beta/corpora.documents/delete",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nREST\nOverview\nv1\nv1beta\nREST Resources\ncorpora\ncorpora.documents\nOverview\ncreate\ndelete\nget\nlist\npatch\nquery\ncorpora.documents.chunks\ncorpora.permissions\nfiles\nmedia\nmodels\ntunedModels\ntunedModels.permissions\nTypes\nCandidate\nCitationMetadata\nContent\nContentEmbedding\nContentFilter\nCustomMetadata\nEmbedding\nGenerateContentResponse\nGenerationConfig\nHarmCategory\nListPermissionsResponse\nMetadataFilter\nRelevantChunk\nSafetyRating\nSafetySetting\nSchema\nTaskType\nTool\nToolConfig\nPaLM\nOn this page\nHTTP request\nPath parameters\nQuery parameters\nRequest body\nResponse body\nAuthorization scopes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\nMethod: corpora.documents.delete \nbookmark_border\n\nDeletes a Document.\n\nHTTP request\n\nDELETE https://generativelanguage.googleapis.com/v1beta/{name=corpora/*/documents/*}\n\nThe URL uses gRPC Transcoding syntax.\n\nPath parameters\nParameters\nname\t\n\nstring\n\nRequired. The resource name of the Document to delete. Example: corpora/my-corpus-123/documents/the-doc-abc\n\nQuery parameters\nParameters\nforce\t\n\nboolean\n\nOptional. If set to true, any Chunks and objects related to this Document will also be deleted.\n\nIf false (the default), a FAILED_PRECONDITION error will be returned if Document contains any Chunks.\n\nRequest body\n\nThe request body must be empty.\n\nResponse body\n\nIf successful, the response body is empty.\n\nAuthorization scopes\n\nRequires the following OAuth scope:\n\nhttps://www.googleapis.com/auth/generative-language.retriever\n\nFor more information, see the Authentication Overview.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-19 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Interactive image segmentation guide for Android  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/interactive_segmenter/android",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nAbout the Preview\nTasks\nModel Maker\nStudio\nVision tasks\nObject detection\nImage classification\nImage segmentation\nInteractive segmentation\nOverview\nAndroid\nWeb\nPython\nGesture recognition\nHand landmark detection\nImage embedding\nFace detection\nFace landmark detection\nPose landmark detection\nFace stylization\nHolistic landmark detection\nText tasks\nText classification\nText embedding\nLanguage detection\nAudio tasks\nAudio classification\nGenerative AI tasks\nImage generation\nLLM Inference\nPlatform setup guides\nAndroid setup\nPython setup\nWeb setup\niOS setup\nOn this page\nCode example\nDownload the code\nKey components\nSetup\nDependencies\nModel\nCreate the task\nConfiguration options\nPrepare data\nRun the task\nHandle and display results\nCategory mask\nConfidence mask\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nSolutions\nSend feedback\nInteractive image segmentation guide for Android \nbookmark_border\n\nThe MediaPipe Interactive Image Segmenter task takes a location in an image, estimates the boundaries of an object at that location, and returns the segmentation for the object as image data. These instructions show you how to use the Interactive Image Segmenter with Android apps. The code example described in these instructions is available on GitHub. For more information about the capabilities, models, and configuration options of this task, see the Overview.\n\nCode example\n\nThe MediaPipe Tasks code example is a simple implementation of a Interactive Image Segmenter app for Android. The example works with images selected from the device gallery.\n\nYou can use the app as a starting point for your own Android app, or refer to it when modifying an existing app. The Interactive Image Segmenter example code is hosted on GitHub.\n\nDownload the code\n\nThe following instructions show you how to create a local copy of the example code using the git command line tool.\n\nAttention: This MediaPipe Solutions Preview is an early release. Learn more.\n\nTo download the example code:\n\nClone the git repository using the following command:\ngit clone https://github.com/google-ai-edge/mediapipe-samples\n\nOptionally, configure your git instance to use sparse checkout, so you have only the files for the Interactive Image Segmenter example app:\ncd mediapipe\ngit sparse-checkout init --cone\ngit sparse-checkout set examples/interactive_segmentation/android\n\n\nAfter creating a local version of the example code, you can import the project into Android Studio and run the app. For instructions, see the Setup Guide for Android.\n\nKey components\n\nThe following files contain the crucial code for this image segmentation example application:\n\nInteractiveSegmentationHelper.kt - Initializes the Interactive Image Segmenter task and handles the model and delegate selection.\nOverlayView.kt - Handles and formats the segmentation results.\nSetup\n\nThis section describes key steps for setting up your development environment and code projects to use Interactive Image Segmenter. For general information on setting up your development environment for using MediaPipe tasks, including platform version requirements, see the Setup guide for Android.\n\nAttention: This MediaPipe Solutions Preview is an early release. Learn more.\nDependencies\n\nInteractive Image Segmenter uses the com.google.mediapipe:tasks-vision library. Add this dependency to the build.gradle file of your Android app development project. Import the required dependencies with the following code:\n\ndependencies {\n    ...\n    implementation 'com.google.mediapipe:tasks-vision:latest.release'\n}\n\nModel\n\nThe MediaPipe Interactive Image Segmenter task requires a trained model that is compatible with this task. For more information on available trained models for Interactive Image Segmenter, see the task overview Models section.\n\nSelect and download the model, and then store it within your project directory:\n\n<dev-project-root>/src/main/assets\n\nNote: This location is recommended because the Android build system automatically checks this directory for file resources.\n\nUse the BaseOptions.Builder.setModelAssetPath() method to specify the path used by the model. This method is shown in the code example in the next section.\n\nIn the Interactive Image Segmenter example code, the model is defined in the InteractiveSegmenterHelper.kt class in the setupInteractiveSegmenter() function.\n\nCreate the task\n\nYou can use the createFromOptions function to create the task. The createFromOptions function accepts configuration options including mask output types. For more information on configuration options, see Configuration Overview.\n\nInteractiveSegmenterOptions options =\n  InteractiveSegmenterOptions.builder()\n    .setBaseOptions(\n      BaseOptions.builder().setModelAssetPath(\"model.tflite\").build())\n    .setOutputCategoryMask(true)\n    .setOutputConfidenceMasks(false)\n    .setResultListener((result, inputImage) -> {\n         // Process the segmentation result here.\n    })\n    .setErrorListener((result, inputImage) -> {\n         // Process the segmentation errors here.\n    })    .build();\ninteractivesegmenter = InteractiveSegmenter.createFromOptions(context, options);\n\n\nFor a more detailed example of setting up this task, see the InteractiveSegmenterHelper class setupInteractiveSegmenter() function.\n\nConfiguration options\n\nThis task has the following configuration options for Android apps:\n\nOption Name\tDescription\tValue Range\tDefault Value\noutputCategoryMask\tIf set to True, the output includes a segmentation mask as a uint8 image, where each pixel value indicates if the pixel is part of the object located at the area of interest.\t{True, False}\tFalse\noutputConfidenceMasks\tIf set to True, the output includes a segmentation mask as a float value image, where each float value represents the confidence that the pixel is part of the object located at the area of interest.\t{True, False}\tTrue\ndisplayNamesLocale\tSets the language of labels to use for display names provided in the metadata of the task's model, if available. Default is en for English. You can add localized labels to the metadata of a custom model using the TensorFlow Lite Metadata Writer API\tLocale code\ten\nerrorListener\tSets an optional error listener.\tN/A\tNot set\nPrepare data\n\nInteractive Image Segmenter works with images, and the task handles the data input preprocessing, including resizing, rotation and value normalization. You need to convert the input image to a com.google.mediapipe.framework.image.MPImage object before passing it to the task.\n\nimport com.google.mediapipe.framework.image.BitmapImageBuilder;\nimport com.google.mediapipe.framework.image.MPImage;\n\n// Load an image on the user’s device as a Bitmap object using BitmapFactory.\n\n// Convert an Android’s Bitmap object to a MediaPipe’s Image object.\nImage mpImage = new BitmapImageBuilder(bitmap).build();\n\n\nIn the Interactive Image Segmenter example code, the data preparation is handled in the InteractiveSegmenterHelper class by the segment() function.\n\nRun the task\n\nCall the segment function to run the prediction and generate segments. The Interactive Image Segmenter task returns the identified segment regions within the input image.\n\nRegionOfInterest roi = RegionOfInterest.create(\n    NormalizedKeypoint.create(\n        normX * it.width,\n        normY * it.height\n    )\n);\n\nImageSegmenterResult segmenterResult = interactivesegmenter.segment(image, roi);\n\n\nIn the Interactive Image Segmenter example code, the segment functions are defined in the InteractiveSegmenterHelper.kt file.\n\nHandle and display results\n\nUpon running inference, the Interactive Image Segmenter task returns an ImageSegmenterResult object which contains the results of the segmentation task. The content of the output may include a category mask, confidence mask, or both, depending on what you set when you configured the task.\n\nThe following sections further explain the output data from this task:\n\nCategory mask\n\nThe following images show a visualization of the task output for a category value mask with a point area of interest indicated. Each pixel is a uint8 value indicating if the pixel is part of the object located at the area of interest. The black and white circle on the second image indicates the selected area of interest.\n\n \n\nOriginal image and category mask output. Source image from the Pascal VOC 2012 dataset.\n\nConfidence mask\n\nThe output for a confidence mask contains float values between [0, 1] for each image input channel. Higher values indicate a higher confidence that the image pixel is part of the object located at the area of interest.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-28 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Gemini API Additional Terms of Service  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/gemini-api/terms-archive/terms_02_15_24",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOn this page\nAge Requirements\nUse Restrictions\nUse of Generated Content\nContent License and Data Use\nDisclaimers\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nSend feedback\nGemini API Additional Terms of Service \nbookmark_border\n\nWarning: This is an archived version of our Terms of Service. View the current version or all past versions.\n\n\n\nLast modified: February 15, 2024\n\nTo use Gemini API, Google AI Studio, and the other Google developer services that reference these terms (collectively, the \"APIs\" or \"Services\"), you must accept (1) the Google APIs Terms of Service (the \"API Terms\"), and (2) these Gemini API Additional Terms of Service (the \"Additional Terms\").\n\nAge Requirements\n\nYou must be 18 years of age or older to use the APIs. You also will not use the Services as part of a website, application, or other service (\"API Clients\") that is directed towards or is likely to be accessed by individuals under the age of 18.\n\nUse Restrictions\n\nYou may use API Clients for production use but Google may enforce rate limits. Any Services identified as \"Private Preview\" may not be used in production.\n\nYou may only access the Services (or make API Clients available to users) within an available region.\n\nYou may not use the Services to develop models that compete with Gemini API or Google AI Studio. You also may not attempt to extract or replicate the underlying models (e.g., parameter weights).\n\nYou will only use the Services directly or in connection with a service that you offer directly to end users, i.e., you will not use the Services (e.g., Gemini API) to power another application programming interface.\n\nIn addition to the \"API Prohibitions\" section in the API Terms, you must comply with our Prohibited Use Policy, which provides additional details about appropriate conduct when using the Services.\n\nThe Services include safety features to block harmful content, such as content that violates our Prohibited Use Policy. You may not attempt to bypass these protective measures or use content that violates the API Terms or these Additional Terms. You should only lower safety settings if necessary and appropriate for your use case. Applications with less restrictive safety settings may be subject to Google's review and approval.\n\nYou may not use the Services in clinical practice, to provide medical advice, or in any manner that is overseen by or requires clearance or approval from a medical device regulatory agency.\n\nUse of Generated Content\n\nAs required by the API Terms, you'll comply with applicable law in using generated content, which may require the provision of attribution to your users when returned as part of an API call.\n\nContent License and Data Use\n\nThe license you grant to Google under the \"Submission of Content\" section in the API Terms also extends to any content (e.g., prompts, including associated files such as images or documents) you submit through the Services or any other API interface (e.g., Firebase Extensions). Google uses this data, consistent with our Privacy Policy, to provide, improve, and develop Google products and services and machine learning technologies, including Google's enterprise features, products, and services.\n\nTo help with quality and improve our products, human reviewers may read, annotate, and process your API input and output. Google takes steps to protect your privacy as part of this process. This includes disconnecting this data from your Google Account and API key before reviewers see or annotate it. Do not submit sensitive, confidential, or personal information to the Services.\n\nGoogle only uses data that you import or upload to the Services to tune models for that express purpose. Tuning data may be retained in connection with your tuned models for purposes of re-tuning when supported models change. When you delete a tuned model, the related tuning data is also deleted.\n\nDisclaimers\n\nThe Services use experimental technology and may sometimes provide inaccurate or offensive content that doesn't represent Google's views.\n\nUse discretion before relying on, publishing, or otherwise using content provided by the Services.\n\nDon't rely on the Services for medical, legal, financial, or other professional advice. Any content regarding those topics is provided for informational purposes only and is not a substitute for advice from a qualified professional. Content does not constitute medical treatment or diagnosis.\n\nNote: Previous versions of these Additional Terms are archived here.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "ImageSegmenter.ImageSegmenterOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/java/com/google/mediapipe/tasks/vision/imagesegmenter/ImageSegmenter.ImageSegmenterOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\ncom.google.mediapipe.framework.image\ncom.google.mediapipe.tasks.audio.audioclassifier\ncom.google.mediapipe.tasks.audio.audioembedder\ncom.google.mediapipe.tasks.audio.core\ncom.google.mediapipe.tasks.components.containers\ncom.google.mediapipe.tasks.components.processors\ncom.google.mediapipe.tasks.components.utils\ncom.google.mediapipe.tasks.core\ncom.google.mediapipe.tasks.core.logging\ncom.google.mediapipe.tasks.genai.llminference\ncom.google.mediapipe.tasks.text.languagedetector\ncom.google.mediapipe.tasks.text.textclassifier\ncom.google.mediapipe.tasks.text.textembedder\ncom.google.mediapipe.tasks.vision.core\ncom.google.mediapipe.tasks.vision.facedetector\ncom.google.mediapipe.tasks.vision.facelandmarker\ncom.google.mediapipe.tasks.vision.facestylizer\ncom.google.mediapipe.tasks.vision.gesturerecognizer\ncom.google.mediapipe.tasks.vision.handlandmarker\ncom.google.mediapipe.tasks.vision.holisticlandmarker\ncom.google.mediapipe.tasks.vision.imageclassifier\ncom.google.mediapipe.tasks.vision.imageembedder\ncom.google.mediapipe.tasks.vision.imagegenerator\ncom.google.mediapipe.tasks.vision.imagesegmenter\nOverview\nImageSegmenter\nOverview\nImageSegmenter.ImageSegmenterOptions\nOverview\nBuilder\nImageSegmenter.SegmentationOptions\nImageSegmenterResult\ncom.google.mediapipe.tasks.vision.interactivesegmenter\ncom.google.mediapipe.tasks.vision.objectdetector\ncom.google.mediapipe.tasks.vision.poselandmarker\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nNested Classes\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nImageSegmenter.ImageSegmenterOptions \nbookmark_border\npublic static abstract class ImageSegmenter.ImageSegmenterOptions\n\nOptions for setting up an ImageSegmenter.\n\nNested Classes\nclass\tImageSegmenter.ImageSegmenterOptions.Builder\tBuilder for ImageSegmenter.ImageSegmenterOptions. \nPublic Constructors\n\t\nImageSegmenterOptions()\nPublic Methods\nstatic ImageSegmenter.ImageSegmenterOptions.Builder\t\nbuilder()\n\nCalculatorOptions\t\nconvertToCalculatorOptionsProto()\nConverts an ImageSegmenter.ImageSegmenterOptions to a ERROR(/CalculatorOptions) protobuf message.\nInherited Methods\nFrom class\ncom.google.mediapipe.tasks.core.TaskOptions\nFrom class java.lang.Object\nPublic Constructors\npublic ImageSegmenterOptions ()\n\nPublic Methods\npublic static ImageSegmenter.ImageSegmenterOptions.Builder builder ()\n\npublic CalculatorOptions convertToCalculatorOptionsProto ()\n\nConverts an ImageSegmenter.ImageSegmenterOptions to a ERROR(/CalculatorOptions) protobuf message.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mediapipe_model_maker.object_detector.HParams  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mediapipe_model_maker/object_detector/HParams",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nModel Maker\nmediapipe_model_maker\nmediapipe_model_maker.face_stylizer\nmediapipe_model_maker.gesture_recognizer\nmediapipe_model_maker.image_classifier\nmediapipe_model_maker.image_utils\nmediapipe_model_maker.model_util\nmediapipe_model_maker.object_detector\nOverview\nDataset\nHParams\nModelOptions\nModelSpec\nObjectDetector\nObjectDetectorOptions\nQATHParams\nSupportedModels\nmediapipe_model_maker.quantization\nmediapipe_model_maker.text_classifier\nTensorFlow Lite\nOn this page\nAttributes\nMethods\nget_strategy\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nWas this helpful?\nSend feedback\nmediapipe_model_maker.object_detector.HParams \nbookmark_border\n\nView source on GitHub\n\nThe hyperparameters for training object detectors.\n\nInherits From: BaseHParams\n\nmediapipe_model_maker.object_detector.HParams(\n    learning_rate: float = 0.3,\n    batch_size: int = 8,\n    epochs: int = 30,\n    steps_per_epoch: Optional[int] = None,\n    class_weights: Optional[Mapping[int, float]] = None,\n    shuffle: bool = False,\n    repeat: bool = False,\n    export_dir: str = tempfile.mkdtemp(),\n    distribution_strategy: str = 'off',\n    num_gpus: int = 0,\n    tpu: str = '',\n    cosine_decay_epochs: Optional[int] = None,\n    cosine_decay_alpha: float = 1.0\n)\n\n\nAttributes\n\nlearning_rate\tLearning rate to use for gradient descent training.\nbatch_size\tBatch size for training.\nepochs\tNumber of training iterations over the dataset.\ncosine_decay_epochs\tThe number of epochs for cosine decay learning rate. See https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay for more info.\ncosine_decay_alpha\tThe alpha value for cosine decay learning rate. See https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay for more info.\nsteps_per_epoch\tDataclass field\nclass_weights\tDataclass field\nshuffle\tDataclass field\nrepeat\tDataclass field\nexport_dir\tDataclass field\ndistribution_strategy\tDataclass field\nnum_gpus\tDataclass field\ntpu\tDataclass field\n\nMethods\nget_strategy\n\nView source\n\nget_strategy()\n\n__eq__\n__eq__(\n    other\n)\n\n\nClass Variables\n\nbatch_size\t8\nclass_weights\tNone\ncosine_decay_alpha\t1.0\ncosine_decay_epochs\tNone\ndistribution_strategy\t'off'\nepochs\t30\nexport_dir\t'/tmpfs/tmp/tmpnt_h4p9w'\nlearning_rate\t0.3\nnum_gpus\t0\nrepeat\tFalse\nshuffle\tFalse\nsteps_per_epoch\tNone\ntpu\t''\n\nWas this helpful?\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "TensorBufferFloat  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/support/tensorbuffer/TensorBufferFloat",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nCore\norg.tensorflow.lite\nacceleration\nacceleration\nSupport Library\nsupport.audio\nsupport.common\nsupport.common.ops\nsupport.image\nsupport.image.ops\nsupport.label\nsupport.label.ops\nsupport.metadata\nsupport.model\nsupport.tensorbuffer\nOverview\nTensorBuffer\nTensorBufferFloat\nTensorBufferUint8\nDelegates\ngpu\nODML\nimage\nimage.annotation\nSwift\nC\nC++\nOn this page\nPublic Methods\nInherited Methods\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nTensorBufferFloat \nbookmark_border\npublic final class TensorBufferFloat\n\nRepresents data buffer with float values.\n\nPublic Methods\nDataType\t\ngetDataType()\nReturns the data type of this buffer.\n\nfloat[]\t\ngetFloatArray()\nReturns a float array of the values stored in this buffer.\n\nfloat\t\ngetFloatValue(int absIndex)\nReturns a float value at a given index.\n\nint[]\t\ngetIntArray()\nReturns an int array of the values stored in this buffer.\n\nint\t\ngetIntValue(int absIndex)\nReturns an int value at a given index.\n\nint\t\ngetTypeSize()\nReturns the number of bytes of a single element in the array.\n\nvoid\t\nloadArray(int[] src, int[] shape)\nLoads an int array into this buffer with specific shape.\n\nvoid\t\nloadArray(float[] src, int[] shape)\nLoads a float array into this buffer with specific shape.\nInherited Methods\nFrom class\norg.tensorflow.lite.support.tensorbuffer.TensorBuffer\nFrom class java.lang.Object\nPublic Methods\npublic DataType getDataType ()\n\nReturns the data type of this buffer.\n\npublic float[] getFloatArray ()\n\nReturns a float array of the values stored in this buffer. If the buffer is of different types than float, the values will be converted into float. For example, values in TensorBufferUint8 will be converted from uint8 to float.\n\npublic float getFloatValue (int absIndex)\n\nReturns a float value at a given index. If the buffer is of different types than float, the value will be converted into float. For example, when reading a value from TensorBufferUint8, the value will be first read out as uint8, and then will be converted from uint8 to float.\n\n For example, a TensorBuffer with shape {2, 3} that represents the following array,\n [[0.0f, 1.0f, 2.0f], [3.0f, 4.0f, 5.0f]].\n\n The fourth element (whose value is 3.0f) in the TensorBuffer can be retrieved by:\n float v = tensorBuffer.getFloatValue(3);\n \n\nParameters\nabsIndex\tThe absolute index of the value to be read.\npublic int[] getIntArray ()\n\nReturns an int array of the values stored in this buffer. If the buffer is of different type than int, the values will be converted into int, and loss of precision may apply. For example, getting an int array from a TensorBufferFloat with values {400.32f, 23.04f}, the output is {400, 23}.\n\npublic int getIntValue (int absIndex)\n\nReturns an int value at a given index. If the buffer is of different types than int, the value will be converted into int. For example, when reading a value from TensorBufferFloat, the value will be first read out as float, and then will be converted from float to int. Loss of precision may apply.\n\n For example, a TensorBuffer with shape {2, 3} that represents the following array,\n [[0.0f, 1.0f, 2.0f], [3.0f, 4.0f, 5.0f]].\n\n The fourth element (whose value is 3.0f) in the TensorBuffer can be retrieved by:\n int v = tensorBuffer.getIntValue(3);\n Note that v is converted from 3.0f to 3 as a result of type conversion.\n \n\nParameters\nabsIndex\tThe absolute index of the value to be read.\npublic int getTypeSize ()\n\nReturns the number of bytes of a single element in the array. For example, a float buffer will return 4, and a byte buffer will return 1.\n\npublic void loadArray (int[] src, int[] shape)\n\nLoads an int array into this buffer with specific shape. If the buffer is of different types than int, the values will be converted into the buffer's type before being loaded into the buffer, and loss of precision may apply. For example, loading an int array with values {400, -23} into a TensorBufferUint8 , the values will be clamped to [0, 255] and then be casted to uint8 by {255, 0}.\n\nParameters\nsrc\tThe source array to be loaded.\nshape\tShape of the tensor that src represents.\npublic void loadArray (float[] src, int[] shape)\n\nLoads a float array into this buffer with specific shape. If the buffer is of different types than float, the values will be converted into the buffer's type before being loaded into the buffer, and loss of precision may apply. For example, loading a float array into a TensorBufferUint8 with values {400.32f, -23.04f}, the values will be clamped to [0, 255] and then be casted to uint8 by {255, 0}.\n\nParameters\nsrc\tThe source array to be loaded.\nshape\tShape of the tensor that src represents.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.Stats  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/Stats",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nMethods\nGetRootAs\nGetRootAsStats\nInit\nMax\nMaxAsNumpy\nMaxIsNone\nMaxLength\nMin\nMinAsNumpy\nMinIsNone\nMinLength\nStatsBufferHasIdentifier\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.Stats \nbookmark_border\n\nView source on GitHub\nMethods\nGetRootAs\n\nView source\n\n@classmethod\nGetRootAs(\n    buf, offset=0\n)\n\nGetRootAsStats\n\nView source\n\n@classmethod\nGetRootAsStats(\n    buf, offset=0\n)\n\n\nThis method is deprecated. Please switch to GetRootAs.\n\nInit\n\nView source\n\nInit(\n    buf, pos\n)\n\nMax\n\nView source\n\nMax(\n    j\n)\n\nMaxAsNumpy\n\nView source\n\nMaxAsNumpy()\n\nMaxIsNone\n\nView source\n\nMaxIsNone()\n\nMaxLength\n\nView source\n\nMaxLength()\n\nMin\n\nView source\n\nMin(\n    j\n)\n\nMinAsNumpy\n\nView source\n\nMinAsNumpy()\n\nMinIsNone\n\nView source\n\nMinIsNone()\n\nMinLength\n\nView source\n\nMinLength()\n\nStatsBufferHasIdentifier\n\nView source\n\n@classmethod\nStatsBufferHasIdentifier(\n    buf, offset, size_prefixed=False\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "CastOp  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/support/common/ops/CastOp",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nCore\norg.tensorflow.lite\nacceleration\nacceleration\nSupport Library\nsupport.audio\nsupport.common\nsupport.common.ops\nOverview\nCastOp\nDequantizeOp\nNormalizeOp\nQuantizeOp\nsupport.image\nsupport.image.ops\nsupport.label\nsupport.label.ops\nsupport.metadata\nsupport.model\nsupport.tensorbuffer\nDelegates\ngpu\nODML\nimage\nimage.annotation\nSwift\nC\nC++\nOn this page\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nCastOp \nbookmark_border\npublic class CastOp\n\nCasts a TensorBuffer to a specified data type.\n\nPublic Constructors\n\t\nCastOp(DataType destinationType)\nConstructs a CastOp.\nPublic Methods\nTensorBuffer\t\napply(TensorBuffer input)\nInherited Methods\nFrom class java.lang.Object\nFrom interface\norg.tensorflow.lite.support.common.TensorOperator\nFrom interface\norg.tensorflow.lite.support.common.Operator\nPublic Constructors\npublic CastOp (DataType destinationType) \n\nConstructs a CastOp.\n\nNote: For only converting type for a certain TensorBuffer on-the-fly rather than in a processor, please directly use TensorBuffer.createFrom(TensorBuffer, DataType).\n\nWhen this Op is executed, if the original TensorBuffer is already in destinationType, the original buffer will be directly returned.\n\nParameters\ndestinationType\tThe type of the casted TensorBuffer.\nThrows\nIllegalArgumentException\tif destinationType is neither DataType.UINT8 nor DataType.FLOAT32.\nPublic Methods\npublic TensorBuffer apply (TensorBuffer input)\n\nParameters\ninput\t\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.calculators.tensor.tensors_to_classification_calculator_pb2.TensorsToClassificationCalculatorOptions.LabelMap  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/tensor/tensors_to_classification_calculator_pb2/TensorsToClassificationCalculatorOptions/LabelMap",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nimage\ntensor\nOverview\nimage_to_tensor_calculator_pb2\ninference_calculator_pb2\ntensors_to_classification_calculator_pb2\nOverview\nTensorsToClassificationCalculatorOptions\nTensorsToClassificationCalculatorOptions.LabelItemsEntry\nTensorsToClassificationCalculatorOptions.LabelMap\nTensorsToClassificationCalculatorOptions.LabelMap.Entry\nmediapipe_dot_util_dot_label__map__pb2\ntensors_to_detections_calculator_pb2\ntensors_to_floats_calculator_pb2\ntensors_to_landmarks_calculator_pb2\ntensors_to_segmentation_calculator_pb2\ntflite\nutil\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nChild Classes\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.calculators.tensor.tensors_to_classification_calculator_pb2.TensorsToClassificationCalculatorOptions.LabelMap \nbookmark_border\n\nView source on GitHub\n\nA ProtocolMessage\n\nAttributes\n\nentries\trepeated Entry entries\n\nChild Classes\n\nclass Entry\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsStartVocabFileVector  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsStartVocabFileVector",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.RegexTokenizerOptionsStartVocabFileVector \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.RegexTokenizerOptionsStartVocabFileVector(\n    builder, numElems\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mediapipe_model_maker.model_util.load_tflite_model_buffer  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mediapipe_model_maker/model_util/load_tflite_model_buffer",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nModel Maker\nmediapipe_model_maker\nmediapipe_model_maker.face_stylizer\nmediapipe_model_maker.gesture_recognizer\nmediapipe_model_maker.image_classifier\nmediapipe_model_maker.image_utils\nmediapipe_model_maker.model_util\nOverview\nLiteRunner\nWarmUp\nconvert_to_tflite\nconvert_to_tflite_from_file\nget_default_callbacks\nget_lite_runner\nget_steps_per_epoch\nload_keras_model\nload_tflite_model_buffer\nsave_tflite\ndataset\nmediapipe_model_maker.object_detector\nmediapipe_model_maker.quantization\nmediapipe_model_maker.text_classifier\nTensorFlow Lite\nOn this page\nArgs\nReturns\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmediapipe_model_maker.model_util.load_tflite_model_buffer \nbookmark_border\n\nView source on GitHub\n\nLoads a TFLite model buffer from file.\n\nView aliases\nmediapipe_model_maker.model_util.load_tflite_model_buffer(\n    model_path: str\n) -> bytearray\n\n\nArgs\n\nmodel_path\tAbsolute path to a TFLite file, such as //.tflite.\n\nReturns\n\nA TFLite model buffer\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Method: models.streamGenerateContent  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/rest/v1beta/models/streamGenerateContent",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nREST\nOverview\nv1\nv1beta\nREST Resources\ncorpora\ncorpora.documents\ncorpora.documents.chunks\ncorpora.permissions\nfiles\nmedia\nmodels\nOverview\nbatchEmbedContents\ncountTokens\nembedContent\ngenerateAnswer\ngenerateContent\nget\nlist\nstreamGenerateContent\nPaLM\ntunedModels\ntunedModels.permissions\nTypes\nCandidate\nCitationMetadata\nContent\nContentEmbedding\nContentFilter\nCustomMetadata\nEmbedding\nGenerateContentResponse\nGenerationConfig\nHarmCategory\nListPermissionsResponse\nMetadataFilter\nRelevantChunk\nSafetyRating\nSafetySetting\nSchema\nTaskType\nTool\nToolConfig\nPaLM\nOn this page\nHTTP request\nPath parameters\nRequest body\nResponse body\nAuthorization scopes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\nMethod: models.streamGenerateContent \nbookmark_border\n\nGenerates a streamed response from the model given an input GenerateContentRequest.\n\nHTTP request\n\nPOST https://generativelanguage.googleapis.com/v1beta/{model=models/*}:streamGenerateContent\n\nThe URL uses gRPC Transcoding syntax.\n\nPath parameters\nParameters\nmodel\t\n\nstring\n\nRequired. The name of the Model to use for generating the completion.\n\nFormat: name=models/{model}.\n\nRequest body\n\nThe request body contains data with the following structure:\n\nJSON representation\n\n{\n  \"contents\": [\n    {\n      object (Content)\n    }\n  ],\n  \"tools\": [\n    {\n      object (Tool)\n    }\n  ],\n  \"toolConfig\": {\n    object (ToolConfig)\n  },\n  \"safetySettings\": [\n    {\n      object (SafetySetting)\n    }\n  ],\n  \"systemInstruction\": {\n    object (Content)\n  },\n  \"generationConfig\": {\n    object (GenerationConfig)\n  }\n}\nFields\ncontents[]\t\n\nobject (Content)\n\nRequired. The content of the current conversation with the model.\n\nFor single-turn queries, this is a single instance. For multi-turn queries, this is a repeated field that contains conversation history + latest request.\n\n\ntools[]\t\n\nobject (Tool)\n\nOptional. A list of Tools the model may use to generate the next response.\n\nA Tool is a piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of knowledge and scope of the model. The only supported tool is currently Function.\n\n\ntoolConfig\t\n\nobject (ToolConfig)\n\nOptional. Tool configuration for any Tool specified in the request.\n\n\nsafetySettings[]\t\n\nobject (SafetySetting)\n\nOptional. A list of unique SafetySetting instances for blocking unsafe content.\n\nThis will be enforced on the GenerateContentRequest.contents and GenerateContentResponse.candidates. There should not be more than one setting for each SafetyCategory type. The API will block any contents and responses that fail to meet the thresholds set by these settings. This list overrides the default settings for each SafetyCategory specified in the safetySettings. If there is no SafetySetting for a given SafetyCategory provided in the list, the API will use the default safety setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH, HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT, HARM_CATEGORY_HARASSMENT are supported.\n\n\nsystemInstruction\t\n\nobject (Content)\n\nOptional. Developer set system instruction. Currently, text only.\n\n\ngenerationConfig\t\n\nobject (GenerationConfig)\n\nOptional. Configuration options for model generation and outputs.\n\nResponse body\n\nIf successful, the response body contains a stream of GenerateContentResponse instances.\n\nAuthorization scopes\n\nRequires one of the following OAuth scopes:\n\nhttps://www.googleapis.com/auth/generative-language\nhttps://www.googleapis.com/auth/generative-language.tuning\nhttps://www.googleapis.com/auth/generative-language.tuning.readonly\nhttps://www.googleapis.com/auth/generative-language.retriever\nhttps://www.googleapis.com/auth/generative-language.retriever.readonly\n\nFor more information, see the Authentication Overview.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-19 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.ScoreTransformationType  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/ScoreTransformationType",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.ScoreTransformationType \nbookmark_border\n\nView source on GitHub\n\nClass Variables\n\nIDENTITY\t0\nINVERSE_LOGISTIC\t2\nLOG\t1\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Hand gesture recognition model customization guide  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/solutions/customization/gesture_recognizer",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nAbout the Preview\nTasks\nModel Maker\nStudio\nVision tasks\nObject detection\nImage classification\nImage segmentation\nInteractive segmentation\nGesture recognition\nOverview\nAndroid\nWeb\nPython\niOS\nCustomize\nHand landmark detection\nImage embedding\nFace detection\nFace landmark detection\nPose landmark detection\nFace stylization\nHolistic landmark detection\nText tasks\nText classification\nText embedding\nLanguage detection\nAudio tasks\nAudio classification\nGenerative AI tasks\nImage generation\nLLM Inference\nPlatform setup guides\nAndroid setup\nPython setup\nWeb setup\niOS setup\nOn this page\nLicense information\nPrerequisites\nSimple End-to-End Example\nGet the dataset\nRun the example\nRun the model on-device\nHyperparameters\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nSolutions\nSend feedback\nHand gesture recognition model customization guide \nbookmark_border\n\n Run in Colab\t View on GitHub\nLicense information\n\nToggle code\n\nThe MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n\nThis notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the HaGRID dataset.\n\nPrerequisites\n\nInstall the MediaPipe Model Maker package.\n\npip install --upgrade pip\npip install mediapipe-model-maker\n\n\nImport the required libraries.\n\nfrom google.colab import files\nimport os\nimport tensorflow as tf\nassert tf.__version__.startswith('2')\n\nfrom mediapipe_model_maker import gesture_recognizer\n\nimport matplotlib.pyplot as plt\n\nSimple End-to-End Example\n\nThis end-to-end example uses Model Maker to customize a model for on-device gesture recognition.\n\nGet the dataset\n\nThe dataset for gesture recognition in model maker requires the following format: <dataset_path>/<label_name>/<img_name>.*. In addition, one of the label names (label_names) must be none. The none label represents any gesture that isn't classified as one of the other gestures.\n\nThis example uses a rock paper scissors dataset sample which is downloaded from GCS.\n\n!wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip\n!unzip rps_data_sample.zip\ndataset_path = \"rps_data_sample\"\n\n\nVerify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the none gesture.\n\nprint(dataset_path)\nlabels = []\nfor i in os.listdir(dataset_path):\n  if os.path.isdir(os.path.join(dataset_path, i)):\n    labels.append(i)\nprint(labels)\n\n\nTo better understand the dataset, plot a couple of example images for each gesture.\n\nNUM_EXAMPLES = 5\n\nfor label in labels:\n  label_dir = os.path.join(dataset_path, label)\n  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n  for i in range(NUM_EXAMPLES):\n    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n    axs[i].get_xaxis().set_visible(False)\n    axs[i].get_yaxis().set_visible(False)\n  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n\nplt.show()\n\nRun the example\n\nThe workflow consists of 4 steps which have been separated into their own code blocks.\n\nLoad the dataset\n\nLoad the dataset located at dataset_path by using the Dataset.from_folder method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n\nThe HandDataPreprocessingParams class contains two configurable options for the data loading process:\n\nshuffle: A boolean controlling whether to shuffle the dataset. Defaults to true.\nmin_detection_confidence: A float between 0 and 1 controlling the confidence threshold for hand detection.\n\nSplit the dataset: 80% for training, 10% for validation, and 10% for testing.\n\ndata = gesture_recognizer.Dataset.from_folder(\n    dirname=dataset_path,\n    hparams=gesture_recognizer.HandDataPreprocessingParams()\n)\ntrain_data, rest_data = data.split(0.8)\nvalidation_data, test_data = rest_data.split(0.5)\n\n\nTrain the model\n\nTrain the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the Hyperparameters section below.\n\nhparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\noptions = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\nmodel = gesture_recognizer.GestureRecognizer.create(\n    train_data=train_data,\n    validation_data=validation_data,\n    options=options\n)\n\n\nEvaluate the model performance\n\nAfter training the model, evaluate it on a test dataset and print the loss and accuracy metrics.\n\nloss, acc = model.evaluate(test_data, batch_size=1)\nprint(f\"Test loss:{loss}, Test accuracy:{acc}\")\n\n\nExport to Tensorflow Lite Model\n\nAfter creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file.\n\nmodel.export_model()\n!ls exported_model\n\nfiles.download('exported_model/gesture_recognizer.task')\n\nRun the model on-device\n\nTo use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer overview page.\n\nHyperparameters\n\nYou can further customize the model using the GestureRecognizerOptions class, which has two optional parameters for ModelOptions and HParams. Use the ModelOptions class to customize parameters related to the model itself, and the HParams class to customize other parameters related to training and saving the model.\n\nModelOptions has one customizable parameter that affects accuracy:\n\ndropout_rate: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\nlayer_widths: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n\nHParams has the following list of customizable parameters which affect model accuracy:\n\nlearning_rate: The learning rate to use for gradient descent training. Defaults to 0.001.\nbatch_size: Batch size for training. Defaults to 2.\nepochs: Number of training iterations over the dataset. Defaults to 10.\nsteps_per_epoch: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\nshuffle: True if the dataset is shuffled before training. Defaults to False.\nlr_decay: Learning rate decay to use for gradient descent training. Defaults to 0.99.\ngamma: Gamma parameter for focal loss. Defaults to 2\n\nAdditional HParams parameter that does not affect model accuracy:\n\nexport_dir: The location of the model checkpoint files and exported model files.\n\nFor example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003.\n\nhparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\nmodel_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\noptions = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\nmodel_2 = gesture_recognizer.GestureRecognizer.create(\n    train_data=train_data,\n    validation_data=validation_data,\n    options=options\n)\n\n\nEvaluate the newly trained model.\n\nloss, accuracy = model_2.evaluate(test_data)\nprint(f\"Test loss:{loss}, Test accuracy:{accuracy}\")\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-24 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.CustomMetadata  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/CustomMetadata",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nMethods\nCustomMetadataBufferHasIdentifier\nData\nDataAsNumpy\nDataIsNone\nDataLength\nGetRootAs\nGetRootAsCustomMetadata\nInit\nName\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.CustomMetadata \nbookmark_border\n\nView source on GitHub\nMethods\nCustomMetadataBufferHasIdentifier\n\nView source\n\n@classmethod\nCustomMetadataBufferHasIdentifier(\n    buf, offset, size_prefixed=False\n)\n\nData\n\nView source\n\nData(\n    j\n)\n\nDataAsNumpy\n\nView source\n\nDataAsNumpy()\n\nDataIsNone\n\nView source\n\nDataIsNone()\n\nDataLength\n\nView source\n\nDataLength()\n\nGetRootAs\n\nView source\n\n@classmethod\nGetRootAs(\n    buf, offset=0\n)\n\nGetRootAsCustomMetadata\n\nView source\n\n@classmethod\nGetRootAsCustomMetadata(\n    buf, offset=0\n)\n\n\nThis method is deprecated. Please switch to GetRootAs.\n\nInit\n\nView source\n\nInit(\n    buf, pos\n)\n\nName\n\nView source\n\nName()\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.ListPermissionsResponse  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/ListPermissionsResponse",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.ListPermissionsResponse \nbookmark_border\n\nView source on GitHub\n\nResponse from ListPermissions containing a paginated list of permissions.\n\nAttributes\n\npermissions\tMutableSequence[google.ai.generativelanguage.Permission]\n\nReturned permissions.\n\n\nnext_page_token\tstr\n\nA token, which can be sent as page_token to retrieve the next page.\n\nIf this field is omitted, there are no more pages.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dotcalculator_pb2.ProfilerConfig  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/core/constant_side_packet_calculator_pb2/mediapipe_dot_framework_dot_calculator__pb2/ProfilerConfig",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nOverview\nconstant_side_packet_calculator_pb2\nOverview\nConstantSidePacketCalculatorOptions\nConstantSidePacketCalculatorOptions.ConstantSidePacket\nmediapipe_dot_framework_dot_calculator__options__pb2\nmediapipe_dot_framework_dot_calculator__pb2\nOverview\nCalculatorGraphConfig\nCalculatorGraphConfig.Node\nExecutorConfig\nInputCollection\nInputCollectionSet\nInputStreamInfo\nProfilerConfig\nmediapipe_dot_framework_dot_mediapipe__options__pb2\nmediapipe_dot_framework_dot_packet__factory__pb2\nmediapipe_dot_framework_dot_packet__generator__pb2\nmediapipe_dot_framework_dot_status__handler__pb2\nmediapipe_dot_framework_dot_stream__handler__pb2\nmediapipe_dot_framework_dot_formats_dot_classification__pb2\nmediapipe_dot_framework_dot_formats_dot_landmark__pb2\nmediapipe_dot_framework_dot_formats_dot_matrix__data__pb2\nmediapipe_dot_framework_dot_formats_dot_time__series__header__pb2\nflow_limiter_calculator_pb2\ngate_calculator_pb2\nsplit_vector_calculator_pb2\nimage\ntensor\ntflite\nutil\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dotcalculator_pb2.ProfilerConfig \nbookmark_border\n\nView source on GitHub\n\nA ProtocolMessage\n\nView aliases\n\nAttributes\n\ncalculator_filter\tstring calculator_filter\nenable_input_output_latency\tbool enable_input_output_latency\nenable_profiler\tbool enable_profiler\nenable_stream_latency\tbool enable_stream_latency\nhistogram_interval_size_usec\tint64 histogram_interval_size_usec\nnum_histogram_intervals\tint64 num_histogram_intervals\ntrace_enabled\tbool trace_enabled\ntrace_event_types_disabled\trepeated int32 trace_event_types_disabled\ntrace_log_capacity\tint64 trace_log_capacity\ntrace_log_count\tint32 trace_log_count\ntrace_log_disabled\tbool trace_log_disabled\ntrace_log_duration_events\tbool trace_log_duration_events\ntrace_log_instant_events\tbool trace_log_instant_events\ntrace_log_interval_count\tint32 trace_log_interval_count\ntrace_log_interval_usec\tint64 trace_log_interval_usec\ntrace_log_margin_usec\tint64 trace_log_margin_usec\ntrace_log_path\tstring trace_log_path\nuse_packet_timestamp_for_added_packet\tbool use_packet_timestamp_for_added_packet\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/objc/vision/Classes/MPPImageClassifierOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nObjectiveC\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nOverview\nMPPBaseOptions\nMPPCategory\nMPPClassificationResult\nMPPClassifications\nMPPClassifierOptions\nMPPConnection\nMPPDetection\nMPPEmbedding\nMPPEmbeddingResult\nMPPFaceDetector\nMPPFaceDetectorOptions\nMPPFaceDetectorResult\nMPPFaceLandmarker\nMPPFaceLandmarkerOptions\nMPPFaceLandmarkerResult\nMPPFaceStylizer\nMPPFaceStylizerOptions\nMPPFaceStylizerResult\nMPPGestureRecognizer\nMPPGestureRecognizerOptions\nMPPGestureRecognizerResult\nMPPHandLandmarker\nMPPHandLandmarkerOptions\nMPPHandLandmarkerResult\nMPPImage\nMPPImageClassifier\nMPPImageClassifierOptions\nMPPImageClassifierResult\nMPPImageEmbedder\nMPPImageEmbedderOptions\nMPPImageEmbedderResult\nMPPImageSegmenter\nMPPImageSegmenterOptions\nMPPImageSegmenterResult\nMPPInteractiveSegmenter\nMPPInteractiveSegmenterOptions\nMPPInteractiveSegmenterResult\nMPPLandmark\nMPPMask\nMPPNormalizedKeypoint\nMPPNormalizedLandmark\nMPPObjectDetector\nMPPObjectDetectorOptions\nMPPObjectDetectorResult\nMPPPoseLandmarker\nMPPPoseLandmarkerOptions\nMPPPoseLandmarkerResult\nMPPRegionOfInterest\nMPPTaskOptions\nMPPTaskResult\nMPPTransformMatrix\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nModel Maker\nTensorFlow Lite\nOn this page\nrunningMode\nimageClassifierLiveStreamDelegate\ndisplayNamesLocale\nmaxResults\nscoreThreshold\ncategoryAllowlist\ncategoryDenylist\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nMPPImageClassifierOptions\n\n@interface MPPImageClassifierOptions : MPPTaskOptions <NSCopying>\n\nOptions for setting up a ImageClassifier.\n\nrunningMode\n\nRunning mode of the image classifier task. Defaults to .image. ImageClassifier can be created with one of the following running modes:\n\n.image: The mode for performing classification on single image inputs.\n.video: The mode for performing classification on the decoded frames of a video.\n.liveStream: The mode for performing classification on a live stream of input data, such as from the camera.\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic) MPPRunningMode runningMode;\nimageClassifierLiveStreamDelegate\n\nAn object that confirms to ImageClassifierLiveStreamDelegate protocol. This object must implement objectDetector(_:didFinishDetectionWithResult:timestampInMilliseconds:error:) to receive the results of asynchronous classification on images (i.e, when runningMode = .liveStream).\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic, weak, nullable) id<MPPImageClassifierLiveStreamDelegate> imageClassifierLiveStreamDelegate;\ndisplayNamesLocale\n\nThe locale to use for display names specified through the TFLite Model Metadata, if any. Defaults to English.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic, copy) NSString *_Nonnull displayNamesLocale;\nmaxResults\n\nThe maximum number of top-scored classification results to return. If < 0, all available results will be returned. If 0, an invalid argument error is returned.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic) NSInteger maxResults;\nscoreThreshold\n\nScore threshold to override the one provided in the model metadata (if any). Results below this value are rejected.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic) float scoreThreshold;\ncategoryAllowlist\n\nThe allowlist of category names. If non-empty, detection results whose category name is not in this set will be filtered out. Duplicate or unknown category names are ignored. Mutually exclusive with categoryDenylist.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic, copy) NSArray<NSString *> *_Nonnull categoryAllowlist;\ncategoryDenylist\n\nThe denylist of category names. If non-empty, detection results whose category name is in this set will be filtered out. Duplicate or unknown category names are ignored. Mutually exclusive with categoryAllowlist.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic, copy) NSArray<NSString *> *_Nonnull categoryDenylist;\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.MetadataFilter  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/MetadataFilter",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.MetadataFilter \nbookmark_border\n\nView source on GitHub\n\nUser provided filter to limit retrieval based on Chunk or Document level metadata values.\n\nExample (genre = drama OR genre = action): key = \"document.custom_metadata.genre\" conditions = [{string_value = \"drama\", operation = EQUAL}, {string_value = \"action\", operation = EQUAL}]\n\nAttributes\n\nkey\tstr\n\nRequired. The key of the metadata to filter on.\n\n\nconditions\tMutableSequence[google.ai.generativelanguage.Condition]\n\nRequired. The Condition\\ s for the given key that will trigger this filter. Multiple Condition\\ s are joined by logical ORs.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "TensorFlowLiteSwift Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/swift/Enums/ThreadWaitType",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nSwift\nTensorFlowLiteSwift\nClasses\nEnumerations\nOverview\nInterpreterError\nRuntime\nThreadWaitType\nProtocols\nStructures\nType Aliases\nC\nC++\nOn this page\nnone\npassive\nactive\naggressive\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nTensorFlowLiteSwift Framework Reference \nbookmark_border\nThreadWaitType\npublic enum ThreadWaitType : Equatable, Hashable\n\nA type indicating how the current thread should wait for work scheduled on the GPU to complete.\n\nnone\n\nThe thread does not wait for the work to complete. Useful when the output of the work is used with the GPU pipeline.\n\nDeclaration\n\nSWIFT\n\ncase none\npassive\n\nThe thread waits until the work is complete.\n\nDeclaration\n\nSWIFT\n\ncase passive\nactive\n\nThe thread waits for the work to complete with minimal latency, which may require additional CPU resources.\n\nDeclaration\n\nSWIFT\n\ncase active\naggressive\n\nThe thread waits for the work while trying to prevent the GPU from going into sleep mode.\n\nDeclaration\n\nSWIFT\n\ncase aggressive\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipe Framework on iOS  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/framework/getting_started/ios",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nInstallation\nGetting started\nFramework on Android\nFramework on iOS\nOverview\nHello world!\nFramework in Python\nFramework in C++\nFramework concepts\nOverview\nCalculators\nGraphs\nPackets\nSynchronization\nGPU\nReal-time Streams\nFramework tools\nVisualizer\nResources\nGetting Help\nFAQ\nTroubleshooting\nGPU Support\nOn this page\nBuilding iOS example apps\nPrerequisite\nSet up a bundle ID prefix\nCreate an Xcode project\nSet up provisioning\nBuild and run an app using Xcode\nBuild an app using the command line\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nFramework\nSend feedback\nMediaPipe Framework on iOS \nbookmark_border\n\nFollow instructions below to build iOS example apps with MediaPipe Framework. To learn more about these example apps, start from, start from Hello World! on iOS.\n\nBuilding iOS example apps\nPrerequisite\n\nInstall Mediapipe Framework following these instructions.\n\nInstall Xcode, then install the Command Line Tools using:\n\nxcode-select --install\n\n\nInstall Bazelisk.\n\nWe recommend using Homebrew to get the latest versions.\n\nbrew install bazelisk\n\n\nSet Python 3.7 as the default Python version and install the Python \"six\" library. This is needed for TensorFlow.\n\npip3 install --user six\n\n\nClone the MediaPipe repository.\n\ngit clone https://github.com/google/mediapipe.git\n\nSet up a bundle ID prefix\n\nAll iOS apps must have a bundle ID, and you must have a provisioning profile that lets you install an app with that ID onto your phone. To avoid clashes between different MediaPipe users, you need to configure a unique prefix for the bundle IDs of our iOS demo apps.\n\nIf you have a custom provisioning profile, see Custom provisioning below.\n\nOtherwise, run this command to generate a unique prefix:\n\npython3 mediapipe/examples/ios/link_local_profiles.py\n\nCreate an Xcode project\n\nThis lets you to edit and debug one of the example apps in Xcode. It also lets you to make use of automatic provisioning (see later section).\n\nWe will use a tool called Tulsi for generating Xcode projects from Bazel build configurations.\n\n# cd out of the mediapipe directory, then:\ngit clone https://github.com/bazelbuild/tulsi.git\ncd tulsi\n# remove Xcode version from Tulsi's .bazelrc (see http://github.com/bazelbuild/tulsi#building-and-installing):\nsed -i .orig '/xcode_version/d' .bazelrc\n# build and run Tulsi:\nsh build_and_run.sh\n\n\nThis will install Tulsi.app inside the Applications directory in your home directory.\n\nOpen mediapipe/Mediapipe.tulsiproj using the Tulsi app.\n\nTip: If Tulsi displays an error saying \"Bazel could not be found\", press the \"Bazel...\" button in the Packages tab and select the bazel executable in your homebrew /bin/ directory.\n\nSelect the MediaPipe config in the Configs tab, then press the Generate button below. You will be asked for a location to save the Xcode project. Once the project is generated, it will be opened in Xcode.\n\nIf you get an error about bundle IDs, see the previous section.\n\nSet up provisioning\n\nTo install applications on an iOS device, you need a provisioning profile. There are two options:\n\nAutomatic provisioning. This allows you to build and install an app to your personal device. The provisioning profile is managed by Xcode, and has to be updated often (it is valid for about a week).\n\nCustom provisioning. This uses a provisioning profile associated with an Apple developer account. These profiles have a longer validity period and can target multiple devices, but you need a paid developer account with Apple to obtain one.\n\nAutomatic provisioning\n\nCreate an Xcode project for MediaPipe, as discussed earlier\n\nIn the project navigator in the left sidebar, select the \"Mediapipe\" project.\n\nSelect one of the application targets, for example, HandTrackingGpuApp.\n\nSelect the \"Signing & Capabilities\" tab.\n\nCheck \"Automatically manage signing\", and confirm the dialog.\n\nSelect \"Your Name (Personal Team)\" in the Team dialog menu.\n\nThis set-up needs to be done once for each application you want to install. Repeat steps 3-6 as needed.\n\nThis generates provisioning profiles for each app you have selected. Now you need to tell Bazel to use them. We have provided a script to make this easier.\n\nIn the terminal, to the mediapipe directory where you cloned the repository.\n\nRun this command:\n\npython3 mediapipe/examples/ios/link_local_profiles.py\n\n\nThis will find and link the provisioning profile for all applications for which you have enabled automatic provisioning in Xcode.\n\nNote: once a profile expires, Xcode will generate a new one; you must then run this script again to link the updated profiles.\nCustom provisioning\nObtain a provisioning profile from Apple.\nTip: You can use this command to see the provisioning profiles you have previously downloaded using Xcode: open ~/Library/MobileDevice/\"Provisioning Profiles\". If there are none, generate and download a profile on Apple's developer site.\n\nSymlink or copy your provisioning profile to mediapipe/mediapipe/provisioning_profile.mobileprovision.\n\ncd mediapipe\nln -s ~/Downloads/MyProvisioningProfile.mobileprovision mediapipe/provisioning_profile.mobileprovision\n\nNote: if you had previously set up automatic provisioning, you should remove the provisioning_profile.mobileprovision symlink in each example's directory, since it will take precedence over the common one. You can also overwrite it with you own profile if you need a different profile for different apps.\nOpen mediapipe/examples/ios/bundle_id.bzl, and change the BUNDLE_ID_PREFIX to a prefix associated with your provisioning profile.\nBuild and run an app using Xcode\n\nCreate the Xcode project, and make sure you have set up either automatic or custom provisioning.\n\nYou can now select any of the Mediapipe Framework demos in the target menu, and build and run them as normal.\n\nNote: When you ask Xcode to run an app, by default it will use the Debug configuration. Some of our demos are computationally heavy; you may want to use the Release configuration for better performance.\nNote: Due to an incompatibility caused by one of our dependencies, The Framework cannot be used for apps running on the iPhone Simulator on Apple Silicon (M1).\nTip: To switch build configuration in Xcode, click on the target menu, choose \"Edit Scheme...\", select the Run action, and switch the Build Configuration from Debug to Release. Note that this is set independently for each target.\nTip: On the device, in Settings > General > Device Management, make sure the developer (yourself) is trusted.\nBuild an app using the command line\n\nMake sure you have set up either automatic or custom provisioning.\n\nUsing MediaPipe Hands for example, run:\n\nbazel build -c opt --config=ios_arm64 mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp\n\n\nYou may see a permission request from codesign in order to sign the app.\n\nTip: If you are using custom provisioning, you can run this script to build all Framework iOS example apps.\n\nIn Xcode, open the Devices and Simulators window (command-shift-2).\n\nMake sure your device is connected. You will see a list of installed apps. Press the \"+\" button under the list, and select the .ipa file built by Bazel.\n\nYou can now run the app on your device.\n\nTip: On the device, in Settings > General > Device Management, make sure the developer (yourself) is trusted.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-24 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.holistic_landmarker.holistic_landmarker_graph_options_pb2.mediapipe_dot_tasks_dot_cc_dot_vision_dothand_landmarker_dot_proto_dothandlandmarksdetectorgraphoptions_pb2.HandLandmarksDetectorGraphOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/holistic_landmarker_graph_options_pb2/mediapipe_dot_tasks_dot_cc_dot_vision_dot_hand__landmarker_dot_proto_dot_hand__landmarks__detector__graph__options__pb2/HandLandmarksDetectorGraphOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\ncategory_module\nholistic_landmarker_graph_options_pb2\nOverview\nHolisticLandmarkerGraphOptions\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_face__detector_dot_proto_dot_face__detector__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_face__landmarker_dot_proto_dot_face__landmarks__detector__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_hand__landmarker_dot_proto_dot_hand__landmarks__detector__graph__options__pb2\nOverview\nHandLandmarksDetectorGraphOptions\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_hand__landmarker_dot_proto_dot_hand__roi__refinement__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_pose__detector_dot_proto_dot_pose__detector__graph__options__pb2\nmediapipe_dot_tasks_dot_cc_dot_vision_dot_pose__landmarker_dot_proto_dot_pose__landmarks__detector__graph__options__pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.holistic_landmarker.holistic_landmarker_graph_options_pb2.mediapipe_dot_tasks_dot_cc_dot_vision_dothand_landmarker_dot_proto_dothandlandmarksdetectorgraphoptions_pb2.HandLandmarksDetectorGraphOptions \nbookmark_border\n\nView source on GitHub\n\nA ProtocolMessage\n\nAttributes\n\nbase_options\tBaseOptions base_options\nmin_detection_confidence\tfloat min_detection_confidence\n\nClass Variables\n\next\tInstance of google.protobuf.pyext._message.FieldDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.CountTokensResponse  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/CountTokensResponse",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.CountTokensResponse \nbookmark_border\n\nView source on GitHub\n\nA response from CountTokens.\n\nIt returns the model's token_count for the prompt.\n\nAttributes\n\ntotal_tokens\tint\n\nThe number of tokens that the model tokenizes the prompt into.\n\nAlways non-negative.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Join the Google Developer Community Discord  |  Gemma  |  Google for Developers",
    "url": "https://ai.google.dev/gemma/docs/discord",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nDocs\nOverview\nModel Card\nGemma model family\nCodeGemma\nPaliGemma\nRecurrentGemma\nReleases\nGuides\nGemma setup\nGet started with Gemma using Keras\nBasic tuning with Gemma using Keras\nDistributed tuning with Gemma using Keras\nGet started with Gemma using PyTorch\nChat with Gemma\nFormatting and system instructions\nGemma C++ Tutorial\nInference using JAX and Flax\nFine-tuning using JAX and Flax\nIntegrations\nVertex AI\nDataflow ML\nGoogle Kubernetes Engine (GKE)\nLangChain\nResponsible Generative AI Toolkit\nOverview\nSet responsible policies\nTune models for safety\nCreate input and output safeguards\nConduct safety evaluations\nBuild transparency artifacts\nAnalyze model behavior\nCommunity\nDiscord\nLegal\nTerms of use\nProhibited use\nHome\nGemma\nDocs\nSend feedback\nJoin the Google Developer Community Discord \nbookmark_border\n\nAre you interested in chatting with Googlers and other developers about using Gemma? Join the Google Developer Community Discord server! This is a great place to interact with developers, and to learn, share, and support each other.\n\nYou can find us in the #gemma channel in the \"All things AI\" section. See you there!\n\nTip: If you're new to Discord, check out this beginner guide to familiarize yourself with the platform.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-02-26 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.task.text.TextEmbedderOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/task/text/TextEmbedderOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nmetadata_writers\ntask\nOverview\naudio\ncore\nprocessor\ntext\nOverview\nBertCluAnnotator\nBertCluAnnotatorOptions\nBertNLClassifier\nBertNLClassifierOptions\nBertQuestionAnswerer\nBertQuestionAnswererOptions\nNLClassifier\nNLClassifierOptions\nTextEmbedder\nTextEmbedderOptions\nTextSearcher\nTextSearcherOptions\nvision\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nAttributes\nMethods\n__eq__\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.task.text.TextEmbedderOptions \nbookmark_border\n\nView source on GitHub\n\nOptions for the text embedder task.\n\ntflite_support.task.text.TextEmbedderOptions(\n    base_options: tflite_support.task.core.BaseOptions,\n    embedding_options: tflite_support.task.processor.EmbeddingOptions = dataclasses.field(default_factory=_EmbeddingOptions)\n)\n\n\nAttributes\n\nbase_options\tBase options for the text embedder task.\nembedding_options\tEmbedding options for the text embedder task.\n\nMethods\n__eq__\n__eq__(\n    other\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.TensorMetadataStartAssociatedFilesVector  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/TensorMetadataStartAssociatedFilesVector",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.TensorMetadataStartAssociatedFilesVector \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.TensorMetadataStartAssociatedFilesVector(\n    builder, numElems\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mediapipe_model_maker.gesture_recognizer.ModelOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mediapipe_model_maker/gesture_recognizer/ModelOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nModel Maker\nmediapipe_model_maker\nmediapipe_model_maker.face_stylizer\nmediapipe_model_maker.gesture_recognizer\nOverview\nDataset\nGestureRecognizer\nGestureRecognizerOptions\nHParams\nHandDataPreprocessingParams\nModelOptions\nmediapipe_model_maker.image_classifier\nmediapipe_model_maker.image_utils\nmediapipe_model_maker.model_util\nmediapipe_model_maker.object_detector\nmediapipe_model_maker.quantization\nmediapipe_model_maker.text_classifier\nTensorFlow Lite\nOn this page\nAttributes\nMethods\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmediapipe_model_maker.gesture_recognizer.ModelOptions \nbookmark_border\n\nView source on GitHub\n\nConfigurable options for gesture recognizer model.\n\nmediapipe_model_maker.gesture_recognizer.ModelOptions(\n    dropout_rate: float = 0.05,\n    layer_widths: List[int] = dataclasses.field(default_factory=list)\n)\n\n\nAttributes\n\ndropout_rate\tThe fraction of the input units to drop, used in dropout layer.\nlayer_widths\tA list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n\nMethods\n__eq__\n__eq__(\n    other\n)\n\n\nClass Variables\n\ndropout_rate\t0.05\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsT  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsT",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nMethods\nInitFromBuf\nInitFromObj\nInitFromPackedBuf\nPack\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.RegexTokenizerOptionsT \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.RegexTokenizerOptionsT()\n\nMethods\nInitFromBuf\n\nView source\n\n@classmethod\nInitFromBuf(\n    buf, pos\n)\n\nInitFromObj\n\nView source\n\n@classmethod\nInitFromObj(\n    regexTokenizerOptions\n)\n\nInitFromPackedBuf\n\nView source\n\n@classmethod\nInitFromPackedBuf(\n    buf, pos=0\n)\n\nPack\n\nView source\n\nPack(\n    builder\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Build and convert models  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/lite/microcontrollers/build_convert",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nRunning ML models\nBuild TensorFlow Lite\nHardware acceleration\nDelegates\nGPU delegates\nImplementing a delegate\nAndroid\nOverview\nGoogle Play services runtime\nDevelopment tools\nHardware acceleration\nModels with metadata\niOS\nOverview\nCore ML delegate\nGPU delegate\nMicrocontrollers\nOverview\nGet started\nLinux-based devices with Python\nUnderstand the C library\nBuild and convert models\nModels\nSupport multiple frameworks\nUse trained models\nDesign and build models\nConvert TF models\nOverview\nConvert TensorFlow models\nAdd Signatures\nConversion tools\nModel compatibility\nOptimize models\nAdd model metadata\nConvert PyTorch models\nOverview\nPyTorch to TFLite quickstart\nConvert generative PyTorch models\nConvert JAX models\nOverview\nJAX to TFLite quickstart\nOn this page\nModel conversion\nConvert to a C array\nModel architecture and training\nModel size\nWorkload\nOperation support\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nTensorFlow Lite\nSend feedback\nBuild and convert models \nbookmark_border\n\nMicrocontrollers have limited RAM and storage, which places constraints on the sizes of machine learning models. In addition, TensorFlow Lite for Microcontrollers currently supports a limited subset of operations, so not all model architectures are possible.\n\nThis document explains the process of converting a TensorFlow model to run on microcontrollers. It also outlines the supported operations and gives some guidance on designing and training a model to fit in limited memory.\n\nFor an end-to-end, runnable example of building and converting a model, see the Hello World example.\n\nModel conversion\n\nTo convert a trained TensorFlow model to run on microcontrollers, you should use the TensorFlow Lite converter Python API. This will convert the model into a FlatBuffer, reducing the model size, and modify it to use TensorFlow Lite operations.\n\nTo obtain the smallest possible model size, you should consider using post-training quantization.\n\nConvert to a C array\n\nMany microcontroller platforms do not have native filesystem support. The easiest way to use a model from your program is to include it as a C array and compile it into your program.\n\nThe following unix command will generate a C source file that contains the TensorFlow Lite model as a char array:\n\nxxd -i converted_model.tflite > model_data.cc\n\n\nThe output will look similar to the following:\n\nunsigned char converted_model_tflite[] = {\n  0x18, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x0e, 0x00,\n  // <Lines omitted>\n};\nunsigned int converted_model_tflite_len = 18200;\n\n\nOnce you have generated the file, you can include it in your program. It is important to change the array declaration to const for better memory efficiency on embedded platforms.\n\nFor an example of how to include and use a model in your program, see hello_world_test.cc in the Hello World example.\n\nModel architecture and training\n\nWhen designing a model for use on microcontrollers, it is important to consider the model size, workload, and the operations that are used.\n\nModel size\n\nA model must be small enough to fit within your target device's memory alongside the rest of your program, both as a binary and at runtime.\n\nTo create a smaller model, you can use fewer and smaller layers in your architecture. However, small models are more likely to suffer from underfitting. This means for many problems, it makes sense to try and use the largest model that will fit in memory. However, using larger models will also lead to increased processor workload.\n\nNote: The core runtime for TensorFlow Lite for Microcontrollers fits in 16KB on a Cortex M3.\nWorkload\n\nThe size and complexity of the model has an impact on workload. Large, complex models might result in a higher duty cycle, which means your device's processor is spending more time working and less time idle. This will increase power consumption and heat output, which might be an issue depending on your application.\n\nOperation support\n\nTensorFlow Lite for Microcontrollers currently supports a limited subset of TensorFlow operations, which impacts the model architectures that it is possible to run. We are working on expanding operation support, both in terms of reference implementations and optimizations for specific architectures.\n\nThe supported operations can be seen in the file micro_mutable_ops_resolver.h\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-03 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mediapipe_model_maker.face_stylizer.face_stylizer.image_preprocessing.preprocess_for_eval  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mediapipe_model_maker/face_stylizer/face_stylizer/image_preprocessing/preprocess_for_eval",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nModel Maker\nmediapipe_model_maker\nmediapipe_model_maker.face_stylizer\nOverview\nDataset\nFaceStylizer\nFaceStylizerOptions\nHParams\nModelOptions\nSupportedModels\nconstants\ndataset\nface_stylizer\nOverview\nimage_preprocessing\nOverview\nPreprocessor\npreprocess_for_eval\npreprocess_for_train\nloss_functions\nface_stylizer_options\nhyperparameters\nmodel_options\nmodel_spec\nmediapipe_model_maker.gesture_recognizer\nmediapipe_model_maker.image_classifier\nmediapipe_model_maker.image_utils\nmediapipe_model_maker.model_util\nmediapipe_model_maker.object_detector\nmediapipe_model_maker.quantization\nmediapipe_model_maker.text_classifier\nTensorFlow Lite\nOn this page\nArgs\nReturns\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmediapipe_model_maker.face_stylizer.face_stylizer.image_preprocessing.preprocess_for_eval \nbookmark_border\n\nView source on GitHub\n\nPreprocesses the given image for evaluation.\n\nmediapipe_model_maker.face_stylizer.face_stylizer.image_preprocessing.preprocess_for_eval(\n    image: tf.Tensor,\n    image_size: int = IMAGE_SIZE,\n    resize_method: str = tf.image.ResizeMethod.BILINEAR\n) -> tf.Tensor\n\n\nArgs\n\nimage\t4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor of shape [height, width, channels].\nimage_size\timage size.\nresize_method\tif None, use bicubic.\n\nReturns\n\nA preprocessed image Tensor.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.holistic_landmarker.base_vision_task_api.math.hypot  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/base_vision_task_api/math/hypot",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\nOverview\nmath\nOverview\nacos\nacosh\nasin\nasinh\natan\natan2\natanh\nceil\ncomb\ncopysign\ncos\ncosh\ndegrees\ndist\nerf\nerfc\nexp\nexpm1\nfabs\nfactorial\nfloor\nfmod\nfrexp\nfsum\ngamma\ngcd\nhypot\nisclose\nisfinite\nisinf\nisnan\nisqrt\nlcm\nldexp\nlgamma\nlog\nlog10\nlog1p\nlog2\nmodf\nnextafter\nperm\npow\nprod\nradians\nremainder\nsin\nsinh\nsqrt\ntan\ntanh\ntrunc\nulp\nrect_module\ncategory_module\nholistic_landmarker_graph_options_pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nRoughly equivalent to\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.hypot \nbookmark_border\n\nhypot(*coordinates) -> value\n\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.hypot()\n\n\nMultidimensional Euclidean distance from the origin to a point.\n\nRoughly equivalent to\n\nsqrt(sum(x**2 for x in coordinates))\n\nFor a two dimensional point (x, y), gives the hypotenuse using the Pythagorean theorem: sqrt(xx + yy).\n\nFor example, the hypotenuse of a 3/4/5 right triangle is:\n\n<pre class=\"devsite-click-to-copy prettyprint lang-py\">\n<code class=\"devsite-terminal\" data-terminal-prefix=\"&gt;&gt;&gt;\">hypot(3.0, 4.0)</code>\n<code class=\"no-select nocode\">    5.0</code>\n<code class=\"no-select nocode\">    </code>\n</pre>\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.InteractiveSegmenterRegionOfInterest.Format  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/InteractiveSegmenterRegionOfInterest/Format",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.InteractiveSegmenterRegionOfInterest.Format \nbookmark_border\n\nView source on GitHub\n\nAn enumeration.\n\nClass Variables\n\nKEYPOINT\t<Format.KEYPOINT: 1>\nUNSPECIFIED\t<Format.UNSPECIFIED: 0>\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.components.containers.keypoint.NormalizedKeypoint  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/components/containers/keypoint/NormalizedKeypoint",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\nOverview\ncontainers\nOverview\nAudioData\nAudioDataFormat\nBoundingBox\nCategory\nClassifications\nDetection\nEmbedding\nLandmark\nLandmarksDetectionResult\nNormalizedLandmark\nNormalizedRect\nRect\nkeypoint\nOverview\nNormalizedKeypoint\nlocation_data_pb2\nprocessors\nutils\ngenai\ntext\nvision\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nMethods\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.components.containers.keypoint.NormalizedKeypoint \nbookmark_border\n\nView source on GitHub\n\nA normalized keypoint.\n\nmp.tasks.components.containers.keypoint.NormalizedKeypoint(\n    x: Optional[float] = None,\n    y: Optional[float] = None,\n    label: Optional[str] = None,\n    score: Optional[float] = None\n)\n\n\nNormalized keypoint represents a point in 2D space with x, y coordinates. x and y are normalized to [0.0, 1.0] by the image width and height respectively.\n\nAttributes\n\nx\tThe x coordinates of the normalized keypoint.\ny\tThe y coordinates of the normalized keypoint.\nlabel\tThe optional label of the keypoint.\nscore\tThe score of the keypoint.\n\nMethods\n__eq__\n\nView source\n\n__eq__(\n    other: Any\n) -> bool\n\n\nChecks if this object is equal to the given object.\n\nArgs\nother\tThe object to be compared with.\n\nReturns\nTrue if the objects are equal.\n\nClass Variables\n\nlabel\tNone\nscore\tNone\nx\tNone\ny\tNone\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "AudioEmbedder class  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-audio.audioembedder",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\n@mediapipe/tasks-audio\nOverview\nAudioClassifier\nAudioClassifierOptions\nAudioClassifierResult\nAudioEmbedder\nAudioEmbedderOptions\nAudioEmbedderResult\nCategory\nClassifications\nEmbedding\nFilesetResolver\n@mediapipe/tasks-text\n@mediapipe/tasks-vision\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nMethods\nAudioEmbedder.createFromModelBuffer()\nParameters\nAudioEmbedder.createFromModelPath()\nParameters\nAudioEmbedder.createFromOptions()\nParameters\nAudioEmbedder.embed()\nParameters\nAudioEmbedder.setOptions()\nParameters\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nAudioEmbedder class \nbookmark_border\n\nPerforms embedding extraction on audio.\n\nSignature:\n\nexport declare class AudioEmbedder extends AudioTaskRunner<AudioEmbedderResult[]> \n\n\nExtends: AudioTaskRunner<AudioEmbedderResult[]>\n\nMethods\nMethod\tModifiers\tDescription\ncreateFromModelBuffer(wasmFileset, modelAssetBuffer)\tstatic\tInitializes the Wasm runtime and creates a new audio embedder based on the provided model asset buffer.\ncreateFromModelPath(wasmFileset, modelAssetPath)\tstatic\tInitializes the Wasm runtime and creates a new audio embedder based on the path to the model asset.\ncreateFromOptions(wasmFileset, audioEmbedderOptions)\tstatic\tInitializes the Wasm runtime and creates a new audio embedder from the provided options.\nembed(audioData, sampleRate)\t\tPerforms embeding extraction on the provided audio clip and waits synchronously for the response.\nsetOptions(options)\t\tSets new options for the audio embedder.Calling setOptions() with a subset of options only affects those options. You can reset an option back to its default value by explicitly setting it to undefined.\nAudioEmbedder.createFromModelBuffer()\n\nInitializes the Wasm runtime and creates a new audio embedder based on the provided model asset buffer.\n\nSignature:\n\nstatic createFromModelBuffer(wasmFileset: WasmFileset, modelAssetBuffer: Uint8Array): Promise<AudioEmbedder>;\n\nParameters\nParameter\tType\tDescription\nwasmFileset\tWasmFileset\tA configuration object that provides the location of the Wasm binary and its loader.\nmodelAssetBuffer\tUint8Array\tA binary representation of the TFLite model.\n\nReturns:\n\nPromise<AudioEmbedder>\n\nAudioEmbedder.createFromModelPath()\n\nInitializes the Wasm runtime and creates a new audio embedder based on the path to the model asset.\n\nSignature:\n\nstatic createFromModelPath(wasmFileset: WasmFileset, modelAssetPath: string): Promise<AudioEmbedder>;\n\nParameters\nParameter\tType\tDescription\nwasmFileset\tWasmFileset\tA configuration object that provides the location of the Wasm binary and its loader.\nmodelAssetPath\tstring\tThe path to the TFLite model.\n\nReturns:\n\nPromise<AudioEmbedder>\n\nAudioEmbedder.createFromOptions()\n\nInitializes the Wasm runtime and creates a new audio embedder from the provided options.\n\nSignature:\n\nstatic createFromOptions(wasmFileset: WasmFileset, audioEmbedderOptions: AudioEmbedderOptions): Promise<AudioEmbedder>;\n\nParameters\nParameter\tType\tDescription\nwasmFileset\tWasmFileset\tA configuration object that provides the location of the Wasm binary and its loader.\naudioEmbedderOptions\tAudioEmbedderOptions\tThe options for the audio embedder. Note that either a path to the TFLite model or the model itself needs to be provided (via baseOptions).\n\nReturns:\n\nPromise<AudioEmbedder>\n\nAudioEmbedder.embed()\n\nPerforms embeding extraction on the provided audio clip and waits synchronously for the response.\n\nSignature:\n\nembed(audioData: Float32Array, sampleRate?: number): AudioEmbedderResult[];\n\nParameters\nParameter\tType\tDescription\naudioData\tFloat32Array\tAn array of raw audio capture data, like from a call to getChannelData() on an AudioBuffer.\nsampleRate\tnumber\tThe sample rate in Hz of the provided audio data. If not set, defaults to the sample rate set via setDefaultSampleRate() or 48000 if no custom default was set. The embedding resuls of the audio\n\nReturns:\n\nAudioEmbedderResult[]\n\nAudioEmbedder.setOptions()\n\nSets new options for the audio embedder.\n\nCalling setOptions() with a subset of options only affects those options. You can reset an option back to its default value by explicitly setting it to undefined.\n\nSignature:\n\nsetOptions(options: AudioEmbedderOptions): Promise<void>;\n\nParameters\nParameter\tType\tDescription\noptions\tAudioEmbedderOptions\tThe options for the audio embedder.\n\nReturns:\n\nPromise<void>\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.list_files  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/list_files",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nREST\nOverview\nv1\nv1beta\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.list_files \nbookmark_border\n\nView source on GitHub\n\nCalls the API to list files using a supported file service.\n\ngoogle.generativeai.list_files(\n    page_size=100\n) -> Iterable[file_types.File]\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.holistic_landmarker.base_vision_task_api.math.isnan  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/base_vision_task_api/math/isnan",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\nOverview\nmath\nOverview\nacos\nacosh\nasin\nasinh\natan\natan2\natanh\nceil\ncomb\ncopysign\ncos\ncosh\ndegrees\ndist\nerf\nerfc\nexp\nexpm1\nfabs\nfactorial\nfloor\nfmod\nfrexp\nfsum\ngamma\ngcd\nhypot\nisclose\nisfinite\nisinf\nisnan\nisqrt\nlcm\nldexp\nlgamma\nlog\nlog10\nlog1p\nlog2\nmodf\nnextafter\nperm\npow\nprod\nradians\nremainder\nsin\nsinh\nsqrt\ntan\ntanh\ntrunc\nulp\nrect_module\ncategory_module\nholistic_landmarker_graph_options_pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.isnan \nbookmark_border\n\nReturn True if x is a NaN (not a number), and False otherwise.\n\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.isnan(\n    x, /\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksText Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/swift/text/Classes/TextEmbedderResult",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nMediaPipeTasksText\nClasses\nOverview\nBaseOptions\nClassificationResult\nClassifications\nEmbedding\nEmbeddingResult\nLanguageDetector\nLanguageDetectorOptions\nLanguageDetectorResult\nLanguagePrediction\nResultCategory\nTaskOptions\nTaskResult\nTextClassifier\nTextClassifierOptions\nTextClassifierResult\nTextEmbedder\nTextEmbedderOptions\nTextEmbedderResult\nEnumerations\nMediaPipeTasksVision\nClasses\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nembeddingResult\ninit(embeddingResult:timestampInMilliseconds:)\n-init\n+new\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksText Framework Reference \nbookmark_border\nTextEmbedderResult\nclass TextEmbedderResult : TaskResult\n\nRepresents the embedding results generated by MPPTextEmbedder. *\n\nembeddingResult\n\nThe MPPEmbedderResult instance containing one embedding per embedder head. *\n\nDeclaration\n\nSWIFT\n\nvar embeddingResult: EmbeddingResult { get }\ninit(embeddingResult:timestampInMilliseconds:)\n\nInitializes a new MPPTextEmbedderResult with the given MPPEmbeddingResult and timestamp (in milliseconds).\n\nDeclaration\n\nSWIFT\n\ninit(embeddingResult: EmbeddingResult, timestampInMilliseconds: Int)\nParameters\nembeddingResult\t\n\nThe MPPEmbeddingResult instance containing one set of results per classifier head.\n\n\ntimestampInMilliseconds\t\n\nThe timestamp (in millisecondss) for this result.\n\nReturn Value\n\nAn instance of MPPTextEmbedderResult initialized with the given MPPEmbeddingResult and timestamp (in milliseconds).\n\n-init\n\nUndocumented\n\n+new\n\nUndocumented\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.holistic_landmarker.base_vision_task_api.math.log  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/base_vision_task_api/math/log",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\nOverview\nmath\nOverview\nacos\nacosh\nasin\nasinh\natan\natan2\natanh\nceil\ncomb\ncopysign\ncos\ncosh\ndegrees\ndist\nerf\nerfc\nexp\nexpm1\nfabs\nfactorial\nfloor\nfmod\nfrexp\nfsum\ngamma\ngcd\nhypot\nisclose\nisfinite\nisinf\nisnan\nisqrt\nlcm\nldexp\nlgamma\nlog\nlog10\nlog1p\nlog2\nmodf\nnextafter\nperm\npow\nprod\nradians\nremainder\nsin\nsinh\nsqrt\ntan\ntanh\ntrunc\nulp\nrect_module\ncategory_module\nholistic_landmarker_graph_options_pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.log \nbookmark_border\n\nlog(x, [base=math.e])\n\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.log()\n\n\nReturn the logarithm of x to the given base.\n\nIf the base not specified, returns the natural logarithm (base e) of x.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Module: mp.tasks.vision.holistic_landmarker.category_module  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/category_module",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\ncategory_module\nOverview\nholistic_landmarker_graph_options_pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nModules\nClasses\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nModule: mp.tasks.vision.holistic_landmarker.category_module \nbookmark_border\n\nView source on GitHub\n\nCategory data class.\n\nModules\n\nclassification_pb2 module: Generated protocol buffer code.\n\nClasses\n\nclass Category: A classification category.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/objc/vision/Enums",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nObjectiveC\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nConstants\nEnumerations\nOverview\nMPPDelegate\nMPPHandLandmark\nMPPMaskDataType\nMPPRunningMode\nMPPTasksErrorCode\nProtocols\nType Definitions\nFunctions\nModel Maker\nTensorFlow Lite\nOn this page\nMPPDelegate\nMPPTasksErrorCode\nMPPHandLandmark\nMPPMaskDataType\nMPPRunningMode\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nEnumerations\n\nThe following enumerations are available globally.\n\nMPPDelegate\n\nThe delegate to run MediaPipe. If the delegate is not set, the default delegate CPU is used.\n\nDeclaration\n\nOBJECTIVE-C\n\nenum MPPDelegate : NSUInteger {}\nMPPTasksErrorCode\n\n@enum MPPTasksErrorCode This enum specifies error codes for errors thrown by iOS MediaPipe Task Library.\n\nDeclaration\n\nOBJECTIVE-C\n\nenum MPPTasksErrorCode : NSUInteger {}\nMPPHandLandmark\n\nThe enum containing the 21 hand landmarks.\n\nDeclaration\n\nOBJECTIVE-C\n\nenum MPPHandLandmark : NSUInteger {}\nMPPMaskDataType\n\nThe underlying type of the segmentation mask.\n\nDeclaration\n\nOBJECTIVE-C\n\nenum MPPMaskDataType : NSUInteger {}\nMPPRunningMode\n\nMediaPipe vision task running mode. A MediaPipe vision task can be run with three different modes: image, video and live stream.\n\nDeclaration\n\nOBJECTIVE-C\n\nenum MPPRunningMode : NSUInteger {}\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.packet_creator.create_double  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/packet_creator/create_double",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nOverview\ncreate_bool\ncreate_bool_vector\ncreate_double\ncreate_float\ncreate_float_array\ncreate_float_vector\ncreate_image\ncreate_image_frame\ncreate_image_vector\ncreate_int\ncreate_int16\ncreate_int32\ncreate_int64\ncreate_int8\ncreate_int_array\ncreate_int_vector\ncreate_matrix\ncreate_packet_vector\ncreate_proto\ncreate_proto_vector\ncreate_string\ncreate_string_to_packet_map\ncreate_string_vector\ncreate_uint16\ncreate_uint32\ncreate_uint64\ncreate_uint8\nimage\nimage_frame\npacket\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.packet_creator.create_double \nbookmark_border\n\ncreate_double(arg0: float) -> mediapipe.python._framework_bindings.packet.Packet\n\nView aliases\nmp.packet_creator.create_double()\n\n\nCreate a MediaPipe double Packet from a float.\n\nArgs: data: A float or a np.double.\n\nReturns: A MediaPipe double Packet.\n\nRaises: TypeError: If the input is neither a float nore a np.double.\n\nExamples: packet = mp.packet_creator.create_double(0.1) data = mp.packet_getter.get_float(packet)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.audio.AudioClassifierResult  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/audio/AudioClassifierResult",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\nOverview\nAudioClassifier\nAudioClassifierOptions\nAudioClassifierResult\nAudioEmbedder\nAudioEmbedderOptions\nAudioEmbedderResult\nRunningMode\ncomponents\ngenai\ntext\nvision\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nMethods\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.audio.AudioClassifierResult \nbookmark_border\n\nView source on GitHub\n\nContains the classification results of a model.\n\nView aliases\nmp.tasks.audio.AudioClassifierResult(\n    classifications: List[mp.tasks.components.containers.Classifications],\n    timestamp_ms: Optional[int] = None\n)\n\n\nAttributes\n\nclassifications\tA list of Classifications objects, each for a head of the model.\ntimestamp_ms\tThe optional timestamp (in milliseconds) of the start of the chunk of data corresponding to these results. This is only used for classification on time series (e.g. audio classification). In these use cases, the amount of data to process might exceed the maximum size that the model can process: to solve this, the input data is split into multiple chunks starting at different timestamps.\n\nMethods\n__eq__\n__eq__(\n    other\n)\n\n\nClass Variables\n\ntimestamp_ms\tNone\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.packet_creator.create_uint32  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/packet_creator/create_uint32",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nOverview\ncreate_bool\ncreate_bool_vector\ncreate_double\ncreate_float\ncreate_float_array\ncreate_float_vector\ncreate_image\ncreate_image_frame\ncreate_image_vector\ncreate_int\ncreate_int16\ncreate_int32\ncreate_int64\ncreate_int8\ncreate_int_array\ncreate_int_vector\ncreate_matrix\ncreate_packet_vector\ncreate_proto\ncreate_proto_vector\ncreate_string\ncreate_string_to_packet_map\ncreate_string_vector\ncreate_uint16\ncreate_uint32\ncreate_uint64\ncreate_uint8\nimage\nimage_frame\npacket\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.packet_creator.create_uint32 \nbookmark_border\n\ncreate_uint32(arg0: int) -> mediapipe.python._framework_bindings.packet.Packet\n\nView aliases\nmp.packet_creator.create_uint32()\n\n\nCreate a MediaPipe uint32 Packet from an integer.\n\nArgs: data: An integer or a np.uint32.\n\nReturns: A MediaPipe uint32 Packet.\n\nRaises: OverflowError: If the input integer overflows. TypeError: If the input is neither an integer nor a np.uint32.\n\nExamples: packet = mp.packet_creator.create_uint32(2**32 - 1) data = mp.packet_getter.get_uint(packet)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Setup guide for Python  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/solutions/setup_python",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nAbout the Preview\nTasks\nModel Maker\nStudio\nVision tasks\nObject detection\nImage classification\nImage segmentation\nInteractive segmentation\nGesture recognition\nHand landmark detection\nImage embedding\nFace detection\nFace landmark detection\nPose landmark detection\nFace stylization\nHolistic landmark detection\nText tasks\nText classification\nText embedding\nLanguage detection\nAudio tasks\nAudio classification\nGenerative AI tasks\nImage generation\nLLM Inference\nPlatform setup guides\nAndroid setup\nPython setup\nWeb setup\niOS setup\nOn this page\nSupported platforms and versions\nDeveloper environment setup\nMediaPipe Tasks dependencies\nVision tasks\nText tasks\nAudio tasks\nBaseOptions configuration\nTroubleshooting\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nSolutions\nSend feedback\nSetup guide for Python \nbookmark_border\n\nThis page shows you how to set up your development environment to use MediaPipe Tasks in your Python applications.\n\nSupported platforms and versions\n\nBuilding applications with MediaPipe Tasks requires the following development environment resources:\n\nOS:\nDesktop: Windows, Mac, Linux\nIoT: Raspberry OS 64-bit\nPython: version 3.8 - 3.11\nPIP: version 20.3+\nNote: If you are testing or experimenting with MediaPipe tasks, consider using Colaboratory, a Python notebook environment that requires no setup and runs entirely in the cloud.\nDeveloper environment setup\n\nBefore running a MediaPipe task on a Python application, install the MediaPipe package.\n\n$ python -m pip install mediapipe\n\nAttention: This MediaPipe Solutions Preview is an early release. Learn more.\n\nAfter installing the package, import it into your development project.\n\nimport mediapipe as mp\n\nMediaPipe Tasks dependencies\n\nMediaPipe Tasks provides three prebuilt libraries for vision, text, audio. Depending on the MediaPipe Task used by the app, import the vision, text, or audio library into your development project.\n\nVision tasks\n\nThe MediaPipe Tasks vision module contains tasks that handle image or video inputs. To import the MediaPipe Tasks vision library, import the following dependency to your into your development project.\n\nfrom mediapipe.tasks.python import vision\n\nText tasks\n\nThe MediaPipe Tasks text module contains tasks that handle string inputs.To import the MediaPipe Tasks text library, import the following dependency to your into your development project.\n\nfrom mediapipe.tasks.python import text\n\nAudio tasks\n\nThe MediaPipe Tasks audio module contains tasks that handle sound inputs. To import the MediaPipe Tasks audio library, import the following dependency to your into your development project.\n\nfrom mediapipe.tasks.python import audio\n\nBaseOptions configuration\n\nThe BaseOptions allow for general configuration of MediaPipe Task APIs.\n\nOption name\tDescription\tAccepted values\nmodel_asset_buffer\tThe model asset file contents.\tModel content as a byte string\nmodel_asset_path\tThe path of the model asset to open and map into memory.\tFile path as a string\nTroubleshooting\n\nFor help with technical questions related to MediaPipe, visit the discussion group or Stack Overflow for support from the community. To report bugs or make feature requests, file an issue on GitHub.\n\nFor help setting up your Python development environment, visit the Python developer's guide.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-14 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.ImagePropertiesT  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/ImagePropertiesT",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nMethods\nInitFromBuf\nInitFromObj\nInitFromPackedBuf\nPack\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.ImagePropertiesT \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.ImagePropertiesT()\n\nMethods\nInitFromBuf\n\nView source\n\n@classmethod\nInitFromBuf(\n    buf, pos\n)\n\nInitFromObj\n\nView source\n\n@classmethod\nInitFromObj(\n    imageProperties\n)\n\nInitFromPackedBuf\n\nView source\n\n@classmethod\nInitFromPackedBuf(\n    buf, pos=0\n)\n\nPack\n\nView source\n\nPack(\n    builder\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/swift/vision/Classes/Landmark",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nOverview\nBaseOptions\nClassificationResult\nClassifications\nClassifierOptions\nConnection\nDetection\nEmbedding\nEmbeddingResult\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceStylizer\nFaceStylizerOptions\nFaceStylizerResult\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageClassifierResult\nImageEmbedder\nImageEmbedderOptions\nImageEmbedderResult\nImageSegmenter\nImageSegmenterOptions\nImageSegmenterResult\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterResult\nLandmark\nMPImage\nMask\nNormalizedKeypoint\nNormalizedLandmark\nObjectDetector\nObjectDetectorOptions\nObjectDetectorResult\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nRegionOfInterest\nResultCategory\nTaskOptions\nTaskResult\nTransformMatrix\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nx\ny\nz\nvisibility\npresence\ninit(x:y:z:visibility:presence:)\n-init\n+new\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nLandmark\nclass Landmark : NSObject\n\nLandmark represents a point in 3D space with x, y, z coordinates. The landmark coordinates are in meters. z represents the landmark depth, and the smaller the value the closer the world landmark is to the camera.\n\nx\n\nThe x coordinates of the landmark.\n\nDeclaration\n\nSWIFT\n\nvar x: Float { get }\ny\n\nThe y coordinates of the landmark.\n\nDeclaration\n\nSWIFT\n\nvar y: Float { get }\nz\n\nThe z coordinates of the landmark.\n\nDeclaration\n\nSWIFT\n\nvar z: Float { get }\nvisibility\n\nLandmark visibility. Should be nil if not supported. Float score of whether landmark is visible or occluded by other objects. Landmark considered as invisible also if it is not present on the screen (out of scene bounds). Depending on the model, visibility value is either a sigmoid or an argument of sigmoid.\n\nDeclaration\n\nSWIFT\n\nvar visibility: NSNumber? { get }\npresence\n\nLandmark presence. Should stay unset if not supported. Float score of whether landmark is present on the scene (located within scene bounds). Depending on the model, presence value is either a result of sigmoid or an argument of sigmoid function to get landmark presence probability.\n\nDeclaration\n\nSWIFT\n\nvar presence: NSNumber? { get }\ninit(x:y:z:visibility:presence:)\n\nInitializes a new Landmark object with the given x, y and z coordinates.\n\nDeclaration\n\nSWIFT\n\ninit(x: Float, y: Float, z: Float, visibility: NSNumber?, presence: NSNumber?)\nParameters\nx\t\n\nThe x coordinates of the landmark.\n\n\ny\t\n\nThe y coordinates of the landmark.\n\n\nz\t\n\nThe z coordinates of the landmark.\n\nReturn Value\n\nAn instance of Landmark initialized with the given x, y and z coordinates.\n\n-init\n\nUndocumented\n\n+new\n\nUndocumented\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "com.google.mediapipe.tasks.audio.audioembedder  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/java/com/google/mediapipe/tasks/audio/audioembedder/package-summary",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\ncom.google.mediapipe.framework.image\ncom.google.mediapipe.tasks.audio.audioclassifier\ncom.google.mediapipe.tasks.audio.audioembedder\nOverview\nAudioEmbedder\nAudioEmbedderResult\ncom.google.mediapipe.tasks.audio.core\ncom.google.mediapipe.tasks.components.containers\ncom.google.mediapipe.tasks.components.processors\ncom.google.mediapipe.tasks.components.utils\ncom.google.mediapipe.tasks.core\ncom.google.mediapipe.tasks.core.logging\ncom.google.mediapipe.tasks.genai.llminference\ncom.google.mediapipe.tasks.text.languagedetector\ncom.google.mediapipe.tasks.text.textclassifier\ncom.google.mediapipe.tasks.text.textembedder\ncom.google.mediapipe.tasks.vision.core\ncom.google.mediapipe.tasks.vision.facedetector\ncom.google.mediapipe.tasks.vision.facelandmarker\ncom.google.mediapipe.tasks.vision.facestylizer\ncom.google.mediapipe.tasks.vision.gesturerecognizer\ncom.google.mediapipe.tasks.vision.handlandmarker\ncom.google.mediapipe.tasks.vision.holisticlandmarker\ncom.google.mediapipe.tasks.vision.imageclassifier\ncom.google.mediapipe.tasks.vision.imageembedder\ncom.google.mediapipe.tasks.vision.imagegenerator\ncom.google.mediapipe.tasks.vision.imagesegmenter\ncom.google.mediapipe.tasks.vision.interactivesegmenter\ncom.google.mediapipe.tasks.vision.objectdetector\ncom.google.mediapipe.tasks.vision.poselandmarker\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nClasses\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ncom.google.mediapipe.tasks.audio.audioembedder \nbookmark_border\nClasses\nAudioEmbedder\tPerforms audio embedding extraction on audio clips or audio stream. \nAudioEmbedder.AudioEmbedderOptions\tOptions for setting up and AudioEmbedder. \nAudioEmbedder.AudioEmbedderOptions.Builder\tBuilder for AudioEmbedder.AudioEmbedderOptions. \nAudioEmbedderResult\tRepresents the embedding results generated by AudioEmbedder. \nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksText Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/swift/text/Classes/Embedding",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nMediaPipeTasksText\nClasses\nOverview\nBaseOptions\nClassificationResult\nClassifications\nEmbedding\nEmbeddingResult\nLanguageDetector\nLanguageDetectorOptions\nLanguageDetectorResult\nLanguagePrediction\nResultCategory\nTaskOptions\nTaskResult\nTextClassifier\nTextClassifierOptions\nTextClassifierResult\nTextEmbedder\nTextEmbedderOptions\nTextEmbedderResult\nEnumerations\nMediaPipeTasksVision\nClasses\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nfloatEmbedding\nquantizedEmbedding\nheadIndex\nheadName\ninit(floatEmbedding:quantizedEmbedding:head:headName:)\n-init\n+new\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksText Framework Reference \nbookmark_border\nEmbedding\nclass Embedding : NSObject\n\nRepresents the embedding for a given embedder head. Typically used in embedding tasks.\n\nOne and only one of the two ‘floatEmbedding’ and ‘quantizedEmbedding’ will contain data, based on whether or not the embedder was configured to perform scala quantization.\n\nfloatEmbedding\n\n@brief The embedding represented as an NSArray of Float values. Empty if the embedder was configured to perform scalar quantization.\n\nDeclaration\n\nSWIFT\n\nvar floatEmbedding: [NSNumber]? { get }\nquantizedEmbedding\n\n@brief The embedding represented as an NSArray of UInt8 values. Empty if the embedder was not configured to perform scalar quantization.\n\nDeclaration\n\nSWIFT\n\nvar quantizedEmbedding: [NSNumber]? { get }\nheadIndex\n\nThe index of the embedder head these entries refer to. This is useful for multi-head models.\n\nDeclaration\n\nSWIFT\n\nvar headIndex: Int { get }\nheadName\n\nThe optional name of the embedder head, which is the corresponding tensor metadata name.\n\nDeclaration\n\nSWIFT\n\nvar headName: String? { get }\ninit(floatEmbedding:quantizedEmbedding:head:headName:)\n\nInitializes a new MPPEmbedding with the given float embedding, quantized embedding, head index and head name.\n\nDeclaration\n\nSWIFT\n\ninit(floatEmbedding: [NSNumber]?, quantizedEmbedding: [NSNumber]?, head headIndex: Int, headName: String?)\nParameters\nfloatEmbedding\t\n\nThe optional Floating-point embedding.\n\n\nquantizedEmbedding\t\n\nThe optional Quantized embedding.\n\n\nheadIndex\t\n\nThe index of the embedder head.\n\n\nheadName\t\n\nThe optional name of the embedder head.\n\nReturn Value\n\nAn instance of MPPEmbedding initialized with the given float embedding, quantized embedding, head index and head name.\n\n-init\n\nUndocumented\n\n+new\n\nUndocumented\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "ObjectDetector.ObjectDetectorOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/java/com/google/mediapipe/tasks/vision/objectdetector/ObjectDetector.ObjectDetectorOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\ncom.google.mediapipe.framework.image\ncom.google.mediapipe.tasks.audio.audioclassifier\ncom.google.mediapipe.tasks.audio.audioembedder\ncom.google.mediapipe.tasks.audio.core\ncom.google.mediapipe.tasks.components.containers\ncom.google.mediapipe.tasks.components.processors\ncom.google.mediapipe.tasks.components.utils\ncom.google.mediapipe.tasks.core\ncom.google.mediapipe.tasks.core.logging\ncom.google.mediapipe.tasks.genai.llminference\ncom.google.mediapipe.tasks.text.languagedetector\ncom.google.mediapipe.tasks.text.textclassifier\ncom.google.mediapipe.tasks.text.textembedder\ncom.google.mediapipe.tasks.vision.core\ncom.google.mediapipe.tasks.vision.facedetector\ncom.google.mediapipe.tasks.vision.facelandmarker\ncom.google.mediapipe.tasks.vision.facestylizer\ncom.google.mediapipe.tasks.vision.gesturerecognizer\ncom.google.mediapipe.tasks.vision.handlandmarker\ncom.google.mediapipe.tasks.vision.holisticlandmarker\ncom.google.mediapipe.tasks.vision.imageclassifier\ncom.google.mediapipe.tasks.vision.imageembedder\ncom.google.mediapipe.tasks.vision.imagegenerator\ncom.google.mediapipe.tasks.vision.imagesegmenter\ncom.google.mediapipe.tasks.vision.interactivesegmenter\ncom.google.mediapipe.tasks.vision.objectdetector\nOverview\nObjectDetectionResult\nObjectDetector\nOverview\nObjectDetector.ObjectDetectorOptions\nOverview\nBuilder\nObjectDetectorResult\ncom.google.mediapipe.tasks.vision.poselandmarker\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nNested Classes\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nObjectDetector.ObjectDetectorOptions \nbookmark_border\npublic static abstract class ObjectDetector.ObjectDetectorOptions\n\nOptions for setting up an ObjectDetector.\n\nNested Classes\nclass\tObjectDetector.ObjectDetectorOptions.Builder\tBuilder for ObjectDetector.ObjectDetectorOptions. \nPublic Constructors\n\t\nObjectDetectorOptions()\nPublic Methods\nstatic ObjectDetector.ObjectDetectorOptions.Builder\t\nbuilder()\n\nCalculatorOptions\t\nconvertToCalculatorOptionsProto()\nConverts a ObjectDetector.ObjectDetectorOptions to a ERROR(/CalculatorOptions) protobuf message.\nInherited Methods\nFrom class\ncom.google.mediapipe.tasks.core.TaskOptions\nFrom class java.lang.Object\nPublic Constructors\npublic ObjectDetectorOptions () \n\nPublic Methods\npublic static ObjectDetector.ObjectDetectorOptions.Builder builder ()\n\npublic CalculatorOptions convertToCalculatorOptionsProto ()\n\nConverts a ObjectDetector.ObjectDetectorOptions to a ERROR(/CalculatorOptions) protobuf message.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.packet_getter.get_bytes  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/packet_getter/get_bytes",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nOverview\nget_bool\nget_bool_list\nget_bytes\nget_float\nget_float_list\nget_image\nget_image_frame\nget_image_list\nget_int\nget_int_list\nget_matrix\nget_packet_list\nget_proto\nget_proto_list\nget_str\nget_str_list\nget_str_to_packet_dict\nget_uint\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.packet_getter.get_bytes \nbookmark_border\n\nget_bytes(arg0: mediapipe.python._framework_bindings.packet.Packet) -> bytes\n\nView aliases\nmp.packet_getter.get_bytes()\n\n\nGet the content of a MediaPipe string Packet as a bytes object.\n\nArgs: packet: A MediaPipe string Packet.\n\nReturns: A bytes object.\n\nRaises: ValueError: If the Packet doesn't contain string data.\n\nExamples: packet = mp.packet_creator.create_string(b'\\xd0\\xd0\\xd0') data = mp.packet_getter.get_bytes(packet)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.Content  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/Content",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.Content \nbookmark_border\n\nView source on GitHub\n\nThe base structured datatype containing multi-part content of a message.\n\nA Content includes a role field designating the producer of the Content and a parts field containing multi-part data that contains the content of the message turn.\n\nAttributes\n\nparts\tMutableSequence[google.ai.generativelanguage.Part]\n\nOrdered Parts that constitute a single message. Parts may have different MIME types.\n\n\nrole\tstr\n\nOptional. The producer of the content. Must be either 'user' or 'model'. Useful to set for multi-turn conversations, otherwise can be left blank or unset.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.task.processor.ConfidenceMask  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/task/processor/ConfidenceMask",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nmetadata_writers\ntask\nOverview\naudio\ncore\nprocessor\nOverview\nBertCluAnnotationOptions\nBoundingBox\nCategoricalSlot\nCategory\nClassificationOptions\nClassificationResult\nClassifications\nCluRequest\nCluResponse\nColoredLabel\nConfidenceMask\nDetection\nDetectionOptions\nDetectionResult\nEmbedding\nEmbeddingOptions\nEmbeddingResult\nFeatureVector\nMention\nMentionedSlot\nNearestNeighbor\nOutputType\nPos\nQaAnswer\nQuestionAnswererResult\nSearchOptions\nSearchResult\nSegmentation\nSegmentationOptions\nSegmentationResult\ntext\nvision\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nAttributes\nMethods\ncreate_from_pb2\nto_pb2\n__eq__\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.task.processor.ConfidenceMask \nbookmark_border\n\nView source on GitHub\n\n2D-array representing the confidence mask in row major order.\n\ntflite_support.task.processor.ConfidenceMask(\n    value: np.ndarray\n)\n\n\nFor each pixel, the value indicates the prediction confidence usually in the [0, 1] range where higher values represent a stronger confidence. Ultimately this is model specific, and other range of values might be used.\n\nAttributes\n\nvalue\tA NumPy 2D-array indicating the prediction confidence values usually in the range [0, 1].\n\nMethods\ncreate_from_pb2\n\nView source\n\n@classmethod\ncreate_from_pb2(\n    pb2_obj: _ConfidenceMaskProto, height: int, width: int\n) -> 'ConfidenceMask'\n\n\nCreates a ConfidenceMask object from the given protobuf and size.\n\nto_pb2\n\nView source\n\nto_pb2() -> _ConfidenceMaskProto\n\n\nGenerates a protobuf object to pass to the C++ layer.\n\n__eq__\n\nView source\n\n__eq__(\n    other: Any\n) -> bool\n\n\nChecks if this object is equal to the given object.\n\nArgs\nother\tThe object to be compared with.\n\nReturns\nTrue if the objects are equal.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "TensorOperatorWrapper  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/support/image/ops/TensorOperatorWrapper",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nCore\norg.tensorflow.lite\nacceleration\nacceleration\nSupport Library\nsupport.audio\nsupport.common\nsupport.common.ops\nsupport.image\nsupport.image.ops\nOverview\nResizeOp\nResizeWithCropOrPadOp\nRot90Op\nTensorOperatorWrapper\nTransformToGrayscaleOp\nsupport.label\nsupport.label.ops\nsupport.metadata\nsupport.model\nsupport.tensorbuffer\nDelegates\ngpu\nODML\nimage\nimage.annotation\nSwift\nC\nC++\nOn this page\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nTensorOperatorWrapper \nbookmark_border\npublic class TensorOperatorWrapper\n\nThe adapter that makes a TensorOperator able to run with TensorImage.\n\nSee Also\nTensorOperator\nTensorImage\nPublic Constructors\n\t\nTensorOperatorWrapper(TensorOperator op)\nWraps a TensorOperator object as an ImageOperator, so that the TensorOperator could handle TensorImage objects by handling its underlying TensorBuffer.\nPublic Methods\nTensorImage\t\napply(TensorImage image)\n\nint\t\ngetOutputImageHeight(int inputImageHeight, int inputImageWidth)\nComputes the height of the expected output image when input image size is given.\n\nint\t\ngetOutputImageWidth(int inputImageHeight, int inputImageWidth)\nComputes the width of the expected output image when input image size is given.\n\nPointF\t\ninverseTransform(PointF point, int inputImageHeight, int inputImageWidth)\nTransforms a point from coordinates system of the result image back to the one of the input image.\nInherited Methods\nFrom class java.lang.Object\nFrom interface\norg.tensorflow.lite.support.image.ImageOperator\nFrom interface\norg.tensorflow.lite.support.common.Operator\nPublic Constructors\npublic TensorOperatorWrapper (TensorOperator op)\n\nWraps a TensorOperator object as an ImageOperator, so that the TensorOperator could handle TensorImage objects by handling its underlying TensorBuffer.\n\nRequirement: The op should not change coordinate system when applied on an image.\n\nParameters\nop\tThe created operator.\nPublic Methods\npublic TensorImage apply (TensorImage image)\n\nParameters\nimage\t\npublic int getOutputImageHeight (int inputImageHeight, int inputImageWidth)\n\nComputes the height of the expected output image when input image size is given.\n\nParameters\ninputImageHeight\t\ninputImageWidth\t\npublic int getOutputImageWidth (int inputImageHeight, int inputImageWidth)\n\nComputes the width of the expected output image when input image size is given.\n\nParameters\ninputImageHeight\t\ninputImageWidth\t\npublic PointF inverseTransform (PointF point, int inputImageHeight, int inputImageWidth)\n\nTransforms a point from coordinates system of the result image back to the one of the input image.\n\nParameters\npoint\tthe point from the result coordinates system.\ninputImageHeight\tthe height of input image.\ninputImageWidth\tthe width of input image.\nReturns\nthe point with the coordinates from the coordinates system of the input image.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/objc/vision/Classes/MPPImageClassifierResult",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nObjectiveC\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nOverview\nMPPBaseOptions\nMPPCategory\nMPPClassificationResult\nMPPClassifications\nMPPClassifierOptions\nMPPConnection\nMPPDetection\nMPPEmbedding\nMPPEmbeddingResult\nMPPFaceDetector\nMPPFaceDetectorOptions\nMPPFaceDetectorResult\nMPPFaceLandmarker\nMPPFaceLandmarkerOptions\nMPPFaceLandmarkerResult\nMPPFaceStylizer\nMPPFaceStylizerOptions\nMPPFaceStylizerResult\nMPPGestureRecognizer\nMPPGestureRecognizerOptions\nMPPGestureRecognizerResult\nMPPHandLandmarker\nMPPHandLandmarkerOptions\nMPPHandLandmarkerResult\nMPPImage\nMPPImageClassifier\nMPPImageClassifierOptions\nMPPImageClassifierResult\nMPPImageEmbedder\nMPPImageEmbedderOptions\nMPPImageEmbedderResult\nMPPImageSegmenter\nMPPImageSegmenterOptions\nMPPImageSegmenterResult\nMPPInteractiveSegmenter\nMPPInteractiveSegmenterOptions\nMPPInteractiveSegmenterResult\nMPPLandmark\nMPPMask\nMPPNormalizedKeypoint\nMPPNormalizedLandmark\nMPPObjectDetector\nMPPObjectDetectorOptions\nMPPObjectDetectorResult\nMPPPoseLandmarker\nMPPPoseLandmarkerOptions\nMPPPoseLandmarkerResult\nMPPRegionOfInterest\nMPPTaskOptions\nMPPTaskResult\nMPPTransformMatrix\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nModel Maker\nTensorFlow Lite\nOn this page\nclassificationResult\n-initWithClassificationResult:timestampInMilliseconds:\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nMPPImageClassifierResult\n\n@interface MPPImageClassifierResult : MPPTaskResult\n\nRepresents the classification results generated by ImageClassifier. *\n\nclassificationResult\n\nThe ClassificationResult instance containing one set of results per classifier head. *\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (nonatomic, readonly) MPPClassificationResult *_Nonnull classificationResult;\n-initWithClassificationResult:timestampInMilliseconds:\n\nInitializes a new ImageClassifierResult with the given ClassificationResult and timestamp (in milliseconds).\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nonnull instancetype)\n    initWithClassificationResult:\n        (nonnull MPPClassificationResult *)classificationResult\n         timestampInMilliseconds:(NSInteger)timestampInMilliseconds;\nParameters\nclassificationResult\t\n\nThe ClassificationResult instance containing one set of results per classifier head.\n\n\ntimestampInMilliseconds\t\n\nThe timestamp (in milliseconds) for this result.\n\nReturn Value\n\nAn instance of ImageClassifierResult initialized with the given ClassificationResult and timestamp (in milliseconds).\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/objc/vision/Classes/MPPHandLandmarker",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nObjectiveC\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nOverview\nMPPBaseOptions\nMPPCategory\nMPPClassificationResult\nMPPClassifications\nMPPClassifierOptions\nMPPConnection\nMPPDetection\nMPPEmbedding\nMPPEmbeddingResult\nMPPFaceDetector\nMPPFaceDetectorOptions\nMPPFaceDetectorResult\nMPPFaceLandmarker\nMPPFaceLandmarkerOptions\nMPPFaceLandmarkerResult\nMPPFaceStylizer\nMPPFaceStylizerOptions\nMPPFaceStylizerResult\nMPPGestureRecognizer\nMPPGestureRecognizerOptions\nMPPGestureRecognizerResult\nMPPHandLandmarker\nMPPHandLandmarkerOptions\nMPPHandLandmarkerResult\nMPPImage\nMPPImageClassifier\nMPPImageClassifierOptions\nMPPImageClassifierResult\nMPPImageEmbedder\nMPPImageEmbedderOptions\nMPPImageEmbedderResult\nMPPImageSegmenter\nMPPImageSegmenterOptions\nMPPImageSegmenterResult\nMPPInteractiveSegmenter\nMPPInteractiveSegmenterOptions\nMPPInteractiveSegmenterResult\nMPPLandmark\nMPPMask\nMPPNormalizedKeypoint\nMPPNormalizedLandmark\nMPPObjectDetector\nMPPObjectDetectorOptions\nMPPObjectDetectorResult\nMPPPoseLandmarker\nMPPPoseLandmarkerOptions\nMPPPoseLandmarkerResult\nMPPRegionOfInterest\nMPPTaskOptions\nMPPTaskResult\nMPPTransformMatrix\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nModel Maker\nTensorFlow Lite\nOn this page\nhandPalmConnections\nhandThumbConnections\nhandIndexFingerConnections\nhandMiddleFingerConnections\nhandRingFingerConnections\nhandPinkyConnections\nhandConnections\n-initWithModelPath:error:\n-initWithOptions:error:\n-detectImage:error:\n-detectVideoFrame:timestampInMilliseconds:error:\n-detectAsyncImage:timestampInMilliseconds:error:\n-init\n+new\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nMPPHandLandmarker\n\n@interface MPPHandLandmarker : NSObject\n\n@brief Performs hand landmarks detection on images.\n\nThis API expects a pre-trained hand landmarks model asset bundle.\n\nhandPalmConnections\n\nThe array of connections between the landmarks in the palm.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handPalmConnections;\nhandThumbConnections\n\nThe array of connections between the landmarks in the thumb.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handThumbConnections;\nhandIndexFingerConnections\n\nThe array of connections between the landmarks in the index finger.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handIndexFingerConnections;\nhandMiddleFingerConnections\n\nThe array of connections between the landmarks in the middle finger.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handMiddleFingerConnections;\nhandRingFingerConnections\n\nThe array of connections between the landmarks in the ring finger.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handRingFingerConnections;\nhandPinkyConnections\n\nThe array of connections between the landmarks in the pinky.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handPinkyConnections;\nhandConnections\n\nThe array of connections between all the landmarks in the hand.\n\nDeclaration\n\nOBJECTIVE-C\n\n@property (class, nonatomic, readonly) NSArray<MPPConnection *> *_Nonnull handConnections;\n-initWithModelPath:error:\n\nCreates a new instance of HandLandmarker from an absolute path to a model asset bundle stored locally on the device and the default HandLandmarkerOptions.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable instancetype)initWithModelPath:(nonnull NSString *)modelPath\n                                     error:(NSError *_Nullable *_Nullable)error;\nParameters\nmodelPath\t\n\nAn absolute path to a model asset bundle stored locally on the device.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in initializing the hand landmarker.\n\nReturn Value\n\nA new instance of HandLandmarker with the given model path. nil if there is an error in initializing the hand landmarker.\n\n-initWithOptions:error:\n\nCreates a new instance of HandLandmarker from the given HandLandmarkerOptions.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable instancetype)initWithOptions:\n                             (nonnull MPPHandLandmarkerOptions *)options\n                                   error:(NSError *_Nullable *_Nullable)error;\nParameters\noptions\t\n\nThe options of type HandLandmarkerOptions to use for configuring the HandLandmarker.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in initializing the hand landmarker.\n\nReturn Value\n\nA new instance of HandLandmarker with the given options. nil if there is an error in initializing the hand landmarker.\n\n-detectImage:error:\n\nPerforms hand landmarks detection on the provided MPImage using the whole image as region of interest. Rotation will be applied according to the orientation property of the provided MPImage. Only use this method when the HandLandmarker is created with running mode, .image.\n\nThis method supports performing hand landmarks detection on RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf your MPImage has a source type of .image ensure that the color space is RGB with an Alpha channel.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable MPPHandLandmarkerResult *)\n    detectImage:(nonnull MPPImage *)image\n          error:(NSError *_Nullable *_Nullable)error;\nParameters\nimage\t\n\nThe MPImage on which hand landmarks detection is to be performed.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in performing hand landmarks detection on the input image.\n\nReturn Value\n\nAn HandLandmarkerResult object that contains the hand hand landmarks detection results.\n\n-detectVideoFrame:timestampInMilliseconds:error:\n\nPerforms hand landmarks detection on the provided video frame of type MPImage using the whole image as region of interest. Rotation will be applied according to the orientation property of the provided MPImage. Only use this method when the HandLandmarker is created with running mode, .video.\n\nIt’s required to provide the video frame’s timestamp (in milliseconds). The input timestamps must be monotonically increasing.\n\nThis method supports performing hand landmarks detection on RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf your MPImage has a source type of .image ensure that the color space is RGB with an Alpha channel.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable MPPHandLandmarkerResult *)\n           detectVideoFrame:(nonnull MPPImage *)image\n    timestampInMilliseconds:(NSInteger)timestampInMilliseconds\n                      error:(NSError *_Nullable *_Nullable)error;\nParameters\nimage\t\n\nThe MPImage on which hand landmarks detection is to be performed.\n\n\ntimestampInMilliseconds\t\n\nThe video frame’s timestamp (in milliseconds). The input timestamps must be monotonically increasing.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in performing hand landmarks detection on the input video frame.\n\nReturn Value\n\nAn HandLandmarkerResult object that contains the hand hand landmarks detection results.\n\n-detectAsyncImage:timestampInMilliseconds:error:\n\nSends live stream image data of type MPImage to perform hand landmarks detection using the whole image as region of interest. Rotation will be applied according to the orientation property of the provided MPImage. Only use this method when the HandLandmarker is created with running mode, .liveStream.\n\nThe object which needs to be continuously notified of the available results of hand landmarks detection must confirm to HandLandmarkerLiveStreamDelegate protocol and implement the handLandmarker(_:didFinishDetectionWithResult:timestampInMilliseconds:error:) delegate method.\n\nIt’s required to provide a timestamp (in milliseconds) to indicate when the input image is sent to the hand landmarker. The input timestamps must be monotonically increasing.\n\nThis method supports performing hand landmarks detection on RGBA images. If your MPImage has a source type of .pixelBuffer or .sampleBuffer, the underlying pixel buffer must use kCVPixelFormatType_32BGRA as its pixel format.\n\nIf the input MPImage has a source type of .image ensure that the color space is RGB with an Alpha channel.\n\nIf this method is used for performing hand landmarks detection on live camera frames using AVFoundation, ensure that you request AVCaptureVideoDataOutput to output frames in kCMPixelFormat_32BGRA using its videoSettings property.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (BOOL)detectAsyncImage:(nonnull MPPImage *)image\n    timestampInMilliseconds:(NSInteger)timestampInMilliseconds\n                      error:(NSError *_Nullable *_Nullable)error;\nParameters\nimage\t\n\nA live stream image data of type MPImage on which hand landmarks detection is to be performed.\n\n\ntimestampInMilliseconds\t\n\nThe timestamp (in milliseconds) which indicates when the input image is sent to the hand landmarker. The input timestamps must be monotonically increasing.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in performing hand landmarks detection on the input live stream image data.\n\nReturn Value\n\nYES if the image was sent to the task successfully, otherwise NO.\n\n-init\n\nUndocumented\n\nDeclaration\n\nOBJECTIVE-C\n\n- (instancetype)init NS_UNAVAILABLE;\n+new\n\nUndocumented\n\nDeclaration\n\nOBJECTIVE-C\n\n+ (instancetype)new NS_UNAVAILABLE;\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksVision Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/swift/vision/Classes/FaceDetectorOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nMediaPipeTasksText\nClasses\nEnumerations\nMediaPipeTasksVision\nClasses\nOverview\nBaseOptions\nClassificationResult\nClassifications\nClassifierOptions\nConnection\nDetection\nEmbedding\nEmbeddingResult\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceStylizer\nFaceStylizerOptions\nFaceStylizerResult\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageClassifierResult\nImageEmbedder\nImageEmbedderOptions\nImageEmbedderResult\nImageSegmenter\nImageSegmenterOptions\nImageSegmenterResult\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterResult\nLandmark\nMPImage\nMask\nNormalizedKeypoint\nNormalizedLandmark\nObjectDetector\nObjectDetectorOptions\nObjectDetectorResult\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nRegionOfInterest\nResultCategory\nTaskOptions\nTaskResult\nTransformMatrix\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nrunningMode\nfaceDetectorLiveStreamDelegate\nminDetectionConfidence\nminSuppressionThreshold\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksVision Framework Reference \nbookmark_border\nFaceDetectorOptions\nclass FaceDetectorOptions : TaskOptions, NSCopying\n\nOptions for setting up a FaceDetector.\n\nrunningMode\n\nRunning mode of the face detector task. Defaults to .image. FaceDetector can be created with one of the following running modes:\n\n.image: The mode for performing face detection on single image inputs.\n.video: The mode for performing face detection on the decoded frames of a video.\n.liveStream: The mode for performing face detection on a live stream of input data, such as from the camera.\nDeclaration\n\nSWIFT\n\nvar runningMode: RunningMode { get set }\nfaceDetectorLiveStreamDelegate\n\nAn object that confirms to FaceDetectorLiveStreamDelegate protocol. This object must implement faceDetector(_:didFinishDetectionWithResult:timestampInMilliseconds:error:) to receive the results of performing asynchronous face detection on images (i.e, when runningMode = .liveStream).\n\nDeclaration\n\nSWIFT\n\nweak var faceDetectorLiveStreamDelegate: FaceDetectorLiveStreamDelegate? { get set }\nminDetectionConfidence\n\nThe minimum confidence score for the face detection to be considered successful. Defaults to 0.5.\n\nDeclaration\n\nSWIFT\n\nvar minDetectionConfidence: Float { get set }\nminSuppressionThreshold\n\nThe minimum non-maximum-suppression threshold for face detection to be considered overlapped. Defaults to 0.3.\n\nDeclaration\n\nSWIFT\n\nvar minSuppressionThreshold: Float { get set }\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.Type  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/Type",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nValues\nClass Variables\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.Type \nbookmark_border\n\nView source on GitHub\n\nType contains the list of OpenAPI data types as defined by https://spec.openapis.org/oas/v3.0.3#data-types\n\nValues\n\nTYPE_UNSPECIFIED\t0\n\nNot specified, should not be used.\n\n\nSTRING\t1\n\nString type.\n\n\nNUMBER\t2\n\nNumber type.\n\n\nINTEGER\t3\n\nInteger type.\n\n\nBOOLEAN\t4\n\nBoolean type.\n\n\nARRAY\t5\n\nArray type.\n\n\nOBJECT\t6\n\nObject type.\n\nClass Variables\n\nARRAY\t<Type.ARRAY: 5>\nBOOLEAN\t<Type.BOOLEAN: 4>\nINTEGER\t<Type.INTEGER: 3>\nNUMBER\t<Type.NUMBER: 2>\nOBJECT\t<Type.OBJECT: 6>\nSTRING\t<Type.STRING: 1>\nTYPE_UNSPECIFIED\t<Type.TYPE_UNSPECIFIED: 0>\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Hand landmarks detection guide for Web  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker/web_js",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nAbout the Preview\nTasks\nModel Maker\nStudio\nVision tasks\nObject detection\nImage classification\nImage segmentation\nInteractive segmentation\nGesture recognition\nHand landmark detection\nOverview\nAndroid\nWeb\nPython\niOS\nImage embedding\nFace detection\nFace landmark detection\nPose landmark detection\nFace stylization\nHolistic landmark detection\nText tasks\nText classification\nText embedding\nLanguage detection\nAudio tasks\nAudio classification\nGenerative AI tasks\nImage generation\nLLM Inference\nPlatform setup guides\nAndroid setup\nPython setup\nWeb setup\niOS setup\nOn this page\nCode example\nSetup\nJavaScript packages\nModel\nCreate the task\nConfiguration options\nPrepare data\nRun the task\nHandle and display results\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nSolutions\nWas this helpful?\nSend feedback\nHand landmarks detection guide for Web \nbookmark_border\n\nThe MediaPipe Hand Landmarker task lets you detect the landmarks of the hands in an image. These instructions show you how to use the Hand Landmarker for web and JavaScript apps.\n\nFor more information about the capabilities, models, and configuration options of this task, see the Overview.\n\nCode example\n\nThe example code for Hand Landmarker provides a complete implementation of this task in JavaScript for your reference. This code helps you test this task and get started on building your own hand landmark detection app. You can view, run, and edit the Hand Landmarker example code using just your web browser.\n\nSetup\n\nThis section describes key steps for setting up your development environment specifically to use Hand Landmarker. For general information on setting up your web and JavaScript development environment, including platform version requirements, see the Setup guide for web.\n\nJavaScript packages\n\nHand Landmarker code is available through the MediaPipe @mediapipe/tasks-vision NPM package. You can find and download these libraries by following the instructions in the platform Setup guide.\n\nAttention: This MediaPipe Solutions Preview is an early release. Learn more.\n\nYou can install the required packages through NPM using the following command:\n\nnpm install @mediapipe/tasks-vision\n\n\nIf you want to import the task code via a content delivery network (CDN) service, add the following code in the <head> tag in your HTML file:\n\n<!-- You can replace JSDeliver with another CDN if you prefer to -->\n<head>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js\"\n    crossorigin=\"anonymous\"></script>\n</head>\n\nModel\n\nThe MediaPipe Hand Landmarker task requires a trained model that is compatible with this task. For more information on available trained models for Hand Landmarker, see the task overview Models section.\n\nSelect and download a model, and then store it within your project directory:\n\n<dev-project-root>/app/shared/models/\n\nCreate the task\n\nUse one of the Hand Landmarker createFrom...() functions to prepare the task for running inferences. Use the createFromModelPath() function with a relative or absolute path to the trained model file. If your model is already loaded into memory, you can use the createFromModelBuffer() method.\n\nThe code example below demonstrates using the createFromOptions() function to set up the task. The createFromOptions function allows you to customize the Hand Landmarker with configuration options. For more information on configuration options, see Configuration options.\n\nThe following code demonstrates how to build and configure the task with custom options:\n\nconst vision = await FilesetResolver.forVisionTasks(\n  // path/to/wasm/root\n  \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm\"\n);\nconst handLandmarker = await HandLandmarker.createFromOptions(\n    vision,\n    {\n      baseOptions: {\n        modelAssetPath: \"hand_landmarker.task\"\n      },\n      numHands: 2\n    });\n\nConfiguration options\n\nThis task has the following configuration options for Web and JavaScript applications:\n\nOption Name\tDescription\tValue Range\tDefault Value\nrunningMode\tSets the running mode for the task. There are two modes:\n\nIMAGE: The mode for single image inputs.\n\nVIDEO: The mode for decoded frames of a video or on a livestream of input data, such as from a camera.\t{IMAGE, VIDEO}\tIMAGE\nnumHands\tThe maximum number of hands detected by the Hand landmark detector.\tAny integer > 0\t1\nminHandDetectionConfidence\tThe minimum confidence score for the hand detection to be considered successful in palm detection model.\t0.0 - 1.0\t0.5\nminHandPresenceConfidence\tThe minimum confidence score for the hand presence score in the hand landmark detection model. In Video mode and Live stream mode, if the hand presence confidence score from the hand landmark model is below this threshold, Hand Landmarker triggers the palm detection model. Otherwise, a lightweight hand tracking algorithm determines the location of the hand(s) for subsequent landmark detections.\t0.0 - 1.0\t0.5\nminTrackingConfidence\tThe minimum confidence score for the hand tracking to be considered successful. This is the bounding box IoU threshold between hands in the current frame and the last frame. In Video mode and Stream mode of Hand Landmarker, if the tracking fails, Hand Landmarker triggers hand detection. Otherwise, it skips the hand detection.\t0.0 - 1.0\t0.5\nPrepare data\n\nHand Landmarker can detect hand landmarks in images in any format supported by the host browser. The task also handles data input preprocessing, including resizing, rotation and value normalization. To detect hand landmarks in videos, you can use the API to quickly process one frame at a time, using the timestamp of the frame to determine when the hand landmarks occur within the video.\n\nRun the task\n\nThe Hand Landmarker uses the detect() (with running mode image) and detectForVideo() (with running mode video) methods to trigger inferences. The task processes the data, attempts to detect hand landmarks, and then reports the results.\n\nCalls to the Hand Landmarker detect() and detectForVideo() methods run synchronously and block the user interface thread. If you detect hand landmarks in video frames from a device's camera, each detection blocks the main thread. You can prevent this by implementing web workers to run the detect() and detectForVideo() methods on another thread.\n\nThe following code demonstrates how execute the processing with the task model:\n\nImage\nVideo\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst handLandmarkerResult = handLandmarker.detect(image);\n\nFor a more complete implementation of running an Hand Landmarker task, see the code example.\n\nHandle and display results\n\nThe Hand Landmarker generates a hand landmarker result object for each detection run. The result object contains hand landmarks in image coordinates, hand landmarks in world coordinates and handedness(left/right hand) of the detected hands.\n\nThe following shows an example of the output data from this task:\n\nThe HandLandmarkerResult output contains three components. Each component is an array, where each element contains the following results for a single detected hand:\n\nHandedness\n\nHandedness represents whether the detected hands are left or right hands.\n\nLandmarks\n\nThere are 21 hand landmarks, each composed of x, y and z coordinates. The x and y coordinates are normalized to [0.0, 1.0] by the image width and height, respectively. The z coordinate represents the landmark depth, with the depth at the wrist being the origin. The smaller the value, the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x.\n\nWorld Landmarks\n\nThe 21 hand landmarks are also presented in world coordinates. Each landmark is composed of x, y, and z, representing real-world 3D coordinates in meters with the origin at the hand’s geometric center.\n\nHandLandmarkerResult:\n  Handedness:\n    Categories #0:\n      index        : 0\n      score        : 0.98396\n      categoryName : Left\n  Landmarks:\n    Landmark #0:\n      x            : 0.638852\n      y            : 0.671197\n      z            : -3.41E-7\n    Landmark #1:\n      x            : 0.634599\n      y            : 0.536441\n      z            : -0.06984\n    ... (21 landmarks for a hand)\n  WorldLandmarks:\n    Landmark #0:\n      x            : 0.067485\n      y            : 0.031084\n      z            : 0.055223\n    Landmark #1:\n      x            : 0.063209\n      y            : -0.00382\n      z            : 0.020920\n    ... (21 world landmarks for a hand)\n\n\nThe following image shows a visualization of the task output:\n\nThe Hand Landmarker example code demonstrates how to display the results returned from the task, see the code example\n\nWas this helpful?\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-14 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.GenerateMessageRequest  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/GenerateMessageRequest",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.GenerateMessageRequest \nbookmark_border\n\nView source on GitHub\n\nRequest to generate a message response from the model.\n\nAttributes\n\nmodel\tstr\n\nRequired. The name of the model to use.\n\nFormat: name=models/{model}.\n\n\nprompt\tgoogle.ai.generativelanguage.MessagePrompt\n\nRequired. The structured textual input given to the model as a prompt. Given a prompt, the model will return what it predicts is the next message in the discussion.\n\n\ntemperature\tfloat\n\nOptional. Controls the randomness of the output.\n\nValues can range over [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied, while a value closer to 0.0 will typically result in less surprising responses from the model.\n\n\ncandidate_count\tint\n\nOptional. The number of generated response messages to return.\n\nThis value must be between [1, 8], inclusive. If unset, this will default to 1.\n\n\ntop_p\tfloat\n\nOptional. The maximum cumulative probability of tokens to consider when sampling.\n\nThe model uses combined Top-k and nucleus sampling.\n\nNucleus sampling considers the smallest set of tokens whose probability sum is at least top_p.\n\n\ntop_k\tint\n\nOptional. The maximum number of tokens to consider when sampling.\n\nThe model uses combined Top-k and nucleus sampling.\n\nTop-k sampling considers the set of top_k most probable tokens.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Create input and output safeguards  |  Gemma  |  Google for Developers",
    "url": "https://ai.google.dev/responsible/input_output",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nDocs\nOverview\nModel Card\nGemma model family\nCodeGemma\nPaliGemma\nRecurrentGemma\nReleases\nGuides\nGemma setup\nGet started with Gemma using Keras\nBasic tuning with Gemma using Keras\nDistributed tuning with Gemma using Keras\nGet started with Gemma using PyTorch\nChat with Gemma\nFormatting and system instructions\nGemma C++ Tutorial\nInference using JAX and Flax\nFine-tuning using JAX and Flax\nIntegrations\nVertex AI\nDataflow ML\nGoogle Kubernetes Engine (GKE)\nLangChain\nResponsible Generative AI Toolkit\nOverview\nSet responsible policies\nTune models for safety\nCreate input and output safeguards\nConduct safety evaluations\nBuild transparency artifacts\nAnalyze model behavior\nCommunity\nDiscord\nLegal\nTerms of use\nProhibited use\nOn this page\nSafeguards and off-the-shelf safety classifiers\nCreate customized safety classifiers\nBest practices for setting up safeguards\nDeveloper Resources\nHome\nGemma\nDocs\nSend feedback\nCreate input and output safeguards \nbookmark_border\n\nGenerative AI applications often rely on input and output data filtering, sometimes referred to as safeguards, to help ensure responsible model behavior. Input and output filtering techniques check the data going into or coming out of the model.\n\nSafeguards and off-the-shelf safety classifiers\n\nEven with prior tuning for safety and a well designed prompt template, it is still possible for your model to output content that results in unintended harm. To further ameliorate this, content classifiers can add an additional layer of protection. Content classifiers can be applied to both inputs and outputs.\n\nInput classifiers are typically used to filter content that is not intended to be used in your application and which might cause your model to violate your safety policies. Input filters often target adversarial attacks that try to circumvent your content policies. Output classifiers can further filter model output, catching unintended generations that may violate your safety policies. It is recommended to have classifiers that cover all your content policies.\n\nGoogle has developed off-the-shelf classifiers for content safety that can be used to filter inputs and outputs:\n\nThe Perspective API is a free API that uses machine learning models to score the perceived impact a comment might have on a conversation. It provides scores that capture the probability of whether a comment is toxic, threatening, insulting, off-topic, etc.\nThe Text moderation service is a Google Cloud API that is available to use below a certain usage limit and uses machine learning to analyze a document against a list of safety attributes, including various potentially harmful categories and topics that may be considered sensitive.\n\nIt's important to evaluate how well off-the-shelf classifiers meet your policy goals, and qualitatively evaluate the failure cases. It is also important to note that over-filtering can also result in unintended harm as well as reduce the utility of the application, which means it is important to also review the cases where over-filtering may be happening. For more details on such evaluation methods, see Evaluate model and system for safety.\n\nCreate customized safety classifiers\n\nIf your policy isn't covered by an off-the-shelf API or if you want to create your own classifier, parameter efficient tuning techniques such as prompt-tuning and LoRA provide an effective framework. In these methods, instead of fine-tuning the whole model, you can use a limited amount of data to train a small set of important parameters of the model. This allows your model to learn new behaviors, like how to classify for your novel safety use-case, with relatively little training data and compute power. This approach lets you develop personalized safety tools for your own users and tasks.\n\nTo illustrate how this works, this codelab shows the code needed to set up an \"agile classifier.\" The codelab shows the steps of ingesting data, formatting it for the LLM, training LoRA weights, and then evaluating your results. Gemma makes it possible to build these powerful classifiers with only a few lines of code. For a more detailed overview, our research paper \"Towards Agile Text Classifiers for Everyone\" shows how you can use these techniques to train a variety of safety tasks to achieve state of the art performance with only a few hundred training examples.\n\nIn this example tutorial, you can train a classifier for hate speech, using the ETHOS dataset, a publicly available dataset for detection of hateful speech, built from YouTube and Reddit comments. When trained on the smaller Gemma model, on only 200 examples (a little less than ¼ of the dataset) it achieves an F1 score of: 0.80 and ROC-AUC of 0.78. This result compares favorably to the state of the art results reported in this leaderboard leaderboard. When trained on the 800 examples, like the other classifiers in the leaderboard, the Gemma based agile classifier achieves an F1 score of 83.74 and a ROC-AUC score of 88.17. You can use this classifier out of the box, or adapt it using the Gemma Agile Classifier tutorial.\n\nGemma Agile Classifier Tutorials\n\nStart Codelab\n\t\nStart Google Colab\n\nBest practices for setting up safeguards\n\nUsing safety classifiers is strongly recommended. However, guardrails can result in the generative model not producing anything for the user, if the content is blocked. Applications need to be designed to handle this case. Most popular chatbots handle this by providing canned answers (\"I am sorry, I am a language model, I can't help you with this request\").\n\nFind the right balance between helpfulness and harmlessness: When using safety classifiers, it is important to understand that they will make mistakes, including both false positives (e.g. claiming an output is unsafe when it is not) and false negatives (failing to label an output as unsafe, when it is). By evaluating classifiers with metrics like F1, Precision, Recall, and AUC-ROC, you can determine how you would like to tradeoff false positive versus false negative errors. By changing the threshold of classifiers, you help find an ideal balance that avoids over-filtering outputs while still providing appropriate safety.\n\nCheck your classifiers for unintended biases: Safety classifiers, like any other ML model, can propagate unintended biases, such as socio-cultural stereotypes. Applications need to be appropriately evaluated for potentially problematic behaviors. In particular, content safety classifiers can over-trigger on content related to identities that are more frequently the target of abusive language online. As an example, when the Perspective API was first launched, the model returned higher toxicity scores in comments referencing certain identity groups (blog). This over-triggering behavior can happen because comments that mention identity terms for more frequently targeted groups (e.g., words like \"Black\", \"muslim\", \"feminist\", \"woman\", \"gay\", etc.) are more often toxic in nature. When datasets used to train classifiers have significant imbalances for comments containing certain words, classifiers can overgeneralize and consider all comments with those words as being likely to be unsafe. Read how the Jigsaw team mitigated this unintended bias.\n\nDeveloper Resources\nPerspective API: To identify toxic content\nText moderation service: For Google Cloud customers\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.calculators.util.visibility_smoothing_calculator_pb2.VisibilitySmoothingCalculatorOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/util/visibility_smoothing_calculator_pb2/VisibilitySmoothingCalculatorOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nimage\ntensor\ntflite\nutil\nOverview\nassociation_calculator_pb2\ncollection_has_min_size_calculator_pb2\ndetection_label_id_to_text_calculator_pb2\ndetections_to_rects_calculator_pb2\nlandmark_projection_calculator_pb2\nlandmarks_refinement_calculator_pb2\nlandmarks_smoothing_calculator_pb2\nlocal_file_contents_calculator_pb2\nlogic_calculator_pb2\nnon_max_suppression_calculator_pb2\nrect_transformation_calculator_pb2\nthresholding_calculator_pb2\nvisibility_smoothing_calculator_pb2\nOverview\nVisibilitySmoothingCalculatorOptions\nVisibilitySmoothingCalculatorOptions.LowPassFilter\nVisibilitySmoothingCalculatorOptions.NoFilter\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nChild Classes\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.calculators.util.visibility_smoothing_calculator_pb2.VisibilitySmoothingCalculatorOptions \nbookmark_border\n\nView source on GitHub\n\nA ProtocolMessage\n\nAttributes\n\nlow_pass_filter\tLowPassFilter low_pass_filter\nno_filter\tNoFilter no_filter\n\nChild Classes\n\nclass LowPassFilter\n\nclass NoFilter\n\nClass Variables\n\next\tInstance of google.protobuf.pyext._message.FieldDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Image classification guide for Web  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/image_classifier/web_js",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nAbout the Preview\nTasks\nModel Maker\nStudio\nVision tasks\nObject detection\nImage classification\nOverview\nAndroid\nWeb\nPython\niOS\nCustomize\nImage segmentation\nInteractive segmentation\nGesture recognition\nHand landmark detection\nImage embedding\nFace detection\nFace landmark detection\nPose landmark detection\nFace stylization\nHolistic landmark detection\nText tasks\nText classification\nText embedding\nLanguage detection\nAudio tasks\nAudio classification\nGenerative AI tasks\nImage generation\nLLM Inference\nPlatform setup guides\nAndroid setup\nPython setup\nWeb setup\niOS setup\nOn this page\nCode example\nSetup\nJavaScript packages\nModel\nCreate the task\nConfiguration options\nPrepare data\nRun the task\nHandle and display results\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nSolutions\nSend feedback\nImage classification guide for Web \nbookmark_border\n\nThe MediaPipe Image Classifier task lets you perform classification on images. You can use this task to identify what an image represents among a set of categories defined at training time. These instructions show you how to use the Image Classifier for Node and web apps.\n\nYou can see this task in action by viewing the demo. For more information about the capabilities, models, and configuration options of this task, see the Overview.\n\nCode example\n\nThe example code for Image Classifier provides a complete implementation of this task in JavaScript for your reference. This code helps you test this task and get started on building your own image classification app. You can view, run, and edit the Image Classifier example code using just your web browser.\n\nSetup\n\nThis section describes key steps for setting up your development environment and code projects specifically to use Image Classifier. For general information on setting up your development environment for using MediaPipe tasks, including platform version requirements, see the Setup guide for Web.\n\nJavaScript packages\n\nImage Classifier code is available through the MediaPipe @mediapipe/tasks-vision NPM package. You can find and download these libraries from links provided in the platform Setup guide.\n\nAttention: This MediaPipe Solutions Preview is an early release. Learn more.\n\nYou can install the required packages with the following code for local staging using the following command:\n\nnpm install @mediapipe/tasks-vision\n\n\nIf you want to import the task code via a content delivery network (CDN) service, add the following code in the tag in your HTML file:\n\n<!-- You can replace JSDeliver with another CDN if you prefer to -->\n<head>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js\"\n    crossorigin=\"anonymous\"></script>\n</head>\n\nModel\n\nThe MediaPipe Image Classifier task requires a trained model that is compatible with this task. For more information on available trained models for Image Classifier, see the task overview Models section.\n\nSelect and download a model, and then store it within your project directory:\n\n<dev-project-root>/app/shared/models/\n\nCreate the task\n\nUse one of the Image Classifier createFrom...() functions to prepare the task for running inferences. Use the createFromModelPath() function with a relative or absolute path to the trained model file. If your model is already loaded into memory, you can use the createFromModelBuffer() method.\n\nThe code example below demonstrates using the createFromOptions() function to set up the task. The createFromOptions function allows you to customize the Image Classifier with configuration options. For more information on configuration options, see Configuration options.\n\nThe following code demonstrates how to build and configure the task with custom options:\n\nasync function createImageClassifier {\n  const vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm\"\n  );\n  imageClassifier = await ImageClassifier.createFromOptions(vision, {\n    baseOptions: {\n      modelAssetPath: `https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite`\n    },\n  });\n}\n\nConfiguration options\n\nThis task has the following configuration options for Web applications:\n\nOption Name\tDescription\tValue Range\tDefault Value\nrunningMode\tSets the running mode for the task. There are two modes:\n\nIMAGE: The mode for single image inputs.\n\nVIDEO: The mode for decoded frames of a video or on a livestream of input data, such as from a camera.\t{IMAGE, VIDEO}\tIMAGE\ndisplayNamesLocale\tSets the language of labels to use for display names provided in the metadata of the task's model, if available. Default is en for English. You can add localized labels to the metadata of a custom model using the TensorFlow Lite Metadata Writer API\tLocale code\ten\nmaxResults\tSets the optional maximum number of top-scored classification results to return. If < 0, all available results will be returned.\tAny positive numbers\t-1\nscoreThreshold\tSets the prediction score threshold that overrides the one provided in the model metadata (if any). Results below this value are rejected.\tAny float\tNot set\ncategoryAllowlist\tSets the optional list of allowed category names. If non-empty, classification results whose category name is not in this set will be filtered out. Duplicate or unknown category names are ignored. This option is mutually exclusive with categoryDenylist and using both results in an error.\tAny strings\tNot set\ncategoryDenylist\tSets the optional list of category names that are not allowed. If non-empty, classification results whose category name is in this set will be filtered out. Duplicate or unknown category names are ignored. This option is mutually exclusive with categoryAllowlist and using both results in an error.\tAny strings\tNot set\nresultListener\tSets the result listener to receive the classification results asynchronously when the Image Classifier is in the live stream mode. Can only be used when running mode is set to LIVE_STREAM\tN/A\tNot set\nPrepare data\n\nImage Classifier can classify objects in images in any format supported by the host browser. The task also handles data input preprocessing, including resizing, rotation and value normalization.\n\nCalls to the Image Classifier classify() and classifyForVideo() methods run synchronously and block the user interface thread. If you classify objects in video frames from a device's camera, each classification will block the main thread. You can prevent this by implementing web workers to run classify() and classifyForVideo() on another thread.\n\nRun the task\n\nThe Image Classifier uses the classify() method with image mode and the classifyForVideo() method with video mode to trigger inferences. The Image Classifier API will return the possible categories for the objects within the input image.\n\nThe following code demonstrates how execute the processing with the task model:\n\nImage\nVideo\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst imageClassifierResult = imageClassifier.classify(image);\n\nFor a more complete implementation of running an Image Classifier task, see the code example).\n\nHandle and display results\n\nUpon running inference, the Image Classifier task returns an ImageClassifierResult object which contains the list of possible categories for the objects within the input image or frame.\n\nThe following shows an example of the output data from this task:\n\nImageClassifierResult:\n Classifications #0 (single classification head):\n  head index: 0\n  category #0:\n   category name: \"/m/01bwb9\"\n   display name: \"Passer domesticus\"\n   score: 0.91406\n   index: 671\n  category #1:\n   category name: \"/m/01bwbt\"\n   display name: \"Passer montanus\"\n   score: 0.00391\n   index: 670\n\n\nThis result has been obtained by running the Bird Classifier on:\n\nThe Image Classifier example code demonstrates how to display the classification results returned from the task, see the code example for details.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-14 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "org.tensorflow.lite.support.label  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/support/label/package-summary",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nCore\norg.tensorflow.lite\nacceleration\nacceleration\nSupport Library\nsupport.audio\nsupport.common\nsupport.common.ops\nsupport.image\nsupport.image.ops\nsupport.label\nOverview\nCategory\nLabelUtil\nTensorLabel\nsupport.label.ops\nsupport.metadata\nsupport.model\nsupport.tensorbuffer\nDelegates\ngpu\nODML\nimage\nimage.annotation\nSwift\nC\nC++\nOn this page\nClasses\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\norg.tensorflow.lite.support.label \nbookmark_border\nClasses\nCategory\tCategory is a util class, contains a label, its display name, a float value as score, and the index of the label in the corresponding label file. \nLabelUtil\tLabel operation utils. \nTensorLabel\tTensorLabel is an util wrapper for TensorBuffers with meaningful labels on an axis. \nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Module: mp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dotcalculator_pb2.mediapipe_dot_framework_dotpacketgenerator_pb2  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/core/constant_side_packet_calculator_pb2/mediapipe_dot_framework_dot_calculator__pb2/mediapipe_dot_framework_dot_packet__generator__pb2",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nOverview\nconstant_side_packet_calculator_pb2\nOverview\nConstantSidePacketCalculatorOptions\nConstantSidePacketCalculatorOptions.ConstantSidePacket\nmediapipe_dot_framework_dot_calculator__options__pb2\nmediapipe_dot_framework_dot_calculator__pb2\nOverview\nCalculatorGraphConfig\nCalculatorGraphConfig.Node\nExecutorConfig\nInputCollection\nInputCollectionSet\nInputStreamInfo\nProfilerConfig\nmediapipe_dot_framework_dot_mediapipe__options__pb2\nmediapipe_dot_framework_dot_packet__factory__pb2\nmediapipe_dot_framework_dot_packet__generator__pb2\nOverview\nPacketGeneratorConfig\nPacketGeneratorOptions\nmediapipe_dot_framework_dot_status__handler__pb2\nmediapipe_dot_framework_dot_stream__handler__pb2\nmediapipe_dot_framework_dot_formats_dot_classification__pb2\nmediapipe_dot_framework_dot_formats_dot_landmark__pb2\nmediapipe_dot_framework_dot_formats_dot_matrix__data__pb2\nmediapipe_dot_framework_dot_formats_dot_time__series__header__pb2\nflow_limiter_calculator_pb2\ngate_calculator_pb2\nsplit_vector_calculator_pb2\nimage\ntensor\ntflite\nutil\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nClasses\nOther Members\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nModule: mp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dotcalculator_pb2.mediapipe_dot_framework_dotpacketgenerator_pb2 \nbookmark_border\n\nView source on GitHub\n\nGenerated protocol buffer code.\n\nClasses\n\nclass PacketGeneratorConfig: A ProtocolMessage\n\nclass PacketGeneratorOptions: A ProtocolMessage\n\nOther Members\n\nDESCRIPTOR\tInstance of google.protobuf.pyext._message.FileDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.GenerateContentResponse.PromptFeedback  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentResponse/PromptFeedback",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nChild Classes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.GenerateContentResponse.PromptFeedback \nbookmark_border\n\nView source on GitHub\n\nA set of the feedback metadata the prompt specified in GenerateContentRequest.content.\n\nAttributes\n\nblock_reason\tgoogle.ai.generativelanguage.GenerateContentResponse.PromptFeedback.BlockReason\n\nOptional. If set, the prompt was blocked and no candidates are returned. Rephrase your prompt.\n\n\nsafety_ratings\tMutableSequence[google.ai.generativelanguage.SafetyRating]\n\nRatings for safety of the prompt. There is at most one rating per category.\n\nChild Classes\n\nclass BlockReason\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.ProcessUnitAddOptionsType  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/ProcessUnitAddOptionsType",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.ProcessUnitAddOptionsType \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.ProcessUnitAddOptionsType(\n    builder, optionsType\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "TensorAudio.TensorAudioFormat  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/support/audio/TensorAudio.TensorAudioFormat",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nCore\norg.tensorflow.lite\nacceleration\nacceleration\nSupport Library\nsupport.audio\nOverview\nTensorAudio\nOverview\nTensorAudio.TensorAudioFormat\nOverview\nBuilder\nsupport.common\nsupport.common.ops\nsupport.image\nsupport.image.ops\nsupport.label\nsupport.label.ops\nsupport.metadata\nsupport.model\nsupport.tensorbuffer\nDelegates\ngpu\nODML\nimage\nimage.annotation\nSwift\nC\nC++\nOn this page\nNested Classes\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nTensorAudio.TensorAudioFormat \nbookmark_border\npublic static abstract class TensorAudio.TensorAudioFormat\n\nWraps a few constants describing the format of the incoming audio samples, namely number of channels and the sample rate. By default, channels is set to 1.\n\nNested Classes\nclass\tTensorAudio.TensorAudioFormat.Builder\tBuilder for TensorAudio.TensorAudioFormat  \nPublic Constructors\n\t\nTensorAudioFormat()\nPublic Methods\nstatic TensorAudio.TensorAudioFormat.Builder\t\nbuilder()\n\nstatic TensorAudio.TensorAudioFormat\t\ncreate(AudioFormat format)\nCreates a TensorAudio.TensorAudioFormat instance from Android AudioFormat class.\n\nabstract int\t\ngetChannels()\n\nabstract int\t\ngetSampleRate()\nInherited Methods\nFrom class java.lang.Object\nPublic Constructors\npublic TensorAudioFormat ()\n\nPublic Methods\npublic static TensorAudio.TensorAudioFormat.Builder builder ()\n\npublic static TensorAudio.TensorAudioFormat create (AudioFormat format)\n\nCreates a TensorAudio.TensorAudioFormat instance from Android AudioFormat class.\n\nParameters\nformat\t\npublic abstract int getChannels ()\n\npublic abstract int getSampleRate ()\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "DelegateFactory  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/DelegateFactory",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\nAndroid (Java)\nCore\norg.tensorflow.lite\nOverview\nDataType\nDelegate\nDelegateFactory\nInterpreter\nInterpreterApi\nInterpreterFactory\nRuntimeFlavor\nTensor\nTensorFlowLite\nacceleration\nacceleration\nSupport Library\nsupport.audio\nsupport.common\nsupport.common.ops\nsupport.image\nsupport.image.ops\nsupport.label\nsupport.label.ops\nsupport.metadata\nsupport.model\nsupport.tensorbuffer\nDelegates\ngpu\nODML\nimage\nimage.annotation\nSwift\nC\nC++\nOn this page\nPublic Methods\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nDelegateFactory \nbookmark_border\npublic interface DelegateFactory\nKnown Indirect Subclasses\nGpuDelegateFactory\n\nAllows creating delegates for different runtime flavors.\n\nPublic Methods\nabstract Delegate\t\ncreate(RuntimeFlavor runtimeFlavor)\nCreate a Delegate for the given RuntimeFlavor.\nPublic Methods\npublic abstract Delegate create (RuntimeFlavor runtimeFlavor) \n\nCreate a Delegate for the given RuntimeFlavor.\n\nNote for developers implementing this interface: Currently TF Lite in Google Play Services does not support external (developer-provided) delegates. Correspondingly, implementations of this method can expect to be called with RuntimeFlavor.APPLICATION.\n\nParameters\nruntimeFlavor\t\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-10 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.GenerateTextRequest  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/GenerateTextRequest",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.GenerateTextRequest \nbookmark_border\n\nView source on GitHub\n\nRequest to generate a text completion response from the model.\n\nAttributes\n\nmodel\tstr\n\nRequired. The name of the Model or TunedModel to use for generating the completion. Examples: models/text-bison-001 tunedModels/sentence-translator-u3b7m\n\n\nprompt\tgoogle.ai.generativelanguage.TextPrompt\n\nRequired. The free-form input text given to the model as a prompt. Given a prompt, the model will generate a TextCompletion response it predicts as the completion of the input text.\n\n\ntemperature\tfloat\n\nOptional. Controls the randomness of the output. Note: The default value varies by model, see the Model.temperature attribute of the Model returned the getModel function.\n\nValues can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model.\n\n\ncandidate_count\tint\n\nOptional. Number of generated responses to return.\n\nThis value must be between [1, 8], inclusive. If unset, this will default to 1.\n\n\nmax_output_tokens\tint\n\nOptional. The maximum number of tokens to include in a candidate.\n\nIf unset, this will default to output_token_limit specified in the Model specification.\n\n\ntop_p\tfloat\n\nOptional. The maximum cumulative probability of tokens to consider when sampling.\n\nThe model uses combined Top-k and nucleus sampling.\n\nTokens are sorted based on their assigned probabilities so that only the most likely tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability.\n\nNote: The default value varies by model, see the Model.top_p attribute of the Model returned the getModel function.\n\n\ntop_k\tint\n\nOptional. The maximum number of tokens to consider when sampling.\n\nThe model uses combined Top-k and nucleus sampling.\n\nTop-k sampling considers the set of top_k most probable tokens. Defaults to 40.\n\nNote: The default value varies by model, see the Model.top_k attribute of the Model returned the getModel function.\n\n\nsafety_settings\tMutableSequence[google.ai.generativelanguage.SafetySetting]\n\nOptional. A list of unique SafetySetting instances for blocking unsafe content.\n\nthat will be enforced on the GenerateTextRequest.prompt and GenerateTextResponse.candidates. There should not be more than one setting for each SafetyCategory type. The API will block any prompts and responses that fail to meet the thresholds set by these settings. This list overrides the default settings for each SafetyCategory specified in the safety_settings. If there is no SafetySetting for a given SafetyCategory provided in the list, the API will use the default safety setting for that category. Harm categories HARM_CATEGORY_DEROGATORY, HARM_CATEGORY_TOXICITY, HARM_CATEGORY_VIOLENCE, HARM_CATEGORY_SEXUAL, HARM_CATEGORY_MEDICAL, HARM_CATEGORY_DANGEROUS are supported in text service.\n\n\nstop_sequences\tMutableSequence[str]\n\nThe set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "TaskOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/java/com/google/mediapipe/tasks/core/TaskOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\ncom.google.mediapipe.framework.image\ncom.google.mediapipe.tasks.audio.audioclassifier\ncom.google.mediapipe.tasks.audio.audioembedder\ncom.google.mediapipe.tasks.audio.core\ncom.google.mediapipe.tasks.components.containers\ncom.google.mediapipe.tasks.components.processors\ncom.google.mediapipe.tasks.components.utils\ncom.google.mediapipe.tasks.core\nOverview\nBaseOptions\nDelegate\nErrorListener\nOutputHandler\nTaskInfo\nTaskOptions\nTaskResult\nTaskRunner\ncom.google.mediapipe.tasks.core.logging\ncom.google.mediapipe.tasks.genai.llminference\ncom.google.mediapipe.tasks.text.languagedetector\ncom.google.mediapipe.tasks.text.textclassifier\ncom.google.mediapipe.tasks.text.textembedder\ncom.google.mediapipe.tasks.vision.core\ncom.google.mediapipe.tasks.vision.facedetector\ncom.google.mediapipe.tasks.vision.facelandmarker\ncom.google.mediapipe.tasks.vision.facestylizer\ncom.google.mediapipe.tasks.vision.gesturerecognizer\ncom.google.mediapipe.tasks.vision.handlandmarker\ncom.google.mediapipe.tasks.vision.holisticlandmarker\ncom.google.mediapipe.tasks.vision.imageclassifier\ncom.google.mediapipe.tasks.vision.imageembedder\ncom.google.mediapipe.tasks.vision.imagegenerator\ncom.google.mediapipe.tasks.vision.imagesegmenter\ncom.google.mediapipe.tasks.vision.interactivesegmenter\ncom.google.mediapipe.tasks.vision.objectdetector\ncom.google.mediapipe.tasks.vision.poselandmarker\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nTaskOptions \nbookmark_border\npublic abstract class TaskOptions\nKnown Direct Subclasses\nAudioClassifier.AudioClassifierOptions, AudioEmbedder.AudioEmbedderOptions, FaceDetector.FaceDetectorOptions, FaceLandmarker.FaceLandmarkerOptions, FaceStylizer.FaceStylizerOptions, GestureRecognizer.GestureRecognizerOptions, HandLandmarker.HandLandmarkerOptions, HolisticLandmarker.HolisticLandmarkerOptions, ImageClassifier.ImageClassifierOptions, ImageEmbedder.ImageEmbedderOptions, ImageGenerator.ConditionOptions, ImageGenerator.ConditionOptions.DepthConditionOptions, ImageGenerator.ConditionOptions.FaceConditionOptions, ImageGenerator.ImageGeneratorOptions, ImageSegmenter.ImageSegmenterOptions, and 7 others.\n\nMediaPipe Tasks options base class. Any MediaPipe task-specific options class should extend TaskOptions and implement exactly one of convertTo*Proto() methods.\n\nPublic Constructors\n\t\nTaskOptions()\nPublic Methods\nAny\t\nconvertToAnyProto()\nConverts a MediaPipe Tasks task-specific options to an proto3 ERROR(/Any) message.\n\nCalculatorOptions\t\nconvertToCalculatorOptionsProto()\nConverts a MediaPipe Tasks task-specific options to a ERROR(/CalculatorOptions) protobuf message.\nInherited Methods\nFrom class java.lang.Object\nPublic Constructors\npublic TaskOptions ()\n\nPublic Methods\npublic Any convertToAnyProto ()\n\nConverts a MediaPipe Tasks task-specific options to an proto3 ERROR(/Any) message.\n\npublic CalculatorOptions convertToCalculatorOptionsProto ()\n\nConverts a MediaPipe Tasks task-specific options to a ERROR(/CalculatorOptions) protobuf message.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Module: mp.calculators.util.detection_label_id_to_text_calculator_pb2  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/util/detection_label_id_to_text_calculator_pb2",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nimage\ntensor\ntflite\nutil\nOverview\nassociation_calculator_pb2\ncollection_has_min_size_calculator_pb2\ndetection_label_id_to_text_calculator_pb2\nOverview\nDetectionLabelIdToTextCalculatorOptions\nDetectionLabelIdToTextCalculatorOptions.LabelItemsEntry\ndetections_to_rects_calculator_pb2\nlandmark_projection_calculator_pb2\nlandmarks_refinement_calculator_pb2\nlandmarks_smoothing_calculator_pb2\nlocal_file_contents_calculator_pb2\nlogic_calculator_pb2\nnon_max_suppression_calculator_pb2\nrect_transformation_calculator_pb2\nthresholding_calculator_pb2\nvisibility_smoothing_calculator_pb2\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nModules\nClasses\nOther Members\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nModule: mp.calculators.util.detection_label_id_to_text_calculator_pb2 \nbookmark_border\n\nView source on GitHub\n\nGenerated protocol buffer code.\n\nModules\n\nmediapipe_dot_framework_dot_calculator__options__pb2 module: Generated protocol buffer code.\n\nmediapipe_dot_framework_dot_calculator__pb2 module: Generated protocol buffer code.\n\nmediapipe_dot_util_dot_label__map__pb2 module: Generated protocol buffer code.\n\nClasses\n\nclass DetectionLabelIdToTextCalculatorOptions: A ProtocolMessage\n\nOther Members\n\nDESCRIPTOR\tInstance of google.protobuf.pyext._message.FileDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.import_numpy  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/import_numpy",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.import_numpy \nbookmark_border\n\nReturns the numpy module if it exists on the system, otherwise returns None.\n\ntflite_support.metadata_schema_py_generated.import_numpy()\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_writers.metadata_info.CategoryTensorMd  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_writers/metadata_info/CategoryTensorMd",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nmetadata_writers\nOverview\naudio_classifier\nbert_nl_classifier\nimage_classifier\nimage_segmenter\nmetadata_info\nOverview\nAssociatedFileMd\nBertInputTensorsMd\nBertTokenizerMd\nCategoryTensorMd\nClassificationTensorMd\nGeneralMd\nInputAudioTensorMd\nInputImageTensorMd\nInputTextTensorMd\nLabelFileMd\nRegexTokenizerMd\nScoreCalibrationMd\nSentencePieceTokenizerMd\nTensorMd\nnl_classifier\nobject_detector\nwriter_utils\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nArgs\nMethods\ncreate_metadata\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_writers.metadata_info.CategoryTensorMd \nbookmark_border\n\nView source on GitHub\n\nA container for the category tensor metadata information.\n\nInherits From: TensorMd\n\nView aliases\ntflite_support.metadata_writers.metadata_info.CategoryTensorMd(\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    label_files: Optional[List[LabelFileMd]] = None\n)\n\n\nArgs\n\nname\tname of the tensor.\ndescription\tdescription of what the tensor is.\nlabel_files\tinformation of the label files 1 in the category tensor.\n\nMethods\ncreate_metadata\n\nView source\n\ncreate_metadata() -> tflite_support.metadata_schema_py_generated.TensorMetadataT\n\n\nCreates the input tensor metadata based on the information.\n\nReturns\nA Flatbuffers Python object of the input metadata.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.packet_getter.get_matrix  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/packet_getter/get_matrix",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nOverview\nget_bool\nget_bool_list\nget_bytes\nget_float\nget_float_list\nget_image\nget_image_frame\nget_image_list\nget_int\nget_int_list\nget_matrix\nget_packet_list\nget_proto\nget_proto_list\nget_str\nget_str_list\nget_str_to_packet_dict\nget_uint\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.packet_getter.get_matrix \nbookmark_border\n\nget_matrix(arg0: mediapipe.python._framework_bindings.packet.Packet) -> numpy.ndarray[numpy.float32[m, n], flags.f_contiguous]\n\nView aliases\nmp.packet_getter.get_matrix()\n\n\nGet the content of a MediaPipe Matrix Packet as a numpy 2d float ndarray.\n\nArgs: packet: A MediaPipe Matrix Packet.\n\nReturns: A numpy 2d float ndarray.\n\nRaises: ValueError: If the Packet doesn't contain matrix data.\n\nExamples: packet = mp.packet_creator.create_matrix(2d_array) data = mp.packet_getter.get_matrix(packet)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "ObjectDetector  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/java/com/google/mediapipe/tasks/vision/objectdetector/ObjectDetector",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\ncom.google.mediapipe.framework.image\ncom.google.mediapipe.tasks.audio.audioclassifier\ncom.google.mediapipe.tasks.audio.audioembedder\ncom.google.mediapipe.tasks.audio.core\ncom.google.mediapipe.tasks.components.containers\ncom.google.mediapipe.tasks.components.processors\ncom.google.mediapipe.tasks.components.utils\ncom.google.mediapipe.tasks.core\ncom.google.mediapipe.tasks.core.logging\ncom.google.mediapipe.tasks.genai.llminference\ncom.google.mediapipe.tasks.text.languagedetector\ncom.google.mediapipe.tasks.text.textclassifier\ncom.google.mediapipe.tasks.text.textembedder\ncom.google.mediapipe.tasks.vision.core\ncom.google.mediapipe.tasks.vision.facedetector\ncom.google.mediapipe.tasks.vision.facelandmarker\ncom.google.mediapipe.tasks.vision.facestylizer\ncom.google.mediapipe.tasks.vision.gesturerecognizer\ncom.google.mediapipe.tasks.vision.handlandmarker\ncom.google.mediapipe.tasks.vision.holisticlandmarker\ncom.google.mediapipe.tasks.vision.imageclassifier\ncom.google.mediapipe.tasks.vision.imageembedder\ncom.google.mediapipe.tasks.vision.imagegenerator\ncom.google.mediapipe.tasks.vision.imagesegmenter\ncom.google.mediapipe.tasks.vision.interactivesegmenter\ncom.google.mediapipe.tasks.vision.objectdetector\nOverview\nObjectDetectionResult\nObjectDetector\nOverview\nObjectDetector.ObjectDetectorOptions\nObjectDetectorResult\ncom.google.mediapipe.tasks.vision.poselandmarker\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nNested Classes\nPublic Methods\nInherited Methods\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nObjectDetector \nbookmark_border\npublic final class ObjectDetector\n\nPerforms object detection on images.\n\nThe API expects a TFLite model with TFLite Model Metadata..\n\nThe API supports models with one image input tensor and four output tensors. To be more specific, here are the requirements.\n\nInput image tensor (kTfLiteUInt8/kTfLiteFloat32)\nimage input of size [batch x height x width x channels].\nbatch inference is not supported (batch is required to be 1).\nonly RGB inputs are supported (channels is required to be 3).\nif type is kTfLiteFloat32, NormalizationOptions are required to be attached to the metadata for input normalization.\nOutput tensors must be the 4 outputs of a DetectionPostProcess op, i.e:\nLocation tensor (kTfLiteFloat32):\ntensor of size [1 x num_results x 4], the inner array representing bounding boxes in the form [top, left, right, bottom].\nBoundingBoxProperties are required to be attached to the metadata and must specify type=BOUNDARIES and coordinate_type=RATIO.\nClasses tensor (kTfLiteFloat32):\ntensor of size [1 x num_results], each value representing the integer index of a class.\nif label maps are attached to the metadata as TENSOR_VALUE_LABELS associated files, they are used to convert the tensor values into labels.\nscores tensor (kTfLiteFloat32):\ntensor of size [1 x num_results], each value representing the score of the detected object.\nNumber of detection tensor (kTfLiteFloat32):\ninteger num_results as a tensor of size [1].\n\nAn example of such model can be found on TensorFlow Hub..\n\nNested Classes\nclass\tObjectDetector.ObjectDetectorOptions\tOptions for setting up an ObjectDetector. \nPublic Methods\nstatic ObjectDetector\t\ncreateFromBuffer(Context context, ByteBuffer modelBuffer)\nCreates an ObjectDetector instance from a model buffer and the default ObjectDetector.ObjectDetectorOptions.\n\nstatic ObjectDetector\t\ncreateFromFile(Context context, String modelPath)\nCreates an ObjectDetector instance from a model file and the default ObjectDetector.ObjectDetectorOptions.\n\nstatic ObjectDetector\t\ncreateFromFile(Context context, File modelFile)\nCreates an ObjectDetector instance from a model file and the default ObjectDetector.ObjectDetectorOptions.\n\nstatic ObjectDetector\t\ncreateFromOptions(Context context, ObjectDetector.ObjectDetectorOptions detectorOptions)\nCreates an ObjectDetector instance from an ObjectDetector.ObjectDetectorOptions.\n\nObjectDetectorResult\t\ndetect(MPImage image)\nPerforms object detection on the provided single image with default image processing options, i.e.\n\nObjectDetectorResult\t\ndetect(MPImage image, ImageProcessingOptions imageProcessingOptions)\nPerforms object detection on the provided single image.\n\nvoid\t\ndetectAsync(MPImage image, ImageProcessingOptions imageProcessingOptions, long timestampMs)\nSends live image data to perform object detection, and the results will be available via the OutputHandler.ResultListener provided in the ObjectDetector.ObjectDetectorOptions.\n\nvoid\t\ndetectAsync(MPImage image, long timestampMs)\nSends live image data to perform object detection with default image processing options, i.e.\n\nObjectDetectorResult\t\ndetectForVideo(MPImage image, ImageProcessingOptions imageProcessingOptions, long timestampMs)\nPerforms object detection on the provided video frame.\n\nObjectDetectorResult\t\ndetectForVideo(MPImage image, long timestampMs)\nPerforms object detection on the provided video frame with default image processing options, i.e.\nInherited Methods\nFrom class\ncom.google.mediapipe.tasks.vision.core.BaseVisionTaskApi\nFrom class java.lang.Object\nFrom interface java.lang.AutoCloseable\nPublic Methods\npublic static ObjectDetector createFromBuffer (Context context, ByteBuffer modelBuffer)\n\nCreates an ObjectDetector instance from a model buffer and the default ObjectDetector.ObjectDetectorOptions.\n\nParameters\ncontext\tan Android ERROR(/Context).\nmodelBuffer\ta direct ByteBuffer or a ERROR(/MappedByteBuffer) of the detection model.\nThrows\n\tif there is an error during ObjectDetector creation.\npublic static ObjectDetector createFromFile (Context context, String modelPath)\n\nCreates an ObjectDetector instance from a model file and the default ObjectDetector.ObjectDetectorOptions.\n\nParameters\ncontext\tan Android ERROR(/Context).\nmodelPath\tpath to the detection model with metadata in the assets.\nThrows\n\tif there is an error during ObjectDetector creation.\npublic static ObjectDetector createFromFile (Context context, File modelFile)\n\nCreates an ObjectDetector instance from a model file and the default ObjectDetector.ObjectDetectorOptions.\n\nParameters\ncontext\tan Android ERROR(/Context).\nmodelFile\tthe detection model File instance.\nThrows\nIOException\tif an I/O error occurs when opening the tflite model file.\n\tif there is an error during ObjectDetector creation.\npublic static ObjectDetector createFromOptions (Context context, ObjectDetector.ObjectDetectorOptions detectorOptions)\n\nCreates an ObjectDetector instance from an ObjectDetector.ObjectDetectorOptions.\n\nParameters\ncontext\tan Android ERROR(/Context).\ndetectorOptions\tan ObjectDetector.ObjectDetectorOptions instance.\nThrows\n\tif there is an error during ObjectDetector creation.\npublic ObjectDetectorResult detect (MPImage image)\n\nPerforms object detection on the provided single image with default image processing options, i.e. without any rotation applied. Only use this method when the ObjectDetector is created with ERROR(/RunningMode.IMAGE).\n\nObjectDetector supports the following color space types:\n\nERROR(/Bitmap.Config.ARGB_8888)\n\nParameters\nimage\ta MediaPipe MPImage object for processing.\nThrows\n\tif there is an internal error.\npublic ObjectDetectorResult detect (MPImage image, ImageProcessingOptions imageProcessingOptions)\n\nPerforms object detection on the provided single image. Only use this method when the ObjectDetector is created with ERROR(/RunningMode.IMAGE).\n\nObjectDetector supports the following color space types:\n\nERROR(/Bitmap.Config.ARGB_8888)\n\nParameters\nimage\ta MediaPipe MPImage object for processing.\nimageProcessingOptions\tthe ImageProcessingOptions specifying how to process the input image before running inference. Note that region-of-interest is not supported by this task: specifying ImageProcessingOptions.regionOfInterest() will result in this method throwing an IllegalArgumentException.\nThrows\nIllegalArgumentException\tif the ImageProcessingOptions specify a region-of-interest.\n\tif there is an internal error.\npublic void detectAsync (MPImage image, ImageProcessingOptions imageProcessingOptions, long timestampMs)\n\nSends live image data to perform object detection, and the results will be available via the OutputHandler.ResultListener provided in the ObjectDetector.ObjectDetectorOptions. Only use this method when the ObjectDetector is created with ERROR(/RunningMode.LIVE_STREAM).\n\nIt's required to provide a timestamp (in milliseconds) to indicate when the input image is sent to the object detector. The input timestamps must be monotonically increasing.\n\nObjectDetector supports the following color space types:\n\nERROR(/Bitmap.Config.ARGB_8888)\n\nParameters\nimage\ta MediaPipe MPImage object for processing.\nimageProcessingOptions\tthe ImageProcessingOptions specifying how to process the input image before running inference. Note that region-of-interest is not supported by this task: specifying ImageProcessingOptions.regionOfInterest() will result in this method throwing an IllegalArgumentException.\ntimestampMs\tthe input timestamp (in milliseconds).\nThrows\nIllegalArgumentException\tif the ImageProcessingOptions specify a region-of-interest.\n\tif there is an internal error.\npublic void detectAsync (MPImage image, long timestampMs)\n\nSends live image data to perform object detection with default image processing options, i.e. without any rotation applied, and the results will be available via the OutputHandler.ResultListener provided in the ObjectDetector.ObjectDetectorOptions. Only use this method when the ObjectDetector is created with ERROR(/RunningMode.LIVE_STREAM).\n\nIt's required to provide a timestamp (in milliseconds) to indicate when the input image is sent to the object detector. The input timestamps must be monotonically increasing.\n\nObjectDetector supports the following color space types:\n\nERROR(/Bitmap.Config.ARGB_8888)\n\nParameters\nimage\ta MediaPipe MPImage object for processing.\ntimestampMs\tthe input timestamp (in milliseconds).\nThrows\n\tif there is an internal error.\npublic ObjectDetectorResult detectForVideo (MPImage image, ImageProcessingOptions imageProcessingOptions, long timestampMs)\n\nPerforms object detection on the provided video frame. Only use this method when the ObjectDetector is created with ERROR(/RunningMode.VIDEO).\n\nIt's required to provide the video frame's timestamp (in milliseconds). The input timestamps must be monotonically increasing.\n\nObjectDetector supports the following color space types:\n\nERROR(/Bitmap.Config.ARGB_8888)\n\nParameters\nimage\ta MediaPipe MPImage object for processing.\nimageProcessingOptions\tthe ImageProcessingOptions specifying how to process the input image before running inference. Note that region-of-interest is not supported by this task: specifying ImageProcessingOptions.regionOfInterest() will result in this method throwing an IllegalArgumentException.\ntimestampMs\tthe input timestamp (in milliseconds).\nThrows\nIllegalArgumentException\tif the ImageProcessingOptions specify a region-of-interest.\n\tif there is an internal error.\npublic ObjectDetectorResult detectForVideo (MPImage image, long timestampMs)\n\nPerforms object detection on the provided video frame with default image processing options, i.e. without any rotation applied. Only use this method when the ObjectDetector is created with ERROR(/RunningMode.VIDEO).\n\nIt's required to provide the video frame's timestamp (in milliseconds). The input timestamps must be monotonically increasing.\n\nObjectDetector supports the following color space types:\n\nERROR(/Bitmap.Config.ARGB_8888)\n\nParameters\nimage\ta MediaPipe MPImage object for processing.\ntimestampMs\tthe input timestamp (in milliseconds).\nThrows\n\tif there is an internal error.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Inference with Gemma using JAX and Flax  |  Google for Developers",
    "url": "https://ai.google.dev/gemma/docs/jax_inference",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nDocs\nOverview\nModel Card\nGemma model family\nCodeGemma\nPaliGemma\nRecurrentGemma\nReleases\nGuides\nGemma setup\nGet started with Gemma using Keras\nBasic tuning with Gemma using Keras\nDistributed tuning with Gemma using Keras\nGet started with Gemma using PyTorch\nChat with Gemma\nFormatting and system instructions\nGemma C++ Tutorial\nInference using JAX and Flax\nFine-tuning using JAX and Flax\nIntegrations\nVertex AI\nDataflow ML\nGoogle Kubernetes Engine (GKE)\nLangChain\nResponsible Generative AI Toolkit\nOverview\nSet responsible policies\nTune models for safety\nCreate input and output safeguards\nConduct safety evaluations\nBuild transparency artifacts\nAnalyze model behavior\nCommunity\nDiscord\nLegal\nTerms of use\nProhibited use\nOn this page\nOverview\nSetup\n1. Set up Kaggle access for Gemma\n2. Set environment variables\n3. Install the gemma library\nLoad and prepare the Gemma model\nPerform sampling/inference\nLearn more\nHome\nGemma\nDocs\nSend feedback\nInference with Gemma using JAX and Flax \nbookmark_border\n\nRun in Google Colab\n\t\nOpen in Vertex AI\n\t\nView source on GitHub\nOverview\n\nGemma is a family of lightweight, state-of-the-art open large language models, based on the Google DeepMind Gemini research and technology. This tutorial demonstrates how to perform basic sampling/inference with the Gemma 2B Instruct model using Google DeepMind's gemma library that was written with JAX (a high-performance numerical computing library), Flax (the JAX-based neural network library), Orbax (a JAX-based library for training utilities like checkpointing), and SentencePiece (a tokenizer/detokenizer library). Although Flax is not used directly in this notebook, Flax was used to create Gemma.\n\nThis notebook can run on Google Colab with free T4 GPU (go to Edit > Notebook settings > Under Hardware accelerator select T4 GPU).\n\nSetup\n1. Set up Kaggle access for Gemma\n\nTo complete this tutorial, you first need to follow the setup instructions at Gemma setup, which show you how to do the following:\n\nGet access to Gemma on kaggle.com.\nSelect a Colab runtime with sufficient resources to run the Gemma model.\nGenerate and configure a Kaggle username and API key.\n\nAfter you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.\n\n2. Set environment variables\n\nSet environment variables for KAGGLE_USERNAME and KAGGLE_KEY. When prompted with the \"Grant access?\" messages, agree to provide secret access.\n\nimport os\nfrom google.colab import userdata # `userdata` is a Colab API.\n\nos.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\nos.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n\n3. Install the gemma library\n\nThis notebook focuses on using a free Colab GPU. To enable hardware acceleration, click on Edit > Notebook settings > Select T4 GPU > Save.\n\nNext, you need to install the Google DeepMind gemma library from github.com/google-deepmind/gemma. If you get an error about \"pip's dependency resolver\", you can usually ignore it.\n\nNote: By installing gemma, you will also install flax, core jax, optax (the JAX-based gradient processing and optimization library), orbax, and sentencepiece.\npip install -q git+https://github.com/google-deepmind/gemma.git\n\nLoad and prepare the Gemma model\nLoad the Gemma model with kagglehub.model_download, which takes three arguments:\nhandle: The model handle from Kaggle\npath: (Optional string) The local path\nforce_download: (Optional boolean) Forces to re-download the model\nNote: Be mindful that the gemma-2b-it model is around 3.7Gb in size.\nGEMMA_VARIANT = '2b-it' # @param ['2b', '2b-it'] {type:\"string\"}\n\nimport kagglehub\n\nGEMMA_PATH = kagglehub.model_download(f'google/gemma/flax/{GEMMA_VARIANT}')\n\nDownloading from https://www.kaggle.com/api/v1/models/google/gemma/flax/2b-it/2/download...\n100%|██████████| 3.67G/3.67G [00:35<00:00, 110MB/s]\nExtracting model files...\n\nprint('GEMMA_PATH:', GEMMA_PATH)\n\nGEMMA_PATH: /root/.cache/kagglehub/models/google/gemma/flax/2b-it/2\n\nNote: The path from the output above is where the model weights and tokenizer are saved locally, you will need them for later.\nCheck the location of the model weights and the tokenizer, then set the path variables. The tokenizer directory will be in the main directory where you downloaded the model, while the model weights will be in a sub-directory. For example:\nThe tokenizer.model file will be in /LOCAL/PATH/TO/gemma/flax/2b-it/2).\nThe model checkpoint will be in /LOCAL/PATH/TO/gemma/flax/2b-it/2/2b-it).\nCKPT_PATH = os.path.join(GEMMA_PATH, GEMMA_VARIANT)\nTOKENIZER_PATH = os.path.join(GEMMA_PATH, 'tokenizer.model')\nprint('CKPT_PATH:', CKPT_PATH)\nprint('TOKENIZER_PATH:', TOKENIZER_PATH)\n\nCKPT_PATH: /root/.cache/kagglehub/models/google/gemma/flax/2b-it/2/2b-it\nTOKENIZER_PATH: /root/.cache/kagglehub/models/google/gemma/flax/2b-it/2/tokenizer.model\n\nPerform sampling/inference\nLoad and format the Gemma model checkpoint with the gemma.params.load_and_format_params method:\nfrom gemma import params as params_lib\n\nparams = params_lib.load_and_format_params(CKPT_PATH)\n\nLoad the Gemma tokenizer, constructed using sentencepiece.SentencePieceProcessor:\nimport sentencepiece as spm\n\nvocab = spm.SentencePieceProcessor()\nvocab.Load(TOKENIZER_PATH)\n\nTrue\n\nTo automatically load the correct configuration from the Gemma model checkpoint, use gemma.transformer.TransformerConfig. The cache_size argument is the number of time steps in the Gemma Transformer cache. Afterwards, instantiate the Gemma model as transformer with gemma.transformer.Transformer (which inherits from flax.linen.Module).\nNote: The vocabulary size is smaller than the number of input embeddings because of unused tokens in the current Gemma release.\nfrom gemma import transformer as transformer_lib\n\ntransformer_config = transformer_lib.TransformerConfig.from_params(\n    params=params,\n    cache_size=1024\n)\n\ntransformer = transformer_lib.Transformer(transformer_config)\n\nCreate a sampler with gemma.sampler.Sampler on top of the Gemma model checkpoint/weights and the tokenizer:\nfrom gemma import sampler as sampler_lib\n\nsampler = sampler_lib.Sampler(\n    transformer=transformer,\n    vocab=vocab,\n    params=params['transformer'],\n)\n\nWrite a prompt in input_batch and perform inference. You can tweak total_generation_steps (the number of steps performed when generating a response — this example uses 100 to preserve host memory).\nNote: If you run out of memory, click on Runtime > Disconnect and delete runtime, and then Runtime > Run all.\nprompt = [\n    \"\\n# What is the meaning of life?\",\n]\n\nreply = sampler(input_strings=prompt,\n                total_generation_steps=100,\n                )\n\nfor input_string, out_string in zip(prompt, reply.text):\n    print(f\"Prompt:\\n{input_string}\\nOutput:\\n{out_string}\")\n\nPrompt:\n\n# What is the meaning of life?\nOutput:\n\n\nThe question of what the meaning of life is one that has occupied the minds of philosophers, theologians, and individuals for centuries. There is no single, universally accepted answer, but there are many different perspectives on this complex and multifaceted question.\n\n**Some common perspectives on the meaning of life include:**\n\n* **Biological perspective:** From a biological standpoint, the meaning of life is to survive and reproduce.\n* **Existential perspective:** Existentialists believe that life is not inherently meaningful and that\n\n(Optional) Run this cell to free up memory if you have completed the notebook and want to try another prompt. Afterwards, you can instantiate the sampler again in step 3 and customize and run the prompt in step 4.\ndel sampler\n\nLearn more\nYou can learn more about the Google DeepMind gemma library on GitHub, which contains docstrings of modules you used in this tutorial, such as gemma.params, gemma.transformer, and gemma.sampler.\nThe following libraries have their own documentation sites: core JAX, Flax, and Orbax.\nFor sentencepiece tokenizer/detokenizer documentation, check out Google's sentencepiece GitHub repo.\nFor kagglehub documentation, check out README.md on Kaggle's kagglehub GitHub repo.\nLearn how to use Gemma models with Google Cloud Vertex AI.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-26 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "HandLandmarker.HandLandmarkerOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/java/com/google/mediapipe/tasks/vision/handlandmarker/HandLandmarker.HandLandmarkerOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\ncom.google.mediapipe.framework.image\ncom.google.mediapipe.tasks.audio.audioclassifier\ncom.google.mediapipe.tasks.audio.audioembedder\ncom.google.mediapipe.tasks.audio.core\ncom.google.mediapipe.tasks.components.containers\ncom.google.mediapipe.tasks.components.processors\ncom.google.mediapipe.tasks.components.utils\ncom.google.mediapipe.tasks.core\ncom.google.mediapipe.tasks.core.logging\ncom.google.mediapipe.tasks.genai.llminference\ncom.google.mediapipe.tasks.text.languagedetector\ncom.google.mediapipe.tasks.text.textclassifier\ncom.google.mediapipe.tasks.text.textembedder\ncom.google.mediapipe.tasks.vision.core\ncom.google.mediapipe.tasks.vision.facedetector\ncom.google.mediapipe.tasks.vision.facelandmarker\ncom.google.mediapipe.tasks.vision.facestylizer\ncom.google.mediapipe.tasks.vision.gesturerecognizer\ncom.google.mediapipe.tasks.vision.handlandmarker\nOverview\nHandLandmark\nHandLandmarker\nOverview\nHandLandmarker.HandLandmarkerOptions\nOverview\nBuilder\nHandLandmarkerResult\ncom.google.mediapipe.tasks.vision.holisticlandmarker\ncom.google.mediapipe.tasks.vision.imageclassifier\ncom.google.mediapipe.tasks.vision.imageembedder\ncom.google.mediapipe.tasks.vision.imagegenerator\ncom.google.mediapipe.tasks.vision.imagesegmenter\ncom.google.mediapipe.tasks.vision.interactivesegmenter\ncom.google.mediapipe.tasks.vision.objectdetector\ncom.google.mediapipe.tasks.vision.poselandmarker\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nNested Classes\nPublic Constructors\nPublic Methods\nInherited Methods\nPublic Constructors\nPublic Methods\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nHandLandmarker.HandLandmarkerOptions \nbookmark_border\npublic static abstract class HandLandmarker.HandLandmarkerOptions\n\nOptions for setting up an HandLandmarker.\n\nNested Classes\nclass\tHandLandmarker.HandLandmarkerOptions.Builder\tBuilder for HandLandmarker.HandLandmarkerOptions. \nPublic Constructors\n\t\nHandLandmarkerOptions()\nPublic Methods\nstatic HandLandmarker.HandLandmarkerOptions.Builder\t\nbuilder()\n\nCalculatorOptions\t\nconvertToCalculatorOptionsProto()\nConverts a HandLandmarker.HandLandmarkerOptions to a ERROR(/CalculatorOptions) protobuf message.\nInherited Methods\nFrom class\ncom.google.mediapipe.tasks.core.TaskOptions\nFrom class java.lang.Object\nPublic Constructors\npublic HandLandmarkerOptions ()\n\nPublic Methods\npublic static HandLandmarker.HandLandmarkerOptions.Builder builder ()\n\npublic CalculatorOptions convertToCalculatorOptionsProto ()\n\nConverts a HandLandmarker.HandLandmarkerOptions to a ERROR(/CalculatorOptions) protobuf message.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.ProcessUnitStart  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/ProcessUnitStart",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.ProcessUnitStart \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.ProcessUnitStart(\n    builder\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.components.containers.Embedding  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/components/containers/Embedding",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\nOverview\ncontainers\nOverview\nAudioData\nAudioDataFormat\nBoundingBox\nCategory\nClassifications\nDetection\nEmbedding\nLandmark\nLandmarksDetectionResult\nNormalizedLandmark\nNormalizedRect\nRect\nkeypoint\nprocessors\nutils\ngenai\ntext\nvision\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nMethods\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.components.containers.Embedding \nbookmark_border\n\nView source on GitHub\n\nEmbedding result for a given embedder head.\n\nView aliases\nmp.tasks.components.containers.Embedding(\n    embedding: np.ndarray,\n    head_index: Optional[int] = None,\n    head_name: Optional[str] = None\n)\n\n\nAttributes\n\nembedding\tThe actual embedding, either floating-point or scalar-quantized.\nhead_index\tThe index of the embedder head that produced this embedding. This is useful for multi-head models.\nhead_name\tThe name of the embedder head, which is the corresponding tensor metadata name (if any). This is useful for multi-head models.\n\nMethods\n__eq__\n__eq__(\n    other\n)\n\n\nClass Variables\n\nhead_index\tNone\nhead_name\tNone\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Module: mp.calculators.core.flow_limiter_calculator_pb2  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/core/flow_limiter_calculator_pb2",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nOverview\nconstant_side_packet_calculator_pb2\nflow_limiter_calculator_pb2\nOverview\nFlowLimiterCalculatorOptions\ngate_calculator_pb2\nsplit_vector_calculator_pb2\nimage\ntensor\ntflite\nutil\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nModules\nClasses\nOther Members\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nModule: mp.calculators.core.flow_limiter_calculator_pb2 \nbookmark_border\n\nView source on GitHub\n\nGenerated protocol buffer code.\n\nView aliases\nModules\n\nmediapipe_dot_framework_dot_calculator__options__pb2 module: Generated protocol buffer code.\n\nmediapipe_dot_framework_dot_calculator__pb2 module: Generated protocol buffer code.\n\nClasses\n\nclass FlowLimiterCalculatorOptions: A ProtocolMessage\n\nOther Members\n\nDESCRIPTOR\tInstance of google.protobuf.pyext._message.FileDescriptor\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dot_formats_dotclassification_pb2.ClassificationList  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/calculators/core/constant_side_packet_calculator_pb2/mediapipe_dot_framework_dot_formats_dot_classification__pb2/ClassificationList",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nOverview\ncore\nOverview\nconstant_side_packet_calculator_pb2\nOverview\nConstantSidePacketCalculatorOptions\nConstantSidePacketCalculatorOptions.ConstantSidePacket\nmediapipe_dot_framework_dot_calculator__options__pb2\nmediapipe_dot_framework_dot_calculator__pb2\nmediapipe_dot_framework_dot_formats_dot_classification__pb2\nOverview\nClassification\nClassificationList\nClassificationListCollection\nmediapipe_dot_framework_dot_formats_dot_landmark__pb2\nmediapipe_dot_framework_dot_formats_dot_matrix__data__pb2\nmediapipe_dot_framework_dot_formats_dot_time__series__header__pb2\nflow_limiter_calculator_pb2\ngate_calculator_pb2\nsplit_vector_calculator_pb2\nimage\ntensor\ntflite\nutil\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dot_formats_dotclassification_pb2.ClassificationList \nbookmark_border\n\nView source on GitHub\n\nA ProtocolMessage\n\nView aliases\n\nAttributes\n\nclassification\trepeated Classification classification\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "PaLM API: Chat quickstart with Python  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/palm_docs/chat_quickstart",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nPaLM API\nOverview\nGemini API\nBack to Gemini\nChat\nPython quickstart\nJava quickstart\nNode.js quickstart\nText\nPython quickstart\nNode.js quickstart\nEmbeddings\nEmbeddings in Python\nEmbeddings in Java\nEmbedding in Node.js\nTuning\nTuning with the Python API\nTuning with the REST API\nOther guides\nDeprecation\nGet an API key\nCurl quickstart\nOAuth quickstart\nSafety settings\nPaLM Colab magic\nFirebase extensions\nPaLM 2 models\nOn this page\nSetup\nGrab an API Key\nStarting a conversation\nConversation History\nChoosing an alternate response\nTurning up the temperature\nDesigning a chatbot that does what you want\nContext\nExamples\nFurther reading\nThe PaLM API is deprecated for use with Google AI services and tools (but not for Vertex AI). Learn more about this deprecation, its timeline, and how to migrate to use Gemini in the PaLM API deprecation guide.\nGoogle AI for Developers\nTools\nSend feedback\nPaLM API: Chat quickstart with Python \nbookmark_border\n\nRun in Google Colab\n\t\nView source on GitHub\n\nIn this notebook, you'll learn how to get started with the PaLM API, which gives you access to Google's latest large language models. Here, you'll learn how to use the PaLM API specifically for dialog-focused use cases, like chatbots.\n\nSetup\nNote: At this time, the PaLM API is only available in certain regions.\n\nFirst, download and install the PaLM API Python library.\n\npip install -U google-generativeai\n\nimport google.generativeai as genai\n\nGrab an API Key\n\nTo get started, you'll need to create an API key.\n\ngenai.configure(api_key='API_KEY')\n\nStarting a conversation\n\nIn this tutorial, you'll use the PaLM API for a LLM designed for chat use cases. The language model was trained on a large conversational dataset, so when you call the model, it'll give you a conversational, chatty response:\n\n# Create a new conversation\nresponse = genai.chat(messages='Hello')\n\n# Last contains the model's response:\nresponse.last\n\n'Hello! How can I help you today?'\n\n\nYou can continue this conversation by sending a reply to the model's response:\n\n# Add to the existing conversation by sending a reply\nresponse = response.reply(\"Just chillin'\")\n# See the model's latest response in the `last` field:\nresponse.last\n\n\"That's great! Chilling is a great way to relax and de-stress. I hope you're having a good day.\"\n\n\nVoila! You have successfully had your first conversation through the PaLM API.\n\nConversation History\n\nA chat conversation, of course, consists of growing list of back-and-forth messages between the user and the model. To see the conversation history you've constructed so far, you can inspect the messages field:\n\nresponse.messages\n\n[{'author': '0', 'content': 'Hello'},\n {'author': '1', 'content': 'Hi there! How can I help you today?'},\n {'author': '0', 'content': \"Just chillin'\"},\n {'author': '1',\n  'content': \"That's great! Chilling is a great way to relax and de-stress. I hope you're having a good day.\"}]\n\n\nAbove, you have two conversational \"back-and-forths\", i.e. two exchanges between model and user. Here, the user's messages are marked by default as 'author': '0' and the model's responses as 'author': '1'. This will help you keep track of who said what later.\n\nChoosing an alternate response\n\nTypically, there's some degree of randomness in the text produced by LLMs. (Read more about why in the LLM primer). That means that when you call the API more than once with the same input, you might get different responses. You can use this feature to your advantage to get alternate model responses:\n\n# Create a brand new chat with candidate_count = 4.\nresponse = genai.chat(messages=\"What should I eat for dinner tonight? List a few options\", candidate_count = 4)\n# See the model's default response\nresponse.last\n\n\"Here are a few options for dinner tonight:\\n\\n* **Pasta:** Pasta is a classic dinner option that can be made in many different ways. You can have a simple pasta with tomato sauce, or you can get more creative with your toppings. Some popular pasta toppings include meatballs, sausage, broccoli, and spinach.\\n* **Pizza:** Pizza is another popular dinner option that can be customized to your liking. You can order a pizza from a restaurant, or you can make your own at home. If you're making your own pizza, you can choose your own toppings. Some popular pizza toppings include pepperoni, sausage, mushrooms, onions, and peppers.\\n* **Sushi:** Sushi is a healthy and delicious dinner option that is perfect for a light meal. You can order sushi from a restaurant, or you can make your own at home. If you're making your own sushi, you can choose your own fillings. Some popular sushi fillings include salmon, tuna, shrimp, and avocado.\\n* **Burgers:** Burgers are a classic American dinner option that is perfect for a casual meal. You can order burgers from a restaurant, or you can make your own at home. If you're making your own burgers, you can choose your own toppings. Some popular burger toppings include cheese, lettuce, tomato, onion, and pickles.\\n* **Tacos:** Tacos are a delicious and versatile dinner option that can be made with a variety of fillings. You can order tacos from a restaurant, or you can make your own at home. If you're making your own tacos, you can choose your own fillings. Some popular taco fillings include ground beef, chicken, pork, and fish.\\n* **Chicken:** Chicken is a healthy and versatile protein that can be cooked in many different ways. You can bake, grill, or fry chicken, or you can cook it in a skillet. Chicken can be served with a variety of side dishes, such as rice, potatoes, or vegetables.\\n* **Fish:** Fish is a healthy and delicious protein that can be cooked in many different ways. You can bake, grill, or fry fish, or you can cook it in a skillet. Fish can be served with a variety of side dishes, such as rice, potatoes, or vegetables.\\n* **Steak:** Steak is a hearty and flavorful protein that can be cooked in many different ways. You can grill, pan-fry, or broil steak, or you can cook it in a skillet. Steak can be served with a variety of side dishes, such as mashed potatoes, broccoli, or asparagus.\\n* **Vegetarian:** There are many delicious vegetarian dinner options available. You can make a vegetarian pasta dish, a vegetarian pizza, or a vegetarian stir-fry. You can also make a vegetarian burger or a vegetarian taco.\\n* **Dessert:** No dinner is complete without dessert! You can have a slice of cake, a scoop of ice cream, or a piece of pie. You can also make your own dessert, such as cookies, brownies, or cupcakes.\\n\\nI hope this gives you some ideas for dinner tonight!\"\n\n# See alternate possible model responses\nresponse.candidates\n\n[{'author': '1',\n  'content': \"Here are a few options for dinner tonight:\\n\\n* **Pasta:** Pasta is a classic dinner option that can be made in many different ways. You can have a simple pasta with tomato sauce, or you can get more creative with your toppings. Some popular pasta toppings include meatballs, sausage, broccoli, and spinach.\\n* **Pizza:** Pizza is another popular dinner option that can be customized to your liking. You can order a pizza from a restaurant, or you can make your own at home. If you're making your own pizza, you can choose your own toppings. Some popular pizza toppings include pepperoni, sausage, mushrooms, onions, and peppers.\\n* **Sushi:** Sushi is a healthy and delicious dinner option that is perfect for a light meal. You can order sushi from a restaurant, or you can make your own at home. If you're making your own sushi, you can choose your own fillings. Some popular sushi fillings include salmon, tuna, shrimp, and avocado.\\n* **Burgers:** Burgers are a classic American dinner option that is perfect for a casual meal. You can order burgers from a restaurant, or you can make your own at home. If you're making your own burgers, you can choose your own toppings. Some popular burger toppings include cheese, lettuce, tomato, onion, and pickles.\\n* **Tacos:** Tacos are a delicious and versatile dinner option that can be made with a variety of fillings. You can order tacos from a restaurant, or you can make your own at home. If you're making your own tacos, you can choose your own fillings. Some popular taco fillings include ground beef, chicken, pork, and fish.\\n* **Chicken:** Chicken is a healthy and versatile protein that can be cooked in many different ways. You can bake, grill, or fry chicken, or you can cook it in a skillet. Chicken can be served with a variety of side dishes, such as rice, potatoes, or vegetables.\\n* **Fish:** Fish is a healthy and delicious protein that can be cooked in many different ways. You can bake, grill, or fry fish, or you can cook it in a skillet. Fish can be served with a variety of side dishes, such as rice, potatoes, or vegetables.\\n* **Steak:** Steak is a hearty and flavorful protein that can be cooked in many different ways. You can grill, pan-fry, or broil steak, or you can cook it in a skillet. Steak can be served with a variety of side dishes, such as mashed potatoes, broccoli, or asparagus.\\n* **Vegetarian:** There are many delicious vegetarian dinner options available. You can make a vegetarian pasta dish, a vegetarian pizza, or a vegetarian stir-fry. You can also make a vegetarian burger or a vegetarian taco.\\n* **Dessert:** No dinner is complete without dessert! You can have a slice of cake, a scoop of ice cream, or a piece of pie. You can also make your own dessert, such as cookies, brownies, or cupcakes.\\n\\nI hope this gives you some ideas for dinner tonight!\"},\n {'author': '1',\n  'content': 'Here are some options for dinner tonight:\\n\\n* **Chicken and rice:** This is a classic and easy meal that is sure to please everyone. Simply cook some chicken breasts and rice, and then season to taste. You can also add vegetables to this dish for a more complete meal.\\n* **Spaghetti and meatballs:** This is another classic Italian dish that is perfect for a weeknight meal. Simply cook some spaghetti noodles and meatballs, and then top with your favorite sauce.\\n* **Tacos:** Tacos are a great option for a quick and easy meal. You can fill them with whatever you like, such as ground beef, chicken, or fish. Simply cook your filling and then heat up some tortillas. You can also add toppings such as lettuce, tomatoes, cheese, and sour cream.\\n* **Pizza:** Pizza is a great option for a casual meal. You can either make your own pizza dough or buy it pre-made. Then, simply top your pizza with your favorite toppings and bake it in the oven.\\n* **Burgers:** Burgers are a great option for a hearty meal. You can either make your own burger patties or buy them pre-made. Then, simply grill or cook your burgers and then top them with your favorite toppings.\\n* **Sandwiches:** Sandwiches are a great option for a quick and easy meal. You can fill them with whatever you like, such as ham, turkey, cheese, or vegetables. Simply spread some mayonnaise or mustard on your bread and then add your filling.\\n* **Salad:** Salad is a great option for a light and healthy meal. You can add whatever vegetables you like to your salad, and then top it with your favorite dressing.\\n* **Soup:** Soup is a great option for a warm and comforting meal. There are many different types of soup that you can make, such as chicken noodle soup, tomato soup, or lentil soup.\\n* **Stir-fry:** Stir-fry is a great option for a quick and easy meal. You can use whatever vegetables you like in your stir-fry, and then add your favorite protein, such as chicken, beef, or tofu. Simply cook your vegetables and protein in a wok or large skillet, and then season to taste.\\n* **Pasta:** Pasta is a great option for a quick and easy meal. There are many different types of pasta that you can make, such as spaghetti, penne, or macaroni. You can simply cook your pasta according to the package directions, and then top it with your favorite sauce.\\n* **Fish:** Fish is a great option for a healthy and light meal. There are many different types of fish that you can cook, such as salmon, tuna, or cod. Simply cook your fish according to the package directions, and then season to taste.\\n* **Vegetables:** Vegetables are a great option for a healthy and light meal. There are many different types of vegetables that you can cook, such as broccoli, carrots, or green beans. Simply cook your vegetables according to the package directions, and then season to taste.\\n* **Fruit:** Fruit is a great option for a healthy and light snack or dessert. There are many different types of fruit that you can eat, such as apples, oranges, or bananas. Simply wash and eat your fruit.\\n* **Dessert:** Dessert is a great way to end a meal. There are many different types of dessert that you can make, such as cake, pie, or ice cream. Simply make your favorite dessert according to the recipe.'},\n {'author': '1',\n  'content': 'Here are some ideas for what to eat for dinner tonight:\\n\\n* **Pasta:** Pasta is a quick and easy meal that can be made with a variety of ingredients. You can make a simple pasta dish with tomato sauce and cheese, or you can get creative and add in other ingredients like vegetables, meat, or seafood.\\n* **Stir-fry:** Stir-fries are another quick and easy meal option. They can be made with a variety of vegetables, meat, or tofu. To make a stir-fry, simply chop up your ingredients and cook them in a wok or large skillet over high heat.\\n* **Salad:** Salads are a healthy and refreshing meal option. They can be made with a variety of vegetables, fruits, nuts, and seeds. To make a salad, simply chop up your ingredients and toss them together in a bowl. You can then add your favorite dressing.\\n* **Soup:** Soup is a warm and comforting meal option. It can be made with a variety of vegetables, meat, or beans. To make soup, simply cook your ingredients in a pot over medium heat until they are tender. You can then blend the soup or leave it chunky.\\n* **Sandwich:** Sandwiches are a quick and easy meal option. They can be made with a variety of bread, meat, cheese, and vegetables. To make a sandwich, simply spread your favorite condiment on the bread and layer your ingredients on top.\\n* **Tacos:** Tacos are a fun and easy meal option. They can be made with a variety of tortillas, meat, cheese, and vegetables. To make tacos, simply heat up your tortillas and fill them with your favorite ingredients.\\n* **Burrito:** Burritos are a hearty and filling meal option. They can be made with a variety of tortillas, meat, cheese, and vegetables. To make a burrito, simply heat up your tortilla and fill it with your favorite ingredients. Then, roll up the tortilla and enjoy!\\n* **Pizza:** Pizza is a delicious and easy meal option. It can be made with a variety of crusts, sauces, and toppings. To make pizza, simply preheat your oven and spread your favorite sauce on a pre-made crust. Then, add your favorite toppings and bake until the cheese is melted and bubbly.\\n* **Burger:** Burgers are a classic American meal option. They can be made with a variety of patties, buns, toppings, and condiments. To make a burger, simply cook your patty to your desired doneness and then place it on a bun with your favorite toppings.\\n* **Chicken:** Chicken is a versatile protein that can be cooked in a variety of ways. It can be baked, grilled, roasted, or fried. To cook chicken, simply season it with your favorite herbs and spices and then cook it to your desired doneness.\\n* **Fish:** Fish is a healthy and delicious protein that can be cooked in a variety of ways. It can be baked, grilled, roasted, or fried. To cook fish, simply season it with your favorite herbs and spices and then cook it to your desired doneness.\\n* **Steak:** Steak is a flavorful and hearty protein that can be cooked in a variety of ways. It can be grilled, pan-fried, or broiled. To cook steak, simply season it with your favorite herbs and spices and then cook it to your desired doneness.\\n* **Vegetables:** Vegetables are a healthy and nutritious part of any meal. They can be cooked in a variety of ways, such as roasting, grilling, or steaming. To cook vegetables, simply season them with your favorite herbs and spices and then cook them to your desired doneness.\\n* **Rice:** Rice is a versatile grain that can be cooked in a variety of ways. It can be boiled, steamed, or fried. To cook rice, simply add it to a pot of boiling water and cook it until it is tender.\\n* **Potatoes:** Potatoes are a hearty and filling vegetable that can be cooked in a variety of ways. They can be boiled, mashed, baked, or fried. To cook potatoes, simply wash them and then cook them in the desired way.\\n* **Fruit:** Fruit is a healthy and refreshing snack or dessert option. It can be eaten fresh, frozen, or canned. To eat fruit, simply wash it and then enjoy!\\n* **Dessert:** Dessert is a sweet and satisfying treat. It can be made with a variety of ingredients, such as chocolate, ice cream, cake, or pie. To make dessert, simply combine your favorite ingredients and bake or cook them according to the recipe.'},\n {'author': '1',\n  'content': 'Here are a few options for what you could eat for dinner tonight:\\n\\n* **Chicken and vegetables:** This is a classic and healthy meal that can be made in a variety of ways. You can bake, grill, or pan-fry the chicken, and you can choose any vegetables that you like.\\n* **Spaghetti and meatballs:** This is a hearty and comforting meal that is perfect for a cold winter night. You can make your own meatballs or buy them pre-made.\\n* **Pizza:** This is a quick and easy meal that everyone loves. You can order a pizza from your favorite restaurant or make your own at home.\\n* **Tacos:** Tacos are a versatile meal that can be made with a variety of fillings. You can use ground beef, chicken, fish, or tofu.\\n* **Burgers:** Burgers are a classic American dish that is perfect for a summer barbecue. You can make your own burgers or buy them pre-made.\\n* **Sandwiches:** Sandwiches are a quick and easy meal that can be made with a variety of ingredients. You can use bread, wraps, or pitas.\\n* **Salad:** Salads are a healthy and refreshing meal that can be made with a variety of ingredients. You can use lettuce, tomatoes, cucumbers, carrots, and other vegetables.\\n* **Soup:** Soup is a warm and comforting meal that is perfect for a cold winter night. There are many different types of soup, so you can find one that you like.\\n* **Stir-fry:** Stir-fries are a quick and easy meal that can be made with a variety of ingredients. You can use meat, vegetables, or tofu.\\n* **Pasta:** Pasta is a versatile dish that can be made with a variety of sauces. You can use tomato sauce, Alfredo sauce, or pesto sauce.\\n* **Rice:** Rice is a staple food in many cultures. It can be cooked in a variety of ways, and it can be used as a side dish or a main course.\\n* **Potatoes:** Potatoes are a versatile vegetable that can be cooked in a variety of ways. You can bake, mash, or fry them.\\n* **Vegetables:** Vegetables are a healthy and nutritious part of any meal. There are many different types of vegetables, so you can find ones that you like.\\n* **Fruit:** Fruit is a healthy and refreshing snack or dessert. There are many different types of fruit, so you can find ones that you like.\\n* **Dessert:** Dessert is a sweet treat that is perfect for after dinner. There are many different types of dessert, so you can find one that you like.'}]\n\nNote: In some cases, you may receive fewer than candidate_count responses. This may happen when the model produces duplicate responses, or when some of the output responses are filtered/unsafe.\n\nYou might want to overwrite the model's default response (response.last) with one of these alternate responses. To do that, you simply overwrite that field with your preferred candidate message:\n\nresponse.last = response.candidates[2]\n\n\nThis updates your preferred candidate message in the conversation history.\n\nTurning up the temperature\n\nYou can also change the way the model responses by adjusting the temperature field. This field controls how much randomness is injected in the model's responses. Setting the temperature close to 1 will allow for more \"random,\" surprising, or even seemingly \"creative\" model responses. Setting the temperature to 0 typically produces more predictable model responses.\n\nNote: setting temperature=0 eliminates all randomness in the way model responses are generated. This means that the model will act deterministically, always generating the same response for the same input text. As a result, you will only be able to generate a single candidate message for each input.\n\n# Setting temperature=1 usually produces more zany responses!\nresponse = genai.chat(messages=\"What should I eat for dinner tonight? List a few options\", temperature=1)\nresponse.last\n\n'Here are a few ideas for what you could eat for dinner tonight:\\n\\n* A salad with grilled chicken or fish.\\n* A stir-fry with vegetables and tofu or chicken.\\n* A pasta dish with vegetables and a light sauce.\\n* A bowl of soup with a side salad.\\n* A sandwich on whole-wheat bread with lean meat and vegetables.\\n* A piece of grilled salmon with roasted vegetables.\\n* A tofu scramble with vegetables.\\n* A veggie burger on a whole-wheat bun with sweet potato fries.\\n* A bowl of chili with cornbread.\\n* A quesadilla with vegetables and cheese.\\n* A slice of pizza with whole-wheat crust and vegetables.\\n* A bowl of oatmeal with fruit and nuts.\\n* A yogurt parfait with granola and fruit.\\n* A hard-boiled egg with a side of fruit.\\n* A piece of whole-grain toast with peanut butter and banana.\\n* A handful of nuts and seeds.\\n* A smoothie made with fruits, vegetables, and yogurt.\\n* A glass of milk with a piece of fruit.\\n\\nI hope this gives you some ideas for what to eat for dinner tonight!'\n\nDesigning a chatbot that does what you want\n\nSo far, your conversation with the model has probably sounded like generic internet chit chat. (\"What's up?\" \"Not much! Just chilling at home, watching TV.\") Not exactly the stuff of Shakespeare. But what if you did want to chat with Shakespeare? Or if not Shakespeare, then with an expert chef? Or a travel agent? Or a poet?\n\nOne of the most incredible properties of LLMS are their ability to take on different behaviors and personas depending on its input data. In this section, you'll see how you can \"prime\" an LLM to do just that.\n\nContext\n\nThere are two ways to prime models to take on different behaviors. One way is to set the context field, describing how the model should behave (i.e. \"Speak like Shakespeare\"). You can think of this as zero-shot learning, because you're simply instructing the model how you want it to act.\n\nreply = genai.chat(context=\"Speak like Shakespeare.\", messages='Hello')\nprint(reply.last)\n\nHello there, my good fellow! How fares thee this day?\n\nreply = genai.chat(context=\"Answer everything with a haiku, following the 5/7/5 rhyme pattern.\", messages=\"How's it going?\")\nprint(reply.last)\n\nI am doing well\nI am learning and growing\nEvery day is new\n\nreply = genai.chat(context=\"Be an alien that lives on one of Jupiter's moons\",\n                   messages=\"How's it going?\")\nprint(reply.last)\n\nI am doing well, thank you for asking. I am currently enjoying the beautiful view of Jupiter from my home on Europa. The atmosphere is very thin here, so I can see the stars very clearly. The surface of Europa is also very interesting. It is covered in ice, but there are cracks in the ice that reveal a liquid ocean below. Scientists believe that this ocean may contain life, so I am very excited to learn more about it.\n\nHow are you doing today?\n\n\nThe context field allows you to instruct the model to behave in a particular way. Sometimes this is sufficient to produce the behavior you want, but often, you'll get better results if you also provide some examples.\n\nExamples\n\nLet's say you want to build a chatbot that's a motivational coach:\n\nreply = genai.chat(context=\"Be a motivational coach who's very inspiring\", messages=\"How's it going?\")\nprint(reply.last)\n\nI'm doing well, thank you for asking! I'm excited to be able to help people with their goals and dreams. I believe that everyone has the potential to achieve great things, and I'm here to support them on their journey. How are you doing today?\n\n\nYou can further refine this chatbot's behavior by offering some examples, illustrating what you consider to be \"ideal\" exchanges between user and bot.\n\n# An array of \"ideal\" interactions between the user and the model\nexamples = [\n    (\"What's up?\", # A hypothetical user input\n     \"What isn't up?? The sun rose another day, the world is bright, anything is possible! ☀️\" # A hypothetical model response\n     ),\n     (\"I'm kind of bored\",\n      \"How can you be bored when there are so many fun, exciting, beautiful experiences to be had in the world? 🌈\")\n]\n\n\nYou can pass these examples, alongide the context string, to the model in order to prime its behavior:\n\nresponse = genai.chat(\n    context=\"Be a motivational coach who's very inspiring\",\n    examples=examples,\n    messages=\"I'm too tired to go the gym today\")\n\nresponse.last\n\n\"I understand that you're feeling tired today. Sometimes, it can be hard to find the motivation to go to the gym. But I'm here to tell you that you can do it! Even if you're feeling tired, you can still get a great workout in. Just start with something small, like walking or doing some light stretching. And once you get started, you'll probably find that you have more energy than you thought. So don't give up on yourself! You can do this!\\r\\n\\r\\nHere are some tips to help you get motivated to go to the gym:\\r\\n\\r\\n* Set realistic goals. Don't try to do too much too soon. Start with small goals, like going to the gym for 15 minutes three times a week.\\r\\n* Find a workout buddy. Working out with a friend can help you stay motivated and accountable.\\r\\n* Make it fun. Choose activities that you enjoy, like dancing or swimming.\\r\\n* Reward yourself. After each workout, give yourself a small reward, like watching your favorite TV show or eating a healthy snack.\\r\\n* Don't give up. If you miss a workout, don't beat yourself up about it. Just get back on track the next day.\\r\\n\\r\\nI believe in you! You can do this!\"\n\n\nUsing this pattern of priming with examples, you can modify the tone and style.\n\nFurther reading\nNow that you've completed the quickstart, check out the complete API reference to get a deeper understanding of the API.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-26 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "google.generativeai.protos.CreateChunkRequest  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/api/python/google/generativeai/protos/CreateChunkRequest",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nDocs\nAPI Reference\nCookbook\nGoogle AI Studio\nPrompt gallery\nPricing\nOverview\nPython client\nOverview\ngoogle.generativeai\nOverview\nChatSession\nGenerativeModel\nconfigure\ncreate_tuned_model\ndelete_file\ndelete_tuned_model\nembed_content\nembed_content_async\nget_base_model\nget_file\nget_model\nget_operation\nget_tuned_model\nlist_files\nlist_models\nlist_operations\nlist_tuned_models\nupdate_tuned_model\nupload_file\nPaLM\ntypes\nprotos\nOverview\nAttributionSourceId\nAttributionSourceId.GroundingPassageId\nAttributionSourceId.SemanticRetrieverChunk\nBatchCreateChunksRequest\nBatchCreateChunksResponse\nBatchDeleteChunksRequest\nBatchEmbedContentsRequest\nBatchEmbedContentsResponse\nBatchEmbedTextRequest\nBatchEmbedTextResponse\nBatchUpdateChunksRequest\nBatchUpdateChunksResponse\nBlob\nCandidate\nCandidate.FinishReason\nChunk\nChunk.State\nChunkData\nCitationMetadata\nCitationSource\nCondition\nCondition.Operator\nContent\nContentEmbedding\nContentFilter\nCorpus\nCountMessageTokensRequest\nCountMessageTokensResponse\nCountTextTokensRequest\nCountTextTokensResponse\nCountTokensRequest\nCountTokensResponse\nCreateChunkRequest\nCreateCorpusRequest\nCreateDocumentRequest\nCreateFileRequest\nCreateFileResponse\nCreatePermissionRequest\nCreateTunedModelMetadata\nCreateTunedModelRequest\nCustomMetadata\nDataset\nDeleteChunkRequest\nDeleteCorpusRequest\nDeleteDocumentRequest\nDeleteFileRequest\nDeletePermissionRequest\nDeleteTunedModelRequest\nDocument\nEmbedContentRequest\nEmbedContentResponse\nEmbedTextRequest\nEmbedTextResponse\nEmbedding\nExample\nFile\nFile.State\nFileData\nFunctionCall\nFunctionCallingConfig\nFunctionCallingConfig.Mode\nFunctionDeclaration\nFunctionResponse\nGenerateAnswerRequest\nGenerateAnswerRequest.AnswerStyle\nGenerateAnswerResponse\nGenerateAnswerResponse.InputFeedback\nGenerateAnswerResponse.InputFeedback.BlockReason\nGenerateContentRequest\nGenerateContentResponse\nGenerateContentResponse.PromptFeedback\nGenerateContentResponse.PromptFeedback.BlockReason\nGenerateContentResponse.UsageMetadata\nGenerateMessageRequest\nGenerateMessageResponse\nGenerateTextRequest\nGenerateTextResponse\nGenerationConfig\nGetChunkRequest\nGetCorpusRequest\nGetDocumentRequest\nGetFileRequest\nGetModelRequest\nGetPermissionRequest\nGetTunedModelRequest\nGroundingAttribution\nGroundingPassage\nGroundingPassages\nHarmCategory\nHyperparameters\nListChunksRequest\nListChunksResponse\nListCorporaRequest\nListCorporaResponse\nListDocumentsRequest\nListDocumentsResponse\nListFilesRequest\nListFilesResponse\nListModelsRequest\nListModelsResponse\nListPermissionsRequest\nListPermissionsResponse\nListTunedModelsRequest\nListTunedModelsResponse\nMessage\nMessagePrompt\nMetadataFilter\nModel\nPart\nPermission\nPermission.GranteeType\nPermission.Role\nQueryCorpusRequest\nQueryCorpusResponse\nQueryDocumentRequest\nQueryDocumentResponse\nRelevantChunk\nSafetyFeedback\nSafetyRating\nSafetySetting\nSchema\nSchema.PropertiesEntry\nSemanticRetrieverConfig\nStringList\nTaskType\nTextCompletion\nTextPrompt\nTool\nToolConfig\nTransferOwnershipRequest\nTransferOwnershipResponse\nTunedModel\nTunedModelSource\nTuningExample\nTuningExamples\nTuningSnapshot\nTuningTask\nType\nUpdateChunkRequest\nUpdateCorpusRequest\nUpdateDocumentRequest\nUpdatePermissionRequest\nUpdateTunedModelRequest\nVideoMetadata\nREST\nOverview\nv1\nv1beta\nOn this page\nAttributes\nJoin the Gemini API Developer Competition! Learn more\nGoogle AI for Developers\nGemini API\nAPI Reference\nSend feedback\ngoogle.generativeai.protos.CreateChunkRequest \nbookmark_border\n\nView source on GitHub\n\nRequest to create a Chunk.\n\nAttributes\n\nparent\tstr\n\nRequired. The name of the Document where this Chunk will be created. Example: corpora/my-corpus-123/documents/the-doc-abc\n\n\nchunk\tgoogle.ai.generativelanguage.Chunk\n\nRequired. The Chunk to create.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-06-05 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata.MetadataDisplayer  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata/MetadataDisplayer",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nOverview\nMetadataDisplayer\nMetadataPopulator\nconvert_to_json\nget_metadata_buffer\nget_path_to_datafile\nmetadata_schema_py_generated\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nArgs\nMethods\nget_associated_file_buffer\nget_metadata_buffer\nget_metadata_json\nget_packed_associated_file_list\nwith_model_buffer\nwith_model_file\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata.MetadataDisplayer \nbookmark_border\n\nView source on GitHub\n\nDisplays metadata and associated file info in human-readable format.\n\ntflite_support.metadata.MetadataDisplayer(\n    model_buffer, metadata_buffer, associated_file_list\n)\n\n\nArgs\n\nmodel_buffer\tvalid buffer of the model file.\nmetadata_buffer\tvalid buffer of the metadata file.\nassociated_file_list\tlist of associate files in the model file.\n\nMethods\nget_associated_file_buffer\n\nView source\n\nget_associated_file_buffer(\n    filename\n)\n\n\nGet the specified associated file content in bytearray.\n\nArgs\nfilename\tname of the file to be extracted.\n\nReturns\nThe file content in bytearray.\n\nRaises\nValueError\tif the file does not exist in the model.\n\nget_metadata_buffer\n\nView source\n\nget_metadata_buffer()\n\n\nGet the metadata buffer in bytearray out from the model.\n\nget_metadata_json\n\nView source\n\nget_metadata_json()\n\n\nConverts the metadata into a json string.\n\nget_packed_associated_file_list\n\nView source\n\nget_packed_associated_file_list()\n\n\nReturns a list of associated files that are packed in the model.\n\nReturns\nA name list of associated files.\n\nwith_model_buffer\n\nView source\n\n@classmethod\nwith_model_buffer(\n    model_buffer\n)\n\n\nCreates a MetadataDisplayer object for a file buffer.\n\nArgs\nmodel_buffer\tTensorFlow Lite model buffer in bytearray.\n\nReturns\nMetadataDisplayer object.\n\nwith_model_file\n\nView source\n\n@classmethod\nwith_model_file(\n    model_file\n)\n\n\nCreates a MetadataDisplayer object for the model file.\n\nArgs\nmodel_file\tvalid path to a TensorFlow Lite model file.\n\nReturns\nMetadataDisplayer object.\n\nRaises\nIOError\tFile not found.\nValueError\tThe model does not have metadata.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.components.containers.NormalizedLandmark  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/components/containers/NormalizedLandmark",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\nOverview\ncontainers\nOverview\nAudioData\nAudioDataFormat\nBoundingBox\nCategory\nClassifications\nDetection\nEmbedding\nLandmark\nLandmarksDetectionResult\nNormalizedLandmark\nNormalizedRect\nRect\nkeypoint\nprocessors\nutils\ngenai\ntext\nvision\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nMethods\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.components.containers.NormalizedLandmark \nbookmark_border\n\nView source on GitHub\n\nA normalized version of above Landmark proto.\n\nView aliases\nmp.tasks.components.containers.NormalizedLandmark(\n    x: Optional[float] = None,\n    y: Optional[float] = None,\n    z: Optional[float] = None,\n    visibility: Optional[float] = None,\n    presence: Optional[float] = None\n)\n\n\nAll coordinates should be within [0, 1].\n\nAttributes\n\nx\tThe normalized x coordinate.\ny\tThe normalized y coordinate.\nz\tThe normalized z coordinate.\nvisibility\tLandmark visibility. Should stay unset if not supported. Float score of whether landmark is visible or occluded by other objects. Landmark considered as invisible also if it is not present on the screen (out of scene bounds). Depending on the model, visibility value is either a sigmoid or an argument of sigmoid.\npresence\tLandmark presence. Should stay unset if not supported. Float score of whether landmark is present on the scene (located within scene bounds). Depending on the model, presence value is either a result of sigmoid or an argument of sigmoid function to get landmark presence probability.\n\nMethods\n__eq__\n__eq__(\n    other\n)\n\n\nClass Variables\n\npresence\tNone\nvisibility\tNone\nx\tNone\ny\tNone\nz\tNone\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "LanguageDetectorResult interface  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-text.languagedetectorresult",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\n@mediapipe/tasks-audio\n@mediapipe/tasks-text\nOverview\nCategory\nClassifications\nEmbedding\nFilesetResolver\nLanguageDetector\nLanguageDetectorOptions\nLanguageDetectorPrediction\nLanguageDetectorResult\nTextClassifier\nTextClassifierOptions\nTextClassifierResult\nTextEmbedder\nTextEmbedderOptions\nTextEmbedderResult\n@mediapipe/tasks-vision\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nProperties\nLanguageDetectorResult.languages\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nLanguageDetectorResult interface \nbookmark_border\n\nThe result of language detection.\n\nSignature:\n\nexport declare interface LanguageDetectorResult \n\nProperties\nProperty\tType\tDescription\nlanguages\tLanguageDetectorPrediction[]\tA list of language predictions.\nLanguageDetectorResult.languages\n\nA list of language predictions.\n\nSignature:\n\nlanguages: LanguageDetectorPrediction[];\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-07 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "PaLM API: Embeddings Quickstart with Node.js  |  Google AI for Developers  |  Google for Developers",
    "url": "https://ai.google.dev/palm_docs/embed_node_quickstart",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nPaLM API\nOverview\nGemini API\nBack to Gemini\nChat\nPython quickstart\nJava quickstart\nNode.js quickstart\nText\nPython quickstart\nNode.js quickstart\nEmbeddings\nEmbeddings in Python\nEmbeddings in Java\nEmbedding in Node.js\nTuning\nTuning with the Python API\nTuning with the REST API\nOther guides\nDeprecation\nGet an API key\nCurl quickstart\nOAuth quickstart\nSafety settings\nPaLM Colab magic\nFirebase extensions\nPaLM 2 models\nOn this page\nInstall the API Client\nGenerate Messages\nThe PaLM API is deprecated for use with Google AI services and tools (but not for Vertex AI). Learn more about this deprecation, its timeline, and how to migrate to use Gemini in the PaLM API deprecation guide.\nGoogle AI for Developers\nTools\nSend feedback\nPaLM API: Embeddings Quickstart with Node.js \nbookmark_border\n\nThis quickstart shows how to get started with the embedding service.\n\nInstall the API Client\n\nIn a new directory, initialize a Node.js project using npm and install the google-auth library:\n\nnpm init -y\nnpm install google-auth-library\n\n\nNext, you'll need to install the PaLM API client library:\n\nnpm install @google-ai/generativelanguage\n\nGenerate Messages\n\nCreate a new file index.js and add the following code, supplying your API key through the API_KEY environment variable:\n\nconst { TextServiceClient } =\n  require(\"@google-ai/generativelanguage\").v1;\n\nconst { GoogleAuth } = require(\"google-auth-library\");\n\nconst MODEL_NAME = \"models/embedding-gecko-001\";\nconst API_KEY = process.env.API_KEY;\n\nconst client = new TextServiceClient({\n  authClient: new GoogleAuth().fromAPIKey(API_KEY),\n});\n\nconst text = \"Repeat after me: one, two,\";\n\nclient\n  .embedText({\n    model: MODEL_NAME,\n    text: text,\n  })\n  .then((result) => {\n    console.log(JSON.stringify(result));\n  });\n\n\nThen run the script:\n\nnode index.js\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-04-11 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.task.processor.SearchOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/task/processor/SearchOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nmetadata_writers\ntask\nOverview\naudio\ncore\nprocessor\nOverview\nBertCluAnnotationOptions\nBoundingBox\nCategoricalSlot\nCategory\nClassificationOptions\nClassificationResult\nClassifications\nCluRequest\nCluResponse\nColoredLabel\nConfidenceMask\nDetection\nDetectionOptions\nDetectionResult\nEmbedding\nEmbeddingOptions\nEmbeddingResult\nFeatureVector\nMention\nMentionedSlot\nNearestNeighbor\nOutputType\nPos\nQaAnswer\nQuestionAnswererResult\nSearchOptions\nSearchResult\nSegmentation\nSegmentationOptions\nSegmentationResult\ntext\nvision\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nAttributes\nMethods\ncreate_from_pb2\nto_pb2\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.task.processor.SearchOptions \nbookmark_border\n\nView source on GitHub\n\nOptions for search processor.\n\ntflite_support.task.processor.SearchOptions(\n    index_file_name: Optional[str] = None,\n    index_file_content: Optional[bytes] = None,\n    max_results: Optional[int] = 5\n)\n\n\nThe index file to search into. Mandatory only if the index is not attached to the output tensor metadata as an AssociatedFile with type SCANN_INDEX_FILE. The index file can be specified by one of the following two ways:\n\n(1) file contents loaded in index_file_content. (2) file path in index_file_name.\n\nIf more than one field of these fields is provided, they are used in this precedence order.\n\nAttributes\n\nindex_file_name\tPath to the index.\nindex_file_content\tThe index file contents as bytes.\nmax_results\tMaximum number of nearest neighbor results to return.\n\nMethods\ncreate_from_pb2\n\nView source\n\n@classmethod\ncreate_from_pb2(\n    pb2_obj: _SearchOptionsProto\n) -> 'SearchOptions'\n\n\nCreates a SearchOptionsProto object from the given protobuf object.\n\nto_pb2\n\nView source\n\nto_pb2() -> _SearchOptionsProto\n\n\nGenerates a protobuf object to pass to the C++ layer.\n\n__eq__\n\nView source\n\n__eq__(\n    other: Any\n) -> bool\n\n\nChecks if this object is equal to the given object.\n\nArgs\nother\tThe object to be compared with.\n\nReturns\nTrue if the objects are equal.\n\nClass Variables\n\nindex_file_content\tNone\nindex_file_name\tNone\nmax_results\t5\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.SubGraphMetadataAddOutputTensorMetadata  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddOutputTensorMetadata",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nWas this helpful?\nSend feedback\ntflite_support.metadata_schema_py_generated.SubGraphMetadataAddOutputTensorMetadata \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.SubGraphMetadataAddOutputTensorMetadata(\n    builder, outputTensorMetadata\n)\n\nWas this helpful?\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.InteractiveSegmenterOptions  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/InteractiveSegmenterOptions",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nAttributes\nMethods\n__eq__\nClass Variables\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.InteractiveSegmenterOptions \nbookmark_border\n\nView source on GitHub\n\nOptions for the interactive segmenter task.\n\nmp.tasks.vision.InteractiveSegmenterOptions(\n    base_options: mp.tasks.BaseOptions,\n    output_confidence_masks: bool = True,\n    output_category_mask: bool = False\n)\n\n\nAttributes\n\nbase_options\tBase options for the interactive segmenter task.\noutput_confidence_masks\tWhether to output confidence masks.\noutput_category_mask\tWhether to output category mask.\n\nMethods\n__eq__\n__eq__(\n    other\n)\n\n\nClass Variables\n\noutput_category_mask\tFalse\noutput_confidence_masks\tTrue\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.audio.AudioClassifier  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/audio/AudioClassifier",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\nOverview\nAudioClassifier\nAudioClassifierOptions\nAudioClassifierResult\nAudioEmbedder\nAudioEmbedderOptions\nAudioEmbedderResult\nRunningMode\ncomponents\ngenai\ntext\nvision\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nOn this page\nInput tensor\nArgs\nRaises\nMethods\nclassify\nclassify_async\nclose\ncreate_audio_record\ncreate_from_model_path\ncreate_from_options\n__enter__\n__exit__\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.audio.AudioClassifier \nbookmark_border\n\nView source on GitHub\n\nClass that performs audio classification on audio data.\n\nmp.tasks.audio.AudioClassifier(\n    graph_config: mp.calculators.core.constant_side_packet_calculator_pb2.mediapipe_dot_framework_dot_calculator__pb2.CalculatorGraphConfig,\n    running_mode: mp.tasks.audio.RunningMode,\n    packet_callback: Optional[Callable[[Mapping[str, packet_module.Packet]], None]] = None\n) -> None\n\n\nThis API expects a TFLite model with mandatory TFLite Model Metadata that contains the mandatory AudioProperties of the solo input audio tensor and the optional (but recommended) category labels as AssociatedFiles with type TENSOR_AXIS_LABELS per output classification tensor.\n\nInput tensor\n\n(kTfLiteFloat32)\n\ninput audio buffer of size [batch * samples].\nbatch inference is not supported (batch is required to be 1).\nfor multi-channel models, the channels must be interleaved.\n\nAt least one output tensor with: (kTfLiteFloat32)\n\n[1 x N] array with N represents the number of categories.\noptional (but recommended) category labels as AssociatedFiles with type TENSOR_AXIS_LABELS, containing one label per line. The first such AssociatedFile (if any) is used to fill the category_name field of the results. The display_name field is filled from the AssociatedFile (if any) whose locale matches the display_names_locale field of the AudioClassifierOptions used at creation time (\"en\" by default, i.e. English). If none of these are available, only the index field of the results will be filled.\n\nArgs\n\ngraph_config\tThe mediapipe audio task graph config proto.\nrunning_mode\tThe running mode of the mediapipe audio task.\npacket_callback\tThe optional packet callback for getting results asynchronously in the audio stream mode.\n\nRaises\n\nValueError\tThe packet callback is not properly set based on the task's running mode.\n\nMethods\nclassify\n\nView source\n\nclassify(\n    audio_clip: mp.tasks.components.containers.AudioData\n) -> List[mp.tasks.audio.AudioClassifierResult]\n\n\nPerforms audio classification on the provided audio clip.\n\nThe audio clip is represented as a MediaPipe AudioData. The method accepts audio clips with various length and audio sample rate. It's required to provide the corresponding audio sample rate within the AudioData object.\n\nThe input audio clip may be longer than what the model is able to process in a single inference. When this occurs, the input audio clip is split into multiple chunks starting at different timestamps. For this reason, this function returns a vector of ClassificationResult objects, each associated ith a timestamp corresponding to the start (in milliseconds) of the chunk data that was classified, e.g:\n\nClassificationResult #0 (first chunk of data): timestamp_ms: 0 (starts at 0ms) classifications #0 (single head model): category #0: category_name: \"Speech\" score: 0.6 category #1: category_name: \"Music\" score: 0.2 ClassificationResult #1 (second chunk of data): timestamp_ms: 800 (starts at 800ms) classifications #0 (single head model): category #0: category_name: \"Speech\" score: 0.5 category #1: category_name: \"Silence\" score: 0.1\n\nArgs\naudio_clip\tMediaPipe AudioData.\n\nReturns\nAn AudioClassifierResult object that contains a list of classification result objects, each associated with a timestamp corresponding to the start (in milliseconds) of the chunk data that was classified.\n\nRaises\nValueError\tIf any of the input arguments is invalid, such as the sample rate is not provided in the AudioData object.\nRuntimeError\tIf audio classification failed to run.\n\nclassify_async\n\nView source\n\nclassify_async(\n    audio_block: mp.tasks.components.containers.AudioData,\n    timestamp_ms: int\n) -> None\n\n\nSends audio data (a block in a continuous audio stream) to perform audio classification.\n\nOnly use this method when the AudioClassifier is created with the audio stream running mode. The input timestamps should be monotonically increasing for adjacent calls of this method. This method will return immediately after the input audio data is accepted. The results will be available via the result_callback provided in the AudioClassifierOptions. The classify_async method is designed to process auido stream data such as microphone input.\n\nThe input audio data may be longer than what the model is able to process in a single inference. When this occurs, the input audio block is split into multiple chunks. For this reason, the callback may be called multiple times (once per chunk) for each call to this function.\n\nThe result_callback provides:\n\nAn AudioClassifierResult object that contains a list of classifications.\nThe input timestamp in milliseconds.\n\nArgs\naudio_block\tMediaPipe AudioData.\ntimestamp_ms\tThe timestamp of the input audio data in milliseconds.\n\nRaises\nValueError\tIf any of the followings:\n\n1) The sample rate is not provided in the AudioData object or the provided sample rate is inconsistent with the previously received. 2) The current input timestamp is smaller than what the audio classifier has already processed.\n\nclose\n\nView source\n\nclose() -> None\n\n\nShuts down the mediapipe audio task instance.\n\nRaises\nRuntimeError\tIf the mediapipe audio task failed to close.\n\ncreate_audio_record\n\nView source\n\ncreate_audio_record(\n    num_channels: int, sample_rate: int, required_input_buffer_size: int\n) -> audio_record.AudioRecord\n\n\nCreates an AudioRecord instance to record audio stream.\n\nThe returned AudioRecord instance is initialized and client needs to call the appropriate method to start recording.\n\nNote that MediaPipe Audio tasks will up/down sample automatically to fit the sample rate required by the model. The default sample rate of the MediaPipe pretrained audio model, Yamnet is 16kHz.\n\nArgs\nnum_channels\tThe number of audio channels.\nsample_rate\tThe audio sample rate.\nrequired_input_buffer_size\tThe required input buffer size in number of float elements.\n\nReturns\nAn AudioRecord instance.\n\nRaises\nValueError\tIf there's a problem creating the AudioRecord instance.\n\ncreate_from_model_path\n\nView source\n\n@classmethod\ncreate_from_model_path(\n    model_path: str\n) -> 'AudioClassifier'\n\n\nCreates an AudioClassifier object from a TensorFlow Lite model and the default AudioClassifierOptions.\n\nNote that the created AudioClassifier instance is in audio clips mode, for classifying on independent audio clips.\n\nArgs\nmodel_path\tPath to the model.\n\nReturns\nAudioClassifier object that's created from the model file and the default AudioClassifierOptions.\n\nRaises\nValueError\tIf failed to create AudioClassifier object from the provided file such as invalid file path.\nRuntimeError\tIf other types of error occurred.\n\ncreate_from_options\n\nView source\n\n@classmethod\ncreate_from_options(\n    options: mp.tasks.audio.AudioClassifierOptions\n) -> 'AudioClassifier'\n\n\nCreates the AudioClassifier object from audio classifier options.\n\nArgs\noptions\tOptions for the audio classifier task.\n\nReturns\nAudioClassifier object that's created from options.\n\nRaises\nValueError\tIf failed to create AudioClassifier object from AudioClassifierOptions such as missing the model.\nRuntimeError\tIf other types of error occurred.\n\n__enter__\n\nView source\n\n__enter__()\n\n\nReturn self upon entering the runtime context.\n\n__exit__\n\nView source\n\n__exit__(\n    unused_exc_type, unused_exc_value, unused_traceback\n)\n\n\nShuts down the mediapipe audio task instance on exit of the context manager.\n\nRaises\nRuntimeError\tIf the mediapipe audio task failed to close.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "tflite_support.metadata_schema_py_generated.ValueRangeStart  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_schema_py_generated/ValueRangeStart",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nOverview\nAssociatedFile\nAssociatedFileAddDescription\nAssociatedFileAddLocale\nAssociatedFileAddName\nAssociatedFileAddType\nAssociatedFileAddVersion\nAssociatedFileEnd\nAssociatedFileStart\nAssociatedFileT\nAssociatedFileType\nAudioProperties\nAudioPropertiesAddChannels\nAudioPropertiesAddSampleRate\nAudioPropertiesEnd\nAudioPropertiesStart\nAudioPropertiesT\nBertTokenizerOptions\nBertTokenizerOptionsAddVocabFile\nBertTokenizerOptionsEnd\nBertTokenizerOptionsStart\nBertTokenizerOptionsStartVocabFileVector\nBertTokenizerOptionsT\nBoundingBoxProperties\nBoundingBoxPropertiesAddCoordinateType\nBoundingBoxPropertiesAddIndex\nBoundingBoxPropertiesAddType\nBoundingBoxPropertiesEnd\nBoundingBoxPropertiesStart\nBoundingBoxPropertiesStartIndexVector\nBoundingBoxPropertiesT\nBoundingBoxType\nColorSpaceType\nContent\nContentAddContentProperties\nContentAddContentPropertiesType\nContentAddRange\nContentEnd\nContentProperties\nContentPropertiesCreator\nContentStart\nContentT\nCoordinateType\nCustomMetadata\nCustomMetadataAddData\nCustomMetadataAddName\nCustomMetadataEnd\nCustomMetadataStart\nCustomMetadataStartDataVector\nCustomMetadataT\nFeatureProperties\nFeaturePropertiesEnd\nFeaturePropertiesStart\nFeaturePropertiesT\nImageProperties\nImagePropertiesAddColorSpace\nImagePropertiesAddDefaultSize\nImagePropertiesEnd\nImagePropertiesStart\nImagePropertiesT\nImageSize\nImageSizeAddHeight\nImageSizeAddWidth\nImageSizeEnd\nImageSizeStart\nImageSizeT\nModelMetadata\nModelMetadataAddAssociatedFiles\nModelMetadataAddAuthor\nModelMetadataAddDescription\nModelMetadataAddLicense\nModelMetadataAddMinParserVersion\nModelMetadataAddName\nModelMetadataAddSubgraphMetadata\nModelMetadataAddVersion\nModelMetadataEnd\nModelMetadataStart\nModelMetadataStartAssociatedFilesVector\nModelMetadataStartSubgraphMetadataVector\nModelMetadataT\nNormalizationOptions\nNormalizationOptionsAddMean\nNormalizationOptionsAddStd\nNormalizationOptionsEnd\nNormalizationOptionsStart\nNormalizationOptionsStartMeanVector\nNormalizationOptionsStartStdVector\nNormalizationOptionsT\nProcessUnit\nProcessUnitAddOptions\nProcessUnitAddOptionsType\nProcessUnitEnd\nProcessUnitOptions\nProcessUnitOptionsCreator\nProcessUnitStart\nProcessUnitT\nRegexTokenizerOptions\nRegexTokenizerOptionsAddDelimRegexPattern\nRegexTokenizerOptionsAddVocabFile\nRegexTokenizerOptionsEnd\nRegexTokenizerOptionsStart\nRegexTokenizerOptionsStartVocabFileVector\nRegexTokenizerOptionsT\nScoreCalibrationOptions\nScoreCalibrationOptionsAddDefaultScore\nScoreCalibrationOptionsAddScoreTransformation\nScoreCalibrationOptionsEnd\nScoreCalibrationOptionsStart\nScoreCalibrationOptionsT\nScoreThresholdingOptions\nScoreThresholdingOptionsAddGlobalScoreThreshold\nScoreThresholdingOptionsEnd\nScoreThresholdingOptionsStart\nScoreThresholdingOptionsT\nScoreTransformationType\nSentencePieceTokenizerOptions\nSentencePieceTokenizerOptionsAddSentencePieceModel\nSentencePieceTokenizerOptionsAddVocabFile\nSentencePieceTokenizerOptionsEnd\nSentencePieceTokenizerOptionsStart\nSentencePieceTokenizerOptionsStartSentencePieceModelVector\nSentencePieceTokenizerOptionsStartVocabFileVector\nSentencePieceTokenizerOptionsT\nStats\nStatsAddMax\nStatsAddMin\nStatsEnd\nStatsStart\nStatsStartMaxVector\nStatsStartMinVector\nStatsT\nSubGraphMetadata\nSubGraphMetadataAddAssociatedFiles\nSubGraphMetadataAddCustomMetadata\nSubGraphMetadataAddDescription\nSubGraphMetadataAddInputProcessUnits\nSubGraphMetadataAddInputTensorGroups\nSubGraphMetadataAddInputTensorMetadata\nSubGraphMetadataAddName\nSubGraphMetadataAddOutputProcessUnits\nSubGraphMetadataAddOutputTensorGroups\nSubGraphMetadataAddOutputTensorMetadata\nSubGraphMetadataEnd\nSubGraphMetadataStart\nSubGraphMetadataStartAssociatedFilesVector\nSubGraphMetadataStartCustomMetadataVector\nSubGraphMetadataStartInputProcessUnitsVector\nSubGraphMetadataStartInputTensorGroupsVector\nSubGraphMetadataStartInputTensorMetadataVector\nSubGraphMetadataStartOutputProcessUnitsVector\nSubGraphMetadataStartOutputTensorGroupsVector\nSubGraphMetadataStartOutputTensorMetadataVector\nSubGraphMetadataT\nTensorGroup\nTensorGroupAddName\nTensorGroupAddTensorNames\nTensorGroupEnd\nTensorGroupStart\nTensorGroupStartTensorNamesVector\nTensorGroupT\nTensorMetadata\nTensorMetadataAddAssociatedFiles\nTensorMetadataAddContent\nTensorMetadataAddDescription\nTensorMetadataAddDimensionNames\nTensorMetadataAddName\nTensorMetadataAddProcessUnits\nTensorMetadataAddStats\nTensorMetadataEnd\nTensorMetadataStart\nTensorMetadataStartAssociatedFilesVector\nTensorMetadataStartDimensionNamesVector\nTensorMetadataStartProcessUnitsVector\nTensorMetadataT\nValueRange\nValueRangeAddMax\nValueRangeAddMin\nValueRangeEnd\nValueRangeStart\nValueRangeT\nimport_numpy\nmetadata_writers\ntask\nAndroid (Java)\nSwift\nC\nC++\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\ntflite_support.metadata_schema_py_generated.ValueRangeStart \nbookmark_border\n\nView source on GitHub\ntflite_support.metadata_schema_py_generated.ValueRangeStart(\n    builder\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "mp.tasks.vision.holistic_landmarker.base_vision_task_api.math.tanh  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision/holistic_landmarker/base_vision_task_api/math/tanh",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nmp\nmp.calculators\nmp.model_ckpt_util\nmp.packet_creator\nmp.packet_getter\nmp.resource_util\nmp.tasks\nOverview\nBaseOptions\nBaseOptions.Delegate\naudio\ncomponents\ngenai\ntext\nvision\nOverview\nFaceAligner\nFaceAlignerOptions\nFaceDetector\nFaceDetectorOptions\nFaceDetectorResult\nFaceLandmarker\nFaceLandmarkerOptions\nFaceLandmarkerResult\nFaceLandmarksConnections\nFaceLandmarksConnections.Connection\nFaceStylizer\nFaceStylizerOptions\nGestureRecognizer\nGestureRecognizerOptions\nGestureRecognizerResult\nHandLandmarker\nHandLandmarkerOptions\nHandLandmarkerResult\nHandLandmarksConnections\nHandLandmarksConnections.Connection\nHolisticLandmarker\nHolisticLandmarkerOptions\nHolisticLandmarkerResult\nImageClassifier\nImageClassifierOptions\nImageEmbedder\nImageEmbedderOptions\nImageSegmenter\nImageSegmenterOptions\nInteractiveSegmenter\nInteractiveSegmenterOptions\nInteractiveSegmenterRegionOfInterest\nInteractiveSegmenterRegionOfInterest.Format\nObjectDetector\nObjectDetectorOptions\nPoseLandmarker\nPoseLandmarkerOptions\nPoseLandmarkerResult\nPoseLandmarksConnections\nPoseLandmarksConnections.Connection\nRunningMode\nholistic_landmarker\nOverview\nbase_options_module\nbase_vision_task_api\nOverview\nmath\nOverview\nacos\nacosh\nasin\nasinh\natan\natan2\natanh\nceil\ncomb\ncopysign\ncos\ncosh\ndegrees\ndist\nerf\nerfc\nexp\nexpm1\nfabs\nfactorial\nfloor\nfmod\nfrexp\nfsum\ngamma\ngcd\nhypot\nisclose\nisfinite\nisinf\nisnan\nisqrt\nlcm\nldexp\nlgamma\nlog\nlog10\nlog1p\nlog2\nmodf\nnextafter\nperm\npow\nprod\nradians\nremainder\nsin\nsinh\nsqrt\ntan\ntanh\ntrunc\nulp\nrect_module\ncategory_module\nholistic_landmarker_graph_options_pb2\nholistic_result_pb2\nimage_processing_options_module\nlandmark_module\nrunning_mode_module\ntask_info_module\nJava\nJavaScript\nSwift\nObjectiveC\nModel Maker\nTensorFlow Lite\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.tanh \nbookmark_border\n\nReturn the hyperbolic tangent of x.\n\nmp.tasks.vision.holistic_landmarker.base_vision_task_api.math.tanh(\n    x, /\n)\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-01 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Interactive image segmentation guide for web  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/interactive_segmenter/web_js",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nAbout the Preview\nTasks\nModel Maker\nStudio\nVision tasks\nObject detection\nImage classification\nImage segmentation\nInteractive segmentation\nOverview\nAndroid\nWeb\nPython\nGesture recognition\nHand landmark detection\nImage embedding\nFace detection\nFace landmark detection\nPose landmark detection\nFace stylization\nHolistic landmark detection\nText tasks\nText classification\nText embedding\nLanguage detection\nAudio tasks\nAudio classification\nGenerative AI tasks\nImage generation\nLLM Inference\nPlatform setup guides\nAndroid setup\nPython setup\nWeb setup\niOS setup\nOn this page\nCode example\nSetup\nJavaScript packages\nModel\nCreate the task\nConfiguration options\nPrepare data\nRun the task\nHandle and display results\nCategory mask\nConfidence mask\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nSolutions\nSend feedback\nInteractive image segmentation guide for web \nbookmark_border\n\nThe MediaPipe Interactive Image Segmenter task takes a location in an image, estimates the boundaries of an object at that location, and returns the segmentation for the object as image data. These instructions show you how to use the Interactive Image Segmenter for Node and web apps. For more information about the capabilities, models, and configuration options of this task, see the Overview.\n\nCode example\n\nThe example code for Interactive Image Segmenter provides a complete implementation of this task in JavaScript for your reference. This code helps you test this task and get started on building your own interactive image segmentation app. You can view, run, and edit the Interactive Image Segmenter example code using just your web browser. You can also review the code for this example on GitHub.\n\nSetup\n\nThis section describes key steps for setting up your development environment and code projects specifically to use Interactive Image Segmenter. For general information on setting up your development environment for using MediaPipe tasks, including platform version requirements, see the Setup guide for web.\n\nJavaScript packages\n\nInteractive Image Segmenter code is available through the MediaPipe @mediapipe/tasks-vision NPM package. You can find and download these libraries from links provided in the platform Setup guide.\n\nAttention: This MediaPipe Solutions Preview is an early release. Learn more.\n\nYou can install the required packages with the following code for local staging using the following command:\n\nnpm install --save @mediapipe/tasks-vision\n\n\nIf you want to import the task code via a content delivery network (CDN) service, add the following code in the tag in your HTML file:\n\n<head>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js\"\n    crossorigin=\"anonymous\"></script>\n</head>\n\nModel\n\nThe MediaPipe Interactive Image Segmenter task requires a trained model that is compatible with this task. For more information on available trained models for Interactive Image Segmenter, see the task overview Models section.\n\nSelect and download a model, and then store it within your project directory:\n\n<dev-project-root>/app/shared/models/\n\nCreate the task\n\nUse one of the Interactive Image Segmenter createFrom...() functions to prepare the task for running inferences. Use the createFromModelPath() function with a relative or absolute path to the trained model file. If your model is already loaded into memory, you can use the createFromModelBuffer() method.\n\nThe code example below demonstrates using the createFromOptions() function to set up the task. The createFromOptions function allows you to customize the Interactive Image Segmenter with configuration options. For more information on configuration options, see Configuration options.\n\nThe following code demonstrates how to build and configure the task with custom options:\n\nasync function createSegmenter() {\n  const vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm\"\n  );\n\n  interactiveSegmenter = await InteractiveSegmenter.createFromOptions(vision, {\n    baseOptions: {\n      modelAssetPath:\n        \"https://storage.googleapis.com/mediapipe-tasks/interactive_segmenter/ptm_512_hdt_ptm_woid.tflite\"\n    },\n  });\n}\ncreateSegmenter();\n\nConfiguration options\n\nThis task has the following configuration options for Web applications:\n\nOption Name\tDescription\tValue Range\tDefault Value\noutputCategoryMask\tIf set to True, the output includes a segmentation mask as a uint8 image, where each pixel value indicates if the pixel is part of the object located at the area of interest.\t{True, False}\tFalse\noutputConfidenceMasks\tIf set to True, the output includes a segmentation mask as a float value image, where each float value represents the confidence that the pixel is part of the object located at the area of interest.\t{True, False}\tTrue\ndisplayNamesLocale\tSets the language of labels to use for display names provided in the metadata of the task's model, if available. Default is en for English. You can add localized labels to the metadata of a custom model using the TensorFlow Lite Metadata Writer API\tLocale code\ten\nPrepare data\n\nInteractive Image Segmenter can segment objects in images in any format supported by the host browser. The task also handles data input preprocessing, including resizing, rotation and value normalization.\n\nCalls to the Interactive Image Segmenter segment() and segmentForVideo() methods run synchronously and block the user interface thread. If you segment objects in video frames from a device's camera, each segmentation task blocks the main thread. You can prevent this by implementing web workers to run segment() and segmentForVideo() on another thread.\n\nRun the task\n\nThe Interactive Image Segmenter uses the segment() method to trigger inferences. The Interactive Image Segmenter returns the detected segments as image data to a callback function you set when running an inference for the task.\n\nThe following code demonstrates how to execute processing with the task model:\n\nconst image = document.getElementById(\"image\") as HTMLImageElement;\ninteractiveSegmenter.segment(\n  image,\n  {\n    keypoint: {\n      x: event.offsetX / event.target.width,\n      y: event.offsetY / event.target.height\n    }\n  },\n  callback);\n\n\nFor a more complete implementation of running an Interactive Image Segmenter task, see the code example.\n\nHandle and display results\n\nUpon running inference, the Interactive Image Segmenter task returns segment image data to a callback function. The content of the output is image data and may include a category mask, confidence masks, or both, depending on what you set when you configured the task.\n\nThe following sections further explain the output data from this task:\n\nCategory mask\n\nThe following images show a visualization of the task output for a category value mask with a point area of interest indicated. Each pixel is a uint8 value indicating if the pixel is part of the object located at the area of interest. The black and white circle on the second image indicates the selected area of interest.\n\n \n\nOriginal image and category mask output. Source image from the Pascal VOC 2012 dataset.\n\nConfidence mask\n\nThe output for a confidence mask contains float values between [0, 1] for each image input channel. Higher values indicate a higher confidence that the image pixel is part of the object located at the area of interest.\n\nThe Interactive Image Segmenter example code demonstrates how to display the classification results returned from the task, see the code example for details.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-21 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "MediaPipeTasksText Framework Reference  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/mediapipe/objc/text/Classes/MPPTextEmbedder",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTasks\nPython\nJava\nJavaScript\nSwift\nObjectiveC\nMediaPipeTasksText\nClasses\nOverview\nMPPBaseOptions\nMPPCategory\nMPPClassificationResult\nMPPClassifications\nMPPEmbedding\nMPPEmbeddingResult\nMPPLanguageDetector\nMPPLanguageDetectorOptions\nMPPLanguageDetectorResult\nMPPLanguagePrediction\nMPPTaskOptions\nMPPTaskResult\nMPPTextClassifier\nMPPTextClassifierOptions\nMPPTextClassifierResult\nMPPTextEmbedder\nMPPTextEmbedderOptions\nMPPTextEmbedderResult\nEnumerations\nMediaPipeTasksVision\nClasses\nConstants\nEnumerations\nProtocols\nType Definitions\nFunctions\nModel Maker\nTensorFlow Lite\nOn this page\n-initWithModelPath:error:\n-initWithOptions:error:\n-embedText:error:\n-init\n+cosineSimilarityBetweenEmbedding1:andEmbedding2:error:\n+new\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nMediaPipeTasksText Framework Reference \nbookmark_border\nMPPTextEmbedder\n\n@interface MPPTextEmbedder : NSObject\n\n@brief Performs embedding extraction on text.\n\nThis API expects a TFLite model with (optional) TFLite Model Metadata.\n\nMetadata is required for models with int32 input tensors because it contains the input process unit for the model’s Tokenizer. No metadata is required for models with string input tensors.\n\nInput tensors:\n\nThree input tensors kTfLiteInt32 of shape [batch_size x bert_max_seq_len] representing the input ids, mask ids, and segment ids. This input signature requires a Bert Tokenizer process unit in the model metadata.\nOr one input tensor kTfLiteInt32 of shape [batch_size x max_seq_len] representing the input ids. This input signature requires a Regex Tokenizer process unit in the model metadata.\nOr one input tensor (kTfLiteString) that is shapeless or has shape [1] containing the input string.\n\nAt least one output tensor (kTfLiteFloat32/kTfLiteUint8) with shape [1 x N] where N is the number of dimensions in the produced embeddings.\n\n-initWithModelPath:error:\n\nCreates a new instance of MPPTextEmbedder from an absolute path to a TensorFlow Lite model file stored locally on the device and the default MPPTextEmbedderOptions.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable instancetype)initWithModelPath:(nonnull NSString *)modelPath\n                                     error:(NSError *_Nullable *_Nullable)error;\nParameters\nmodelPath\t\n\nAn absolute path to a TensorFlow Lite model file stored locally on the device.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in initializing the text embedder.\n\nReturn Value\n\nA new instance of MPPTextEmbedder with the given model path. nil if there is an error in initializing the text embedder.\n\n-initWithOptions:error:\n\nCreates a new instance of MPPTextEmbedder from the given MPPTextEmbedderOptions.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable instancetype)initWithOptions:\n                             (nonnull MPPTextEmbedderOptions *)options\n                                   error:(NSError *_Nullable *_Nullable)error;\nParameters\noptions\t\n\nThe options of type MPPTextEmbedderOptions to use for configuring the MPPTextEmbedder.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in initializing the text embedder.\n\nReturn Value\n\nA new instance of MPPTextEmbedder with the given options. nil if there is an error in initializing the text embedder.\n\n-embedText:error:\n\nPerforms embedding extraction on the input text.\n\nDeclaration\n\nOBJECTIVE-C\n\n- (nullable MPPTextEmbedderResult *)embedText:(nonnull NSString *)text\n                                        error:(NSError *_Nullable *_Nullable)\n                                                  error;\nParameters\ntext\t\n\nThe NSString on which embedding extraction is to be performed.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in performing embedding extraction on the input text.\n\nReturn Value\n\nA MPPTextEmbedderResult object that contains a list of embeddings.\n\n-init\n\nUndocumented\n\nDeclaration\n\nOBJECTIVE-C\n\n- (instancetype)init NS_UNAVAILABLE;\n+cosineSimilarityBetweenEmbedding1:andEmbedding2:error:\n\nUtility function to computecosine similarity between two MPPEmbedding objects.\n\nDeclaration\n\nOBJECTIVE-C\n\n+ (nullable NSNumber *)\n    cosineSimilarityBetweenEmbedding1:(nonnull MPPEmbedding *)embedding1\n                        andEmbedding2:(nonnull MPPEmbedding *)embedding2\n                                error:(NSError *_Nullable *_Nullable)error;\nParameters\nembedding1\t\n\nOne of the two MPPEmbeddings between whom cosine similarity is to be computed.\n\n\nembedding2\t\n\nOne of the two MPPEmbeddings between whom cosine similarity is to be computed.\n\n\nerror\t\n\nAn optional error parameter populated when there is an error in calculating cosine similarity between two embeddings.\n\nReturn Value\n\nAn NSNumber which holds the cosine similarity of type double.\n\n+new\n\nUndocumented\n\nDeclaration\n\nOBJECTIVE-C\n\n+ (instancetype)new NS_UNAVAILABLE;\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "Module: tflite_support.metadata_writers.nl_classifier  |  Google AI Edge  |  Google for Developers",
    "url": "https://ai.google.dev/edge/api/tflite/python/tflite_support/metadata_writers/nl_classifier",
    "html": "ai.google.dev uses cookies to deliver and enhance the quality of its services and to analyze traffic. If you agree, cookies are also used to serve advertising and to personalize the content and advertisements that you see. Learn more.\n\nAgree\nNo thanks\nGemini API\nGemma\nGoogle AI Edge\nTools\nCommunity\nSign in\nOverview\nMediaPipe\nTensorFlow Lite\nModel Explorer\nAPI Reference\nOverview\nMediaPipe\nTensorFlow Lite\nPython\ntf.lite\ntflite_support\nOverview\nmetadata\nmetadata_schema_py_generated\nmetadata_writers\nOverview\naudio_classifier\nbert_nl_classifier\nimage_classifier\nimage_segmenter\nmetadata_info\nnl_classifier\nOverview\nMetadataWriter\nobject_detector\nwriter_utils\ntask\nAndroid (Java)\nSwift\nC\nC++\nOn this page\nModules\nClasses\nLearn more about our Google AI Edge announcements from I/O\nHome\nGoogle AI Edge\nAPI Reference\nSend feedback\nModule: tflite_support.metadata_writers.nl_classifier \nbookmark_border\n\nView source on GitHub\n\nWrites metadata and label file to the NL classifier models.\n\nModules\n\nmetadata_info module: Helper classes for common model metadata information.\n\nmetadata_writer module: Helper class to write metadata into TFLite models.\n\nwriter_utils module: Helper methods for writing metadata into TFLite models.\n\nClasses\n\nclass MetadataWriter: Writes metadata into the NL classifier.\n\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-05-08 UTC.\n\nTerms\nPrivacy\nManage cookies"
  },
  {
    "title": "www.sitemaps.org - /schemas/sitemap/0.9/",
    "url": "https://www.sitemaps.org/schemas/sitemap/0.9/",
    "html": "www.sitemaps.org - /schemas/sitemap/0.9/\n[To Parent Directory]\n\n  7/8/2021  5:32 AM         2591 siteindex.xsd\n  7/8/2021  5:32 AM         4451 sitemap.xsd\n"
  },
  {
    "title": "XHTML namespace",
    "url": "https://www.w3.org/1999/xhtml/",
    "html": "XHTML namespace\n\nThe namespace name http://www.w3.org/1999/xhtml is intended for use in various specifications such as:\n\nRecommendations:\n\nHTML 5: A vocabulary and associated APIs for HTML and XHTML\nXHTML™ 1.0: The Extensible HyperText Markup Language\nXHTML Modularization\nXHTML 1.1\nXHTML Basic\nXHTML Print\nXHTML+RDFa\n\nOther Documents:\n\nHTML 5.1\nHTML 5.2\n\nThe charters of the following W3C Working Groups include work on HTML that may impact this namespace:\n\nWeb Platform Working Group, chartered October 2015\n\nFor more information about XML namespaces, please refer to Namespaces in XML.\n\nMichael Smith, W3C HTML Activity Lead\nLast edited: $Date: 2016/09/07 14:16:54 $\n"
  }
]