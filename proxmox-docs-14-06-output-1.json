[
  {
    "title": "Proxmox VE Administration Guide",
    "url": "https://pve.proxmox.com/pve-docs/pve-admin-guide.html#_introduction",
    "html": "\nProxmox VE Administration Guide\nProxmox Server Solutions GmbH\n<support@proxmox.com>\nversion 8.2.2, Thu Apr 25 09:24:16 CEST 2024\n↩Index\nTable of Contents\n1. Introduction\n1.1. Central Management\n1.2. Flexible Storage\n1.3. Integrated Backup and Restore\n1.4. High Availability Cluster\n1.5. Flexible Networking\n1.6. Integrated Firewall\n1.7. Hyper-converged Infrastructure\n1.8. Why Open Source\n1.9. Your benefits with Proxmox VE\n1.10. Getting Help\n1.11. Project History\n1.12. Improving the Proxmox VE Documentation\n1.13. Translating Proxmox VE\n2. Installing Proxmox VE\n2.1. System Requirements\n2.2. Prepare Installation Media\n2.3. Using the Proxmox VE Installer\n2.4. Unattended Installation\n2.5. Install Proxmox VE on Debian\n3. Host System Administration\n3.1. Package Repositories\n3.2. System Software Updates\n3.3. Firmware Updates\n3.4. Network Configuration\n3.5. Time Synchronization\n3.6. External Metric Server\n3.7. Disk Health Monitoring\n3.8. Logical Volume Manager (LVM)\n3.9. ZFS on Linux\n3.10. BTRFS\n3.11. Proxmox Node Management\n3.12. Certificate Management\n3.13. Host Bootloader\n3.14. Kernel Samepage Merging (KSM)\n4. Graphical User Interface\n4.1. Features\n4.2. Login\n4.3. GUI Overview\n4.4. Content Panels\n4.5. Tags\n5. Cluster Manager\n5.1. Requirements\n5.2. Preparing Nodes\n5.3. Create a Cluster\n5.4. Adding Nodes to the Cluster\n5.5. Remove a Cluster Node\n5.6. Quorum\n5.7. Cluster Network\n5.8. Corosync Redundancy\n5.9. Role of SSH in Proxmox VE Clusters\n5.10. Corosync External Vote Support\n5.11. Corosync Configuration\n5.12. Cluster Cold Start\n5.13. Guest VMID Auto-Selection\n5.14. Guest Migration\n6. Proxmox Cluster File System (pmxcfs)\n6.1. POSIX Compatibility\n6.2. File Access Rights\n6.3. Technology\n6.4. File System Layout\n6.5. Recovery\n7. Proxmox VE Storage\n7.1. Storage Types\n7.2. Storage Configuration\n7.3. Volumes\n7.4. Using the Command-line Interface\n7.5. Directory Backend\n7.6. NFS Backend\n7.7. CIFS Backend\n7.8. Proxmox Backup Server\n7.9. GlusterFS Backend\n7.10. Local ZFS Pool Backend\n7.11. LVM Backend\n7.12. LVM thin Backend\n7.13. Open-iSCSI initiator\n7.14. User Mode iSCSI Backend\n7.15. Ceph RADOS Block Devices (RBD)\n7.16. Ceph Filesystem (CephFS)\n7.17. BTRFS Backend\n7.18. ZFS over ISCSI Backend\n8. Deploy Hyper-Converged Ceph Cluster\n8.1. Introduction\n8.2. Terminology\n8.3. Recommendations for a Healthy Ceph Cluster\n8.4. Initial Ceph Installation & Configuration\n8.5. Ceph Monitor\n8.6. Ceph Manager\n8.7. Ceph OSDs\n8.8. Ceph Pools\n8.9. Ceph CRUSH & device classes\n8.10. Ceph Client\n8.11. CephFS\n8.12. Ceph maintenance\n8.13. Ceph Monitoring and Troubleshooting\n9. Storage Replication\n9.1. Supported Storage Types\n9.2. Schedule Format\n9.3. Error Handling\n9.4. Managing Jobs\n9.5. Command-line Interface Examples\n10. QEMU/KVM Virtual Machines\n10.1. Emulated devices and paravirtualized devices\n10.2. Virtual Machines Settings\n10.3. Migration\n10.4. Copies and Clones\n10.5. Virtual Machine Templates\n10.6. VM Generation ID\n10.7. Importing Virtual Machines\n10.8. Cloud-Init Support\n10.9. PCI(e) Passthrough\n10.10. Hookscripts\n10.11. Hibernation\n10.12. Resource Mapping\n10.13. Managing Virtual Machines with qm\n10.14. Configuration\n10.15. Locks\n11. Proxmox Container Toolkit\n11.1. Technology Overview\n11.2. Supported Distributions\n11.3. Container Images\n11.4. Container Settings\n11.5. Security Considerations\n11.6. Guest Operating System Configuration\n11.7. Container Storage\n11.8. Backup and Restore\n11.9. Managing Containers with pct\n11.10. Migration\n11.11. Configuration\n11.12. Locks\n12. Software-Defined Network\n12.1. Introduction\n12.2. Support Status\n12.3. Installation\n12.4. Configuration Overview\n12.5. Technology & Configuration\n12.6. Zones\n12.7. VNets\n12.8. Subnets\n12.9. Controllers\n12.10. IPAM\n12.11. DNS\n12.12. DHCP\n12.13. Examples\n12.14. Notes\n13. Proxmox VE Firewall\n13.1. Zones\n13.2. Configuration Files\n13.3. Firewall Rules\n13.4. Security Groups\n13.5. IP Aliases\n13.6. IP Sets\n13.7. Services and Commands\n13.8. Default firewall rules\n13.9. Logging of firewall rules\n13.10. Tips and Tricks\n13.11. Notes on IPv6\n13.12. Ports used by Proxmox VE\n13.13. nftables\n14. User Management\n14.1. Users\n14.2. Groups\n14.3. API Tokens\n14.4. Resource Pools\n14.5. Authentication Realms\n14.6. Two-Factor Authentication\n14.7. Permission Management\n14.8. Command-line Tool\n14.9. Real World Examples\n15. High Availability\n15.1. Requirements\n15.2. Resources\n15.3. Management Tasks\n15.4. How It Works\n15.5. HA Simulator\n15.6. Configuration\n15.7. Fencing\n15.8. Start Failure Policy\n15.9. Error Recovery\n15.10. Package Updates\n15.11. Node Maintenance\n15.12. Cluster Resource Scheduling\n16. Backup and Restore\n16.1. Backup Modes\n16.2. Backup File Names\n16.3. Backup File Compression\n16.4. Backup Encryption\n16.5. Backup Jobs\n16.6. Backup Retention\n16.7. Backup Protection\n16.8. Backup Notes\n16.9. Restore\n16.10. Configuration\n16.11. Hook Scripts\n16.12. File Exclusions\n16.13. Examples\n17. Notifications\n17.1. Overview\n17.2. Notification Targets\n17.3. Notification Matchers\n17.4. Notification Events\n17.5. System Mail Forwarding\n17.6. Permissions\n18. Important Service Daemons\n18.1. pvedaemon - Proxmox VE API Daemon\n18.2. pveproxy - Proxmox VE API Proxy Daemon\n18.3. pvestatd - Proxmox VE Status Daemon\n18.4. spiceproxy - SPICE Proxy Service\n18.5. pvescheduler - Proxmox VE Scheduler Daemon\n19. Useful Command-line Tools\n19.1. pvesubscription - Subscription Management\n19.2. pveperf - Proxmox VE Benchmark Script\n19.3. Shell interface for the Proxmox VE API\n20. Frequently Asked Questions\n21. Bibliography\n22. Appendix A: Command-line Interface\n22.1. Output format options [FORMAT_OPTIONS]\n22.2. pvesm - Proxmox VE Storage Manager\n22.3. pvesubscription - Proxmox VE Subscription Manager\n22.4. pveperf - Proxmox VE Benchmark Script\n22.5. pveceph - Manage CEPH Services on Proxmox VE Nodes\n22.6. pvenode - Proxmox VE Node Management\n22.7. pvesh - Shell interface for the Proxmox VE API\n22.8. qm - QEMU/KVM Virtual Machine Manager\n22.9. qmrestore - Restore QemuServer vzdump Backups\n22.10. pct - Proxmox Container Toolkit\n22.11. pveam - Proxmox VE Appliance Manager\n22.12. pvecm - Proxmox VE Cluster Manager\n22.13. pvesr - Proxmox VE Storage Replication\n22.14. pveum - Proxmox VE User Manager\n22.15. vzdump - Backup Utility for VMs and Containers\n22.16. ha-manager - Proxmox VE HA Manager\n23. Appendix B: Service Daemons\n23.1. pve-firewall - Proxmox VE Firewall Daemon\n23.2. pvedaemon - Proxmox VE API Daemon\n23.3. pveproxy - Proxmox VE API Proxy Daemon\n23.4. pvestatd - Proxmox VE Status Daemon\n23.5. spiceproxy - SPICE Proxy Service\n23.6. pmxcfs - Proxmox Cluster File System\n23.7. pve-ha-crm - Cluster Resource Manager Daemon\n23.8. pve-ha-lrm - Local Resource Manager Daemon\n23.9. pvescheduler - Proxmox VE Scheduler Daemon\n24. Appendix C: Configuration Files\n24.1. Datacenter Configuration\n25. Appendix D: Calendar Events\n25.1. Schedule Format\n25.2. Detailed Specification\n26. Appendix E: QEMU vCPU List\n26.1. Introduction\n26.2. Intel CPU Types\n26.3. AMD CPU Types\n27. Appendix F: Firewall Macro Definitions\n28. Appendix G: Markdown Primer\n28.1. Markdown Basics\n29. Appendix H: GNU Free Documentation License\n1. Introduction\n\nProxmox VE is a platform to run virtual machines and containers. It is based on Debian Linux, and completely open source. For maximum flexibility, we implemented two virtualization technologies - Kernel-based Virtual Machine (KVM) and container-based virtualization (LXC).\n\nOne main design goal was to make administration as easy as possible. You can use Proxmox VE on a single node, or assemble a cluster of many nodes. All management tasks can be done using our web-based management interface, and even a novice user can setup and install Proxmox VE within minutes.\n\n1.1. Central Management\n\nWhile many people start with a single node, Proxmox VE can scale out to a large set of clustered nodes. The cluster stack is fully integrated and ships with the default installation.\n\nUnique Multi-Master Design\n\nThe integrated web-based management interface gives you a clean overview of all your KVM guests and Linux containers and even of your whole cluster. You can easily manage your VMs and containers, storage or cluster from the GUI. There is no need to install a separate, complex, and pricey management server.\n\nProxmox Cluster File System (pmxcfs)\n\nProxmox VE uses the unique Proxmox Cluster file system (pmxcfs), a database-driven file system for storing configuration files. This enables you to store the configuration of thousands of virtual machines. By using corosync, these files are replicated in real time on all cluster nodes. The file system stores all data inside a persistent database on disk, nonetheless, a copy of the data resides in RAM which provides a maximum storage size of 30MB - more than enough for thousands of VMs.\n\nProxmox VE is the only virtualization platform using this unique cluster file system.\n\nWeb-based Management Interface\n\nProxmox VE is simple to use. Management tasks can be done via the included web based management interface - there is no need to install a separate management tool or any additional management node with huge databases. The multi-master tool allows you to manage your whole cluster from any node of your cluster. The central web-based management - based on the JavaScript Framework (ExtJS) - empowers you to control all functionalities from the GUI and overview history and syslogs of each single node. This includes running backup or restore jobs, live-migration or HA triggered activities.\n\nCommand Line\n\nFor advanced users who are used to the comfort of the Unix shell or Windows Powershell, Proxmox VE provides a command-line interface to manage all the components of your virtual environment. This command-line interface has intelligent tab completion and full documentation in the form of UNIX man pages.\n\nREST API\n\nProxmox VE uses a RESTful API. We choose JSON as primary data format, and the whole API is formally defined using JSON Schema. This enables fast and easy integration for third party management tools like custom hosting environments.\n\nRole-based Administration\n\nYou can define granular access for all objects (like VMs, storages, nodes, etc.) by using the role based user- and permission management. This allows you to define privileges and helps you to control access to objects. This concept is also known as access control lists: Each permission specifies a subject (a user or group) and a role (set of privileges) on a specific path.\n\nAuthentication Realms\n\nProxmox VE supports multiple authentication sources like Microsoft Active Directory, LDAP, Linux PAM standard authentication or the built-in Proxmox VE authentication server.\n\n1.2. Flexible Storage\n\nThe Proxmox VE storage model is very flexible. Virtual machine images can either be stored on one or several local storages or on shared storage like NFS and on SAN. There are no limits, you may configure as many storage definitions as you like. You can use all storage technologies available for Debian Linux.\n\nOne major benefit of storing VMs on shared storage is the ability to live-migrate running machines without any downtime, as all nodes in the cluster have direct access to VM disk images.\n\nWe currently support the following Network storage types:\n\nLVM Group (network backing with iSCSI targets)\n\niSCSI target\n\nNFS Share\n\nCIFS Share\n\nCeph RBD\n\nDirectly use iSCSI LUNs\n\nGlusterFS\n\nLocal storage types supported are:\n\nLVM Group (local backing devices like block devices, FC devices, DRBD, etc.)\n\nDirectory (storage on existing filesystem)\n\nZFS\n\n1.3. Integrated Backup and Restore\n\nThe integrated backup tool (vzdump) creates consistent snapshots of running Containers and KVM guests. It basically creates an archive of the VM or CT data which includes the VM/CT configuration files.\n\nKVM live backup works for all storage types including VM images on NFS, CIFS, iSCSI LUN, Ceph RBD. The new backup format is optimized for storing VM backups fast and effective (sparse files, out of order data, minimized I/O).\n\n1.4. High Availability Cluster\n\nA multi-node Proxmox VE HA Cluster enables the definition of highly available virtual servers. The Proxmox VE HA Cluster is based on proven Linux HA technologies, providing stable and reliable HA services.\n\n1.5. Flexible Networking\n\nProxmox VE uses a bridged networking model. All VMs can share one bridge as if virtual network cables from each guest were all plugged into the same switch. For connecting VMs to the outside world, bridges are attached to physical network cards and assigned a TCP/IP configuration.\n\nFor further flexibility, VLANs (IEEE 802.1q) and network bonding/aggregation are possible. In this way it is possible to build complex, flexible virtual networks for the Proxmox VE hosts, leveraging the full power of the Linux network stack.\n\n1.6. Integrated Firewall\n\nThe integrated firewall allows you to filter network packets on any VM or Container interface. Common sets of firewall rules can be grouped into “security groups”.\n\n1.7. Hyper-converged Infrastructure\n\nProxmox VE is a virtualization platform that tightly integrates compute, storage and networking resources, manages highly available clusters, backup/restore as well as disaster recovery. All components are software-defined and compatible with one another.\n\nTherefore it is possible to administrate them like a single system via the centralized web management interface. These capabilities make Proxmox VE an ideal choice to deploy and manage an open source hyper-converged infrastructure.\n\n1.7.1. Benefits of a Hyper-Converged Infrastructure (HCI) with Proxmox VE\n\nA hyper-converged infrastructure (HCI) is especially useful for deployments in which a high infrastructure demand meets a low administration budget, for distributed setups such as remote and branch office environments or for virtual private and public clouds.\n\nHCI provides the following advantages:\n\nScalability: seamless expansion of compute, network and storage devices (i.e. scale up servers and storage quickly and independently from each other).\n\nLow cost: Proxmox VE is open source and integrates all components you need such as compute, storage, networking, backup, and management center. It can replace an expensive compute/storage infrastructure.\n\nData protection and efficiency: services such as backup and disaster recovery are integrated.\n\nSimplicity: easy configuration and centralized administration.\n\nOpen Source: No vendor lock-in.\n\n1.7.2. Hyper-Converged Infrastructure: Storage\n\nProxmox VE has tightly integrated support for deploying a hyper-converged storage infrastructure. You can, for example, deploy and manage the following two storage technologies by using the web interface only:\n\nCeph: a both self-healing and self-managing shared, reliable and highly scalable storage system. Checkout how to manage Ceph services on Proxmox VE nodes\n\nZFS: a combined file system and logical volume manager with extensive protection against data corruption, various RAID modes, fast and cheap snapshots - among other features. Find out how to leverage the power of ZFS on Proxmox VE nodes.\n\nBesides above, Proxmox VE has support to integrate a wide range of additional storage technologies. You can find out about them in the Storage Manager chapter.\n\n1.8. Why Open Source\n\nProxmox VE uses a Linux kernel and is based on the Debian GNU/Linux Distribution. The source code of Proxmox VE is released under the GNU Affero General Public License, version 3. This means that you are free to inspect the source code at any time or contribute to the project yourself.\n\nAt Proxmox we are committed to use open source software whenever possible. Using open source software guarantees full access to all functionalities - as well as high security and reliability. We think that everybody should have the right to access the source code of a software to run it, build on it, or submit changes back to the project. Everybody is encouraged to contribute while Proxmox ensures the product always meets professional quality criteria.\n\nOpen source software also helps to keep your costs low and makes your core infrastructure independent from a single vendor.\n\n1.9. Your benefits with Proxmox VE\n\nOpen source software\n\nNo vendor lock-in\n\nLinux kernel\n\nFast installation and easy-to-use\n\nWeb-based management interface\n\nREST API\n\nHuge active community\n\nLow administration costs and simple deployment\n\n1.10. Getting Help\n1.10.1. Proxmox VE Wiki\n\nThe primary source of information is the Proxmox VE Wiki. It combines the reference documentation with user contributed content.\n\n1.10.2. Community Support Forum\n\nProxmox VE itself is fully open source, so we always encourage our users to discuss and share their knowledge using the Proxmox VE Community Forum. The forum is moderated by the Proxmox support team, and has a large user base from all around the world. Needless to say, such a large forum is a great place to get information.\n\n1.10.3. Mailing Lists\n\nThis is a fast way to communicate with the Proxmox VE community via email.\n\nMailing list for users: Proxmox VE User List\n\nProxmox VE is fully open source and contributions are welcome! The primary communication channel for developers is the:\n\nMailing list for developers: Proxmox VE development discussion\n\n1.10.4. Commercial Support\n\nProxmox Server Solutions GmbH also offers enterprise support available as Proxmox VE Subscription Service Plans. All users with a subscription get access to the Proxmox VE Enterprise Repository, and—with a Basic, Standard or Premium subscription—also to the Proxmox Customer Portal. The customer portal provides help and support with guaranteed response times from the Proxmox VE developers.\n\nFor volume discounts, or more information in general, please contact sales@proxmox.com.\n\n1.10.5. Bug Tracker\n\nProxmox runs a public bug tracker at https://bugzilla.proxmox.com. If an issue appears, file your report there. An issue can be a bug as well as a request for a new feature or enhancement. The bug tracker helps to keep track of the issue and will send a notification once it has been solved.\n\n1.11. Project History\n\nThe project started in 2007, followed by a first stable version in 2008. At the time we used OpenVZ for containers, and KVM for virtual machines. The clustering features were limited, and the user interface was simple (server generated web page).\n\nBut we quickly developed new features using the Corosync cluster stack, and the introduction of the new Proxmox cluster file system (pmxcfs) was a big step forward, because it completely hides the cluster complexity from the user. Managing a cluster of 16 nodes is as simple as managing a single node.\n\nWe also introduced a new REST API, with a complete declarative specification written in JSON-Schema. This enabled other people to integrate Proxmox VE into their infrastructure, and made it easy to provide additional services.\n\nAlso, the new REST API made it possible to replace the original user interface with a modern HTML5 application using JavaScript. We also replaced the old Java based VNC console code with noVNC. So you only need a web browser to manage your VMs.\n\nThe support for various storage types is another big task. Notably, Proxmox VE was the first distribution to ship ZFS on Linux by default in 2014. Another milestone was the ability to run and manage Ceph storage on the hypervisor nodes. Such setups are extremely cost effective.\n\nWhen we started we were among the first companies providing commercial support for KVM. The KVM project itself continuously evolved, and is now a widely used hypervisor. New features arrive with each release. We developed the KVM live backup feature, which makes it possible to create snapshot backups on any storage type.\n\nThe most notable change with version 4.0 was the move from OpenVZ to LXC. Containers are now deeply integrated, and they can use the same storage and network features as virtual machines.\n\n1.12. Improving the Proxmox VE Documentation\n\nContributions and improvements to the Proxmox VE documentation are always welcome. There are several ways to contribute.\n\nIf you find errors or other room for improvement in this documentation, please file a bug at the Proxmox bug tracker to propose a correction.\n\nIf you want to propose new content, choose one of the following options:\n\nThe wiki: For specific setups, how-to guides, or tutorials the wiki is the right option to contribute.\n\nThe reference documentation: For general content that will be helpful to all users please propose your contribution for the reference documentation. This includes all information about how to install, configure, use, and troubleshoot Proxmox VE features. The reference documentation is written in the asciidoc format. To edit the documentation you need to clone the git repository at git://git.proxmox.com/git/pve-docs.git; then follow the README.adoc document.\n\n\tIf you are interested in working on the Proxmox VE codebase, the Developer Documentation wiki article will show you where to start.\n1.13. Translating Proxmox VE\n\nThe Proxmox VE user interface is in English by default. However, thanks to the contributions of the community, translations to other languages are also available. We welcome any support in adding new languages, translating the latest features, and improving incomplete or inconsistent translations.\n\nWe use gettext for the management of the translation files. Tools like Poedit offer a nice user interface to edit the translation files, but you can use whatever editor you’re comfortable with. No programming knowledge is required for translating.\n\n1.13.1. Translating with git\n\nThe language files are available as a git repository. If you are familiar with git, please contribute according to our Developer Documentation.\n\nYou can create a new translation by doing the following (replace <LANG> with the language ID):\n\n# git clone git://git.proxmox.com/git/proxmox-i18n.git\n# cd proxmox-i18n\n# make init-<LANG>.po\n\nOr you can edit an existing translation, using the editor of your choice:\n\n# poedit <LANG>.po\n1.13.2. Translating without git\n\nEven if you are not familiar with git, you can help translate Proxmox VE. To start, you can download the language files here. Find the language you want to improve, then right click on the \"raw\" link of this language file and select Save Link As…. Make your changes to the file, and then send your final translation directly to office(at)proxmox.com, together with a signed contributor license agreement.\n\n1.13.3. Testing the Translation\n\nIn order for the translation to be used in Proxmox VE, you must first translate the .po file into a .js file. You can do this by invoking the following script, which is located in the same repository:\n\n# ./po2js.pl -t pve xx.po >pve-lang-xx.js\n\nThe resulting file pve-lang-xx.js can then be copied to the directory /usr/share/pve-i18n, on your proxmox server, in order to test it out.\n\nAlternatively, you can build a deb package by running the following command from the root of the repository:\n\n# make deb\n\tFor either of these methods to work, you need to have the following perl packages installed on your system. For Debian/Ubuntu:\n# apt-get install perl liblocale-po-perl libjson-perl\n1.13.4. Sending the Translation\n\nYou can send the finished translation (.po file) to the Proxmox team at the address office(at)proxmox.com, along with a signed contributor license agreement. Alternatively, if you have some developer experience, you can send it as a patch to the Proxmox VE development mailing list. See Developer Documentation.\n\n2. Installing Proxmox VE\n\nProxmox VE is based on Debian. This is why the install disk images (ISO files) provided by Proxmox include a complete Debian system as well as all necessary Proxmox VE packages.\n\n\tSee the support table in the FAQ for the relationship between Proxmox VE releases and Debian releases.\n\nThe installer will guide you through the setup, allowing you to partition the local disk(s), apply basic system configurations (for example, timezone, language, network) and install all required packages. This process should not take more than a few minutes. Installing with the provided ISO is the recommended method for new and existing users.\n\nAlternatively, Proxmox VE can be installed on top of an existing Debian system. This option is only recommended for advanced users because detailed knowledge about Proxmox VE is required.\n\n2.1. System Requirements\n\nWe recommend using high quality server hardware, when running Proxmox VE in production. To further decrease the impact of a failed host, you can run Proxmox VE in a cluster with highly available (HA) virtual machines and containers.\n\nProxmox VE can use local storage (DAS), SAN, NAS, and distributed storage like Ceph RBD. For details see chapter storage.\n\n2.1.1. Minimum Requirements, for Evaluation\n\nThese minimum requirements are for evaluation purposes only and should not be used in production.\n\nCPU: 64bit (Intel EMT64 or AMD64)\n\nIntel VT/AMD-V capable CPU/motherboard for KVM full virtualization support\n\nRAM: 1 GB RAM, plus additional RAM needed for guests\n\nHard drive\n\nOne network card (NIC)\n\n2.1.2. Recommended System Requirements\n\nIntel EMT64 or AMD64 with Intel VT/AMD-V CPU flag.\n\nMemory: Minimum 2 GB for the OS and Proxmox VE services, plus designated memory for guests. For Ceph and ZFS, additional memory is required; approximately 1GB of memory for every TB of used storage.\n\nFast and redundant storage, best results are achieved with SSDs.\n\nOS storage: Use a hardware RAID with battery protected write cache (“BBU”) or non-RAID with ZFS (optional SSD for ZIL).\n\nVM storage:\n\nFor local storage, use either a hardware RAID with battery backed write cache (BBU) or non-RAID for ZFS and Ceph. Neither ZFS nor Ceph are compatible with a hardware RAID controller.\n\nShared and distributed storage is possible.\n\nSSDs with Power-Loss-Protection (PLP) are recommended for good performance. Using consumer SSDs is discouraged.\n\nRedundant (Multi-)Gbit NICs, with additional NICs depending on the preferred storage technology and cluster setup.\n\nFor PCI(e) passthrough the CPU needs to support the VT-d/AMD-d flag.\n\n2.1.3. Simple Performance Overview\n\nTo get an overview of the CPU and hard disk performance on an installed Proxmox VE system, run the included pveperf tool.\n\n\tThis is just a very quick and general benchmark. More detailed tests are recommended, especially regarding the I/O performance of your system.\n2.1.4. Supported Web Browsers for Accessing the Web Interface\n\nTo access the web-based user interface, we recommend using one of the following browsers:\n\nFirefox, a release from the current year, or the latest Extended Support Release\n\nChrome, a release from the current year\n\nMicrosoft’s currently supported version of Edge\n\nSafari, a release from the current year\n\nWhen accessed from a mobile device, Proxmox VE will show a lightweight, touch-based interface.\n\n2.2. Prepare Installation Media\n\nDownload the installer ISO image from: https://www.proxmox.com/en/downloads/proxmox-virtual-environment/iso\n\nThe Proxmox VE installation media is a hybrid ISO image. It works in two ways:\n\nAn ISO image file ready to burn to a CD or DVD.\n\nA raw sector (IMG) image file ready to copy to a USB flash drive (USB stick).\n\nUsing a USB flash drive to install Proxmox VE is the recommended way because it is the faster option.\n\n2.2.1. Prepare a USB Flash Drive as Installation Medium\n\nThe flash drive needs to have at least 1 GB of storage available.\n\n\tDo not use UNetbootin. It does not work with the Proxmox VE installation image.\n\tMake sure that the USB flash drive is not mounted and does not contain any important data.\n2.2.2. Instructions for GNU/Linux\n\nOn Unix-like operating system use the dd command to copy the ISO image to the USB flash drive. First find the correct device name of the USB flash drive (see below). Then run the dd command.\n\n# dd bs=1M conv=fdatasync if=./proxmox-ve_*.iso of=/dev/XYZ\n\tBe sure to replace /dev/XYZ with the correct device name and adapt the input filename (if) path.\n\tBe very careful, and do not overwrite the wrong disk!\nFind the Correct USB Device Name\n\nThere are two ways to find out the name of the USB flash drive. The first one is to compare the last lines of the dmesg command output before and after plugging in the flash drive. The second way is to compare the output of the lsblk command. Open a terminal and run:\n\n# lsblk\n\nThen plug in your USB flash drive and run the command again:\n\n# lsblk\n\nA new device will appear. This is the one you want to use. To be on the extra safe side check if the reported size matches your USB flash drive.\n\n2.2.3. Instructions for macOS\n\nOpen the terminal (query Terminal in Spotlight).\n\nConvert the .iso file to .dmg format using the convert option of hdiutil, for example:\n\n# hdiutil convert proxmox-ve_*.iso -format UDRW -o proxmox-ve_*.dmg\n\tmacOS tends to automatically add .dmg to the output file name.\n\nTo get the current list of devices run the command:\n\n# diskutil list\n\nNow insert the USB flash drive and run this command again to determine which device node has been assigned to it. (e.g., /dev/diskX).\n\n# diskutil list\n# diskutil unmountDisk /dev/diskX\n\treplace X with the disk number from the last command.\n# sudo dd if=proxmox-ve_*.dmg bs=1M of=/dev/rdiskX\n\trdiskX, instead of diskX, in the last command is intended. It will increase the write speed.\n2.2.4. Instructions for Windows\nUsing Etcher\n\nEtcher works out of the box. Download Etcher from https://etcher.io. It will guide you through the process of selecting the ISO and your USB flash drive.\n\nUsing Rufus\n\nRufus is a more lightweight alternative, but you need to use the DD mode to make it work. Download Rufus from https://rufus.ie/. Either install it or use the portable version. Select the destination drive and the Proxmox VE ISO file.\n\n\tOnce you Start you have to click No on the dialog asking to download a different version of GRUB. In the next dialog select the DD mode.\n2.3. Using the Proxmox VE Installer\n\nThe installer ISO image includes the following:\n\nComplete operating system (Debian Linux, 64-bit)\n\nThe Proxmox VE installer, which partitions the local disk(s) with ext4, XFS, BTRFS (technology preview), or ZFS and installs the operating system\n\nProxmox VE Linux kernel with KVM and LXC support\n\nComplete toolset for administering virtual machines, containers, the host system, clusters and all necessary resources\n\nWeb-based management interface\n\n\tAll existing data on the selected drives will be removed during the installation process. The installer does not add boot menu entries for other operating systems.\n\nPlease insert the prepared installation media (for example, USB flash drive or CD-ROM) and boot from it.\n\n\tMake sure that booting from the installation medium (for example, USB) is enabled in your server’s firmware settings. Secure boot needs to be disabled when booting an installer prior to Proxmox VE version 8.1.\n\nAfter choosing the correct entry (for example, Boot from USB) the Proxmox VE menu will be displayed, and one of the following options can be selected:\n\nInstall Proxmox VE (Graphical)\n\nStarts the normal installation.\n\n\tIt’s possible to use the installation wizard with a keyboard only. Buttons can be clicked by pressing the ALT key combined with the underlined character from the respective button. For example, ALT + N to press a Next button.\nInstall Proxmox VE (Terminal UI)\n\nStarts the terminal-mode installation wizard. It provides the same overall installation experience as the graphical installer, but has generally better compatibility with very old and very new hardware.\n\nInstall Proxmox VE (Terminal UI, Serial Console)\n\nStarts the terminal-mode installation wizard, additionally setting up the Linux kernel to use the (first) serial port of the machine for in- and output. This can be used if the machine is completely headless and only has a serial console available.\n\nBoth modes use the same code base for the actual installation process to benefit from more than a decade of bug fixes and ensure feature parity.\n\n\tThe Terminal UI option can be used in case the graphical installer does not work correctly, due to e.g. driver issues. See also adding the nomodeset kernel parameter.\nAdvanced Options: Install Proxmox VE (Graphical, Debug Mode)\n\nStarts the installation in debug mode. A console will be opened at several installation steps. This helps to debug the situation if something goes wrong. To exit a debug console, press CTRL-D. This option can be used to boot a live system with all basic tools available. You can use it, for example, to repair a degraded ZFS rpool or fix the bootloader for an existing Proxmox VE setup.\n\nAdvanced Options: Install Proxmox VE (Terminal UI, Debug Mode)\n\nSame as the graphical debug mode, but preparing the system to run the terminal-based installer instead.\n\nAdvanced Options: Install Proxmox VE (Serial Console Debug Mode)\n\nSame the terminal-based debug mode, but additionally sets up the Linux kernel to use the (first) serial port of the machine for in- and output.\n\nAdvanced Options: Rescue Boot\n\nWith this option you can boot an existing installation. It searches all attached hard disks. If it finds an existing installation, it boots directly into that disk using the Linux kernel from the ISO. This can be useful if there are problems with the bootloader (GRUB/systemd-boot) or the BIOS/UEFI is unable to read the boot block from the disk.\n\nAdvanced Options: Test Memory (memtest86+)\n\nRuns memtest86+. This is useful to check if the memory is functional and free of errors. Secure Boot must be turned off in the UEFI firmware setup utility to run this option.\n\nYou normally select Install Proxmox VE (Graphical) to start the installation.\n\nThe first step is to read our EULA (End User License Agreement). Following this, you can select the target hard disk(s) for the installation.\n\n\tBy default, the whole server is used and all existing data is removed. Make sure there is no important data on the server before proceeding with the installation.\n\nThe Options button lets you select the target file system, which defaults to ext4. The installer uses LVM if you select ext4 or xfs as a file system, and offers additional options to restrict LVM space (see below).\n\nProxmox VE can also be installed on ZFS. As ZFS offers several software RAID levels, this is an option for systems that don’t have a hardware RAID controller. The target disks must be selected in the Options dialog. More ZFS specific settings can be changed under Advanced Options.\n\n\tZFS on top of any hardware RAID is not supported and can result in data loss.\n\nThe next page asks for basic configuration options like your location, time zone, and keyboard layout. The location is used to select a nearby download server, in order to increase the speed of updates. The installer is usually able to auto-detect these settings, so you only need to change them in rare situations when auto-detection fails, or when you want to use a keyboard layout not commonly used in your country.\n\nNext the password of the superuser (root) and an email address needs to be specified. The password must consist of at least 5 characters. It’s highly recommended to use a stronger password. Some guidelines are:\n\nUse a minimum password length of at least 12 characters.\n\nInclude lowercase and uppercase alphabetic characters, numbers, and symbols.\n\nAvoid character repetition, keyboard patterns, common dictionary words, letter or number sequences, usernames, relative or pet names, romantic links (current or past), and biographical information (for example ID numbers, ancestors' names or dates).\n\nThe email address is used to send notifications to the system administrator. For example:\n\nInformation about available package updates.\n\nError messages from periodic cron jobs.\n\nAll those notification mails will be sent to the specified email address.\n\nThe last step is the network configuration. Network interfaces that are UP show a filled circle in front of their name in the drop down menu. Please note that during installation you can either specify an IPv4 or IPv6 address, but not both. To configure a dual stack node, add additional IP addresses after the installation.\n\nThe next step shows a summary of the previously selected options. Please re-check every setting and use the Previous button if a setting needs to be changed.\n\nAfter clicking Install, the installer will begin to format the disks and copy packages to the target disk(s). Please wait until this step has finished; then remove the installation medium and restart your system.\n\nCopying the packages usually takes several minutes, mostly depending on the speed of the installation medium and the target disk performance.\n\nWhen copying and setting up the packages has finished, you can reboot the server. This will be done automatically after a few seconds by default.\n\nInstallation Failure\n\nIf the installation failed, check out specific errors on the second TTY (CTRL + ALT + F2) and ensure that the systems meets the minimum requirements.\n\nIf the installation is still not working, look at the how to get help chapter.\n\n2.3.1. Accessing the Management Interface Post-Installation\n\nAfter a succesful installation and reboot of the system you can use the Proxmox VE web interface for further configuration.\n\nPoint your browser to the IP address given during the installation and port 8006, for example: https://youripaddress:8006\n\nLog in using the root (realm PAM) username and the password chosen during installation.\n\nUpload your subscription key to gain access to the Enterprise repository. Otherwise, you will need to set up one of the public, less tested package repositories to get updates for security fixes, bug fixes, and new features.\n\nCheck the IP configuration and hostname.\n\nCheck the timezone.\n\nCheck your Firewall settings.\n\n2.3.2. Advanced LVM Configuration Options\n\nThe installer creates a Volume Group (VG) called pve, and additional Logical Volumes (LVs) called root, data, and swap, if ext4 or xfs is used. To control the size of these volumes use:\n\nhdsize\n\nDefines the total hard disk size to be used. This way you can reserve free space on the hard disk for further partitioning (for example for an additional PV and VG on the same hard disk that can be used for LVM storage).\n\nswapsize\n\nDefines the size of the swap volume. The default is the size of the installed memory, minimum 4 GB and maximum 8 GB. The resulting value cannot be greater than hdsize/8.\n\n\tIf set to 0, no swap volume will be created.\nmaxroot\n\nDefines the maximum size of the root volume, which stores the operation system. The maximum limit of the root volume size is hdsize/4.\n\nmaxvz\n\nDefines the maximum size of the data volume. The actual size of the data volume is:\n\ndatasize = hdsize - rootsize - swapsize - minfree\n\nWhere datasize cannot be bigger than maxvz.\n\n\tIn case of LVM thin, the data pool will only be created if datasize is bigger than 4GB.\n\tIf set to 0, no data volume will be created and the storage configuration will be adapted accordingly.\nminfree\n\nDefines the amount of free space that should be left in the LVM volume group pve. With more than 128GB storage available, the default is 16GB, otherwise hdsize/8 will be used.\n\n\tLVM requires free space in the VG for snapshot creation (not required for lvmthin snapshots).\n2.3.3. Advanced ZFS Configuration Options\n\nThe installer creates the ZFS pool rpool, if ZFS is used. No swap space is created but you can reserve some unpartitioned space on the install disks for swap. You can also create a swap zvol after the installation, although this can lead to problems (see ZFS swap notes).\n\nashift\n\nDefines the ashift value for the created pool. The ashift needs to be set at least to the sector-size of the underlying disks (2 to the power of ashift is the sector-size), or any disk which might be put in the pool (for example the replacement of a defective disk).\n\ncompress\n\nDefines whether compression is enabled for rpool.\n\nchecksum\n\nDefines which checksumming algorithm should be used for rpool.\n\ncopies\n\nDefines the copies parameter for rpool. Check the zfs(8) manpage for the semantics, and why this does not replace redundancy on disk-level.\n\nARC max size\n\nDefines the maximum size the ARC can grow to and thus limits the amount of memory ZFS will use. See also the section on how to limit ZFS memory usage for more details.\n\nhdsize\n\nDefines the total hard disk size to be used. This is useful to save free space on the hard disk(s) for further partitioning (for example to create a swap-partition). hdsize is only honored for bootable disks, that is only the first disk or mirror for RAID0, RAID1 or RAID10, and all disks in RAID-Z[123].\n\n2.3.4. ZFS Performance Tips\n\nZFS works best with a lot of memory. If you intend to use ZFS make sure to have enough RAM available for it. A good calculation is 4GB plus 1GB RAM for each TB RAW disk space.\n\nZFS can use a dedicated drive as write cache, called the ZFS Intent Log (ZIL). Use a fast drive (SSD) for it. It can be added after installation with the following command:\n\n# zpool add <pool-name> log </dev/path_to_fast_ssd>\n2.3.5. Adding the nomodeset Kernel Parameter\n\nProblems may arise on very old or very new hardware due to graphics drivers. If the installation hangs during boot, you can try adding the nomodeset parameter. This prevents the Linux kernel from loading any graphics drivers and forces it to continue using the BIOS/UEFI-provided framebuffer.\n\nOn the Proxmox VE bootloader menu, navigate to Install Proxmox VE (Terminal UI) and press e to edit the entry. Using the arrow keys, navigate to the line starting with linux, move the cursor to the end of that line and add the parameter nomodeset, separated by a space from the pre-existing last parameter.\n\nThen press Ctrl-X or F10 to boot the configuration.\n\n2.4. Unattended Installation\n\nIt is possible to install Proxmox VE automatically in an unattended manner. This enables you to fully automate the setup process on bare-metal. Once the installation is complete and the host has booted up, automation tools like Ansible can be used to further configure the installation.\n\nThe necessary options for the installer must be provided in an answer file. This file allows the use of filter rules to determine which disks and network cards should be used.\n\nTo use the automated installation, it is first necessary to prepare an installation ISO. Visit our wiki for more details and information on the unattended installation.\n\n2.5. Install Proxmox VE on Debian\n\nProxmox VE ships as a set of Debian packages and can be installed on top of a standard Debian installation. After configuring the repositories you need to run the following commands:\n\n# apt-get update\n# apt-get install proxmox-ve\n\nInstalling on top of an existing Debian installation looks easy, but it presumes that the base system has been installed correctly and that you know how you want to configure and use the local storage. You also need to configure the network manually.\n\nIn general, this is not trivial, especially when LVM or ZFS is used.\n\nA detailed step by step how-to can be found on the wiki.\n\n3. Host System Administration\n\nThe following sections will focus on common virtualization tasks and explain the Proxmox VE specifics regarding the administration and management of the host machine.\n\nProxmox VE is based on Debian GNU/Linux with additional repositories to provide the Proxmox VE related packages. This means that the full range of Debian packages is available including security updates and bug fixes. Proxmox VE provides its own Linux kernel based on the Ubuntu kernel. It has all the necessary virtualization and container features enabled and includes ZFS and several extra hardware drivers.\n\nFor other topics not included in the following sections, please refer to the Debian documentation. The Debian Administrator's Handbook is available online, and provides a comprehensive introduction to the Debian operating system (see [Hertzog13]).\n\n3.1. Package Repositories\n\nProxmox VE uses APT as its package management tool like any other Debian-based system.\n\nProxmox VE automatically checks for package updates on a daily basis. The root@pam user is notified via email about available updates. From the GUI, the Changelog button can be used to see more details about an selected update.\n\n3.1.1. Repositories in Proxmox VE\n\nRepositories are a collection of software packages, they can be used to install new software, but are also important to get new updates.\n\n\tYou need valid Debian and Proxmox repositories to get the latest security updates, bug fixes and new features.\n\nAPT Repositories are defined in the file /etc/apt/sources.list and in .list files placed in /etc/apt/sources.list.d/.\n\nRepository Management\n\nSince Proxmox VE 7, you can check the repository state in the web interface. The node summary panel shows a high level status overview, while the separate Repository panel shows in-depth status and list of all configured repositories.\n\nBasic repository management, for example, activating or deactivating a repository, is also supported.\n\nSources.list\n\nIn a sources.list file, each line defines a package repository. The preferred source must come first. Empty lines are ignored. A # character anywhere on a line marks the remainder of that line as a comment. The available packages from a repository are acquired by running apt-get update. Updates can be installed directly using apt-get, or via the GUI (Node → Updates).\n\nFile /etc/apt/sources.list\ndeb http://deb.debian.org/debian bookworm main contrib\ndeb http://deb.debian.org/debian bookworm-updates main contrib\n\n# security updates\ndeb http://security.debian.org/debian-security bookworm-security main contrib\n\nProxmox VE provides three different package repositories.\n\n3.1.2. Proxmox VE Enterprise Repository\n\nThis is the recommended repository and available for all Proxmox VE subscription users. It contains the most stable packages and is suitable for production use. The pve-enterprise repository is enabled by default:\n\nFile /etc/apt/sources.list.d/pve-enterprise.list\ndeb https://enterprise.proxmox.com/debian/pve bookworm pve-enterprise\n\nPlease note that you need a valid subscription key to access the pve-enterprise repository. We offer different support levels, which you can find further details about at https://proxmox.com/en/proxmox-virtual-environment/pricing.\n\n\tYou can disable this repository by commenting out the above line using a # (at the start of the line). This prevents error messages if your host does not have a subscription key. Please configure the pve-no-subscription repository in that case.\n3.1.3. Proxmox VE No-Subscription Repository\n\nAs the name suggests, you do not need a subscription key to access this repository. It can be used for testing and non-production use. It’s not recommended to use this on production servers, as these packages are not always as heavily tested and validated.\n\nWe recommend to configure this repository in /etc/apt/sources.list.\n\nFile /etc/apt/sources.list\ndeb http://ftp.debian.org/debian bookworm main contrib\ndeb http://ftp.debian.org/debian bookworm-updates main contrib\n\n# Proxmox VE pve-no-subscription repository provided by proxmox.com,\n# NOT recommended for production use\ndeb http://download.proxmox.com/debian/pve bookworm pve-no-subscription\n\n# security updates\ndeb http://security.debian.org/debian-security bookworm-security main contrib\n3.1.4. Proxmox VE Test Repository\n\nThis repository contains the latest packages and is primarily used by developers to test new features. To configure it, add the following line to /etc/apt/sources.list:\n\nsources.list entry for pvetest\ndeb http://download.proxmox.com/debian/pve bookworm pvetest\n\tThe pvetest repository should (as the name implies) only be used for testing new features or bug fixes.\n3.1.5. Ceph Reef Enterprise Repository\n\nThis repository holds the enterprise Proxmox VE Ceph 18.2 Reef packages. They are suitable for production. Use this repository if you run the Ceph client or a full Ceph cluster on Proxmox VE.\n\nFile /etc/apt/sources.list.d/ceph.list\ndeb https://enterprise.proxmox.com/debian/ceph-reef bookworm enterprise\n3.1.6. Ceph Reef No-Subscription Repository\n\nThis Ceph repository contains the Ceph 18.2 Reef packages before they are moved to the enterprise repository and after they where on the test repository.\n\n\tIt’s recommended to use the enterprise repository for production machines.\nFile /etc/apt/sources.list.d/ceph.list\ndeb http://download.proxmox.com/debian/ceph-reef bookworm no-subscription\n3.1.7. Ceph Reef Test Repository\n\nThis Ceph repository contains the Ceph 18.2 Reef packages before they are moved to the main repository. It is used to test new Ceph releases on Proxmox VE.\n\nFile /etc/apt/sources.list.d/ceph.list\ndeb http://download.proxmox.com/debian/ceph-reef bookworm test\n3.1.8. Ceph Quincy Enterprise Repository\n\nThis repository holds the enterprise Proxmox VE Ceph Quincy packages. They are suitable for production. Use this repository if you run the Ceph client or a full Ceph cluster on Proxmox VE.\n\nFile /etc/apt/sources.list.d/ceph.list\ndeb https://enterprise.proxmox.com/debian/ceph-quincy bookworm enterprise\n3.1.9. Ceph Quincy No-Subscription Repository\n\nThis Ceph repository contains the Ceph Quincy packages before they are moved to the enterprise repository and after they where on the test repository.\n\n\tIt’s recommended to use the enterprise repository for production machines.\nFile /etc/apt/sources.list.d/ceph.list\ndeb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription\n3.1.10. Ceph Quincy Test Repository\n\nThis Ceph repository contains the Ceph Quincy packages before they are moved to the main repository. It is used to test new Ceph releases on Proxmox VE.\n\nFile /etc/apt/sources.list.d/ceph.list\ndeb http://download.proxmox.com/debian/ceph-quincy bookworm test\n3.1.11. Older Ceph Repositories\n\nProxmox VE 8 doesn’t support Ceph Pacific, Ceph Octopus, or even older releases for hyper-converged setups. For those releases, you need to first upgrade Ceph to a newer release before upgrading to Proxmox VE 8.\n\nSee the respective upgrade guide for details.\n\n3.1.12. Debian Firmware Repository\n\nStarting with Debian Bookworm (Proxmox VE 8) non-free firmware (as defined by DFSG) has been moved to the newly created Debian repository component non-free-firmware.\n\nEnable this repository if you want to set up Early OS Microcode Updates or need additional Runtime Firmware Files not already included in the pre-installed package pve-firmware.\n\nTo be able to install packages from this component, run editor /etc/apt/sources.list, append non-free-firmware to the end of each .debian.org repository line and run apt update.\n\n3.1.13. SecureApt\n\nThe Release files in the repositories are signed with GnuPG. APT is using these signatures to verify that all packages are from a trusted source.\n\nIf you install Proxmox VE from an official ISO image, the key for verification is already installed.\n\nIf you install Proxmox VE on top of Debian, download and install the key with the following commands:\n\n # wget https://enterprise.proxmox.com/debian/proxmox-release-bookworm.gpg -O /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg\n\nVerify the checksum afterwards with the sha512sum CLI tool:\n\n# sha512sum /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg\n7da6fe34168adc6e479327ba517796d4702fa2f8b4f0a9833f5ea6e6b48f6507a6da403a274fe201595edc86a84463d50383d07f64bdde2e3658108db7d6dc87 /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg\n\nor the md5sum CLI tool:\n\n# md5sum /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg\n41558dc019ef90bd0f6067644a51cf5b /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg\n3.2. System Software Updates\n\nProxmox provides updates on a regular basis for all repositories. To install updates use the web-based GUI or the following CLI commands:\n\n# apt-get update\n# apt-get dist-upgrade\n\tThe APT package management system is very flexible and provides many features, see man apt-get, or [Hertzog13] for additional information.\n\tRegular updates are essential to get the latest patches and security related fixes. Major system upgrades are announced in the Proxmox VE Community Forum.\n3.3. Firmware Updates\n\nFirmware updates from this chapter should be applied when running Proxmox VE on a bare-metal server. Whether configuring firmware updates is appropriate within guests, e.g. when using device pass-through, depends strongly on your setup and is therefore out of scope.\n\nIn addition to regular software updates, firmware updates are also important for reliable and secure operation.\n\nWhen obtaining and applying firmware updates, a combination of available options is recommended to get them as early as possible or at all.\n\nThe term firmware is usually divided linguistically into microcode (for CPUs) and firmware (for other devices).\n\n3.3.1. Persistent Firmware\n\nThis section is suitable for all devices. Updated microcode, which is usually included in a BIOS/UEFI update, is stored on the motherboard, whereas other firmware is stored on the respective device. This persistent method is especially important for the CPU, as it enables the earliest possible regular loading of the updated microcode at boot time.\n\n\tWith some updates, such as for BIOS/UEFI or storage controller, the device configuration could be reset. Please follow the vendor’s instructions carefully and back up the current configuration.\n\nPlease check with your vendor which update methods are available.\n\nConvenient update methods for servers can include Dell’s Lifecycle Manager or Service Packs from HPE.\n\nSometimes there are Linux utilities available as well. Examples are mlxup for NVIDIA ConnectX or bnxtnvm/niccli for Broadcom network cards.\n\nLVFS is also an option if there is a cooperation with the hardware vendor and supported hardware in use. The technical requirement for this is that the system was manufactured after 2014 and is booted via UEFI.\n\nProxmox VE ships its own version of the fwupd package to enable Secure Boot Support with the Proxmox signing key. This package consciously dropped the dependency recommendation for the udisks2 package, due to observed issues with its use on hypervisors. That means you must explicitly configure the correct mount point of the EFI partition in /etc/fwupd/daemon.conf, for example:\n\nFile /etc/fwupd/daemon.conf\n# Override the location used for the EFI system partition (ESP) path.\nEspLocation=/boot/efi\n\tIf the update instructions require a host reboot, make sure that it can be done safely. See also Node Maintenance.\n3.3.2. Runtime Firmware Files\n\nThis method stores firmware on the Proxmox VE operating system and will pass it to a device if its persisted firmware is less recent. It is supported by devices such as network and graphics cards, but not by those that rely on persisted firmware such as the motherboard and hard disks.\n\nIn Proxmox VE the package pve-firmware is already installed by default. Therefore, with the normal system updates (APT), included firmware of common hardware is automatically kept up to date.\n\nAn additional Debian Firmware Repository exists, but is not configured by default.\n\nIf you try to install an additional firmware package but it conflicts, APT will abort the installation. Perhaps the particular firmware can be obtained in another way.\n\n3.3.3. CPU Microcode Updates\n\nMicrocode updates are intended to fix found security vulnerabilities and other serious CPU bugs. While the CPU performance can be affected, a patched microcode is usually still more performant than an unpatched microcode where the kernel itself has to do mitigations. Depending on the CPU type, it is possible that performance results of the flawed factory state can no longer be achieved without knowingly running the CPU in an unsafe state.\n\nTo get an overview of present CPU vulnerabilities and their mitigations, run lscpu. Current real-world known vulnerabilities can only show up if the Proxmox VE host is up to date, its version not end of life, and has at least been rebooted since the last kernel update.\n\nBesides the recommended microcode update via persistent BIOS/UEFI updates, there is also an independent method via Early OS Microcode Updates. It is convenient to use and also quite helpful when the motherboard vendor no longer provides BIOS/UEFI updates. Regardless of the method in use, a reboot is always needed to apply a microcode update.\n\nSet up Early OS Microcode Updates\n\nTo set up microcode updates that are applied early on boot by the Linux kernel, you need to:\n\nEnable the Debian Firmware Repository\n\nGet the latest available packages apt update (or use the web interface, under Node → Updates)\n\nInstall the CPU-vendor specific microcode package:\n\nFor Intel CPUs: apt install intel-microcode\n\nFor AMD CPUs: apt install amd64-microcode\n\nReboot the Proxmox VE host\n\nAny future microcode update will also require a reboot to be loaded.\n\nMicrocode Version\n\nTo get the current running microcode revision for comparison or debugging purposes:\n\n# grep microcode /proc/cpuinfo | uniq\nmicrocode       : 0xf0\n\nA microcode package has updates for many different CPUs. But updates specifically for your CPU might not come often. So, just looking at the date on the package won’t tell you when the company actually released an update for your specific CPU.\n\nIf you’ve installed a new microcode package and rebooted your Proxmox VE host, and this new microcode is newer than both, the version baked into the CPU and the one from the motherboard’s firmware, you’ll see a message in the system log saying \"microcode updated early\".\n\n# dmesg | grep microcode\n[    0.000000] microcode: microcode updated early to revision 0xf0, date = 2021-11-12\n[    0.896580] microcode: Microcode Update Driver: v2.2.\nTroubleshooting\n\nFor debugging purposes, the set up Early OS Microcode Update applied regularly at system boot can be temporarily disabled as follows:\n\nmake sure that the host can be rebooted safely\n\nreboot the host to get to the GRUB menu (hold SHIFT if it is hidden)\n\nat the desired Proxmox VE boot entry press E\n\ngo to the line which starts with linux and append separated by a space dis_ucode_ldr\n\npress CTRL-X to boot this time without an Early OS Microcode Update\n\nIf a problem related to a recent microcode update is suspected, a package downgrade should be considered instead of package removal (apt purge <intel-microcode|amd64-microcode>). Otherwise, a too old persisted microcode might be loaded, even though a more recent one would run without problems.\n\nA downgrade is possible if an earlier microcode package version is available in the Debian repository, as shown in this example:\n\n# apt list -a intel-microcode\nListing... Done\nintel-microcode/stable-security,now 3.20230808.1~deb12u1 amd64 [installed]\nintel-microcode/stable 3.20230512.1 amd64\n# apt install intel-microcode=3.202305*\n...\nSelected version '3.20230512.1' (Debian:12.1/stable [amd64]) for 'intel-microcode'\n...\ndpkg: warning: downgrading intel-microcode from 3.20230808.1~deb12u1 to 3.20230512.1\n...\nintel-microcode: microcode will be updated at next boot\n...\n\nMake sure (again) that the host can be rebooted safely. To apply an older microcode potentially included in the microcode package for your CPU type, reboot now.\n\n\t\n\nIt makes sense to hold the downgraded package for a while and try more recent versions again at a later time. Even if the package version is the same in the future, system updates may have fixed the experienced problem in the meantime.\n\n# apt-mark hold intel-microcode\nintel-microcode set on hold.\n# apt-mark unhold intel-microcode\n# apt update\n# apt upgrade\n3.4. Network Configuration\n\nProxmox VE is using the Linux network stack. This provides a lot of flexibility on how to set up the network on the Proxmox VE nodes. The configuration can be done either via the GUI, or by manually editing the file /etc/network/interfaces, which contains the whole network configuration. The interfaces(5) manual page contains the complete format description. All Proxmox VE tools try hard to keep direct user modifications, but using the GUI is still preferable, because it protects you from errors.\n\nA Linux bridge interface (commonly called vmbrX) is needed to connect guests to the underlying physical network. It can be thought of as a virtual switch which the guests and physical interfaces are connected to. This section provides some examples on how the network can be set up to accomodate different use cases like redundancy with a bond, vlans or routed and NAT setups.\n\nThe Software Defined Network is an option for more complex virtual networks in Proxmox VE clusters.\n\n\tIt’s discouraged to use the traditional Debian tools ifup and ifdown if unsure, as they have some pitfalls like interupting all guest traffic on ifdown vmbrX but not reconnecting those guest again when doing ifup on the same bridge later.\n3.4.1. Apply Network Changes\n\nProxmox VE does not write changes directly to /etc/network/interfaces. Instead, we write into a temporary file called /etc/network/interfaces.new, this way you can do many related changes at once. This also allows to ensure your changes are correct before applying, as a wrong network configuration may render a node inaccessible.\n\nLive-Reload Network with ifupdown2\n\nWith the recommended ifupdown2 package (default for new installations since Proxmox VE 7.0), it is possible to apply network configuration changes without a reboot. If you change the network configuration via the GUI, you can click the Apply Configuration button. This will move changes from the staging interfaces.new file to /etc/network/interfaces and apply them live.\n\nIf you made manual changes directly to the /etc/network/interfaces file, you can apply them by running ifreload -a\n\n\tIf you installed Proxmox VE on top of Debian, or upgraded to Proxmox VE 7.0 from an older Proxmox VE installation, make sure ifupdown2 is installed: apt install ifupdown2\nReboot Node to Apply\n\nAnother way to apply a new network configuration is to reboot the node. In that case the systemd service pvenetcommit will activate the staging interfaces.new file before the networking service will apply that configuration.\n\n3.4.2. Naming Conventions\n\nWe currently use the following naming conventions for device names:\n\nEthernet devices: en*, systemd network interface names. This naming scheme is used for new Proxmox VE installations since version 5.0.\n\nEthernet devices: eth[N], where 0 ≤ N (eth0, eth1, …) This naming scheme is used for Proxmox VE hosts which were installed before the 5.0 release. When upgrading to 5.0, the names are kept as-is.\n\nBridge names: Commonly vmbr[N], where 0 ≤ N ≤ 4094 (vmbr0 - vmbr4094), but you can use any alphanumeric string that starts with a character and is at most 10 characters long.\n\nBonds: bond[N], where 0 ≤ N (bond0, bond1, …)\n\nVLANs: Simply add the VLAN number to the device name, separated by a period (eno1.50, bond1.30)\n\nThis makes it easier to debug networks problems, because the device name implies the device type.\n\nSystemd Network Interface Names\n\nSystemd defines a versioned naming scheme for network device names. The scheme uses the two-character prefix en for Ethernet network devices. The next characters depends on the device driver, device location and other attributes. Some possible patterns are:\n\no<index>[n<phys_port_name>|d<dev_port>] — devices on board\n\ns<slot>[f<function>][n<phys_port_name>|d<dev_port>] — devices by hotplug id\n\n[P<domain>]p<bus>s<slot>[f<function>][n<phys_port_name>|d<dev_port>] — devices by bus id\n\nx<MAC> — devices by MAC address\n\nSome examples for the most common patterns are:\n\neno1 — is the first on-board NIC\n\nenp3s0f1 — is function 1 of the NIC on PCI bus 3, slot 0\n\nFor a full list of possible device name patterns, see the systemd.net-naming-scheme(7) manpage.\n\nA new version of systemd may define a new version of the network device naming scheme, which it then uses by default. Consequently, updating to a newer systemd version, for example during a major Proxmox VE upgrade, can change the names of network devices and require adjusting the network configuration. To avoid name changes due to a new version of the naming scheme, you can manually pin a particular naming scheme version (see below).\n\nHowever, even with a pinned naming scheme version, network device names can still change due to kernel or driver updates. In order to avoid name changes for a particular network device altogether, you can manually override its name using a link file (see below).\n\nFor more information on network interface names, see Predictable Network Interface Names.\n\nPinning a specific naming scheme version\n\nYou can pin a specific version of the naming scheme for network devices by adding the net.naming-scheme=<version> parameter to the kernel command line. For a list of naming scheme versions, see the systemd.net-naming-scheme(7) manpage.\n\nFor example, to pin the version v252, which is the latest naming scheme version for a fresh Proxmox VE 8.0 installation, add the following kernel command-line parameter:\n\nnet.naming-scheme=v252\n\nSee also this section on editing the kernel command line. You need to reboot for the changes to take effect.\n\nOverriding network device names\n\nYou can manually assign a name to a particular network device using a custom systemd.link file. This overrides the name that would be assigned according to the latest network device naming scheme. This way, you can avoid naming changes due to kernel updates, driver updates or newer versions of the naming scheme.\n\nCustom link files should be placed in /etc/systemd/network/ and named <n>-<id>.link, where n is a priority smaller than 99 and id is some identifier. A link file has two sections: [Match] determines which interfaces the file will apply to; [Link] determines how these interfaces should be configured, including their naming.\n\nTo assign a name to a particular network device, you need a way to uniquely and permanently identify that device in the [Match] section. One possibility is to match the device’s MAC address using the MACAddress option, as it is unlikely to change. Then, you can assign a name using the Name option in the [Link] section.\n\nFor example, to assign the name enwan0 to the device with MAC address aa:bb:cc:dd:ee:ff, create a file /etc/systemd/network/10-enwan0.link with the following contents:\n\n[Match]\nMACAddress=aa:bb:cc:dd:ee:ff\n\n[Link]\nName=enwan0\n\nDo not forget to adjust /etc/network/interfaces to use the new name. You need to reboot the node for the change to take effect.\n\n\tIt is recommended to assign a name starting with en or eth so that Proxmox VE recognizes the interface as a physical network device which can then be configured via the GUI. Also, you should ensure that the name will not clash with other interface names in the future. One possibility is to assign a name that does not match any name pattern that systemd uses for network interfaces (see above), such as enwan0 in the example above.\n\nFor more information on link files, see the systemd.link(5) manpage.\n\n3.4.3. Choosing a network configuration\n\nDepending on your current network organization and your resources you can choose either a bridged, routed, or masquerading networking setup.\n\nProxmox VE server in a private LAN, using an external gateway to reach the internet\n\nThe Bridged model makes the most sense in this case, and this is also the default mode on new Proxmox VE installations. Each of your Guest system will have a virtual interface attached to the Proxmox VE bridge. This is similar in effect to having the Guest network card directly connected to a new switch on your LAN, the Proxmox VE host playing the role of the switch.\n\nProxmox VE server at hosting provider, with public IP ranges for Guests\n\nFor this setup, you can use either a Bridged or Routed model, depending on what your provider allows.\n\nProxmox VE server at hosting provider, with a single public IP address\n\nIn that case the only way to get outgoing network accesses for your guest systems is to use Masquerading. For incoming network access to your guests, you will need to configure Port Forwarding.\n\nFor further flexibility, you can configure VLANs (IEEE 802.1q) and network bonding, also known as \"link aggregation\". That way it is possible to build complex and flexible virtual networks.\n\n3.4.4. Default Configuration using a Bridge\n\nBridges are like physical network switches implemented in software. All virtual guests can share a single bridge, or you can create multiple bridges to separate network domains. Each host can have up to 4094 bridges.\n\nThe installation program creates a single bridge named vmbr0, which is connected to the first Ethernet card. The corresponding configuration in /etc/network/interfaces might look like this:\n\nauto lo\niface lo inet loopback\n\niface eno1 inet manual\n\nauto vmbr0\niface vmbr0 inet static\n        address 192.168.10.2/24\n        gateway 192.168.10.1\n        bridge-ports eno1\n        bridge-stp off\n        bridge-fd 0\n\nVirtual machines behave as if they were directly connected to the physical network. The network, in turn, sees each virtual machine as having its own MAC, even though there is only one network cable connecting all of these VMs to the network.\n\n3.4.5. Routed Configuration\n\nMost hosting providers do not support the above setup. For security reasons, they disable networking as soon as they detect multiple MAC addresses on a single interface.\n\n\tSome providers allow you to register additional MACs through their management interface. This avoids the problem, but can be clumsy to configure because you need to register a MAC for each of your VMs.\n\nYou can avoid the problem by “routing” all traffic via a single interface. This makes sure that all network packets use the same MAC address.\n\nA common scenario is that you have a public IP (assume 198.51.100.5 for this example), and an additional IP block for your VMs (203.0.113.16/28). We recommend the following setup for such situations:\n\nauto lo\niface lo inet loopback\n\nauto eno0\niface eno0 inet static\n        address  198.51.100.5/29\n        gateway  198.51.100.1\n        post-up echo 1 > /proc/sys/net/ipv4/ip_forward\n        post-up echo 1 > /proc/sys/net/ipv4/conf/eno0/proxy_arp\n\n\nauto vmbr0\niface vmbr0 inet static\n        address  203.0.113.17/28\n        bridge-ports none\n        bridge-stp off\n        bridge-fd 0\n3.4.6. Masquerading (NAT) with iptables\n\nMasquerading allows guests having only a private IP address to access the network by using the host IP address for outgoing traffic. Each outgoing packet is rewritten by iptables to appear as originating from the host, and responses are rewritten accordingly to be routed to the original sender.\n\nauto lo\niface lo inet loopback\n\nauto eno1\n#real IP address\niface eno1 inet static\n        address  198.51.100.5/24\n        gateway  198.51.100.1\n\nauto vmbr0\n#private sub network\niface vmbr0 inet static\n        address  10.10.10.1/24\n        bridge-ports none\n        bridge-stp off\n        bridge-fd 0\n\n        post-up   echo 1 > /proc/sys/net/ipv4/ip_forward\n        post-up   iptables -t nat -A POSTROUTING -s '10.10.10.0/24' -o eno1 -j MASQUERADE\n        post-down iptables -t nat -D POSTROUTING -s '10.10.10.0/24' -o eno1 -j MASQUERADE\n\tIn some masquerade setups with firewall enabled, conntrack zones might be needed for outgoing connections. Otherwise the firewall could block outgoing connections since they will prefer the POSTROUTING of the VM bridge (and not MASQUERADE).\n\nAdding these lines in the /etc/network/interfaces can fix this problem:\n\npost-up   iptables -t raw -I PREROUTING -i fwbr+ -j CT --zone 1\npost-down iptables -t raw -D PREROUTING -i fwbr+ -j CT --zone 1\n\nFor more information about this, refer to the following links:\n\nNetfilter Packet Flow\n\nPatch on netdev-list introducing conntrack zones\n\nBlog post with a good explanation by using TRACE in the raw table\n\n3.4.7. Linux Bond\n\nBonding (also called NIC teaming or Link Aggregation) is a technique for binding multiple NIC’s to a single network device. It is possible to achieve different goals, like make the network fault-tolerant, increase the performance or both together.\n\nHigh-speed hardware like Fibre Channel and the associated switching hardware can be quite expensive. By doing link aggregation, two NICs can appear as one logical interface, resulting in double speed. This is a native Linux kernel feature that is supported by most switches. If your nodes have multiple Ethernet ports, you can distribute your points of failure by running network cables to different switches and the bonded connection will failover to one cable or the other in case of network trouble.\n\nAggregated links can improve live-migration delays and improve the speed of replication of data between Proxmox VE Cluster nodes.\n\nThere are 7 modes for bonding:\n\nRound-robin (balance-rr): Transmit network packets in sequential order from the first available network interface (NIC) slave through the last. This mode provides load balancing and fault tolerance.\n\nActive-backup (active-backup): Only one NIC slave in the bond is active. A different slave becomes active if, and only if, the active slave fails. The single logical bonded interface’s MAC address is externally visible on only one NIC (port) to avoid distortion in the network switch. This mode provides fault tolerance.\n\nXOR (balance-xor): Transmit network packets based on [(source MAC address XOR’d with destination MAC address) modulo NIC slave count]. This selects the same NIC slave for each destination MAC address. This mode provides load balancing and fault tolerance.\n\nBroadcast (broadcast): Transmit network packets on all slave network interfaces. This mode provides fault tolerance.\n\nIEEE 802.3ad Dynamic link aggregation (802.3ad)(LACP): Creates aggregation groups that share the same speed and duplex settings. Utilizes all slave network interfaces in the active aggregator group according to the 802.3ad specification.\n\nAdaptive transmit load balancing (balance-tlb): Linux bonding driver mode that does not require any special network-switch support. The outgoing network packet traffic is distributed according to the current load (computed relative to the speed) on each network interface slave. Incoming traffic is received by one currently designated slave network interface. If this receiving slave fails, another slave takes over the MAC address of the failed receiving slave.\n\nAdaptive load balancing (balance-alb): Includes balance-tlb plus receive load balancing (rlb) for IPV4 traffic, and does not require any special network switch support. The receive load balancing is achieved by ARP negotiation. The bonding driver intercepts the ARP Replies sent by the local system on their way out and overwrites the source hardware address with the unique hardware address of one of the NIC slaves in the single logical bonded interface such that different network-peers use different MAC addresses for their network packet traffic.\n\nIf your switch support the LACP (IEEE 802.3ad) protocol then we recommend using the corresponding bonding mode (802.3ad). Otherwise you should generally use the active-backup mode.\n\nFor the cluster network (Corosync) we recommend configuring it with multiple networks. Corosync does not need a bond for network reduncancy as it can switch between networks by itself, if one becomes unusable.\n\nThe following bond configuration can be used as distributed/shared storage network. The benefit would be that you get more speed and the network will be fault-tolerant.\n\nExample: Use bond with fixed IP address\nauto lo\niface lo inet loopback\n\niface eno1 inet manual\n\niface eno2 inet manual\n\niface eno3 inet manual\n\nauto bond0\niface bond0 inet static\n      bond-slaves eno1 eno2\n      address  192.168.1.2/24\n      bond-miimon 100\n      bond-mode 802.3ad\n      bond-xmit-hash-policy layer2+3\n\nauto vmbr0\niface vmbr0 inet static\n        address  10.10.10.2/24\n        gateway  10.10.10.1\n        bridge-ports eno3\n        bridge-stp off\n        bridge-fd 0\n\nAnother possibility it to use the bond directly as bridge port. This can be used to make the guest network fault-tolerant.\n\nExample: Use a bond as bridge port\nauto lo\niface lo inet loopback\n\niface eno1 inet manual\n\niface eno2 inet manual\n\nauto bond0\niface bond0 inet manual\n      bond-slaves eno1 eno2\n      bond-miimon 100\n      bond-mode 802.3ad\n      bond-xmit-hash-policy layer2+3\n\nauto vmbr0\niface vmbr0 inet static\n        address  10.10.10.2/24\n        gateway  10.10.10.1\n        bridge-ports bond0\n        bridge-stp off\n        bridge-fd 0\n3.4.8. VLAN 802.1Q\n\nA virtual LAN (VLAN) is a broadcast domain that is partitioned and isolated in the network at layer two. So it is possible to have multiple networks (4096) in a physical network, each independent of the other ones.\n\nEach VLAN network is identified by a number often called tag. Network packages are then tagged to identify which virtual network they belong to.\n\nVLAN for Guest Networks\n\nProxmox VE supports this setup out of the box. You can specify the VLAN tag when you create a VM. The VLAN tag is part of the guest network configuration. The networking layer supports different modes to implement VLANs, depending on the bridge configuration:\n\nVLAN awareness on the Linux bridge: In this case, each guest’s virtual network card is assigned to a VLAN tag, which is transparently supported by the Linux bridge. Trunk mode is also possible, but that makes configuration in the guest necessary.\n\n\"traditional\" VLAN on the Linux bridge: In contrast to the VLAN awareness method, this method is not transparent and creates a VLAN device with associated bridge for each VLAN. That is, creating a guest on VLAN 5 for example, would create two interfaces eno1.5 and vmbr0v5, which would remain until a reboot occurs.\n\nOpen vSwitch VLAN: This mode uses the OVS VLAN feature.\n\nGuest configured VLAN: VLANs are assigned inside the guest. In this case, the setup is completely done inside the guest and can not be influenced from the outside. The benefit is that you can use more than one VLAN on a single virtual NIC.\n\nVLAN on the Host\n\nTo allow host communication with an isolated network. It is possible to apply VLAN tags to any network device (NIC, Bond, Bridge). In general, you should configure the VLAN on the interface with the least abstraction layers between itself and the physical NIC.\n\nFor example, in a default configuration where you want to place the host management address on a separate VLAN.\n\nExample: Use VLAN 5 for the Proxmox VE management IP with traditional Linux bridge\nauto lo\niface lo inet loopback\n\niface eno1 inet manual\n\niface eno1.5 inet manual\n\nauto vmbr0v5\niface vmbr0v5 inet static\n        address  10.10.10.2/24\n        gateway  10.10.10.1\n        bridge-ports eno1.5\n        bridge-stp off\n        bridge-fd 0\n\nauto vmbr0\niface vmbr0 inet manual\n        bridge-ports eno1\n        bridge-stp off\n        bridge-fd 0\nExample: Use VLAN 5 for the Proxmox VE management IP with VLAN aware Linux bridge\nauto lo\niface lo inet loopback\n\niface eno1 inet manual\n\n\nauto vmbr0.5\niface vmbr0.5 inet static\n        address  10.10.10.2/24\n        gateway  10.10.10.1\n\nauto vmbr0\niface vmbr0 inet manual\n        bridge-ports eno1\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n\nThe next example is the same setup but a bond is used to make this network fail-safe.\n\nExample: Use VLAN 5 with bond0 for the Proxmox VE management IP with traditional Linux bridge\nauto lo\niface lo inet loopback\n\niface eno1 inet manual\n\niface eno2 inet manual\n\nauto bond0\niface bond0 inet manual\n      bond-slaves eno1 eno2\n      bond-miimon 100\n      bond-mode 802.3ad\n      bond-xmit-hash-policy layer2+3\n\niface bond0.5 inet manual\n\nauto vmbr0v5\niface vmbr0v5 inet static\n        address  10.10.10.2/24\n        gateway  10.10.10.1\n        bridge-ports bond0.5\n        bridge-stp off\n        bridge-fd 0\n\nauto vmbr0\niface vmbr0 inet manual\n        bridge-ports bond0\n        bridge-stp off\n        bridge-fd 0\n3.4.9. Disabling IPv6 on the Node\n\nProxmox VE works correctly in all environments, irrespective of whether IPv6 is deployed or not. We recommend leaving all settings at the provided defaults.\n\nShould you still need to disable support for IPv6 on your node, do so by creating an appropriate sysctl.conf (5) snippet file and setting the proper sysctls, for example adding /etc/sysctl.d/disable-ipv6.conf with content:\n\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n\nThis method is preferred to disabling the loading of the IPv6 module on the kernel commandline.\n\n3.4.10. Disabling MAC Learning on a Bridge\n\nBy default, MAC learning is enabled on a bridge to ensure a smooth experience with virtual guests and their networks.\n\nBut in some environments this can be undesired. Since Proxmox VE 7.3 you can disable MAC learning on the bridge by setting the ‘bridge-disable-mac-learning 1` configuration on a bridge in `/etc/network/interfaces’, for example:\n\n# ...\n\nauto vmbr0\niface vmbr0 inet static\n        address  10.10.10.2/24\n        gateway  10.10.10.1\n        bridge-ports ens18\n        bridge-stp off\n        bridge-fd 0\n        bridge-disable-mac-learning 1\n\nOnce enabled, Proxmox VE will manually add the configured MAC address from VMs and Containers to the bridges forwarding database to ensure that guest can still use the network - but only when they are using their actual MAC address.\n\n3.5. Time Synchronization\n\nThe Proxmox VE cluster stack itself relies heavily on the fact that all the nodes have precisely synchronized time. Some other components, like Ceph, also won’t work properly if the local time on all nodes is not in sync.\n\nTime synchronization between nodes can be achieved using the “Network Time Protocol” (NTP). As of Proxmox VE 7, chrony is used as the default NTP daemon, while Proxmox VE 6 uses systemd-timesyncd. Both come preconfigured to use a set of public servers.\n\n\tIf you upgrade your system to Proxmox VE 7, it is recommended that you manually install either chrony, ntp or openntpd.\n3.5.1. Using Custom NTP Servers\n\nIn some cases, it might be desired to use non-default NTP servers. For example, if your Proxmox VE nodes do not have access to the public internet due to restrictive firewall rules, you need to set up local NTP servers and tell the NTP daemon to use them.\n\nFor systems using chrony:\n\nSpecify which servers chrony should use in /etc/chrony/chrony.conf:\n\nserver ntp1.example.com iburst\nserver ntp2.example.com iburst\nserver ntp3.example.com iburst\n\nRestart chrony:\n\n# systemctl restart chronyd\n\nCheck the journal to confirm that the newly configured NTP servers are being used:\n\n# journalctl --since -1h -u chrony\n...\nAug 26 13:00:09 node1 systemd[1]: Started chrony, an NTP client/server.\nAug 26 13:00:15 node1 chronyd[4873]: Selected source 10.0.0.1 (ntp1.example.com)\nAug 26 13:00:15 node1 chronyd[4873]: System clock TAI offset set to 37 seconds\n...\nFor systems using systemd-timesyncd:\n\nSpecify which servers systemd-timesyncd should use in /etc/systemd/timesyncd.conf:\n\n[Time]\nNTP=ntp1.example.com ntp2.example.com ntp3.example.com ntp4.example.com\n\nThen, restart the synchronization service (systemctl restart systemd-timesyncd), and verify that your newly configured NTP servers are in use by checking the journal (journalctl --since -1h -u systemd-timesyncd):\n\n...\nOct 07 14:58:36 node1 systemd[1]: Stopping Network Time Synchronization...\nOct 07 14:58:36 node1 systemd[1]: Starting Network Time Synchronization...\nOct 07 14:58:36 node1 systemd[1]: Started Network Time Synchronization.\nOct 07 14:58:36 node1 systemd-timesyncd[13514]: Using NTP server 10.0.0.1:123 (ntp1.example.com).\nOct 07 14:58:36 node1 systemd-timesyncd[13514]: interval/delta/delay/jitter/drift 64s/-0.002s/0.020s/0.000s/-31ppm\n...\n3.6. External Metric Server\n\nIn Proxmox VE, you can define external metric servers, which will periodically receive various stats about your hosts, virtual guests and storages.\n\nCurrently supported are:\n\nGraphite (see https://graphiteapp.org )\n\nInfluxDB (see https://www.influxdata.com/time-series-platform/influxdb/ )\n\nThe external metric server definitions are saved in /etc/pve/status.cfg, and can be edited through the web interface.\n\n3.6.1. Graphite server configuration\n\nThe default port is set to 2003 and the default graphite path is proxmox.\n\nBy default, Proxmox VE sends the data over UDP, so the graphite server has to be configured to accept this. Here the maximum transmission unit (MTU) can be configured for environments not using the standard 1500 MTU.\n\nYou can also configure the plugin to use TCP. In order not to block the important pvestatd statistic collection daemon, a timeout is required to cope with network problems.\n\n3.6.2. Influxdb plugin configuration\n\nProxmox VE sends the data over UDP, so the influxdb server has to be configured for this. The MTU can also be configured here, if necessary.\n\nHere is an example configuration for influxdb (on your influxdb server):\n\n[[udp]]\n   enabled = true\n   bind-address = \"0.0.0.0:8089\"\n   database = \"proxmox\"\n   batch-size = 1000\n   batch-timeout = \"1s\"\n\nWith this configuration, your server listens on all IP addresses on port 8089, and writes the data in the proxmox database\n\nAlternatively, the plugin can be configured to use the http(s) API of InfluxDB 2.x. InfluxDB 1.8.x does contain a forwards compatible API endpoint for this v2 API.\n\nTo use it, set influxdbproto to http or https (depending on your configuration). By default, Proxmox VE uses the organization proxmox and the bucket/db proxmox (They can be set with the configuration organization and bucket respectively).\n\nSince InfluxDB’s v2 API is only available with authentication, you have to generate a token that can write into the correct bucket and set it.\n\nIn the v2 compatible API of 1.8.x, you can use user:password as token (if required), and can omit the organization since that has no meaning in InfluxDB 1.x.\n\nYou can also set the HTTP Timeout (default is 1s) with the timeout setting, as well as the maximum batch size (default 25000000 bytes) with the max-body-size setting (this corresponds to the InfluxDB setting with the same name).\n\n3.7. Disk Health Monitoring\n\nAlthough a robust and redundant storage is recommended, it can be very helpful to monitor the health of your local disks.\n\nStarting with Proxmox VE 4.3, the package smartmontools [1] is installed and required. This is a set of tools to monitor and control the S.M.A.R.T. system for local hard disks.\n\nYou can get the status of a disk by issuing the following command:\n\n# smartctl -a /dev/sdX\n\nwhere /dev/sdX is the path to one of your local disks.\n\nIf the output says:\n\nSMART support is: Disabled\n\nyou can enable it with the command:\n\n# smartctl -s on /dev/sdX\n\nFor more information on how to use smartctl, please see man smartctl.\n\nBy default, smartmontools daemon smartd is active and enabled, and scans the disks under /dev/sdX and /dev/hdX every 30 minutes for errors and warnings, and sends an e-mail to root if it detects a problem.\n\nFor more information about how to configure smartd, please see man smartd and man smartd.conf.\n\nIf you use your hard disks with a hardware raid controller, there are most likely tools to monitor the disks in the raid array and the array itself. For more information about this, please refer to the vendor of your raid controller.\n\n3.8. Logical Volume Manager (LVM)\n\nMost people install Proxmox VE directly on a local disk. The Proxmox VE installation CD offers several options for local disk management, and the current default setup uses LVM. The installer lets you select a single disk for such setup, and uses that disk as physical volume for the Volume Group (VG) pve. The following output is from a test installation using a small 8GB disk:\n\n# pvs\n  PV         VG   Fmt  Attr PSize PFree\n  /dev/sda3  pve  lvm2 a--  7.87g 876.00m\n\n# vgs\n  VG   #PV #LV #SN Attr   VSize VFree\n  pve    1   3   0 wz--n- 7.87g 876.00m\n\nThe installer allocates three Logical Volumes (LV) inside this VG:\n\n# lvs\n  LV   VG   Attr       LSize   Pool Origin Data%  Meta%\n  data pve  twi-a-tz--   4.38g             0.00   0.63\n  root pve  -wi-ao----   1.75g\n  swap pve  -wi-ao---- 896.00m\nroot\n\nFormatted as ext4, and contains the operating system.\n\nswap\n\nSwap partition\n\ndata\n\nThis volume uses LVM-thin, and is used to store VM images. LVM-thin is preferable for this task, because it offers efficient support for snapshots and clones.\n\nFor Proxmox VE versions up to 4.1, the installer creates a standard logical volume called “data”, which is mounted at /var/lib/vz.\n\nStarting from version 4.2, the logical volume “data” is a LVM-thin pool, used to store block based guest images, and /var/lib/vz is simply a directory on the root file system.\n\n3.8.1. Hardware\n\nWe highly recommend to use a hardware RAID controller (with BBU) for such setups. This increases performance, provides redundancy, and make disk replacements easier (hot-pluggable).\n\nLVM itself does not need any special hardware, and memory requirements are very low.\n\n3.8.2. Bootloader\n\nWe install two boot loaders by default. The first partition contains the standard GRUB boot loader. The second partition is an EFI System Partition (ESP), which makes it possible to boot on EFI systems and to apply persistent firmware updates from the user space.\n\n3.8.3. Creating a Volume Group\n\nLet’s assume we have an empty disk /dev/sdb, onto which we want to create a volume group named “vmdata”.\n\n\tPlease note that the following commands will destroy all existing data on /dev/sdb.\n\nFirst create a partition.\n\n# sgdisk -N 1 /dev/sdb\n\nCreate a Physical Volume (PV) without confirmation and 250K metadatasize.\n\n# pvcreate --metadatasize 250k -y -ff /dev/sdb1\n\nCreate a volume group named “vmdata” on /dev/sdb1\n\n# vgcreate vmdata /dev/sdb1\n3.8.4. Creating an extra LV for /var/lib/vz\n\nThis can be easily done by creating a new thin LV.\n\n# lvcreate -n <Name> -V <Size[M,G,T]> <VG>/<LVThin_pool>\n\nA real world example:\n\n# lvcreate -n vz -V 10G pve/data\n\nNow a filesystem must be created on the LV.\n\n# mkfs.ext4 /dev/pve/vz\n\nAt last this has to be mounted.\n\n\tbe sure that /var/lib/vz is empty. On a default installation it’s not.\n\nTo make it always accessible add the following line in /etc/fstab.\n\n# echo '/dev/pve/vz /var/lib/vz ext4 defaults 0 2' >> /etc/fstab\n3.8.5. Resizing the thin pool\n\nResize the LV and the metadata pool with the following command:\n\n# lvresize --size +<size[\\M,G,T]> --poolmetadatasize +<size[\\M,G]> <VG>/<LVThin_pool>\n\tWhen extending the data pool, the metadata pool must also be extended.\n3.8.6. Create a LVM-thin pool\n\nA thin pool has to be created on top of a volume group. How to create a volume group see Section LVM.\n\n# lvcreate -L 80G -T -n vmstore vmdata\n3.9. ZFS on Linux\n\nZFS is a combined file system and logical volume manager designed by Sun Microsystems. Starting with Proxmox VE 3.4, the native Linux kernel port of the ZFS file system is introduced as optional file system and also as an additional selection for the root file system. There is no need for manually compile ZFS modules - all packages are included.\n\nBy using ZFS, its possible to achieve maximum enterprise features with low budget hardware, but also high performance systems by leveraging SSD caching or even SSD only setups. ZFS can replace cost intense hardware raid cards by moderate CPU and memory load combined with easy management.\n\nGeneral ZFS advantages\n\nEasy configuration and management with Proxmox VE GUI and CLI.\n\nReliable\n\nProtection against data corruption\n\nData compression on file system level\n\nSnapshots\n\nCopy-on-write clone\n\nVarious raid levels: RAID0, RAID1, RAID10, RAIDZ-1, RAIDZ-2, RAIDZ-3, dRAID, dRAID2, dRAID3\n\nCan use SSD for cache\n\nSelf healing\n\nContinuous integrity checking\n\nDesigned for high storage capacities\n\nAsynchronous replication over network\n\nOpen Source\n\nEncryption\n\n…\n\n3.9.1. Hardware\n\nZFS depends heavily on memory, so you need at least 8GB to start. In practice, use as much as you can get for your hardware/budget. To prevent data corruption, we recommend the use of high quality ECC RAM.\n\nIf you use a dedicated cache and/or log disk, you should use an enterprise class SSD. This can increase the overall performance significantly.\n\n\tDo not use ZFS on top of a hardware RAID controller which has its own cache management. ZFS needs to communicate directly with the disks. An HBA adapter or something like an LSI controller flashed in “IT” mode is more appropriate.\n\nIf you are experimenting with an installation of Proxmox VE inside a VM (Nested Virtualization), don’t use virtio for disks of that VM, as they are not supported by ZFS. Use IDE or SCSI instead (also works with the virtio SCSI controller type).\n\n3.9.2. Installation as Root File System\n\nWhen you install using the Proxmox VE installer, you can choose ZFS for the root file system. You need to select the RAID type at installation time:\n\nRAID0\n\t\n\nAlso called “striping”. The capacity of such volume is the sum of the capacities of all disks. But RAID0 does not add any redundancy, so the failure of a single drive makes the volume unusable.\n\n\nRAID1\n\t\n\nAlso called “mirroring”. Data is written identically to all disks. This mode requires at least 2 disks with the same size. The resulting capacity is that of a single disk.\n\n\nRAID10\n\t\n\nA combination of RAID0 and RAID1. Requires at least 4 disks.\n\n\nRAIDZ-1\n\t\n\nA variation on RAID-5, single parity. Requires at least 3 disks.\n\n\nRAIDZ-2\n\t\n\nA variation on RAID-5, double parity. Requires at least 4 disks.\n\n\nRAIDZ-3\n\t\n\nA variation on RAID-5, triple parity. Requires at least 5 disks.\n\nThe installer automatically partitions the disks, creates a ZFS pool called rpool, and installs the root file system on the ZFS subvolume rpool/ROOT/pve-1.\n\nAnother subvolume called rpool/data is created to store VM images. In order to use that with the Proxmox VE tools, the installer creates the following configuration entry in /etc/pve/storage.cfg:\n\nzfspool: local-zfs\n        pool rpool/data\n        sparse\n        content images,rootdir\n\nAfter installation, you can view your ZFS pool status using the zpool command:\n\n# zpool status\n  pool: rpool\n state: ONLINE\n  scan: none requested\nconfig:\n\n        NAME        STATE     READ WRITE CKSUM\n        rpool       ONLINE       0     0     0\n          mirror-0  ONLINE       0     0     0\n            sda2    ONLINE       0     0     0\n            sdb2    ONLINE       0     0     0\n          mirror-1  ONLINE       0     0     0\n            sdc     ONLINE       0     0     0\n            sdd     ONLINE       0     0     0\n\nerrors: No known data errors\n\nThe zfs command is used to configure and manage your ZFS file systems. The following command lists all file systems after installation:\n\n# zfs list\nNAME               USED  AVAIL  REFER  MOUNTPOINT\nrpool             4.94G  7.68T    96K  /rpool\nrpool/ROOT         702M  7.68T    96K  /rpool/ROOT\nrpool/ROOT/pve-1   702M  7.68T   702M  /\nrpool/data          96K  7.68T    96K  /rpool/data\nrpool/swap        4.25G  7.69T    64K  -\n3.9.3. ZFS RAID Level Considerations\n\nThere are a few factors to take into consideration when choosing the layout of a ZFS pool. The basic building block of a ZFS pool is the virtual device, or vdev. All vdevs in a pool are used equally and the data is striped among them (RAID0). Check the zpoolconcepts(7) manpage for more details on vdevs.\n\nPerformance\n\nEach vdev type has different performance behaviors. The two parameters of interest are the IOPS (Input/Output Operations per Second) and the bandwidth with which data can be written or read.\n\nA mirror vdev (RAID1) will approximately behave like a single disk in regard to both parameters when writing data. When reading data the performance will scale linearly with the number of disks in the mirror.\n\nA common situation is to have 4 disks. When setting it up as 2 mirror vdevs (RAID10) the pool will have the write characteristics as two single disks in regard to IOPS and bandwidth. For read operations it will resemble 4 single disks.\n\nA RAIDZ of any redundancy level will approximately behave like a single disk in regard to IOPS with a lot of bandwidth. How much bandwidth depends on the size of the RAIDZ vdev and the redundancy level.\n\nA dRAID pool should match the performance of an equivalent RAIDZ pool.\n\nFor running VMs, IOPS is the more important metric in most situations.\n\nSize, Space usage and Redundancy\n\nWhile a pool made of mirror vdevs will have the best performance characteristics, the usable space will be 50% of the disks available. Less if a mirror vdev consists of more than 2 disks, for example in a 3-way mirror. At least one healthy disk per mirror is needed for the pool to stay functional.\n\nThe usable space of a RAIDZ type vdev of N disks is roughly N-P, with P being the RAIDZ-level. The RAIDZ-level indicates how many arbitrary disks can fail without losing data. A special case is a 4 disk pool with RAIDZ2. In this situation it is usually better to use 2 mirror vdevs for the better performance as the usable space will be the same.\n\nAnother important factor when using any RAIDZ level is how ZVOL datasets, which are used for VM disks, behave. For each data block the pool needs parity data which is at least the size of the minimum block size defined by the ashift value of the pool. With an ashift of 12 the block size of the pool is 4k. The default block size for a ZVOL is 8k. Therefore, in a RAIDZ2 each 8k block written will cause two additional 4k parity blocks to be written, 8k + 4k + 4k = 16k. This is of course a simplified approach and the real situation will be slightly different with metadata, compression and such not being accounted for in this example.\n\nThis behavior can be observed when checking the following properties of the ZVOL:\n\nvolsize\n\nrefreservation (if the pool is not thin provisioned)\n\nused (if the pool is thin provisioned and without snapshots present)\n\n# zfs get volsize,refreservation,used <pool>/vm-<vmid>-disk-X\n\nvolsize is the size of the disk as it is presented to the VM, while refreservation shows the reserved space on the pool which includes the expected space needed for the parity data. If the pool is thin provisioned, the refreservation will be set to 0. Another way to observe the behavior is to compare the used disk space within the VM and the used property. Be aware that snapshots will skew the value.\n\nThere are a few options to counter the increased use of space:\n\nIncrease the volblocksize to improve the data to parity ratio\n\nUse mirror vdevs instead of RAIDZ\n\nUse ashift=9 (block size of 512 bytes)\n\nThe volblocksize property can only be set when creating a ZVOL. The default value can be changed in the storage configuration. When doing this, the guest needs to be tuned accordingly and depending on the use case, the problem of write amplification is just moved from the ZFS layer up to the guest.\n\nUsing ashift=9 when creating the pool can lead to bad performance, depending on the disks underneath, and cannot be changed later on.\n\nMirror vdevs (RAID1, RAID10) have favorable behavior for VM workloads. Use them, unless your environment has specific needs and characteristics where RAIDZ performance characteristics are acceptable.\n\n3.9.4. ZFS dRAID\n\nIn a ZFS dRAID (declustered RAID) the hot spare drive(s) participate in the RAID. Their spare capacity is reserved and used for rebuilding when one drive fails. This provides, depending on the configuration, faster rebuilding compared to a RAIDZ in case of drive failure. More information can be found in the official OpenZFS documentation. [2]\n\n\tdRAID is intended for more than 10-15 disks in a dRAID. A RAIDZ setup should be better for a lower amount of disks in most use cases.\n\tThe GUI requires one more disk than the minimum (i.e. dRAID1 needs 3). It expects that a spare disk is added as well.\n\ndRAID1 or dRAID: requires at least 2 disks, one can fail before data is lost\n\ndRAID2: requires at least 3 disks, two can fail before data is lost\n\ndRAID3: requires at least 4 disks, three can fail before data is lost\n\nAdditional information can be found on the manual page:\n\n# man zpoolconcepts\nSpares and Data\n\nThe number of spares tells the system how many disks it should keep ready in case of a disk failure. The default value is 0 spares. Without spares, rebuilding won’t get any speed benefits.\n\ndata defines the number of devices in a redundancy group. The default value is 8. Except when disks - parity - spares equal something less than 8, the lower number is used. In general, a smaller number of data devices leads to higher IOPS, better compression ratios and faster resilvering, but defining fewer data devices reduces the available storage capacity of the pool.\n\n3.9.5. Bootloader\n\nProxmox VE uses proxmox-boot-tool to manage the bootloader configuration. See the chapter on Proxmox VE host bootloaders for details.\n\n3.9.6. ZFS Administration\n\nThis section gives you some usage examples for common tasks. ZFS itself is really powerful and provides many options. The main commands to manage ZFS are zfs and zpool. Both commands come with great manual pages, which can be read with:\n\n# man zpool\n# man zfs\nCreate a new zpool\n\nTo create a new pool, at least one disk is needed. The ashift should have the same sector-size (2 power of ashift) or larger as the underlying disk.\n\n# zpool create -f -o ashift=12 <pool> <device>\n\t\n\nPool names must adhere to the following rules:\n\nbegin with a letter (a-z or A-Z)\n\ncontain only alphanumeric, -, _, ., : or ` ` (space) characters\n\nmust not begin with one of mirror, raidz, draid or spare\n\nmust not be log\n\nTo activate compression (see section Compression in ZFS):\n\n# zfs set compression=lz4 <pool>\nCreate a new pool with RAID-0\n\nMinimum 1 disk\n\n# zpool create -f -o ashift=12 <pool> <device1> <device2>\nCreate a new pool with RAID-1\n\nMinimum 2 disks\n\n# zpool create -f -o ashift=12 <pool> mirror <device1> <device2>\nCreate a new pool with RAID-10\n\nMinimum 4 disks\n\n# zpool create -f -o ashift=12 <pool> mirror <device1> <device2> mirror <device3> <device4>\nCreate a new pool with RAIDZ-1\n\nMinimum 3 disks\n\n# zpool create -f -o ashift=12 <pool> raidz1 <device1> <device2> <device3>\nCreate a new pool with RAIDZ-2\n\nMinimum 4 disks\n\n# zpool create -f -o ashift=12 <pool> raidz2 <device1> <device2> <device3> <device4>\n\nPlease read the section for ZFS RAID Level Considerations to get a rough estimate on how IOPS and bandwidth expectations before setting up a pool, especially when wanting to use a RAID-Z mode.\n\nCreate a new pool with cache (L2ARC)\n\nIt is possible to use a dedicated device, or partition, as second-level cache to increase the performance. Such a cache device will especially help with random-read workloads of data that is mostly static. As it acts as additional caching layer between the actual storage, and the in-memory ARC, it can also help if the ARC must be reduced due to memory constraints.\n\nCreate ZFS pool with a on-disk cache\n# zpool create -f -o ashift=12 <pool> <device> cache <cache-device>\n\nHere only a single <device> and a single <cache-device> was used, but it is possible to use more devices, like it’s shown in Create a new pool with RAID.\n\nNote that for cache devices no mirror or raid modi exist, they are all simply accumulated.\n\nIf any cache device produces errors on read, ZFS will transparently divert that request to the underlying storage layer.\n\nCreate a new pool with log (ZIL)\n\nIt is possible to use a dedicated drive, or partition, for the ZFS Intent Log (ZIL), it is mainly used to provide safe synchronous transactions, so often in performance critical paths like databases, or other programs that issue fsync operations more frequently.\n\nThe pool is used as default ZIL location, diverting the ZIL IO load to a separate device can, help to reduce transaction latencies while relieving the main pool at the same time, increasing overall performance.\n\nFor disks to be used as log devices, directly or through a partition, it’s recommend to:\n\nuse fast SSDs with power-loss protection, as those have much smaller commit latencies.\n\nUse at least a few GB for the partition (or whole device), but using more than half of your installed memory won’t provide you with any real advantage.\n\nCreate ZFS pool with separate log device\n# zpool create -f -o ashift=12 <pool> <device> log <log-device>\n\nIn above example a single <device> and a single <log-device> is used, but you can also combine this with other RAID variants, as described in the Create a new pool with RAID section.\n\nYou can also mirror the log device to multiple devices, this is mainly useful to ensure that performance doesn’t immediately degrades if a single log device fails.\n\nIf all log devices fail the ZFS main pool itself will be used again, until the log device(s) get replaced.\n\nAdd cache and log to an existing pool\n\nIf you have a pool without cache and log you can still add both, or just one of them, at any time.\n\nFor example, let’s assume you got a good enterprise SSD with power-loss protection that you want to use for improving the overall performance of your pool.\n\nAs the maximum size of a log device should be about half the size of the installed physical memory, it means that the ZIL will mostly likely only take up a relatively small part of the SSD, the remaining space can be used as cache.\n\nFirst you have to create two GPT partitions on the SSD with parted or gdisk.\n\nThen you’re ready to add them to an pool:\n\nAdd both, a separate log device and a second-level cache, to an existing pool\n# zpool add -f <pool> log <device-part1> cache <device-part2>\n\nJust replay <pool>, <device-part1> and <device-part2> with the pool name and the two /dev/disk/by-id/ paths to the partitions.\n\nYou can also add ZIL and cache separately.\n\nAdd a log device to an existing ZFS pool\n# zpool add <pool> log <log-device>\nChanging a failed device\n# zpool replace -f <pool> <old-device> <new-device>\nChanging a failed bootable device\n\nDepending on how Proxmox VE was installed it is either using systemd-boot or GRUB through proxmox-boot-tool [3] or plain GRUB as bootloader (see Host Bootloader). You can check by running:\n\n# proxmox-boot-tool status\n\nThe first steps of copying the partition table, reissuing GUIDs and replacing the ZFS partition are the same. To make the system bootable from the new disk, different steps are needed which depend on the bootloader in use.\n\n# sgdisk <healthy bootable device> -R <new device>\n# sgdisk -G <new device>\n# zpool replace -f <pool> <old zfs partition> <new zfs partition>\n\tUse the zpool status -v command to monitor how far the resilvering process of the new disk has progressed.\nWith proxmox-boot-tool:\n# proxmox-boot-tool format <new disk's ESP>\n# proxmox-boot-tool init <new disk's ESP> [grub]\n\tESP stands for EFI System Partition, which is setup as partition #2 on bootable disks setup by the Proxmox VE installer since version 5.4. For details, see Setting up a new partition for use as synced ESP.\n\tMake sure to pass grub as mode to proxmox-boot-tool init if proxmox-boot-tool status indicates your current disks are using GRUB, especially if Secure Boot is enabled!\nWith plain GRUB:\n# grub-install <new disk>\n\tPlain GRUB is only used on systems installed with Proxmox VE 6.3 or earlier, which have not been manually migrated to using proxmox-boot-tool yet.\n3.9.7. Configure E-Mail Notification\n\nZFS comes with an event daemon ZED, which monitors events generated by the ZFS kernel module. The daemon can also send emails on ZFS events like pool errors. Newer ZFS packages ship the daemon in a separate zfs-zed package, which should already be installed by default in Proxmox VE.\n\nYou can configure the daemon via the file /etc/zfs/zed.d/zed.rc with your favorite editor. The required setting for email notification is ZED_EMAIL_ADDR, which is set to root by default.\n\nZED_EMAIL_ADDR=\"root\"\n\nPlease note Proxmox VE forwards mails to root to the email address configured for the root user.\n\n3.9.8. Limit ZFS Memory Usage\n\nZFS uses 50 % of the host memory for the Adaptive Replacement Cache (ARC) by default. For new installations starting with Proxmox VE 8.1, the ARC usage limit will be set to 10 % of the installed physical memory, clamped to a maximum of 16 GiB. This value is written to /etc/modprobe.d/zfs.conf.\n\nAllocating enough memory for the ARC is crucial for IO performance, so reduce it with caution. As a general rule of thumb, allocate at least 2 GiB Base + 1 GiB/TiB-Storage. For example, if you have a pool with 8 TiB of available storage space then you should use 10 GiB of memory for the ARC.\n\nZFS also enforces a minimum value of 64 MiB.\n\nYou can change the ARC usage limit for the current boot (a reboot resets this change again) by writing to the zfs_arc_max module parameter directly:\n\n echo \"$[10 * 1024*1024*1024]\" >/sys/module/zfs/parameters/zfs_arc_max\n\nTo permanently change the ARC limits, add (or change if already present) the following line to /etc/modprobe.d/zfs.conf:\n\noptions zfs zfs_arc_max=8589934592\n\nThis example setting limits the usage to 8 GiB (8 * 230).\n\n\tIn case your desired zfs_arc_max value is lower than or equal to zfs_arc_min (which defaults to 1/32 of the system memory), zfs_arc_max will be ignored unless you also set zfs_arc_min to at most zfs_arc_max - 1.\necho \"$[8 * 1024*1024*1024 - 1]\" >/sys/module/zfs/parameters/zfs_arc_min\necho \"$[8 * 1024*1024*1024]\" >/sys/module/zfs/parameters/zfs_arc_max\n\nThis example setting (temporarily) limits the usage to 8 GiB (8 * 230) on systems with more than 256 GiB of total memory, where simply setting zfs_arc_max alone would not work.\n\n\t\n\nIf your root file system is ZFS, you must update your initramfs every time this value changes:\n\n# update-initramfs -u -k all\n\nYou must reboot to activate these changes.\n\n3.9.9. SWAP on ZFS\n\nSwap-space created on a zvol may generate some troubles, like blocking the server or generating a high IO load, often seen when starting a Backup to an external Storage.\n\nWe strongly recommend to use enough memory, so that you normally do not run into low memory situations. Should you need or want to add swap, it is preferred to create a partition on a physical disk and use it as a swap device. You can leave some space free for this purpose in the advanced options of the installer. Additionally, you can lower the “swappiness” value. A good value for servers is 10:\n\n# sysctl -w vm.swappiness=10\n\nTo make the swappiness persistent, open /etc/sysctl.conf with an editor of your choice and add the following line:\n\nvm.swappiness = 10\nTable 1. Linux kernel swappiness parameter values\nValue\tStrategy\n\n\nvm.swappiness = 0\n\n\t\n\nThe kernel will swap only to avoid an out of memory condition\n\n\n\n\nvm.swappiness = 1\n\n\t\n\nMinimum amount of swapping without disabling it entirely.\n\n\n\n\nvm.swappiness = 10\n\n\t\n\nThis value is sometimes recommended to improve performance when sufficient memory exists in a system.\n\n\n\n\nvm.swappiness = 60\n\n\t\n\nThe default value.\n\n\n\n\nvm.swappiness = 100\n\n\t\n\nThe kernel will swap aggressively.\n\n3.9.10. Encrypted ZFS Datasets\n\tNative ZFS encryption in Proxmox VE is experimental. Known limitations and issues include Replication with encrypted datasets [4], as well as checksum errors when using Snapshots or ZVOLs. [5]\n\nZFS on Linux version 0.8.0 introduced support for native encryption of datasets. After an upgrade from previous ZFS on Linux versions, the encryption feature can be enabled per pool:\n\n# zpool get feature@encryption tank\nNAME  PROPERTY            VALUE            SOURCE\ntank  feature@encryption  disabled         local\n\n# zpool set feature@encryption=enabled\n\n# zpool get feature@encryption tank\nNAME  PROPERTY            VALUE            SOURCE\ntank  feature@encryption  enabled         local\n\tThere is currently no support for booting from pools with encrypted datasets using GRUB, and only limited support for automatically unlocking encrypted datasets on boot. Older versions of ZFS without encryption support will not be able to decrypt stored data.\n\tIt is recommended to either unlock storage datasets manually after booting, or to write a custom unit to pass the key material needed for unlocking on boot to zfs load-key.\n\tEstablish and test a backup procedure before enabling encryption of production data. If the associated key material/passphrase/keyfile has been lost, accessing the encrypted data is no longer possible.\n\nEncryption needs to be setup when creating datasets/zvols, and is inherited by default to child datasets. For example, to create an encrypted dataset tank/encrypted_data and configure it as storage in Proxmox VE, run the following commands:\n\n# zfs create -o encryption=on -o keyformat=passphrase tank/encrypted_data\nEnter passphrase:\nRe-enter passphrase:\n\n# pvesm add zfspool encrypted_zfs -pool tank/encrypted_data\n\nAll guest volumes/disks create on this storage will be encrypted with the shared key material of the parent dataset.\n\nTo actually use the storage, the associated key material needs to be loaded and the dataset needs to be mounted. This can be done in one step with:\n\n# zfs mount -l tank/encrypted_data\nEnter passphrase for 'tank/encrypted_data':\n\nIt is also possible to use a (random) keyfile instead of prompting for a passphrase by setting the keylocation and keyformat properties, either at creation time or with zfs change-key on existing datasets:\n\n# dd if=/dev/urandom of=/path/to/keyfile bs=32 count=1\n\n# zfs change-key -o keyformat=raw -o keylocation=file:///path/to/keyfile tank/encrypted_data\n\tWhen using a keyfile, special care needs to be taken to secure the keyfile against unauthorized access or accidental loss. Without the keyfile, it is not possible to access the plaintext data!\n\nA guest volume created underneath an encrypted dataset will have its encryptionroot property set accordingly. The key material only needs to be loaded once per encryptionroot to be available to all encrypted datasets underneath it.\n\nSee the encryptionroot, encryption, keylocation, keyformat and keystatus properties, the zfs load-key, zfs unload-key and zfs change-key commands and the Encryption section from man zfs for more details and advanced usage.\n\n3.9.11. Compression in ZFS\n\nWhen compression is enabled on a dataset, ZFS tries to compress all new blocks before writing them and decompresses them on reading. Already existing data will not be compressed retroactively.\n\nYou can enable compression with:\n\n# zfs set compression=<algorithm> <dataset>\n\nWe recommend using the lz4 algorithm, because it adds very little CPU overhead. Other algorithms like lzjb and gzip-N, where N is an integer from 1 (fastest) to 9 (best compression ratio), are also available. Depending on the algorithm and how compressible the data is, having compression enabled can even increase I/O performance.\n\nYou can disable compression at any time with:\n\n# zfs set compression=off <dataset>\n\nAgain, only new blocks will be affected by this change.\n\n3.9.12. ZFS Special Device\n\nSince version 0.8.0 ZFS supports special devices. A special device in a pool is used to store metadata, deduplication tables, and optionally small file blocks.\n\nA special device can improve the speed of a pool consisting of slow spinning hard disks with a lot of metadata changes. For example workloads that involve creating, updating or deleting a large number of files will benefit from the presence of a special device. ZFS datasets can also be configured to store whole small files on the special device which can further improve the performance. Use fast SSDs for the special device.\n\n\tThe redundancy of the special device should match the one of the pool, since the special device is a point of failure for the whole pool.\n\tAdding a special device to a pool cannot be undone!\nCreate a pool with special device and RAID-1:\n# zpool create -f -o ashift=12 <pool> mirror <device1> <device2> special mirror <device3> <device4>\nAdd a special device to an existing pool with RAID-1:\n# zpool add <pool> special mirror <device1> <device2>\n\nZFS datasets expose the special_small_blocks=<size> property. size can be 0 to disable storing small file blocks on the special device or a power of two in the range between 512B to 1M. After setting the property new file blocks smaller than size will be allocated on the special device.\n\n\tIf the value for special_small_blocks is greater than or equal to the recordsize (default 128K) of the dataset, all data will be written to the special device, so be careful!\n\nSetting the special_small_blocks property on a pool will change the default value of that property for all child ZFS datasets (for example all containers in the pool will opt in for small file blocks).\n\nOpt in for all file smaller than 4K-blocks pool-wide:\n# zfs set special_small_blocks=4K <pool>\nOpt in for small file blocks for a single dataset:\n# zfs set special_small_blocks=4K <pool>/<filesystem>\nOpt out from small file blocks for a single dataset:\n# zfs set special_small_blocks=0 <pool>/<filesystem>\n3.9.13. ZFS Pool Features\n\nChanges to the on-disk format in ZFS are only made between major version changes and are specified through features. All features, as well as the general mechanism are well documented in the zpool-features(5) manpage.\n\nSince enabling new features can render a pool not importable by an older version of ZFS, this needs to be done actively by the administrator, by running zpool upgrade on the pool (see the zpool-upgrade(8) manpage).\n\nUnless you need to use one of the new features, there is no upside to enabling them.\n\nIn fact, there are some downsides to enabling new features:\n\nA system with root on ZFS, that still boots using GRUB will become unbootable if a new feature is active on the rpool, due to the incompatible implementation of ZFS in GRUB.\n\nThe system will not be able to import any upgraded pool when booted with an older kernel, which still ships with the old ZFS modules.\n\nBooting an older Proxmox VE ISO to repair a non-booting system will likewise not work.\n\n\tDo not upgrade your rpool if your system is still booted with GRUB, as this will render your system unbootable. This includes systems installed before Proxmox VE 5.4, and systems booting with legacy BIOS boot (see how to determine the bootloader).\nEnable new features for a ZFS pool:\n# zpool upgrade <pool>\n3.10. BTRFS\n\tBTRFS integration is currently a technology preview in Proxmox VE.\n\nBTRFS is a modern copy on write file system natively supported by the Linux kernel, implementing features such as snapshots, built-in RAID and self healing via checksums for data and metadata. Starting with Proxmox VE 7.0, BTRFS is introduced as optional selection for the root file system.\n\nGeneral BTRFS advantages\n\nMain system setup almost identical to the traditional ext4 based setup\n\nSnapshots\n\nData compression on file system level\n\nCopy-on-write clone\n\nRAID0, RAID1 and RAID10\n\nProtection against data corruption\n\nSelf healing\n\nnatively supported by the Linux kernel\n\n…\n\nCaveats\n\nRAID levels 5/6 are experimental and dangerous\n\n3.10.1. Installation as Root File System\n\nWhen you install using the Proxmox VE installer, you can choose BTRFS for the root file system. You need to select the RAID type at installation time:\n\nRAID0\n\t\n\nAlso called “striping”. The capacity of such volume is the sum of the capacities of all disks. But RAID0 does not add any redundancy, so the failure of a single drive makes the volume unusable.\n\n\nRAID1\n\t\n\nAlso called “mirroring”. Data is written identically to all disks. This mode requires at least 2 disks with the same size. The resulting capacity is that of a single disk.\n\n\nRAID10\n\t\n\nA combination of RAID0 and RAID1. Requires at least 4 disks.\n\nThe installer automatically partitions the disks and creates an additional subvolume at /var/lib/pve/local-btrfs. In order to use that with the Proxmox VE tools, the installer creates the following configuration entry in /etc/pve/storage.cfg:\n\ndir: local\n        path /var/lib/vz\n        content iso,vztmpl,backup\n        disable\n\nbtrfs: local-btrfs\n        path /var/lib/pve/local-btrfs\n        content iso,vztmpl,backup,images,rootdir\n\nThis explicitly disables the default local storage in favor of a BTRFS specific storage entry on the additional subvolume.\n\nThe btrfs command is used to configure and manage the BTRFS file system, After the installation, the following command lists all additional subvolumes:\n\n# btrfs subvolume list /\nID 256 gen 6 top level 5 path var/lib/pve/local-btrfs\n3.10.2. BTRFS Administration\n\nThis section gives you some usage examples for common tasks.\n\nCreating a BTRFS file system\n\nTo create BTRFS file systems, mkfs.btrfs is used. The -d and -m parameters are used to set the profile for metadata and data respectively. With the optional -L parameter, a label can be set.\n\nGenerally, the following modes are supported: single, raid0, raid1, raid10.\n\nCreate a BTRFS file system on a single disk /dev/sdb with the label My-Storage:\n\n # mkfs.btrfs -m single -d single -L My-Storage /dev/sdb\n\nOr create a RAID1 on the two partitions /dev/sdb1 and /dev/sdc1:\n\n # mkfs.btrfs -m raid1 -d raid1 -L My-Storage /dev/sdb1 /dev/sdc1\nMounting a BTRFS file system\n\nThe new file-system can then be mounted either manually, for example:\n\n # mkdir /my-storage\n # mount /dev/sdb /my-storage\n\nA BTRFS can also be added to /etc/fstab like any other mount point, automatically mounting it on boot. It’s recommended to avoid using block-device paths but use the UUID value the mkfs.btrfs command printed, especially there is more than one disk in a BTRFS setup.\n\nFor example:\n\nFile /etc/fstab\n# ... other mount points left out for brevity\n\n# using the UUID from the mkfs.btrfs output is highly recommended\nUUID=e2c0c3ff-2114-4f54-b767-3a203e49f6f3 /my-storage btrfs defaults 0 0\n\tIf you do not have the UUID available anymore you can use the blkid tool to list all properties of block-devices.\n\nAfterwards you can trigger the first mount by executing:\n\nmount /my-storage\n\nAfter the next reboot this will be automatically done by the system at boot.\n\nAdding a BTRFS file system to Proxmox VE\n\nYou can add an existing BTRFS file system to Proxmox VE via the web interface, or using the CLI, for example:\n\npvesm add btrfs my-storage --path /my-storage\nCreating a subvolume\n\nCreating a subvolume links it to a path in the BTRFS file system, where it will appear as a regular directory.\n\n# btrfs subvolume create /some/path\n\nAfterwards /some/path will act like a regular directory.\n\nDeleting a subvolume\n\nContrary to directories removed via rmdir, subvolumes do not need to be empty in order to be deleted via the btrfs command.\n\n# btrfs subvolume delete /some/path\nCreating a snapshot of a subvolume\n\nBTRFS does not actually distinguish between snapshots and normal subvolumes, so taking a snapshot can also be seen as creating an arbitrary copy of a subvolume. By convention, Proxmox VE will use the read-only flag when creating snapshots of guest disks or subvolumes, but this flag can also be changed later on.\n\n# btrfs subvolume snapshot -r /some/path /a/new/path\n\nThis will create a read-only \"clone\" of the subvolume on /some/path at /a/new/path. Any future modifications to /some/path cause the modified data to be copied before modification.\n\nIf the read-only (-r) option is left out, both subvolumes will be writable.\n\nEnabling compression\n\nBy default, BTRFS does not compress data. To enable compression, the compress mount option can be added. Note that data already written will not be compressed after the fact.\n\nBy default, the rootfs will be listed in /etc/fstab as follows:\n\nUUID=<uuid of your root file system> / btrfs defaults 0 1\n\nYou can simply append compress=zstd, compress=lzo, or compress=zlib to the defaults above like so:\n\nUUID=<uuid of your root file system> / btrfs defaults,compress=zstd 0 1\n\nThis change will take effect after rebooting.\n\nChecking Space Usage\n\nThe classic df tool may output confusing values for some BTRFS setups. For a better estimate use the btrfs filesystem usage /PATH command, for example:\n\n# btrfs fi usage /my-storage\n3.11. Proxmox Node Management\n\nThe Proxmox VE node management tool (pvenode) allows you to control node specific settings and resources.\n\nCurrently pvenode allows you to set a node’s description, run various bulk operations on the node’s guests, view the node’s task history, and manage the node’s SSL certificates, which are used for the API and the web GUI through pveproxy.\n\n3.11.1. Wake-on-LAN\n\nWake-on-LAN (WoL) allows you to switch on a sleeping computer in the network, by sending a magic packet. At least one NIC must support this feature, and the respective option needs to be enabled in the computer’s firmware (BIOS/UEFI) configuration. The option name can vary from Enable Wake-on-Lan to Power On By PCIE Device; check your motherboard’s vendor manual, if you’re unsure. ethtool can be used to check the WoL configuration of <interface> by running:\n\nethtool <interface> | grep Wake-on\n\npvenode allows you to wake sleeping members of a cluster via WoL, using the command:\n\npvenode wakeonlan <node>\n\nThis broadcasts the WoL magic packet on UDP port 9, containing the MAC address of <node> obtained from the wakeonlan property. The node-specific wakeonlan property can be set using the following command:\n\npvenode config set -wakeonlan XX:XX:XX:XX:XX:XX\n\nThe interface via which to send the WoL packet is determined from the default route. It can be overwritten by setting the bind-interface via the following command:\n\npvenode config set -wakeonlan XX:XX:XX:XX:XX:XX,bind-interface=<iface-name>\n\nThe broadcast address (default 255.255.255.255) used when sending the WoL packet can further be changed by setting the broadcast-address explicitly using the following command:\n\npvenode config set -wakeonlan XX:XX:XX:XX:XX:XX,broadcast-address=<broadcast-address>\n3.11.2. Task History\n\nWhen troubleshooting server issues, for example, failed backup jobs, it can often be helpful to have a log of the previously run tasks. With Proxmox VE, you can access the nodes’s task history through the pvenode task command.\n\nYou can get a filtered list of a node’s finished tasks with the list subcommand. For example, to get a list of tasks related to VM 100 that ended with an error, the command would be:\n\npvenode task list --errors --vmid 100\n\nThe log of a task can then be printed using its UPID:\n\npvenode task log UPID:pve1:00010D94:001CA6EA:6124E1B9:vzdump:100:root@pam:\n3.11.3. Bulk Guest Power Management\n\nIn case you have many VMs/containers, starting and stopping guests can be carried out in bulk operations with the startall and stopall subcommands of pvenode. By default, pvenode startall will only start VMs/containers which have been set to automatically start on boot (see Automatic Start and Shutdown of Virtual Machines), however, you can override this behavior with the --force flag. Both commands also have a --vms option, which limits the stopped/started guests to the specified VMIDs.\n\nFor example, to start VMs 100, 101, and 102, regardless of whether they have onboot set, you can use:\n\npvenode startall --vms 100,101,102 --force\n\nTo stop these guests (and any other guests that may be running), use the command:\n\npvenode stopall\n\tThe stopall command first attempts to perform a clean shutdown and then waits until either all guests have successfully shut down or an overridable timeout (3 minutes by default) has expired. Once that happens and the force-stop parameter is not explicitly set to 0 (false), all virtual guests that are still running are hard stopped.\n3.11.4. First Guest Boot Delay\n\nIn case your VMs/containers rely on slow-to-start external resources, for example an NFS server, you can also set a per-node delay between the time Proxmox VE boots and the time the first VM/container that is configured to autostart boots (see Automatic Start and Shutdown of Virtual Machines).\n\nYou can achieve this by setting the following (where 10 represents the delay in seconds):\n\npvenode config set --startall-onboot-delay 10\n3.11.5. Bulk Guest Migration\n\nIn case an upgrade situation requires you to migrate all of your guests from one node to another, pvenode also offers the migrateall subcommand for bulk migration. By default, this command will migrate every guest on the system to the target node. It can however be set to only migrate a set of guests.\n\nFor example, to migrate VMs 100, 101, and 102, to the node pve2, with live-migration for local disks enabled, you can run:\n\npvenode migrateall pve2 --vms 100,101,102 --with-local-disks\n3.12. Certificate Management\n3.12.1. Certificates for Intra-Cluster Communication\n\nEach Proxmox VE cluster creates by default its own (self-signed) Certificate Authority (CA) and generates a certificate for each node which gets signed by the aforementioned CA. These certificates are used for encrypted communication with the cluster’s pveproxy service and the Shell/Console feature if SPICE is used.\n\nThe CA certificate and key are stored in the Proxmox Cluster File System (pmxcfs).\n\n3.12.2. Certificates for API and Web GUI\n\nThe REST API and web GUI are provided by the pveproxy service, which runs on each node.\n\nYou have the following options for the certificate used by pveproxy:\n\nBy default the node-specific certificate in /etc/pve/nodes/NODENAME/pve-ssl.pem is used. This certificate is signed by the cluster CA and therefore not automatically trusted by browsers and operating systems.\n\nuse an externally provided certificate (e.g. signed by a commercial CA).\n\nuse ACME (Let’s Encrypt) to get a trusted certificate with automatic renewal, this is also integrated in the Proxmox VE API and web interface.\n\nFor options 2 and 3 the file /etc/pve/local/pveproxy-ssl.pem (and /etc/pve/local/pveproxy-ssl.key, which needs to be without password) is used.\n\n\tKeep in mind that /etc/pve/local is a node specific symlink to /etc/pve/nodes/NODENAME.\n\nCertificates are managed with the Proxmox VE Node management command (see the pvenode(1) manpage).\n\n\tDo not replace or manually modify the automatically generated node certificate files in /etc/pve/local/pve-ssl.pem and /etc/pve/local/pve-ssl.key or the cluster CA files in /etc/pve/pve-root-ca.pem and /etc/pve/priv/pve-root-ca.key.\n3.12.3. Upload Custom Certificate\n\nIf you already have a certificate which you want to use for a Proxmox VE node you can upload that certificate simply over the web interface.\n\nNote that the certificates key file, if provided, mustn’t be password protected.\n\n3.12.4. Trusted certificates via Let’s Encrypt (ACME)\n\nProxmox VE includes an implementation of the Automatic Certificate Management Environment ACME protocol, allowing Proxmox VE admins to use an ACME provider like Let’s Encrypt for easy setup of TLS certificates which are accepted and trusted on modern operating systems and web browsers out of the box.\n\nCurrently, the two ACME endpoints implemented are the Let’s Encrypt (LE) production and its staging environment. Our ACME client supports validation of http-01 challenges using a built-in web server and validation of dns-01 challenges using a DNS plugin supporting all the DNS API endpoints acme.sh does.\n\nACME Account\n\nYou need to register an ACME account per cluster with the endpoint you want to use. The email address used for that account will serve as contact point for renewal-due or similar notifications from the ACME endpoint.\n\nYou can register and deactivate ACME accounts over the web interface Datacenter -> ACME or using the pvenode command-line tool.\n\n pvenode acme account register account-name mail@example.com\n\tBecause of rate-limits you should use LE staging for experiments or if you use ACME for the first time.\nACME Plugins\n\nThe ACME plugins task is to provide automatic verification that you, and thus the Proxmox VE cluster under your operation, are the real owner of a domain. This is the basis building block for automatic certificate management.\n\nThe ACME protocol specifies different types of challenges, for example the http-01 where a web server provides a file with a certain content to prove that it controls a domain. Sometimes this isn’t possible, either because of technical limitations or if the address of a record to is not reachable from the public internet. The dns-01 challenge can be used in these cases. This challenge is fulfilled by creating a certain DNS record in the domain’s zone.\n\nProxmox VE supports both of those challenge types out of the box, you can configure plugins either over the web interface under Datacenter -> ACME, or using the pvenode acme plugin add command.\n\nACME Plugin configurations are stored in /etc/pve/priv/acme/plugins.cfg. A plugin is available for all nodes in the cluster.\n\nNode Domains\n\nEach domain is node specific. You can add new or manage existing domain entries under Node -> Certificates, or using the pvenode config command.\n\nAfter configuring the desired domain(s) for a node and ensuring that the desired ACME account is selected, you can order your new certificate over the web interface. On success the interface will reload after 10 seconds.\n\nRenewal will happen automatically.\n\n3.12.5. ACME HTTP Challenge Plugin\n\nThere is always an implicitly configured standalone plugin for validating http-01 challenges via the built-in webserver spawned on port 80.\n\n\tThe name standalone means that it can provide the validation on it’s own, without any third party service. So, this plugin works also for cluster nodes.\n\nThere are a few prerequisites to use it for certificate management with Let’s Encrypts ACME.\n\nYou have to accept the ToS of Let’s Encrypt to register an account.\n\nPort 80 of the node needs to be reachable from the internet.\n\nThere must be no other listener on port 80.\n\nThe requested (sub)domain needs to resolve to a public IP of the Node.\n\n3.12.6. ACME DNS API Challenge Plugin\n\nOn systems where external access for validation via the http-01 method is not possible or desired, it is possible to use the dns-01 validation method. This validation method requires a DNS server that allows provisioning of TXT records via an API.\n\nConfiguring ACME DNS APIs for validation\n\nProxmox VE re-uses the DNS plugins developed for the acme.sh [6] project, please refer to its documentation for details on configuration of specific APIs.\n\nThe easiest way to configure a new plugin with the DNS API is using the web interface (Datacenter -> ACME).\n\nChoose DNS as challenge type. Then you can select your API provider, enter the credential data to access your account over their API.\n\n\tSee the acme.sh How to use DNS API wiki for more detailed information about getting API credentials for your provider.\n\nAs there are many DNS providers and API endpoints Proxmox VE automatically generates the form for the credentials for some providers. For the others you will see a bigger text area, simply copy all the credentials KEY=VALUE pairs in there.\n\nDNS Validation through CNAME Alias\n\nA special alias mode can be used to handle the validation on a different domain/DNS server, in case your primary/real DNS does not support provisioning via an API. Manually set up a permanent CNAME record for _acme-challenge.domain1.example pointing to _acme-challenge.domain2.example and set the alias property in the Proxmox VE node configuration file to domain2.example to allow the DNS server of domain2.example to validate all challenges for domain1.example.\n\nCombination of Plugins\n\nCombining http-01 and dns-01 validation is possible in case your node is reachable via multiple domains with different requirements / DNS provisioning capabilities. Mixing DNS APIs from multiple providers or instances is also possible by specifying different plugin instances per domain.\n\n\tAccessing the same service over multiple domains increases complexity and should be avoided if possible.\n3.12.7. Automatic renewal of ACME certificates\n\nIf a node has been successfully configured with an ACME-provided certificate (either via pvenode or via the GUI), the certificate will be automatically renewed by the pve-daily-update.service. Currently, renewal will be attempted if the certificate has expired already, or will expire in the next 30 days.\n\n3.12.8. ACME Examples with pvenode\nExample: Sample pvenode invocation for using Let’s Encrypt certificates\nroot@proxmox:~# pvenode acme account register default mail@example.invalid\nDirectory endpoints:\n0) Let's Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory)\n1) Let's Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/directory)\n2) Custom\nEnter selection: 1\n\nTerms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf\nDo you agree to the above terms? [y|N]y\n...\nTask OK\nroot@proxmox:~# pvenode config set --acme domains=example.invalid\nroot@proxmox:~# pvenode acme cert order\nLoading ACME account details\nPlacing ACME order\n...\nStatus is 'valid'!\n\nAll domains validated!\n...\nDownloading certificate\nSetting pveproxy certificate and key\nRestarting pveproxy\nTask OK\nExample: Setting up the OVH API for validating a domain\n\tthe account registration steps are the same no matter which plugins are used, and are not repeated here.\n\tOVH_AK and OVH_AS need to be obtained from OVH according to the OVH API documentation\n\nFirst you need to get all information so you and Proxmox VE can access the API.\n\nroot@proxmox:~# cat /path/to/api-token\nOVH_AK=XXXXXXXXXXXXXXXX\nOVH_AS=YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\nroot@proxmox:~# source /path/to/api-token\nroot@proxmox:~# curl -XPOST -H\"X-Ovh-Application: $OVH_AK\" -H \"Content-type: application/json\" \\\nhttps://eu.api.ovh.com/1.0/auth/credential  -d '{\n  \"accessRules\": [\n    {\"method\": \"GET\",\"path\": \"/auth/time\"},\n    {\"method\": \"GET\",\"path\": \"/domain\"},\n    {\"method\": \"GET\",\"path\": \"/domain/zone/*\"},\n    {\"method\": \"GET\",\"path\": \"/domain/zone/*/record\"},\n    {\"method\": \"POST\",\"path\": \"/domain/zone/*/record\"},\n    {\"method\": \"POST\",\"path\": \"/domain/zone/*/refresh\"},\n    {\"method\": \"PUT\",\"path\": \"/domain/zone/*/record/\"},\n    {\"method\": \"DELETE\",\"path\": \"/domain/zone/*/record/*\"}\n]\n}'\n{\"consumerKey\":\"ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ\",\"state\":\"pendingValidation\",\"validationUrl\":\"https://eu.api.ovh.com/auth/?credentialToken=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"}\n\n(open validation URL and follow instructions to link Application Key with account/Consumer Key)\n\nroot@proxmox:~# echo \"OVH_CK=ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ\" >> /path/to/api-token\n\nNow you can setup the the ACME plugin:\n\nroot@proxmox:~# pvenode acme plugin add dns example_plugin --api ovh --data /path/to/api_token\nroot@proxmox:~# pvenode acme plugin config example_plugin\n┌────────┬──────────────────────────────────────────┐\n│ key    │ value                                    │\n╞════════╪══════════════════════════════════════════╡\n│ api    │ ovh                                      │\n├────────┼──────────────────────────────────────────┤\n│ data   │ OVH_AK=XXXXXXXXXXXXXXXX                  │\n│        │ OVH_AS=YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY  │\n│        │ OVH_CK=ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ  │\n├────────┼──────────────────────────────────────────┤\n│ digest │ 867fcf556363ca1bea866863093fcab83edf47a1 │\n├────────┼──────────────────────────────────────────┤\n│ plugin │ example_plugin                           │\n├────────┼──────────────────────────────────────────┤\n│ type   │ dns                                      │\n└────────┴──────────────────────────────────────────┘\n\nAt last you can configure the domain you want to get certificates for and place the certificate order for it:\n\nroot@proxmox:~# pvenode config set -acmedomain0 example.proxmox.com,plugin=example_plugin\nroot@proxmox:~# pvenode acme cert order\nLoading ACME account details\nPlacing ACME order\nOrder URL: https://acme-staging-v02.api.letsencrypt.org/acme/order/11111111/22222222\n\nGetting authorization details from 'https://acme-staging-v02.api.letsencrypt.org/acme/authz-v3/33333333'\nThe validation for example.proxmox.com is pending!\n[Wed Apr 22 09:25:30 CEST 2020] Using OVH endpoint: ovh-eu\n[Wed Apr 22 09:25:30 CEST 2020] Checking authentication\n[Wed Apr 22 09:25:30 CEST 2020] Consumer key is ok.\n[Wed Apr 22 09:25:31 CEST 2020] Adding record\n[Wed Apr 22 09:25:32 CEST 2020] Added, sleep 10 seconds.\nAdd TXT record: _acme-challenge.example.proxmox.com\nTriggering validation\nSleeping for 5 seconds\nStatus is 'valid'!\n[Wed Apr 22 09:25:48 CEST 2020] Using OVH endpoint: ovh-eu\n[Wed Apr 22 09:25:48 CEST 2020] Checking authentication\n[Wed Apr 22 09:25:48 CEST 2020] Consumer key is ok.\nRemove TXT record: _acme-challenge.example.proxmox.com\n\nAll domains validated!\n\nCreating CSR\nChecking order status\nOrder is ready, finalizing order\nvalid!\n\nDownloading certificate\nSetting pveproxy certificate and key\nRestarting pveproxy\nTask OK\nExample: Switching from the staging to the regular ACME directory\n\nChanging the ACME directory for an account is unsupported, but as Proxmox VE supports more than one account you can just create a new one with the production (trusted) ACME directory as endpoint. You can also deactivate the staging account and recreate it.\n\nExample: Changing the default ACME account from staging to directory using pvenode\nroot@proxmox:~# pvenode acme account deactivate default\nRenaming account file from '/etc/pve/priv/acme/default' to '/etc/pve/priv/acme/_deactivated_default_4'\nTask OK\n\nroot@proxmox:~# pvenode acme account register default example@proxmox.com\nDirectory endpoints:\n0) Let's Encrypt V2 (https://acme-v02.api.letsencrypt.org/directory)\n1) Let's Encrypt V2 Staging (https://acme-staging-v02.api.letsencrypt.org/directory)\n2) Custom\nEnter selection: 0\n\nTerms of Service: https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf\nDo you agree to the above terms? [y|N]y\n...\nTask OK\n3.13. Host Bootloader\n\nProxmox VE currently uses one of two bootloaders depending on the disk setup selected in the installer.\n\nFor EFI Systems installed with ZFS as the root filesystem systemd-boot is used, unless Secure Boot is enabled. All other deployments use the standard GRUB bootloader (this usually also applies to systems which are installed on top of Debian).\n\n3.13.1. Partitioning Scheme Used by the Installer\n\nThe Proxmox VE installer creates 3 partitions on all disks selected for installation.\n\nThe created partitions are:\n\na 1 MB BIOS Boot Partition (gdisk type EF02)\n\na 512 MB EFI System Partition (ESP, gdisk type EF00)\n\na third partition spanning the set hdsize parameter or the remaining space used for the chosen storage type\n\nSystems using ZFS as root filesystem are booted with a kernel and initrd image stored on the 512 MB EFI System Partition. For legacy BIOS systems, and EFI systems with Secure Boot enabled, GRUB is used, for EFI systems without Secure Boot, systemd-boot is used. Both are installed and configured to point to the ESPs.\n\nGRUB in BIOS mode (--target i386-pc) is installed onto the BIOS Boot Partition of all selected disks on all systems booted with GRUB [7].\n\n3.13.2. Synchronizing the content of the ESP with proxmox-boot-tool\n\nproxmox-boot-tool is a utility used to keep the contents of the EFI System Partitions properly configured and synchronized. It copies certain kernel versions to all ESPs and configures the respective bootloader to boot from the vfat formatted ESPs. In the context of ZFS as root filesystem this means that you can use all optional features on your root pool instead of the subset which is also present in the ZFS implementation in GRUB or having to create a separate small boot-pool [8].\n\nIn setups with redundancy all disks are partitioned with an ESP, by the installer. This ensures the system boots even if the first boot device fails or if the BIOS can only boot from a particular disk.\n\nThe ESPs are not kept mounted during regular operation. This helps to prevent filesystem corruption to the vfat formatted ESPs in case of a system crash, and removes the need to manually adapt /etc/fstab in case the primary boot device fails.\n\nproxmox-boot-tool handles the following tasks:\n\nformatting and setting up a new partition\n\ncopying and configuring new kernel images and initrd images to all listed ESPs\n\nsynchronizing the configuration on kernel upgrades and other maintenance tasks\n\nmanaging the list of kernel versions which are synchronized\n\nconfiguring the boot-loader to boot a particular kernel version (pinning)\n\nYou can view the currently configured ESPs and their state by running:\n\n# proxmox-boot-tool status\nSetting up a new partition for use as synced ESP\n\nTo format and initialize a partition as synced ESP, e.g., after replacing a failed vdev in an rpool, or when converting an existing system that pre-dates the sync mechanism, proxmox-boot-tool from proxmox-kernel-helper can be used.\n\n\tthe format command will format the <partition>, make sure to pass in the right device/partition!\n\nFor example, to format an empty partition /dev/sda2 as ESP, run the following:\n\n# proxmox-boot-tool format /dev/sda2\n\nTo setup an existing, unmounted ESP located on /dev/sda2 for inclusion in Proxmox VE’s kernel update synchronization mechanism, use the following:\n\n# proxmox-boot-tool init /dev/sda2\n\nor\n\n# proxmox-boot-tool init /dev/sda2 grub\n\nto force initialization with GRUB instead of systemd-boot, for example for Secure Boot support.\n\nAfterwards /etc/kernel/proxmox-boot-uuids should contain a new line with the UUID of the newly added partition. The init command will also automatically trigger a refresh of all configured ESPs.\n\nUpdating the configuration on all ESPs\n\nTo copy and configure all bootable kernels and keep all ESPs listed in /etc/kernel/proxmox-boot-uuids in sync you just need to run:\n\n# proxmox-boot-tool refresh\n\n(The equivalent to running update-grub systems with ext4 or xfs on root).\n\nThis is necessary should you make changes to the kernel commandline, or want to sync all kernels and initrds.\n\n\tBoth update-initramfs and apt (when necessary) will automatically trigger a refresh.\nKernel Versions considered by proxmox-boot-tool\n\nThe following kernel versions are configured by default:\n\nthe currently running kernel\n\nthe version being newly installed on package updates\n\nthe two latest already installed kernels\n\nthe latest version of the second-to-last kernel series (e.g. 5.0, 5.3), if applicable\n\nany manually selected kernels\n\nManually keeping a kernel bootable\n\nShould you wish to add a certain kernel and initrd image to the list of bootable kernels use proxmox-boot-tool kernel add.\n\nFor example run the following to add the kernel with ABI version 5.0.15-1-pve to the list of kernels to keep installed and synced to all ESPs:\n\n# proxmox-boot-tool kernel add 5.0.15-1-pve\n\nproxmox-boot-tool kernel list will list all kernel versions currently selected for booting:\n\n# proxmox-boot-tool kernel list\nManually selected kernels:\n5.0.15-1-pve\n\nAutomatically selected kernels:\n5.0.12-1-pve\n4.15.18-18-pve\n\nRun proxmox-boot-tool kernel remove to remove a kernel from the list of manually selected kernels, for example:\n\n# proxmox-boot-tool kernel remove 5.0.15-1-pve\n\tIt’s required to run proxmox-boot-tool refresh to update all EFI System Partitions (ESPs) after a manual kernel addition or removal from above.\n3.13.3. Determine which Bootloader is Used\n\nThe simplest and most reliable way to determine which bootloader is used, is to watch the boot process of the Proxmox VE node.\n\nYou will either see the blue box of GRUB or the simple black on white systemd-boot.\n\nDetermining the bootloader from a running system might not be 100% accurate. The safest way is to run the following command:\n\n# efibootmgr -v\n\nIf it returns a message that EFI variables are not supported, GRUB is used in BIOS/Legacy mode.\n\nIf the output contains a line that looks similar to the following, GRUB is used in UEFI mode.\n\nBoot0005* proxmox       [...] File(\\EFI\\proxmox\\grubx64.efi)\n\nIf the output contains a line similar to the following, systemd-boot is used.\n\nBoot0006* Linux Boot Manager    [...] File(\\EFI\\systemd\\systemd-bootx64.efi)\n\nBy running:\n\n# proxmox-boot-tool status\n\nyou can find out if proxmox-boot-tool is configured, which is a good indication of how the system is booted.\n\n3.13.4. GRUB\n\nGRUB has been the de-facto standard for booting Linux systems for many years and is quite well documented [9].\n\nConfiguration\n\nChanges to the GRUB configuration are done via the defaults file /etc/default/grub or config snippets in /etc/default/grub.d. To regenerate the configuration file after a change to the configuration run: [10]\n\n# update-grub\n3.13.5. Systemd-boot\n\nsystemd-boot is a lightweight EFI bootloader. It reads the kernel and initrd images directly from the EFI Service Partition (ESP) where it is installed. The main advantage of directly loading the kernel from the ESP is that it does not need to reimplement the drivers for accessing the storage. In Proxmox VE proxmox-boot-tool is used to keep the configuration on the ESPs synchronized.\n\nConfiguration\n\nsystemd-boot is configured via the file loader/loader.conf in the root directory of an EFI System Partition (ESP). See the loader.conf(5) manpage for details.\n\nEach bootloader entry is placed in a file of its own in the directory loader/entries/\n\nAn example entry.conf looks like this (/ refers to the root of the ESP):\n\ntitle    Proxmox\nversion  5.0.15-1-pve\noptions   root=ZFS=rpool/ROOT/pve-1 boot=zfs\nlinux    /EFI/proxmox/5.0.15-1-pve/vmlinuz-5.0.15-1-pve\ninitrd   /EFI/proxmox/5.0.15-1-pve/initrd.img-5.0.15-1-pve\n3.13.6. Editing the Kernel Commandline\n\nYou can modify the kernel commandline in the following places, depending on the bootloader used:\n\nGRUB\n\nThe kernel commandline needs to be placed in the variable GRUB_CMDLINE_LINUX_DEFAULT in the file /etc/default/grub. Running update-grub appends its content to all linux entries in /boot/grub/grub.cfg.\n\nSystemd-boot\n\nThe kernel commandline needs to be placed as one line in /etc/kernel/cmdline. To apply your changes, run proxmox-boot-tool refresh, which sets it as the option line for all config files in loader/entries/proxmox-*.conf.\n\nA complete list of kernel parameters can be found at https://www.kernel.org/doc/html/v<YOUR-KERNEL-VERSION>/admin-guide/kernel-parameters.html. replace <YOUR-KERNEL-VERSION> with the major.minor version, for example, for kernels based on version 6.5 the URL would be: https://www.kernel.org/doc/html/v6.5/admin-guide/kernel-parameters.html\n\nYou can find your kernel version by checking the web interface (Node → Summary), or by running\n\n# uname -r\n\nUse the first two numbers at the front of the output.\n\n3.13.7. Override the Kernel-Version for next Boot\n\nTo select a kernel that is not currently the default kernel, you can either:\n\nuse the boot loader menu that is displayed at the beginning of the boot process\n\nuse the proxmox-boot-tool to pin the system to a kernel version either once or permanently (until pin is reset).\n\nThis should help you work around incompatibilities between a newer kernel version and the hardware.\n\n\tSuch a pin should be removed as soon as possible so that all current security patches of the latest kernel are also applied to the system.\n\nFor example: To permanently select the version 5.15.30-1-pve for booting you would run:\n\n# proxmox-boot-tool kernel pin 5.15.30-1-pve\n\tThe pinning functionality works for all Proxmox VE systems, not only those using proxmox-boot-tool to synchronize the contents of the ESPs, if your system does not use proxmox-boot-tool for synchronizing you can also skip the proxmox-boot-tool refresh call in the end.\n\nYou can also set a kernel version to be booted on the next system boot only. This is for example useful to test if an updated kernel has resolved an issue, which caused you to pin a version in the first place:\n\n# proxmox-boot-tool kernel pin 5.15.30-1-pve --next-boot\n\nTo remove any pinned version configuration use the unpin subcommand:\n\n# proxmox-boot-tool kernel unpin\n\nWhile unpin has a --next-boot option as well, it is used to clear a pinned version set with --next-boot. As that happens already automatically on boot, invonking it manually is of little use.\n\nAfter setting, or clearing pinned versions you also need to synchronize the content and configuration on the ESPs by running the refresh subcommand.\n\n\tYou will be prompted to automatically do for proxmox-boot-tool managed systems if you call the tool interactively.\n# proxmox-boot-tool refresh\n3.13.8. Secure Boot\n\nSince Proxmox VE 8.1, Secure Boot is supported out of the box via signed packages and integration in proxmox-boot-tool.\n\nThe following packages are required for secure boot to work. You can install them all at once by using the ‘proxmox-secure-boot-support’ meta-package.\n\nshim-signed (shim bootloader signed by Microsoft)\n\nshim-helpers-amd64-signed (fallback bootloader and MOKManager, signed by Proxmox)\n\ngrub-efi-amd64-signed (GRUB EFI bootloader, signed by Proxmox)\n\nproxmox-kernel-6.X.Y-Z-pve-signed (Kernel image, signed by Proxmox)\n\nOnly GRUB is supported as bootloader out of the box, since other bootloader are currently not eligible for secure boot code-signing.\n\nAny new installation of Proxmox VE will automatically have all of the above packages included.\n\nMore details about how Secure Boot works, and how to customize the setup, are available in our wiki.\n\nSwitching an Existing Installation to Secure Boot\n\tThis can lead to an unbootable installation in some cases if not done correctly. Reinstalling the host will setup Secure Boot automatically if available, without any extra interactions. Make sure you have a working and well-tested backup of your Proxmox VE host!\n\nAn existing UEFI installation can be switched over to Secure Boot if desired, without having to reinstall Proxmox VE from scratch.\n\nFirst, ensure all your system is up-to-date. Next, install proxmox-secure-boot-support. GRUB automatically creates the needed EFI boot entry for booting via the default shim.\n\nsystemd-boot\n\nIf systemd-boot is used as a bootloader (see Determine which Bootloader is used), some additional setup is needed. This is only the case if Proxmox VE was installed with ZFS-on-root.\n\nTo check the latter, run:\n\n# findmnt /\n\nIf the host is indeed using ZFS as root filesystem, the FSTYPE column should contain zfs:\n\nTARGET SOURCE           FSTYPE OPTIONS\n/      rpool/ROOT/pve-1 zfs    rw,relatime,xattr,noacl,casesensitive\n\nNext, a suitable potential ESP (EFI system partition) must be found. This can be done using the lsblk command as following:\n\n# lsblk -o +FSTYPE\n\nThe output should look something like this:\n\nNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS FSTYPE\nsda      8:0    0   32G  0 disk\n├─sda1   8:1    0 1007K  0 part\n├─sda2   8:2    0  512M  0 part             vfat\n└─sda3   8:3    0 31.5G  0 part             zfs_member\nsdb      8:16   0   32G  0 disk\n├─sdb1   8:17   0 1007K  0 part\n├─sdb2   8:18   0  512M  0 part             vfat\n└─sdb3   8:19   0 31.5G  0 part             zfs_member\n\nIn this case, the partitions sda2 and sdb2 are the targets. They can be identified by the their size of 512M and their FSTYPE being vfat, in this case on a ZFS RAID-1 installation.\n\nThese partitions must be properly set up for booting through GRUB using proxmox-boot-tool. This command (using sda2 as an example) must be run separately for each individual ESP:\n\n# proxmox-boot-tool init /dev/sda2 grub\n\nAfterwards, you can sanity-check the setup by running the following command:\n\n# efibootmgr -v\n\nThis list should contain an entry looking similar to this:\n\n[..]\nBoot0009* proxmox       HD(2,GPT,..,0x800,0x100000)/File(\\EFI\\proxmox\\shimx64.efi)\n[..]\n\tThe old systemd-boot bootloader will be kept, but GRUB will be preferred. This way, if booting using GRUB in Secure Boot mode does not work for any reason, the system can still be booted using systemd-boot with Secure Boot turned off.\n\nNow the host can be rebooted and Secure Boot enabled in the UEFI firmware setup utility.\n\nOn reboot, a new entry named proxmox should be selectable in the UEFI firmware boot menu, which boots using the pre-signed EFI shim.\n\nIf, for any reason, no proxmox entry can be found in the UEFI boot menu, you can try adding it manually (if supported by the firmware), by adding the file \\EFI\\proxmox\\shimx64.efi as a custom boot entry.\n\n\tSome UEFI firmwares are known to drop the proxmox boot option on reboot. This can happen if the proxmox boot entry is pointing to a GRUB installation on a disk, where the disk itself is not a boot option. If possible, try adding the disk as a boot option in the UEFI firmware setup utility and run proxmox-boot-tool again.\n\tTo enroll custom keys, see the accompanying Secure Boot wiki page.\nUsing DKMS/Third Party Modules With Secure Boot\n\nOn systems with Secure Boot enabled, the kernel will refuse to load modules which are not signed by a trusted key. The default set of modules shipped with the kernel packages is signed with an ephemeral key embedded in the kernel image which is trusted by that specific version of the kernel image.\n\nIn order to load other modules, such as those built with DKMS or manually, they need to be signed with a key trusted by the Secure Boot stack. The easiest way to achieve this is to enroll them as Machine Owner Key (MOK) with mokutil.\n\nThe dkms tool will automatically generate a keypair and certificate in /var/lib/dkms/mok.key and /var/lib/dkms/mok.pub and use it for signing the kernel modules it builds and installs.\n\nYou can view the certificate contents with\n\n# openssl x509 -in /var/lib/dkms/mok.pub -noout -text\n\nand enroll it on your system using the following command:\n\n# mokutil --import /var/lib/dkms/mok.pub\ninput password:\ninput password again:\n\nThe mokutil command will ask for a (temporary) password twice, this password needs to be entered one more time in the next step of the process! Rebooting the system should automatically boot into the MOKManager EFI binary, which allows you to verify the key/certificate and confirm the enrollment using the password selected when starting the enrollment using mokutil. Afterwards, the kernel should allow loading modules built with DKMS (which are signed with the enrolled MOK). The MOK can also be used to sign custom EFI binaries and kernel images if desired.\n\nThe same procedure can also be used for custom/third-party modules not managed with DKMS, but the key/certificate generation and signing steps need to be done manually in that case.\n\n3.14. Kernel Samepage Merging (KSM)\n\nKernel Samepage Merging (KSM) is an optional memory deduplication feature offered by the Linux kernel, which is enabled by default in Proxmox VE. KSM works by scanning a range of physical memory pages for identical content, and identifying the virtual pages that are mapped to them. If identical pages are found, the corresponding virtual pages are re-mapped so that they all point to the same physical page, and the old pages are freed. The virtual pages are marked as \"copy-on-write\", so that any writes to them will be written to a new area of memory, leaving the shared physical page intact.\n\n3.14.1. Implications of KSM\n\nKSM can optimize memory usage in virtualization environments, as multiple VMs running similar operating systems or workloads could potentially share a lot of common memory pages.\n\nHowever, while KSM can reduce memory usage, it also comes with some security risks, as it can expose VMs to side-channel attacks. Research has shown that it is possible to infer information about a running VM via a second VM on the same host, by exploiting certain characteristics of KSM.\n\nThus, if you are using Proxmox VE to provide hosting services, you should consider disabling KSM, in order to provide your users with additional security. Furthermore, you should check your country’s regulations, as disabling KSM may be a legal requirement.\n\n3.14.2. Disabling KSM\n\nTo see if KSM is active, you can check the output of:\n\n# systemctl status ksmtuned\n\nIf it is, it can be disabled immediately with:\n\n# systemctl disable --now ksmtuned\n\nFinally, to unmerge all the currently merged pages, run:\n\n# echo 2 > /sys/kernel/mm/ksm/run\n4. Graphical User Interface\n\nProxmox VE is simple. There is no need to install a separate management tool, and everything can be done through your web browser (Latest Firefox or Google Chrome is preferred). A built-in HTML5 console is used to access the guest console. As an alternative, SPICE can be used.\n\nBecause we use the Proxmox cluster file system (pmxcfs), you can connect to any node to manage the entire cluster. Each node can manage the entire cluster. There is no need for a dedicated manager node.\n\nYou can use the web-based administration interface with any modern browser. When Proxmox VE detects that you are connecting from a mobile device, you are redirected to a simpler, touch-based user interface.\n\nThe web interface can be reached via https://youripaddress:8006 (default login is: root, and the password is specified during the installation process).\n\n4.1. Features\n\nSeamless integration and management of Proxmox VE clusters\n\nAJAX technologies for dynamic updates of resources\n\nSecure access to all Virtual Machines and Containers via SSL encryption (https)\n\nFast search-driven interface, capable of handling hundreds and probably thousands of VMs\n\nSecure HTML5 console or SPICE\n\nRole based permission management for all objects (VMs, storages, nodes, etc.)\n\nSupport for multiple authentication sources (e.g. local, MS ADS, LDAP, …)\n\nTwo-Factor Authentication (OATH, Yubikey)\n\nBased on ExtJS 7.x JavaScript framework\n\n4.2. Login\n\nWhen you connect to the server, you will first see the login window. Proxmox VE supports various authentication backends (Realm), and you can select the language here. The GUI is translated to more than 20 languages.\n\n\tYou can save the user name on the client side by selecting the checkbox at the bottom. This saves some typing when you login next time.\n4.3. GUI Overview\n\nThe Proxmox VE user interface consists of four regions.\n\nHeader\n\t\n\nOn top. Shows status information and contains buttons for most important actions.\n\n\nResource Tree\n\t\n\nAt the left side. A navigation tree where you can select specific objects.\n\n\nContent Panel\n\t\n\nCenter region. Selected objects display configuration options and status here.\n\n\nLog Panel\n\t\n\nAt the bottom. Displays log entries for recent tasks. You can double-click on those log entries to get more details, or to abort a running task.\n\n\tYou can shrink and expand the size of the resource tree and log panel, or completely hide the log panel. This can be helpful when you work on small displays and want more space to view other content.\n4.3.1. Header\n\nOn the top left side, the first thing you see is the Proxmox logo. Next to it is the current running version of Proxmox VE. In the search bar nearside you can search for specific objects (VMs, containers, nodes, …). This is sometimes faster than selecting an object in the resource tree.\n\nThe right part of the header contains four buttons:\n\nDocumentation\n\t\n\nOpens a new browser window showing the reference documentation.\n\n\nCreate VM\n\t\n\nOpens the virtual machine creation wizard.\n\n\nCreate CT\n\t\n\nOpen the container creation wizard.\n\n\nUser Menu\n\t\n\nDisplays the identity of the user you’re currently logged in with, and clicking it opens a menu with user-specific options.\n\nIn the user menu, you’ll find the My Settings dialog, which provides local UI settings. Below that, there are shortcuts for TFA (Two-Factor Authentication) and Password self-service. You’ll also find options to change the Language and the Color Theme. Finally, at the bottom of the menu is the Logout option.\n\n4.3.2. My Settings\n\nThe My Settings window allows you to set locally stored settings. These include the Dashboard Storages which allow you to enable or disable specific storages to be counted towards the total amount visible in the datacenter summary. If no storage is checked the total is the sum of all storages, same as enabling every single one.\n\nBelow the dashboard settings you find the stored user name and a button to clear it as well as a button to reset every layout in the GUI to its default.\n\nOn the right side there are xterm.js Settings. These contain the following options:\n\nFont-Family\n\t\n\nThe font to be used in xterm.js (e.g. Arial).\n\n\nFont-Size\n\t\n\nThe preferred font size to be used.\n\n\nLetter Spacing\n\t\n\nIncreases or decreases spacing between letters in text.\n\n\nLine Height\n\t\n\nSpecify the absolute height of a line.\n\n4.3.3. Resource Tree\n\nThis is the main navigation tree. On top of the tree you can select some predefined views, which change the structure of the tree below. The default view is the Server View, and it shows the following object types:\n\nDatacenter\n\t\n\nContains cluster-wide settings (relevant for all nodes).\n\n\nNode\n\t\n\nRepresents the hosts inside a cluster, where the guests run.\n\n\nGuest\n\t\n\nVMs, containers and templates.\n\n\nStorage\n\t\n\nData Storage.\n\n\nPool\n\t\n\nIt is possible to group guests using a pool to simplify management.\n\nThe following view types are available:\n\nServer View\n\t\n\nShows all kinds of objects, grouped by nodes.\n\n\nFolder View\n\t\n\nShows all kinds of objects, grouped by object type.\n\n\nPool View\n\t\n\nShow VMs and containers, grouped by pool.\n\n4.3.4. Log Panel\n\nThe main purpose of the log panel is to show you what is currently going on in your cluster. Actions like creating an new VM are executed in the background, and we call such a background job a task.\n\nAny output from such a task is saved into a separate log file. You can view that log by simply double-click a task log entry. It is also possible to abort a running task there.\n\nPlease note that we display the most recent tasks from all cluster nodes here. So you can see when somebody else is working on another cluster node in real-time.\n\n\tWe remove older and finished task from the log panel to keep that list short. But you can still find those tasks within the node panel in the Task History.\n\nSome short-running actions simply send logs to all cluster members. You can see those messages in the Cluster log panel.\n\n4.4. Content Panels\n\nWhen you select an item from the resource tree, the corresponding object displays configuration and status information in the content panel. The following sections provide a brief overview of this functionality. Please refer to the corresponding chapters in the reference documentation to get more detailed information.\n\n4.4.1. Datacenter\n\nOn the datacenter level, you can access cluster-wide settings and information.\n\nSearch: perform a cluster-wide search for nodes, VMs, containers, storage devices, and pools.\n\nSummary: gives a brief overview of the cluster’s health and resource usage.\n\nCluster: provides the functionality and information necessary to create or join a cluster.\n\nOptions: view and manage cluster-wide default settings.\n\nStorage: provides an interface for managing cluster storage.\n\nBackup: schedule backup jobs. This operates cluster wide, so it doesn’t matter where the VMs/containers are on your cluster when scheduling.\n\nReplication: view and manage replication jobs.\n\nPermissions: manage user, group, and API token permissions, and LDAP, MS-AD and Two-Factor authentication.\n\nHA: manage Proxmox VE High Availability.\n\nACME: set up ACME (Let’s Encrypt) certificates for server nodes.\n\nFirewall: configure and make templates for the Proxmox Firewall cluster wide.\n\nMetric Server: define external metric servers for Proxmox VE.\n\nNotifications: configurate notification behavior and targets for Proxmox VE.\n\nSupport: display information about your support subscription.\n\n4.4.2. Nodes\n\nNodes in your cluster can be managed individually at this level.\n\nThe top header has useful buttons such as Reboot, Shutdown, Shell, Bulk Actions and Help. Shell has the options noVNC, SPICE and xterm.js. Bulk Actions has the options Bulk Start, Bulk Shutdown and Bulk Migrate.\n\nSearch: search a node for VMs, containers, storage devices, and pools.\n\nSummary: display a brief overview of the node’s resource usage.\n\nNotes: write custom comments in Markdown syntax.\n\nShell: access to a shell interface for the node.\n\nSystem: configure network, DNS and time settings, and access the syslog.\n\nUpdates: upgrade the system and see the available new packages.\n\nFirewall: manage the Proxmox Firewall for a specific node.\n\nDisks: get an overview of the attached disks, and manage how they are used.\n\nCeph: is only used if you have installed a Ceph server on your host. In this case, you can manage your Ceph cluster and see the status of it here.\n\nReplication: view and manage replication jobs.\n\nTask History: see a list of past tasks.\n\nSubscription: upload a subscription key, and generate a system report for use in support cases.\n\n4.4.3. Guests\n\nThere are two different kinds of guests and both can be converted to a template. One of them is a Kernel-based Virtual Machine (KVM) and the other is a Linux Container (LXC). Navigation for these are mostly the same; only some options are different.\n\nTo access the various guest management interfaces, select a VM or container from the menu on the left.\n\nThe header contains commands for items such as power management, migration, console access and type, cloning, HA, and help. Some of these buttons contain drop-down menus, for example, Shutdown also contains other power options, and Console contains the different console types: SPICE, noVNC and xterm.js.\n\nThe panel on the right contains an interface for whatever item is selected from the menu on the left.\n\nThe available interfaces are as follows.\n\nSummary: provides a brief overview of the VM’s activity and a Notes field for Markdown syntax comments.\n\nConsole: access to an interactive console for the VM/container.\n\n(KVM)Hardware: define the hardware available to the KVM VM.\n\n(LXC)Resources: define the system resources available to the LXC.\n\n(LXC)Network: configure a container’s network settings.\n\n(LXC)DNS: configure a container’s DNS settings.\n\nOptions: manage guest options.\n\nTask History: view all previous tasks related to the selected guest.\n\n(KVM) Monitor: an interactive communication interface to the KVM process.\n\nBackup: create and restore system backups.\n\nReplication: view and manage the replication jobs for the selected guest.\n\nSnapshots: create and restore VM snapshots.\n\nFirewall: configure the firewall on the VM level.\n\nPermissions: manage permissions for the selected guest.\n\n4.4.4. Storage\n\nAs with the guest interface, the interface for storage consists of a menu on the left for certain storage elements and an interface on the right to manage these elements.\n\nIn this view we have a two partition split-view. On the left side we have the storage options and on the right side the content of the selected option will be shown.\n\nSummary: shows important information about the storage, such as the type, usage, and content which it stores.\n\nContent: a menu item for each content type which the storage stores, for example, Backups, ISO Images, CT Templates.\n\nPermissions: manage permissions for the storage.\n\n4.4.5. Pools\n\nAgain, the pools view comprises two partitions: a menu on the left, and the corresponding interfaces for each menu item on the right.\n\nSummary: shows a description of the pool.\n\nMembers: display and manage pool members (guests and storage).\n\nPermissions: manage the permissions for the pool.\n\n4.5. Tags\n\nFor organizational purposes, it is possible to set tags for guests. Currently, these only provide informational value to users. Tags are displayed in two places in the web interface: in the Resource Tree and in the status line when a guest is selected.\n\nTags can be added, edited, and removed in the status line of the guest by clicking on the pencil icon. You can add multiple tags by pressing the + button and remove them by pressing the - button. To save or cancel the changes, you can use the ✓ and x button respectively.\n\nTags can also be set via the CLI, where multiple tags are separated by semicolons. For example:\n\n# qm set ID --tags myfirsttag;mysecondtag\n4.5.1. Style Configuration\n\nBy default, the tag colors are derived from their text in a deterministic way. The color, shape in the resource tree, and case-sensitivity, as well as how tags are sorted, can be customized. This can be done via the web interface under Datacenter → Options → Tag Style Override. Alternatively, this can be done via the CLI. For example:\n\n# pvesh set /cluster/options --tag-style color-map=example:000000:FFFFFF\n\nsets the background color of the tag example to black (#000000) and the text color to white (#FFFFFF).\n\n4.5.2. Permissions\n\nBy default, users with the privilege VM.Config.Options on a guest (/vms/ID) can set any tags they want (see Permission Management). If you want to restrict this behavior, appropriate permissions can be set under Datacenter → Options → User Tag Access:\n\nfree: users are not restricted in setting tags (Default)\n\nlist: users can set tags based on a predefined list of tags\n\nexisting: like list but users can also use already existing tags\n\nnone: users are restricted from using tags\n\nThe same can also be done via the CLI.\n\nNote that a user with the Sys.Modify privileges on / is always able to set or delete any tags, regardless of the settings here. Additionally, there is a configurable list of registered tags which can only be added and removed by users with the privilege Sys.Modify on /. The list of registered tags can be edited under Datacenter → Options → Registered Tags or via the CLI.\n\nFor more details on the exact options and how to invoke them in the CLI, see Datacenter Configuration.\n\n5. Cluster Manager\n\nThe Proxmox VE cluster manager pvecm is a tool to create a group of physical servers. Such a group is called a cluster. We use the Corosync Cluster Engine for reliable group communication. There’s no explicit limit for the number of nodes in a cluster. In practice, the actual possible node count may be limited by the host and network performance. Currently (2021), there are reports of clusters (using high-end enterprise hardware) with over 50 nodes in production.\n\npvecm can be used to create a new cluster, join nodes to a cluster, leave the cluster, get status information, and do various other cluster-related tasks. The Proxmox Cluster File System (“pmxcfs”) is used to transparently distribute the cluster configuration to all cluster nodes.\n\nGrouping nodes into a cluster has the following advantages:\n\nCentralized, web-based management\n\nMulti-master clusters: each node can do all management tasks\n\nUse of pmxcfs, a database-driven file system, for storing configuration files, replicated in real-time on all nodes using corosync\n\nEasy migration of virtual machines and containers between physical hosts\n\nFast deployment\n\nCluster-wide services like firewall and HA\n\n5.1. Requirements\n\nAll nodes must be able to connect to each other via UDP ports 5405-5412 for corosync to work.\n\nDate and time must be synchronized.\n\nAn SSH tunnel on TCP port 22 between nodes is required.\n\nIf you are interested in High Availability, you need to have at least three nodes for reliable quorum. All nodes should have the same version.\n\nWe recommend a dedicated NIC for the cluster traffic, especially if you use shared storage.\n\nThe root password of a cluster node is required for adding nodes.\n\nOnline migration of virtual machines is only supported when nodes have CPUs from the same vendor. It might work otherwise, but this is never guaranteed.\n\n\tIt is not possible to mix Proxmox VE 3.x and earlier with Proxmox VE 4.X cluster nodes.\n\tWhile it’s possible to mix Proxmox VE 4.4 and Proxmox VE 5.0 nodes, doing so is not supported as a production configuration and should only be done temporarily, during an upgrade of the whole cluster from one major version to another.\n\tRunning a cluster of Proxmox VE 6.x with earlier versions is not possible. The cluster protocol (corosync) between Proxmox VE 6.x and earlier versions changed fundamentally. The corosync 3 packages for Proxmox VE 5.4 are only intended for the upgrade procedure to Proxmox VE 6.0.\n5.2. Preparing Nodes\n\nFirst, install Proxmox VE on all nodes. Make sure that each node is installed with the final hostname and IP configuration. Changing the hostname and IP is not possible after cluster creation.\n\nWhile it’s common to reference all node names and their IPs in /etc/hosts (or make their names resolvable through other means), this is not necessary for a cluster to work. It may be useful however, as you can then connect from one node to another via SSH, using the easier to remember node name (see also Link Address Types). Note that we always recommend referencing nodes by their IP addresses in the cluster configuration.\n\n5.3. Create a Cluster\n\nYou can either create a cluster on the console (login via ssh), or through the API using the Proxmox VE web interface (Datacenter → Cluster).\n\n\tUse a unique name for your cluster. This name cannot be changed later. The cluster name follows the same rules as node names.\n5.3.1. Create via Web GUI\n\nUnder Datacenter → Cluster, click on Create Cluster. Enter the cluster name and select a network connection from the drop-down list to serve as the main cluster network (Link 0). It defaults to the IP resolved via the node’s hostname.\n\nAs of Proxmox VE 6.2, up to 8 fallback links can be added to a cluster. To add a redundant link, click the Add button and select a link number and IP address from the respective fields. Prior to Proxmox VE 6.2, to add a second link as fallback, you can select the Advanced checkbox and choose an additional network interface (Link 1, see also Corosync Redundancy).\n\n\tEnsure that the network selected for cluster communication is not used for any high traffic purposes, like network storage or live-migration. While the cluster network itself produces small amounts of data, it is very sensitive to latency. Check out full cluster network requirements.\n5.3.2. Create via the Command Line\n\nLogin via ssh to the first Proxmox VE node and run the following command:\n\n hp1# pvecm create CLUSTERNAME\n\nTo check the state of the new cluster use:\n\n hp1# pvecm status\n5.3.3. Multiple Clusters in the Same Network\n\nIt is possible to create multiple clusters in the same physical or logical network. In this case, each cluster must have a unique name to avoid possible clashes in the cluster communication stack. Furthermore, this helps avoid human confusion by making clusters clearly distinguishable.\n\nWhile the bandwidth requirement of a corosync cluster is relatively low, the latency of packages and the package per second (PPS) rate is the limiting factor. Different clusters in the same network can compete with each other for these resources, so it may still make sense to use separate physical network infrastructure for bigger clusters.\n\n5.4. Adding Nodes to the Cluster\n\tAll existing configuration in /etc/pve is overwritten when joining a cluster. In particular, a joining node cannot hold any guests, since guest IDs could otherwise conflict, and the node will inherit the cluster’s storage configuration. To join a node with existing guest, as a workaround, you can create a backup of each guest (using vzdump) and restore it under a different ID after joining. If the node’s storage layout differs, you will need to re-add the node’s storages, and adapt each storage’s node restriction to reflect on which nodes the storage is actually available.\n5.4.1. Join Node to Cluster via GUI\n\nLog in to the web interface on an existing cluster node. Under Datacenter → Cluster, click the Join Information button at the top. Then, click on the button Copy Information. Alternatively, copy the string from the Information field manually.\n\nNext, log in to the web interface on the node you want to add. Under Datacenter → Cluster, click on Join Cluster. Fill in the Information field with the Join Information text you copied earlier. Most settings required for joining the cluster will be filled out automatically. For security reasons, the cluster password has to be entered manually.\n\n\tTo enter all required data manually, you can disable the Assisted Join checkbox.\n\nAfter clicking the Join button, the cluster join process will start immediately. After the node has joined the cluster, its current node certificate will be replaced by one signed from the cluster certificate authority (CA). This means that the current session will stop working after a few seconds. You then might need to force-reload the web interface and log in again with the cluster credentials.\n\nNow your node should be visible under Datacenter → Cluster.\n\n5.4.2. Join Node to Cluster via Command Line\n\nLog in to the node you want to join into an existing cluster via ssh.\n\n # pvecm add IP-ADDRESS-CLUSTER\n\nFor IP-ADDRESS-CLUSTER, use the IP or hostname of an existing cluster node. An IP address is recommended (see Link Address Types).\n\nTo check the state of the cluster use:\n\n # pvecm status\nCluster status after adding 4 nodes\n # pvecm status\nCluster information\n~~~~~~~~~~~~~~~~~~~\nName:             prod-central\nConfig Version:   3\nTransport:        knet\nSecure auth:      on\n\nQuorum information\n~~~~~~~~~~~~~~~~~~\nDate:             Tue Sep 14 11:06:47 2021\nQuorum provider:  corosync_votequorum\nNodes:            4\nNode ID:          0x00000001\nRing ID:          1.1a8\nQuorate:          Yes\n\nVotequorum information\n~~~~~~~~~~~~~~~~~~~~~~\nExpected votes:   4\nHighest expected: 4\nTotal votes:      4\nQuorum:           3\nFlags:            Quorate\n\nMembership information\n~~~~~~~~~~~~~~~~~~~~~~\n    Nodeid      Votes Name\n0x00000001          1 192.168.15.91\n0x00000002          1 192.168.15.92 (local)\n0x00000003          1 192.168.15.93\n0x00000004          1 192.168.15.94\n\nIf you only want a list of all nodes, use:\n\n # pvecm nodes\nList nodes in a cluster\n # pvecm nodes\n\nMembership information\n~~~~~~~~~~~~~~~~~~~~~~\n    Nodeid      Votes Name\n         1          1 hp1\n         2          1 hp2 (local)\n         3          1 hp3\n         4          1 hp4\n5.4.3. Adding Nodes with Separated Cluster Network\n\nWhen adding a node to a cluster with a separated cluster network, you need to use the link0 parameter to set the nodes address on that network:\n\n# pvecm add IP-ADDRESS-CLUSTER --link0 LOCAL-IP-ADDRESS-LINK0\n\nIf you want to use the built-in redundancy of the Kronosnet transport layer, also use the link1 parameter.\n\nUsing the GUI, you can select the correct interface from the corresponding Link X fields in the Cluster Join dialog.\n\n5.5. Remove a Cluster Node\n\tRead the procedure carefully before proceeding, as it may not be what you want or need.\n\nMove all virtual machines from the node. Ensure that you have made copies of any local data or backups that you want to keep. In addition, make sure to remove any scheduled replication jobs to the node to be removed.\n\n\tFailure to remove replication jobs to a node before removing said node will result in the replication job becoming irremovable. Especially note that replication automatically switches direction if a replicated VM is migrated, so by migrating a replicated VM from a node to be deleted, replication jobs will be set up to that node automatically.\n\nIn the following example, we will remove the node hp4 from the cluster.\n\nLog in to a different cluster node (not hp4), and issue a pvecm nodes command to identify the node ID to remove:\n\n hp1# pvecm nodes\n\nMembership information\n~~~~~~~~~~~~~~~~~~~~~~\n    Nodeid      Votes Name\n         1          1 hp1 (local)\n         2          1 hp2\n         3          1 hp3\n         4          1 hp4\n\nAt this point, you must power off hp4 and ensure that it will not power on again (in the network) with its current configuration.\n\n\tAs mentioned above, it is critical to power off the node before removal, and make sure that it will not power on again (in the existing cluster network) with its current configuration. If you power on the node as it is, the cluster could end up broken, and it could be difficult to restore it to a functioning state.\n\nAfter powering off the node hp4, we can safely remove it from the cluster.\n\n hp1# pvecm delnode hp4\n Killing node 4\n\tAt this point, it is possible that you will receive an error message stating Could not kill node (error = CS_ERR_NOT_EXIST). This does not signify an actual failure in the deletion of the node, but rather a failure in corosync trying to kill an offline node. Thus, it can be safely ignored.\n\nUse pvecm nodes or pvecm status to check the node list again. It should look something like:\n\nhp1# pvecm status\n\n...\n\nVotequorum information\n~~~~~~~~~~~~~~~~~~~~~~\nExpected votes:   3\nHighest expected: 3\nTotal votes:      3\nQuorum:           2\nFlags:            Quorate\n\nMembership information\n~~~~~~~~~~~~~~~~~~~~~~\n    Nodeid      Votes Name\n0x00000001          1 192.168.15.90 (local)\n0x00000002          1 192.168.15.91\n0x00000003          1 192.168.15.92\n\nIf, for whatever reason, you want this server to join the same cluster again, you have to:\n\ndo a fresh install of Proxmox VE on it,\n\nthen join it, as explained in the previous section.\n\nThe configuration files for the removed node will still reside in /etc/pve/nodes/hp4. Recover any configuration you still need and remove the directory afterwards.\n\n\tAfter removal of the node, its SSH fingerprint will still reside in the known_hosts of the other nodes. If you receive an SSH error after rejoining a node with the same IP or hostname, run pvecm updatecerts once on the re-added node to update its fingerprint cluster wide.\n5.5.1. Separate a Node Without Reinstalling\n\tThis is not the recommended method, proceed with caution. Use the previous method if you’re unsure.\n\nYou can also separate a node from a cluster without reinstalling it from scratch. But after removing the node from the cluster, it will still have access to any shared storage. This must be resolved before you start removing the node from the cluster. A Proxmox VE cluster cannot share the exact same storage with another cluster, as storage locking doesn’t work over the cluster boundary. Furthermore, it may also lead to VMID conflicts.\n\nIt’s suggested that you create a new storage, where only the node which you want to separate has access. This can be a new export on your NFS or a new Ceph pool, to name a few examples. It’s just important that the exact same storage does not get accessed by multiple clusters. After setting up this storage, move all data and VMs from the node to it. Then you are ready to separate the node from the cluster.\n\n\tEnsure that all shared resources are cleanly separated! Otherwise you will run into conflicts and problems.\n\nFirst, stop the corosync and pve-cluster services on the node:\n\nsystemctl stop pve-cluster\nsystemctl stop corosync\n\nStart the cluster file system again in local mode:\n\npmxcfs -l\n\nDelete the corosync configuration files:\n\nrm /etc/pve/corosync.conf\nrm -r /etc/corosync/*\n\nYou can now start the file system again as a normal service:\n\nkillall pmxcfs\nsystemctl start pve-cluster\n\nThe node is now separated from the cluster. You can deleted it from any remaining node of the cluster with:\n\npvecm delnode oldnode\n\nIf the command fails due to a loss of quorum in the remaining node, you can set the expected votes to 1 as a workaround:\n\npvecm expected 1\n\nAnd then repeat the pvecm delnode command.\n\nNow switch back to the separated node and delete all the remaining cluster files on it. This ensures that the node can be added to another cluster again without problems.\n\nrm /var/lib/corosync/*\n\nAs the configuration files from the other nodes are still in the cluster file system, you may want to clean those up too. After making absolutely sure that you have the correct node name, you can simply remove the entire directory recursively from /etc/pve/nodes/NODENAME.\n\n\tThe node’s SSH keys will remain in the authorized_key file. This means that the nodes can still connect to each other with public key authentication. You should fix this by removing the respective keys from the /etc/pve/priv/authorized_keys file.\n5.6. Quorum\n\nProxmox VE use a quorum-based technique to provide a consistent state among all cluster nodes.\n\nA quorum is the minimum number of votes that a distributed transaction has to obtain in order to be allowed to perform an operation in a distributed system.\n\nQuorum (distributed computing)\n— from Wikipedia\n\nIn case of network partitioning, state changes requires that a majority of nodes are online. The cluster switches to read-only mode if it loses quorum.\n\n\tProxmox VE assigns a single vote to each node by default.\n5.7. Cluster Network\n\nThe cluster network is the core of a cluster. All messages sent over it have to be delivered reliably to all nodes in their respective order. In Proxmox VE this part is done by corosync, an implementation of a high performance, low overhead, high availability development toolkit. It serves our decentralized configuration file system (pmxcfs).\n\n5.7.1. Network Requirements\n\nThe Proxmox VE cluster stack requires a reliable network with latencies under 5 milliseconds (LAN performance) between all nodes to operate stably. While on setups with a small node count a network with higher latencies may work, this is not guaranteed and gets rather unlikely with more than three nodes and latencies above around 10 ms.\n\nThe network should not be used heavily by other members, as while corosync does not uses much bandwidth it is sensitive to latency jitters; ideally corosync runs on its own physically separated network. Especially do not use a shared network for corosync and storage (except as a potential low-priority fallback in a redundant configuration).\n\nBefore setting up a cluster, it is good practice to check if the network is fit for that purpose. To ensure that the nodes can connect to each other on the cluster network, you can test the connectivity between them with the ping tool.\n\nIf the Proxmox VE firewall is enabled, ACCEPT rules for corosync will automatically be generated - no manual action is required.\n\n\tCorosync used Multicast before version 3.0 (introduced in Proxmox VE 6.0). Modern versions rely on Kronosnet for cluster communication, which, for now, only supports regular UDP unicast.\n\tYou can still enable Multicast or legacy unicast by setting your transport to udp or udpu in your corosync.conf, but keep in mind that this will disable all cryptography and redundancy support. This is therefore not recommended.\n5.7.2. Separate Cluster Network\n\nWhen creating a cluster without any parameters, the corosync cluster network is generally shared with the web interface and the VMs' network. Depending on your setup, even storage traffic may get sent over the same network. It’s recommended to change that, as corosync is a time-critical, real-time application.\n\nSetting Up a New Network\n\nFirst, you have to set up a new network interface. It should be on a physically separate network. Ensure that your network fulfills the cluster network requirements.\n\nSeparate On Cluster Creation\n\nThis is possible via the linkX parameters of the pvecm create command, used for creating a new cluster.\n\nIf you have set up an additional NIC with a static address on 10.10.10.1/25, and want to send and receive all cluster communication over this interface, you would execute:\n\npvecm create test --link0 10.10.10.1\n\nTo check if everything is working properly, execute:\n\nsystemctl status corosync\n\nAfterwards, proceed as described above to add nodes with a separated cluster network.\n\nSeparate After Cluster Creation\n\nYou can do this if you have already created a cluster and want to switch its communication to another network, without rebuilding the whole cluster. This change may lead to short periods of quorum loss in the cluster, as nodes have to restart corosync and come up one after the other on the new network.\n\nCheck how to edit the corosync.conf file first. Then, open it and you should see a file similar to:\n\nlogging {\n  debug: off\n  to_syslog: yes\n}\n\nnodelist {\n\n  node {\n    name: due\n    nodeid: 2\n    quorum_votes: 1\n    ring0_addr: due\n  }\n\n  node {\n    name: tre\n    nodeid: 3\n    quorum_votes: 1\n    ring0_addr: tre\n  }\n\n  node {\n    name: uno\n    nodeid: 1\n    quorum_votes: 1\n    ring0_addr: uno\n  }\n\n}\n\nquorum {\n  provider: corosync_votequorum\n}\n\ntotem {\n  cluster_name: testcluster\n  config_version: 3\n  ip_version: ipv4-6\n  secauth: on\n  version: 2\n  interface {\n    linknumber: 0\n  }\n\n}\n\tringX_addr actually specifies a corosync link address. The name \"ring\" is a remnant of older corosync versions that is kept for backwards compatibility.\n\nThe first thing you want to do is add the name properties in the node entries, if you do not see them already. Those must match the node name.\n\nThen replace all addresses from the ring0_addr properties of all nodes with the new addresses. You may use plain IP addresses or hostnames here. If you use hostnames, ensure that they are resolvable from all nodes (see also Link Address Types).\n\nIn this example, we want to switch cluster communication to the 10.10.10.0/25 network, so we change the ring0_addr of each node respectively.\n\n\tThe exact same procedure can be used to change other ringX_addr values as well. However, we recommend only changing one link address at a time, so that it’s easier to recover if something goes wrong.\n\nAfter we increase the config_version property, the new configuration file should look like:\n\nlogging {\n  debug: off\n  to_syslog: yes\n}\n\nnodelist {\n\n  node {\n    name: due\n    nodeid: 2\n    quorum_votes: 1\n    ring0_addr: 10.10.10.2\n  }\n\n  node {\n    name: tre\n    nodeid: 3\n    quorum_votes: 1\n    ring0_addr: 10.10.10.3\n  }\n\n  node {\n    name: uno\n    nodeid: 1\n    quorum_votes: 1\n    ring0_addr: 10.10.10.1\n  }\n\n}\n\nquorum {\n  provider: corosync_votequorum\n}\n\ntotem {\n  cluster_name: testcluster\n  config_version: 4\n  ip_version: ipv4-6\n  secauth: on\n  version: 2\n  interface {\n    linknumber: 0\n  }\n\n}\n\nThen, after a final check to see that all changed information is correct, we save it and once again follow the edit corosync.conf file section to bring it into effect.\n\nThe changes will be applied live, so restarting corosync is not strictly necessary. If you changed other settings as well, or notice corosync complaining, you can optionally trigger a restart.\n\nOn a single node execute:\n\nsystemctl restart corosync\n\nNow check if everything is okay:\n\nsystemctl status corosync\n\nIf corosync begins to work again, restart it on all other nodes too. They will then join the cluster membership one by one on the new network.\n\n5.7.3. Corosync Addresses\n\nA corosync link address (for backwards compatibility denoted by ringX_addr in corosync.conf) can be specified in two ways:\n\nIPv4/v6 addresses can be used directly. They are recommended, since they are static and usually not changed carelessly.\n\nHostnames will be resolved using getaddrinfo, which means that by default, IPv6 addresses will be used first, if available (see also man gai.conf). Keep this in mind, especially when upgrading an existing cluster to IPv6.\n\n\tHostnames should be used with care, since the addresses they resolve to can be changed without touching corosync or the node it runs on - which may lead to a situation where an address is changed without thinking about implications for corosync.\n\nA separate, static hostname specifically for corosync is recommended, if hostnames are preferred. Also, make sure that every node in the cluster can resolve all hostnames correctly.\n\nSince Proxmox VE 5.1, while supported, hostnames will be resolved at the time of entry. Only the resolved IP is saved to the configuration.\n\nNodes that joined the cluster on earlier versions likely still use their unresolved hostname in corosync.conf. It might be a good idea to replace them with IPs or a separate hostname, as mentioned above.\n\n5.8. Corosync Redundancy\n\nCorosync supports redundant networking via its integrated Kronosnet layer by default (it is not supported on the legacy udp/udpu transports). It can be enabled by specifying more than one link address, either via the --linkX parameters of pvecm, in the GUI as Link 1 (while creating a cluster or adding a new node) or by specifying more than one ringX_addr in corosync.conf.\n\n\tTo provide useful failover, every link should be on its own physical network connection.\n\nLinks are used according to a priority setting. You can configure this priority by setting knet_link_priority in the corresponding interface section in corosync.conf, or, preferably, using the priority parameter when creating your cluster with pvecm:\n\n # pvecm create CLUSTERNAME --link0 10.10.10.1,priority=15 --link1 10.20.20.1,priority=20\n\nThis would cause link1 to be used first, since it has the higher priority.\n\nIf no priorities are configured manually (or two links have the same priority), links will be used in order of their number, with the lower number having higher priority.\n\nEven if all links are working, only the one with the highest priority will see corosync traffic. Link priorities cannot be mixed, meaning that links with different priorities will not be able to communicate with each other.\n\nSince lower priority links will not see traffic unless all higher priorities have failed, it becomes a useful strategy to specify networks used for other tasks (VMs, storage, etc.) as low-priority links. If worst comes to worst, a higher latency or more congested connection might be better than no connection at all.\n\n5.8.1. Adding Redundant Links To An Existing Cluster\n\nTo add a new link to a running configuration, first check how to edit the corosync.conf file.\n\nThen, add a new ringX_addr to every node in the nodelist section. Make sure that your X is the same for every node you add it to, and that it is unique for each node.\n\nLastly, add a new interface, as shown below, to your totem section, replacing X with the link number chosen above.\n\nAssuming you added a link with number 1, the new configuration file could look like this:\n\nlogging {\n  debug: off\n  to_syslog: yes\n}\n\nnodelist {\n\n  node {\n    name: due\n    nodeid: 2\n    quorum_votes: 1\n    ring0_addr: 10.10.10.2\n    ring1_addr: 10.20.20.2\n  }\n\n  node {\n    name: tre\n    nodeid: 3\n    quorum_votes: 1\n    ring0_addr: 10.10.10.3\n    ring1_addr: 10.20.20.3\n  }\n\n  node {\n    name: uno\n    nodeid: 1\n    quorum_votes: 1\n    ring0_addr: 10.10.10.1\n    ring1_addr: 10.20.20.1\n  }\n\n}\n\nquorum {\n  provider: corosync_votequorum\n}\n\ntotem {\n  cluster_name: testcluster\n  config_version: 4\n  ip_version: ipv4-6\n  secauth: on\n  version: 2\n  interface {\n    linknumber: 0\n  }\n  interface {\n    linknumber: 1\n  }\n}\n\nThe new link will be enabled as soon as you follow the last steps to edit the corosync.conf file. A restart should not be necessary. You can check that corosync loaded the new link using:\n\njournalctl -b -u corosync\n\nIt might be a good idea to test the new link by temporarily disconnecting the old link on one node and making sure that its status remains online while disconnected:\n\npvecm status\n\nIf you see a healthy cluster state, it means that your new link is being used.\n\n5.9. Role of SSH in Proxmox VE Clusters\n\nProxmox VE utilizes SSH tunnels for various features.\n\nProxying console/shell sessions (node and guests)\n\nWhen using the shell for node B while being connected to node A, connects to a terminal proxy on node A, which is in turn connected to the login shell on node B via a non-interactive SSH tunnel.\n\nVM and CT memory and local-storage migration in secure mode.\n\nDuring the migration, one or more SSH tunnel(s) are established between the source and target nodes, in order to exchange migration information and transfer memory and disk contents.\n\nStorage replication\n\n5.9.1. SSH setup\n\nOn Proxmox VE systems, the following changes are made to the SSH configuration/setup:\n\nthe root user’s SSH client config gets setup to prefer AES over ChaCha20\n\nthe root user’s authorized_keys file gets linked to /etc/pve/priv/authorized_keys, merging all authorized keys within a cluster\n\nsshd is configured to allow logging in as root with a password\n\n\tOlder systems might also have /etc/ssh/ssh_known_hosts set up as symlink pointing to /etc/pve/priv/known_hosts, containing a merged version of all node host keys. This system was replaced with explicit host key pinning in pve-cluster <<INSERT VERSION>>, the symlink can be deconfigured if still in place by running pvecm updatecerts --unmerge-known-hosts.\n5.9.2. Pitfalls due to automatic execution of .bashrc and siblings\n\nIn case you have a custom .bashrc, or similar files that get executed on login by the configured shell, ssh will automatically run it once the session is established successfully. This can cause some unexpected behavior, as those commands may be executed with root permissions on any of the operations described above. This can cause possible problematic side-effects!\n\nIn order to avoid such complications, it’s recommended to add a check in /root/.bashrc to make sure the session is interactive, and only then run .bashrc commands.\n\nYou can add this snippet at the beginning of your .bashrc file:\n\n# Early exit if not running interactively to avoid side-effects!\ncase $- in\n    *i*) ;;\n      *) return;;\nesac\n5.10. Corosync External Vote Support\n\nThis section describes a way to deploy an external voter in a Proxmox VE cluster. When configured, the cluster can sustain more node failures without violating safety properties of the cluster communication.\n\nFor this to work, there are two services involved:\n\nA QDevice daemon which runs on each Proxmox VE node\n\nAn external vote daemon which runs on an independent server\n\nAs a result, you can achieve higher availability, even in smaller setups (for example 2+1 nodes).\n\n5.10.1. QDevice Technical Overview\n\nThe Corosync Quorum Device (QDevice) is a daemon which runs on each cluster node. It provides a configured number of votes to the cluster’s quorum subsystem, based on an externally running third-party arbitrator’s decision. Its primary use is to allow a cluster to sustain more node failures than standard quorum rules allow. This can be done safely as the external device can see all nodes and thus choose only one set of nodes to give its vote. This will only be done if said set of nodes can have quorum (again) after receiving the third-party vote.\n\nCurrently, only QDevice Net is supported as a third-party arbitrator. This is a daemon which provides a vote to a cluster partition, if it can reach the partition members over the network. It will only give votes to one partition of a cluster at any time. It’s designed to support multiple clusters and is almost configuration and state free. New clusters are handled dynamically and no configuration file is needed on the host running a QDevice.\n\nThe only requirements for the external host are that it needs network access to the cluster and to have a corosync-qnetd package available. We provide a package for Debian based hosts, and other Linux distributions should also have a package available through their respective package manager.\n\n\tUnlike corosync itself, a QDevice connects to the cluster over TCP/IP. The daemon can also run outside the LAN of the cluster and isn’t limited to the low latencies requirements of corosync.\n5.10.2. Supported Setups\n\nWe support QDevices for clusters with an even number of nodes and recommend it for 2 node clusters, if they should provide higher availability. For clusters with an odd node count, we currently discourage the use of QDevices. The reason for this is the difference in the votes which the QDevice provides for each cluster type. Even numbered clusters get a single additional vote, which only increases availability, because if the QDevice itself fails, you are in the same position as with no QDevice at all.\n\nOn the other hand, with an odd numbered cluster size, the QDevice provides (N-1) votes — where N corresponds to the cluster node count. This alternative behavior makes sense; if it had only one additional vote, the cluster could get into a split-brain situation. This algorithm allows for all nodes but one (and naturally the QDevice itself) to fail. However, there are two drawbacks to this:\n\nIf the QNet daemon itself fails, no other node may fail or the cluster immediately loses quorum. For example, in a cluster with 15 nodes, 7 could fail before the cluster becomes inquorate. But, if a QDevice is configured here and it itself fails, no single node of the 15 may fail. The QDevice acts almost as a single point of failure in this case.\n\nThe fact that all but one node plus QDevice may fail sounds promising at first, but this may result in a mass recovery of HA services, which could overload the single remaining node. Furthermore, a Ceph server will stop providing services if only ((N-1)/2) nodes or less remain online.\n\nIf you understand the drawbacks and implications, you can decide yourself if you want to use this technology in an odd numbered cluster setup.\n\n5.10.3. QDevice-Net Setup\n\nWe recommend running any daemon which provides votes to corosync-qdevice as an unprivileged user. Proxmox VE and Debian provide a package which is already configured to do so. The traffic between the daemon and the cluster must be encrypted to ensure a safe and secure integration of the QDevice in Proxmox VE.\n\nFirst, install the corosync-qnetd package on your external server\n\nexternal# apt install corosync-qnetd\n\nand the corosync-qdevice package on all cluster nodes\n\npve# apt install corosync-qdevice\n\nAfter doing this, ensure that all the nodes in the cluster are online.\n\nYou can now set up your QDevice by running the following command on one of the Proxmox VE nodes:\n\npve# pvecm qdevice setup <QDEVICE-IP>\n\nThe SSH key from the cluster will be automatically copied to the QDevice.\n\n\tMake sure to setup key-based access for the root user on your external server, or temporarily allow root login with password during the setup phase. If you receive an error such as Host key verification failed. at this stage, running pvecm updatecerts could fix the issue.\n\nAfter all the steps have successfully completed, you will see \"Done\". You can verify that the QDevice has been set up with:\n\npve# pvecm status\n\n...\n\nVotequorum information\n~~~~~~~~~~~~~~~~~~~~~\nExpected votes:   3\nHighest expected: 3\nTotal votes:      3\nQuorum:           2\nFlags:            Quorate Qdevice\n\nMembership information\n~~~~~~~~~~~~~~~~~~~~~~\n    Nodeid      Votes    Qdevice Name\n    0x00000001      1    A,V,NMW 192.168.22.180 (local)\n    0x00000002      1    A,V,NMW 192.168.22.181\n    0x00000000      1            Qdevice\nQDevice Status Flags\n\nThe status output of the QDevice, as seen above, will usually contain three columns:\n\nA / NA: Alive or Not Alive. Indicates if the communication to the external corosync-qnetd daemon works.\n\nV / NV: If the QDevice will cast a vote for the node. In a split-brain situation, where the corosync connection between the nodes is down, but they both can still communicate with the external corosync-qnetd daemon, only one node will get the vote.\n\nMW / NMW: Master wins (MV) or not (NMW). Default is NMW, see [11].\n\nNR: QDevice is not registered.\n\n\tIf your QDevice is listed as Not Alive (NA in the output above), ensure that port 5403 (the default port of the qnetd server) of your external server is reachable via TCP/IP!\n5.10.4. Frequently Asked Questions\nTie Breaking\n\nIn case of a tie, where two same-sized cluster partitions cannot see each other but can see the QDevice, the QDevice chooses one of those partitions randomly and provides a vote to it.\n\nPossible Negative Implications\n\nFor clusters with an even node count, there are no negative implications when using a QDevice. If it fails to work, it is the same as not having a QDevice at all.\n\nAdding/Deleting Nodes After QDevice Setup\n\nIf you want to add a new node or remove an existing one from a cluster with a QDevice setup, you need to remove the QDevice first. After that, you can add or remove nodes normally. Once you have a cluster with an even node count again, you can set up the QDevice again as described previously.\n\nRemoving the QDevice\n\nIf you used the official pvecm tool to add the QDevice, you can remove it by running:\n\npve# pvecm qdevice remove\n5.11. Corosync Configuration\n\nThe /etc/pve/corosync.conf file plays a central role in a Proxmox VE cluster. It controls the cluster membership and its network. For further information about it, check the corosync.conf man page:\n\nman corosync.conf\n\nFor node membership, you should always use the pvecm tool provided by Proxmox VE. You may have to edit the configuration file manually for other changes. Here are a few best practice tips for doing this.\n\n5.11.1. Edit corosync.conf\n\nEditing the corosync.conf file is not always very straightforward. There are two on each cluster node, one in /etc/pve/corosync.conf and the other in /etc/corosync/corosync.conf. Editing the one in our cluster file system will propagate the changes to the local one, but not vice versa.\n\nThe configuration will get updated automatically, as soon as the file changes. This means that changes which can be integrated in a running corosync will take effect immediately. Thus, you should always make a copy and edit that instead, to avoid triggering unintended changes when saving the file while editing.\n\ncp /etc/pve/corosync.conf /etc/pve/corosync.conf.new\n\nThen, open the config file with your favorite editor, such as nano or vim.tiny, which come pre-installed on every Proxmox VE node.\n\n\tAlways increment the config_version number after configuration changes; omitting this can lead to problems.\n\nAfter making the necessary changes, create another copy of the current working configuration file. This serves as a backup if the new configuration fails to apply or causes other issues.\n\ncp /etc/pve/corosync.conf /etc/pve/corosync.conf.bak\n\nThen replace the old configuration file with the new one:\n\nmv /etc/pve/corosync.conf.new /etc/pve/corosync.conf\n\nYou can check if the changes could be applied automatically, using the following commands:\n\nsystemctl status corosync\njournalctl -b -u corosync\n\nIf the changes could not be applied automatically, you may have to restart the corosync service via:\n\nsystemctl restart corosync\n\nOn errors, check the troubleshooting section below.\n\n5.11.2. Troubleshooting\nIssue: quorum.expected_votes must be configured\n\nWhen corosync starts to fail and you get the following message in the system log:\n\n[...]\ncorosync[1647]:  [QUORUM] Quorum provider: corosync_votequorum failed to initialize.\ncorosync[1647]:  [SERV  ] Service engine 'corosync_quorum' failed to load for reason\n    'configuration error: nodelist or quorum.expected_votes must be configured!'\n[...]\n\nIt means that the hostname you set for a corosync ringX_addr in the configuration could not be resolved.\n\nWrite Configuration When Not Quorate\n\nIf you need to change /etc/pve/corosync.conf on a node with no quorum, and you understand what you are doing, use:\n\npvecm expected 1\n\nThis sets the expected vote count to 1 and makes the cluster quorate. You can then fix your configuration, or revert it back to the last working backup.\n\nThis is not enough if corosync cannot start anymore. In that case, it is best to edit the local copy of the corosync configuration in /etc/corosync/corosync.conf, so that corosync can start again. Ensure that on all nodes, this configuration has the same content to avoid split-brain situations.\n\n5.11.3. Corosync Configuration Glossary\nringX_addr\n\nThis names the different link addresses for the Kronosnet connections between nodes.\n\n5.12. Cluster Cold Start\n\nIt is obvious that a cluster is not quorate when all nodes are offline. This is a common case after a power failure.\n\n\tIt is always a good idea to use an uninterruptible power supply (“UPS”, also called “battery backup”) to avoid this state, especially if you want HA.\n\nOn node startup, the pve-guests service is started and waits for quorum. Once quorate, it starts all guests which have the onboot flag set.\n\nWhen you turn on nodes, or when power comes back after power failure, it is likely that some nodes will boot faster than others. Please keep in mind that guest startup is delayed until you reach quorum.\n\n5.13. Guest VMID Auto-Selection\n\nWhen creating new guests the web interface will ask the backend for a free VMID automatically. The default range for searching is 100 to 1000000 (lower than the maximal allowed VMID enforced by the schema).\n\nSometimes admins either want to allocate new VMIDs in a separate range, for example to easily separate temporary VMs with ones that choose a VMID manually. Other times its just desired to provided a stable length VMID, for which setting the lower boundary to, for example, 100000 gives much more room for.\n\nTo accommodate this use case one can set either lower, upper or both boundaries via the datacenter.cfg configuration file, which can be edited in the web interface under Datacenter → Options.\n\n\tThe range is only used for the next-id API call, so it isn’t a hard limit.\n5.14. Guest Migration\n\nMigrating virtual guests to other nodes is a useful feature in a cluster. There are settings to control the behavior of such migrations. This can be done via the configuration file datacenter.cfg or for a specific migration via API or command-line parameters.\n\nIt makes a difference if a guest is online or offline, or if it has local resources (like a local disk).\n\nFor details about virtual machine migration, see the QEMU/KVM Migration Chapter.\n\nFor details about container migration, see the Container Migration Chapter.\n\n5.14.1. Migration Type\n\nThe migration type defines if the migration data should be sent over an encrypted (secure) channel or an unencrypted (insecure) one. Setting the migration type to insecure means that the RAM content of a virtual guest is also transferred unencrypted, which can lead to information disclosure of critical data from inside the guest (for example, passwords or encryption keys).\n\nTherefore, we strongly recommend using the secure channel if you do not have full control over the network and can not guarantee that no one is eavesdropping on it.\n\n\tStorage migration does not follow this setting. Currently, it always sends the storage content over a secure channel.\n\nEncryption requires a lot of computing power, so this setting is often changed to insecure to achieve better performance. The impact on modern systems is lower because they implement AES encryption in hardware. The performance impact is particularly evident in fast networks, where you can transfer 10 Gbps or more.\n\n5.14.2. Migration Network\n\nBy default, Proxmox VE uses the network in which cluster communication takes place to send the migration traffic. This is not optimal both because sensitive cluster traffic can be disrupted and this network may not have the best bandwidth available on the node.\n\nSetting the migration network parameter allows the use of a dedicated network for all migration traffic. In addition to the memory, this also affects the storage traffic for offline migrations.\n\nThe migration network is set as a network using CIDR notation. This has the advantage that you don’t have to set individual IP addresses for each node. Proxmox VE can determine the real address on the destination node from the network specified in the CIDR form. To enable this, the network must be specified so that each node has exactly one IP in the respective network.\n\nExample\n\nWe assume that we have a three-node setup, with three separate networks. One for public communication with the Internet, one for cluster communication, and a very fast one, which we want to use as a dedicated network for migration.\n\nA network configuration for such a setup might look as follows:\n\niface eno1 inet manual\n\n# public network\nauto vmbr0\niface vmbr0 inet static\n    address 192.X.Y.57/24\n    gateway 192.X.Y.1\n    bridge-ports eno1\n    bridge-stp off\n    bridge-fd 0\n\n# cluster network\nauto eno2\niface eno2 inet static\n    address  10.1.1.1/24\n\n# fast network\nauto eno3\niface eno3 inet static\n    address  10.1.2.1/24\n\nHere, we will use the network 10.1.2.0/24 as a migration network. For a single migration, you can do this using the migration_network parameter of the command-line tool:\n\n# qm migrate 106 tre --online --migration_network 10.1.2.0/24\n\nTo configure this as the default network for all migrations in the cluster, set the migration property of the /etc/pve/datacenter.cfg file:\n\n# use dedicated migration network\nmigration: secure,network=10.1.2.0/24\n\tThe migration type must always be set when the migration network is set in /etc/pve/datacenter.cfg.\n6. Proxmox Cluster File System (pmxcfs)\n\nThe Proxmox Cluster file system (“pmxcfs”) is a database-driven file system for storing configuration files, replicated in real time to all cluster nodes using corosync. We use this to store all Proxmox VE related configuration files.\n\nAlthough the file system stores all data inside a persistent database on disk, a copy of the data resides in RAM. This imposes restrictions on the maximum size, which is currently 128 MiB. This is still enough to store the configuration of several thousand virtual machines.\n\nThis system provides the following advantages:\n\nSeamless replication of all configuration to all nodes in real time\n\nProvides strong consistency checks to avoid duplicate VM IDs\n\nRead-only when a node loses quorum\n\nAutomatic updates of the corosync cluster configuration to all nodes\n\nIncludes a distributed locking mechanism\n\n6.1. POSIX Compatibility\n\nThe file system is based on FUSE, so the behavior is POSIX like. But some feature are simply not implemented, because we do not need them:\n\nYou can just generate normal files and directories, but no symbolic links, …\n\nYou can’t rename non-empty directories (because this makes it easier to guarantee that VMIDs are unique).\n\nYou can’t change file permissions (permissions are based on paths)\n\nO_EXCL creates were not atomic (like old NFS)\n\nO_TRUNC creates are not atomic (FUSE restriction)\n\n6.2. File Access Rights\n\nAll files and directories are owned by user root and have group www-data. Only root has write permissions, but group www-data can read most files. Files below the following paths are only accessible by root:\n\n/etc/pve/priv/\n/etc/pve/nodes/${NAME}/priv/\n6.3. Technology\n\nWe use the Corosync Cluster Engine for cluster communication, and SQlite for the database file. The file system is implemented in user space using FUSE.\n\n6.4. File System Layout\n\nThe file system is mounted at:\n\n/etc/pve\n6.4.1. Files\n\nauthkey.pub\n\n\t\n\nPublic key used by the ticket system\n\n\n\n\nceph.conf\n\n\t\n\nCeph configuration file (note: /etc/ceph/ceph.conf is a symbolic link to this)\n\n\n\n\ncorosync.conf\n\n\t\n\nCorosync cluster configuration file (prior to Proxmox VE 4.x, this file was called cluster.conf)\n\n\n\n\ndatacenter.cfg\n\n\t\n\nProxmox VE datacenter-wide configuration (keyboard layout, proxy, …)\n\n\n\n\ndomains.cfg\n\n\t\n\nProxmox VE authentication domains\n\n\n\n\nfirewall/cluster.fw\n\n\t\n\nFirewall configuration applied to all nodes\n\n\n\n\nfirewall/<NAME>.fw\n\n\t\n\nFirewall configuration for individual nodes\n\n\n\n\nfirewall/<VMID>.fw\n\n\t\n\nFirewall configuration for VMs and containers\n\n\n\n\nha/crm_commands\n\n\t\n\nDisplays HA operations that are currently being carried out by the CRM\n\n\n\n\nha/manager_status\n\n\t\n\nJSON-formatted information regarding HA services on the cluster\n\n\n\n\nha/resources.cfg\n\n\t\n\nResources managed by high availability, and their current state\n\n\n\n\nnodes/<NAME>/config\n\n\t\n\nNode-specific configuration\n\n\n\n\nnodes/<NAME>/lxc/<VMID>.conf\n\n\t\n\nVM configuration data for LXC containers\n\n\n\n\nnodes/<NAME>/openvz/\n\n\t\n\nPrior to Proxmox VE 4.0, used for container configuration data (deprecated, removed soon)\n\n\n\n\nnodes/<NAME>/pve-ssl.key\n\n\t\n\nPrivate SSL key for pve-ssl.pem\n\n\n\n\nnodes/<NAME>/pve-ssl.pem\n\n\t\n\nPublic SSL certificate for web server (signed by cluster CA)\n\n\n\n\nnodes/<NAME>/pveproxy-ssl.key\n\n\t\n\nPrivate SSL key for pveproxy-ssl.pem (optional)\n\n\n\n\nnodes/<NAME>/pveproxy-ssl.pem\n\n\t\n\nPublic SSL certificate (chain) for web server (optional override for pve-ssl.pem)\n\n\n\n\nnodes/<NAME>/qemu-server/<VMID>.conf\n\n\t\n\nVM configuration data for KVM VMs\n\n\n\n\npriv/authkey.key\n\n\t\n\nPrivate key used by ticket system\n\n\n\n\npriv/authorized_keys\n\n\t\n\nSSH keys of cluster members for authentication\n\n\n\n\npriv/ceph*\n\n\t\n\nCeph authentication keys and associated capabilities\n\n\n\n\npriv/known_hosts\n\n\t\n\nSSH keys of the cluster members for verification\n\n\n\n\npriv/lock/*\n\n\t\n\nLock files used by various services to ensure safe cluster-wide operations\n\n\n\n\npriv/pve-root-ca.key\n\n\t\n\nPrivate key of cluster CA\n\n\n\n\npriv/shadow.cfg\n\n\t\n\nShadow password file for PVE Realm users\n\n\n\n\npriv/storage/<STORAGE-ID>.pw\n\n\t\n\nContains the password of a storage in plain text\n\n\n\n\npriv/tfa.cfg\n\n\t\n\nBase64-encoded two-factor authentication configuration\n\n\n\n\npriv/token.cfg\n\n\t\n\nAPI token secrets of all tokens\n\n\n\n\npve-root-ca.pem\n\n\t\n\nPublic certificate of cluster CA\n\n\n\n\npve-www.key\n\n\t\n\nPrivate key used for generating CSRF tokens\n\n\n\n\nsdn/*\n\n\t\n\nShared configuration files for Software Defined Networking (SDN)\n\n\n\n\nstatus.cfg\n\n\t\n\nProxmox VE external metrics server configuration\n\n\n\n\nstorage.cfg\n\n\t\n\nProxmox VE storage configuration\n\n\n\n\nuser.cfg\n\n\t\n\nProxmox VE access control configuration (users/groups/…)\n\n\n\n\nvirtual-guest/cpu-models.conf\n\n\t\n\nFor storing custom CPU models\n\n\n\n\nvzdump.cron\n\n\t\n\nCluster-wide vzdump backup-job schedule\n\n6.4.2. Symbolic links\n\nCertain directories within the cluster file system use symbolic links, in order to point to a node’s own configuration files. Thus, the files pointed to in the table below refer to different files on each node of the cluster.\n\nlocal\n\n\t\n\nnodes/<LOCAL_HOST_NAME>\n\n\n\n\nlxc\n\n\t\n\nnodes/<LOCAL_HOST_NAME>/lxc/\n\n\n\n\nopenvz\n\n\t\n\nnodes/<LOCAL_HOST_NAME>/openvz/ (deprecated, removed soon)\n\n\n\n\nqemu-server\n\n\t\n\nnodes/<LOCAL_HOST_NAME>/qemu-server/\n\n6.4.3. Special status files for debugging (JSON)\n\n.version\n\n\t\n\nFile versions (to detect file modifications)\n\n\n\n\n.members\n\n\t\n\nInfo about cluster members\n\n\n\n\n.vmlist\n\n\t\n\nList of all VMs\n\n\n\n\n.clusterlog\n\n\t\n\nCluster log (last 50 entries)\n\n\n\n\n.rrd\n\n\t\n\nRRD data (most recent entries)\n\n6.4.4. Enable/Disable debugging\n\nYou can enable verbose syslog messages with:\n\necho \"1\" >/etc/pve/.debug\n\nAnd disable verbose syslog messages with:\n\necho \"0\" >/etc/pve/.debug\n6.5. Recovery\n\nIf you have major problems with your Proxmox VE host, for example hardware issues, it could be helpful to copy the pmxcfs database file /var/lib/pve-cluster/config.db, and move it to a new Proxmox VE host. On the new host (with nothing running), you need to stop the pve-cluster service and replace the config.db file (required permissions 0600). Following this, adapt /etc/hostname and /etc/hosts according to the lost Proxmox VE host, then reboot and check (and don’t forget your VM/CT data).\n\n6.5.1. Remove Cluster Configuration\n\nThe recommended way is to reinstall the node after you remove it from your cluster. This ensures that all secret cluster/ssh keys and any shared configuration data is destroyed.\n\nIn some cases, you might prefer to put a node back to local mode without reinstalling, which is described in Separate A Node Without Reinstalling\n\n6.5.2. Recovering/Moving Guests from Failed Nodes\n\nFor the guest configuration files in nodes/<NAME>/qemu-server/ (VMs) and nodes/<NAME>/lxc/ (containers), Proxmox VE sees the containing node <NAME> as the owner of the respective guest. This concept enables the usage of local locks instead of expensive cluster-wide locks for preventing concurrent guest configuration changes.\n\nAs a consequence, if the owning node of a guest fails (for example, due to a power outage, fencing event, etc.), a regular migration is not possible (even if all the disks are located on shared storage), because such a local lock on the (offline) owning node is unobtainable. This is not a problem for HA-managed guests, as Proxmox VE’s High Availability stack includes the necessary (cluster-wide) locking and watchdog functionality to ensure correct and automatic recovery of guests from fenced nodes.\n\nIf a non-HA-managed guest has only shared disks (and no other local resources which are only available on the failed node), a manual recovery is possible by simply moving the guest configuration file from the failed node’s directory in /etc/pve/ to an online node’s directory (which changes the logical owner or location of the guest).\n\nFor example, recovering the VM with ID 100 from an offline node1 to another node node2 works by running the following command as root on any member node of the cluster:\n\nmv /etc/pve/nodes/node1/qemu-server/100.conf /etc/pve/nodes/node2/qemu-server/\n\tBefore manually recovering a guest like this, make absolutely sure that the failed source node is really powered off/fenced. Otherwise Proxmox VE’s locking principles are violated by the mv command, which can have unexpected consequences.\n\tGuests with local disks (or other local resources which are only available on the offline node) are not recoverable like this. Either wait for the failed node to rejoin the cluster or restore such guests from backups.\n7. Proxmox VE Storage\n\nThe Proxmox VE storage model is very flexible. Virtual machine images can either be stored on one or several local storages, or on shared storage like NFS or iSCSI (NAS, SAN). There are no limits, and you may configure as many storage pools as you like. You can use all storage technologies available for Debian Linux.\n\nOne major benefit of storing VMs on shared storage is the ability to live-migrate running machines without any downtime, as all nodes in the cluster have direct access to VM disk images. There is no need to copy VM image data, so live migration is very fast in that case.\n\nThe storage library (package libpve-storage-perl) uses a flexible plugin system to provide a common interface to all storage types. This can be easily adopted to include further storage types in the future.\n\n7.1. Storage Types\n\nThere are basically two different classes of storage types:\n\nFile level storage\n\nFile level based storage technologies allow access to a fully featured (POSIX) file system. They are in general more flexible than any Block level storage (see below), and allow you to store content of any type. ZFS is probably the most advanced system, and it has full support for snapshots and clones.\n\nBlock level storage\n\nAllows to store large raw images. It is usually not possible to store other files (ISO, backups, ..) on such storage types. Most modern block level storage implementations support snapshots and clones. RADOS and GlusterFS are distributed systems, replicating storage data to different nodes.\n\nTable 2. Available storage types\nDescription\tPlugin type\tLevel\tShared\tSnapshots\tStable\n\n\nZFS (local)\n\n\t\n\nzfspool\n\n\t\n\nboth1\n\n\t\n\nno\n\n\t\n\nyes\n\n\t\n\nyes\n\n\n\n\nDirectory\n\n\t\n\ndir\n\n\t\n\nfile\n\n\t\n\nno\n\n\t\n\nno2\n\n\t\n\nyes\n\n\n\n\nBTRFS\n\n\t\n\nbtrfs\n\n\t\n\nfile\n\n\t\n\nno\n\n\t\n\nyes\n\n\t\n\ntechnology preview\n\n\n\n\nNFS\n\n\t\n\nnfs\n\n\t\n\nfile\n\n\t\n\nyes\n\n\t\n\nno2\n\n\t\n\nyes\n\n\n\n\nCIFS\n\n\t\n\ncifs\n\n\t\n\nfile\n\n\t\n\nyes\n\n\t\n\nno2\n\n\t\n\nyes\n\n\n\n\nProxmox Backup\n\n\t\n\npbs\n\n\t\n\nboth\n\n\t\n\nyes\n\n\t\n\nn/a\n\n\t\n\nyes\n\n\n\n\nGlusterFS\n\n\t\n\nglusterfs\n\n\t\n\nfile\n\n\t\n\nyes\n\n\t\n\nno2\n\n\t\n\nyes\n\n\n\n\nCephFS\n\n\t\n\ncephfs\n\n\t\n\nfile\n\n\t\n\nyes\n\n\t\n\nyes\n\n\t\n\nyes\n\n\n\n\nLVM\n\n\t\n\nlvm\n\n\t\n\nblock\n\n\t\n\nno3\n\n\t\n\nno\n\n\t\n\nyes\n\n\n\n\nLVM-thin\n\n\t\n\nlvmthin\n\n\t\n\nblock\n\n\t\n\nno\n\n\t\n\nyes\n\n\t\n\nyes\n\n\n\n\niSCSI/kernel\n\n\t\n\niscsi\n\n\t\n\nblock\n\n\t\n\nyes\n\n\t\n\nno\n\n\t\n\nyes\n\n\n\n\niSCSI/libiscsi\n\n\t\n\niscsidirect\n\n\t\n\nblock\n\n\t\n\nyes\n\n\t\n\nno\n\n\t\n\nyes\n\n\n\n\nCeph/RBD\n\n\t\n\nrbd\n\n\t\n\nblock\n\n\t\n\nyes\n\n\t\n\nyes\n\n\t\n\nyes\n\n\n\n\nZFS over iSCSI\n\n\t\n\nzfs\n\n\t\n\nblock\n\n\t\n\nyes\n\n\t\n\nyes\n\n\t\n\nyes\n\n1: Disk images for VMs are stored in ZFS volume (zvol) datasets, which provide block device functionality.\n\n2: On file based storages, snapshots are possible with the qcow2 format.\n\n3: It is possible to use LVM on top of an iSCSI or FC-based storage. That way you get a shared LVM storage\n\n7.1.1. Thin Provisioning\n\nA number of storages, and the QEMU image format qcow2, support thin provisioning. With thin provisioning activated, only the blocks that the guest system actually use will be written to the storage.\n\nSay for instance you create a VM with a 32GB hard disk, and after installing the guest system OS, the root file system of the VM contains 3 GB of data. In that case only 3GB are written to the storage, even if the guest VM sees a 32GB hard drive. In this way thin provisioning allows you to create disk images which are larger than the currently available storage blocks. You can create large disk images for your VMs, and when the need arises, add more disks to your storage without resizing the VMs' file systems.\n\nAll storage types which have the “Snapshots” feature also support thin provisioning.\n\n\tIf a storage runs full, all guests using volumes on that storage receive IO errors. This can cause file system inconsistencies and may corrupt your data. So it is advisable to avoid over-provisioning of your storage resources, or carefully observe free space to avoid such conditions.\n7.2. Storage Configuration\n\nAll Proxmox VE related storage configuration is stored within a single text file at /etc/pve/storage.cfg. As this file is within /etc/pve/, it gets automatically distributed to all cluster nodes. So all nodes share the same storage configuration.\n\nSharing storage configuration makes perfect sense for shared storage, because the same “shared” storage is accessible from all nodes. But it is also useful for local storage types. In this case such local storage is available on all nodes, but it is physically different and can have totally different content.\n\n7.2.1. Storage Pools\n\nEach storage pool has a <type>, and is uniquely identified by its <STORAGE_ID>. A pool configuration looks like this:\n\n<type>: <STORAGE_ID>\n        <property> <value>\n        <property> <value>\n        <property>\n        ...\n\nThe <type>: <STORAGE_ID> line starts the pool definition, which is then followed by a list of properties. Most properties require a value. Some have reasonable defaults, in which case you can omit the value.\n\nTo be more specific, take a look at the default storage configuration after installation. It contains one special local storage pool named local, which refers to the directory /var/lib/vz and is always available. The Proxmox VE installer creates additional storage entries depending on the storage type chosen at installation time.\n\nDefault storage configuration (/etc/pve/storage.cfg)\ndir: local\n        path /var/lib/vz\n        content iso,vztmpl,backup\n\n# default image store on LVM based installation\nlvmthin: local-lvm\n        thinpool data\n        vgname pve\n        content rootdir,images\n\n# default image store on ZFS based installation\nzfspool: local-zfs\n        pool rpool/data\n        sparse\n        content images,rootdir\n\tIt is problematic to have multiple storage configurations pointing to the exact same underlying storage. Such an aliased storage configuration can lead to two different volume IDs (volid) pointing to the exact same disk image. Proxmox VE expects that the images' volume IDs point to, are unique. Choosing different content types for aliased storage configurations can be fine, but is not recommended.\n7.2.2. Common Storage Properties\n\nA few storage properties are common among different storage types.\n\nnodes\n\nList of cluster node names where this storage is usable/accessible. One can use this property to restrict storage access to a limited set of nodes.\n\ncontent\n\nA storage can support several content types, for example virtual disk images, cdrom iso images, container templates or container root directories. Not all storage types support all content types. One can set this property to select what this storage is used for.\n\nimages\n\nQEMU/KVM VM images.\n\nrootdir\n\nAllow to store container data.\n\nvztmpl\n\nContainer templates.\n\nbackup\n\nBackup files (vzdump).\n\niso\n\nISO images\n\nsnippets\n\nSnippet files, for example guest hook scripts\n\nshared\n\nIndicate that this is a single storage with the same contents on all nodes (or all listed in the nodes option). It will not make the contents of a local storage automatically accessible to other nodes, it just marks an already shared storage as such!\n\ndisable\n\nYou can use this flag to disable the storage completely.\n\nmaxfiles\n\nDeprecated, please use prune-backups instead. Maximum number of backup files per VM. Use 0 for unlimited.\n\nprune-backups\n\nRetention options for backups. For details, see Backup Retention.\n\nformat\n\nDefault image format (raw|qcow2|vmdk)\n\npreallocation\n\nPreallocation mode (off|metadata|falloc|full) for raw and qcow2 images on file-based storages. The default is metadata, which is treated like off for raw images. When using network storages in combination with large qcow2 images, using off can help to avoid timeouts.\n\n\tIt is not advisable to use the same storage pool on different Proxmox VE clusters. Some storage operation need exclusive access to the storage, so proper locking is required. While this is implemented within a cluster, it does not work between different clusters.\n7.3. Volumes\n\nWe use a special notation to address storage data. When you allocate data from a storage pool, it returns such a volume identifier. A volume is identified by the <STORAGE_ID>, followed by a storage type dependent volume name, separated by colon. A valid <VOLUME_ID> looks like:\n\nlocal:230/example-image.raw\nlocal:iso/debian-501-amd64-netinst.iso\nlocal:vztmpl/debian-5.0-joomla_1.5.9-1_i386.tar.gz\niscsi-storage:0.0.2.scsi-14f504e46494c4500494b5042546d2d646744372d31616d61\n\nTo get the file system path for a <VOLUME_ID> use:\n\npvesm path <VOLUME_ID>\n7.3.1. Volume Ownership\n\nThere exists an ownership relation for image type volumes. Each such volume is owned by a VM or Container. For example volume local:230/example-image.raw is owned by VM 230. Most storage backends encodes this ownership information into the volume name.\n\nWhen you remove a VM or Container, the system also removes all associated volumes which are owned by that VM or Container.\n\n7.4. Using the Command-line Interface\n\nIt is recommended to familiarize yourself with the concept behind storage pools and volume identifiers, but in real life, you are not forced to do any of those low level operations on the command line. Normally, allocation and removal of volumes is done by the VM and Container management tools.\n\nNevertheless, there is a command-line tool called pvesm (“Proxmox VE Storage Manager”), which is able to perform common storage management tasks.\n\n7.4.1. Examples\n\nAdd storage pools\n\npvesm add <TYPE> <STORAGE_ID> <OPTIONS>\npvesm add dir <STORAGE_ID> --path <PATH>\npvesm add nfs <STORAGE_ID> --path <PATH> --server <SERVER> --export <EXPORT>\npvesm add lvm <STORAGE_ID> --vgname <VGNAME>\npvesm add iscsi <STORAGE_ID> --portal <HOST[:PORT]> --target <TARGET>\n\nDisable storage pools\n\npvesm set <STORAGE_ID> --disable 1\n\nEnable storage pools\n\npvesm set <STORAGE_ID> --disable 0\n\nChange/set storage options\n\npvesm set <STORAGE_ID> <OPTIONS>\npvesm set <STORAGE_ID> --shared 1\npvesm set local --format qcow2\npvesm set <STORAGE_ID> --content iso\n\nRemove storage pools. This does not delete any data, and does not disconnect or unmount anything. It just removes the storage configuration.\n\npvesm remove <STORAGE_ID>\n\nAllocate volumes\n\npvesm alloc <STORAGE_ID> <VMID> <name> <size> [--format <raw|qcow2>]\n\nAllocate a 4G volume in local storage. The name is auto-generated if you pass an empty string as <name>\n\npvesm alloc local <VMID> '' 4G\n\nFree volumes\n\npvesm free <VOLUME_ID>\n\tThis really destroys all volume data.\n\nList storage status\n\npvesm status\n\nList storage contents\n\npvesm list <STORAGE_ID> [--vmid <VMID>]\n\nList volumes allocated by VMID\n\npvesm list <STORAGE_ID> --vmid <VMID>\n\nList iso images\n\npvesm list <STORAGE_ID> --content iso\n\nList container templates\n\npvesm list <STORAGE_ID> --content vztmpl\n\nShow file system path for a volume\n\npvesm path <VOLUME_ID>\n\nExporting the volume local:103/vm-103-disk-0.qcow2 to the file target. This is mostly used internally with pvesm import. The stream format qcow2+size is different to the qcow2 format. Consequently, the exported file cannot simply be attached to a VM. This also holds for the other formats.\n\npvesm export local:103/vm-103-disk-0.qcow2 qcow2+size target --with-snapshots 1\n7.5. Directory Backend\n\nStorage pool type: dir\n\nProxmox VE can use local directories or locally mounted shares for storage. A directory is a file level storage, so you can store any content type like virtual disk images, containers, templates, ISO images or backup files.\n\n\tYou can mount additional storages via standard linux /etc/fstab, and then define a directory storage for that mount point. This way you can use any file system supported by Linux.\n\nThis backend assumes that the underlying directory is POSIX compatible, but nothing else. This implies that you cannot create snapshots at the storage level. But there exists a workaround for VM images using the qcow2 file format, because that format supports snapshots internally.\n\n\tSome storage types do not support O_DIRECT, so you can’t use cache mode none with such storages. Simply use cache mode writeback instead.\n\nWe use a predefined directory layout to store different content types into different sub-directories. This layout is used by all file level storage backends.\n\nTable 3. Directory layout\nContent type\tSubdir\n\n\nVM images\n\n\t\n\nimages/<VMID>/\n\n\n\n\nISO images\n\n\t\n\ntemplate/iso/\n\n\n\n\nContainer templates\n\n\t\n\ntemplate/cache/\n\n\n\n\nBackup files\n\n\t\n\ndump/\n\n\n\n\nSnippets\n\n\t\n\nsnippets/\n\n7.5.1. Configuration\n\nThis backend supports all common storage properties, and adds two additional properties. The path property is used to specify the directory. This needs to be an absolute file system path.\n\nThe optional content-dirs property allows for the default layout to be changed. It consists of a comma-separated list of identifiers in the following format:\n\nvtype=path\n\nWhere vtype is one of the allowed content types for the storage, and path is a path relative to the mountpoint of the storage.\n\nConfiguration Example (/etc/pve/storage.cfg)\ndir: backup\n        path /mnt/backup\n        content backup\n        prune-backups keep-last=7\n        max-protected-backups 3\n        content-dirs backup=custom/backup/dir\n\nThe above configuration defines a storage pool called backup. That pool can be used to store up to 7 regular backups (keep-last=7) and 3 protected backups per VM. The real path for the backup files is /mnt/backup/custom/backup/dir/....\n\n7.5.2. File naming conventions\n\nThis backend uses a well defined naming scheme for VM images:\n\nvm-<VMID>-<NAME>.<FORMAT>\n<VMID>\n\nThis specifies the owner VM.\n\n<NAME>\n\nThis can be an arbitrary name (ascii) without white space. The backend uses disk-[N] as default, where [N] is replaced by an integer to make the name unique.\n\n<FORMAT>\n\nSpecifies the image format (raw|qcow2|vmdk).\n\nWhen you create a VM template, all VM images are renamed to indicate that they are now read-only, and can be used as a base image for clones:\n\nbase-<VMID>-<NAME>.<FORMAT>\n\tSuch base images are used to generate cloned images. So it is important that those files are read-only, and never get modified. The backend changes the access mode to 0444, and sets the immutable flag (chattr +i) if the storage supports that.\n7.5.3. Storage Features\n\nAs mentioned above, most file systems do not support snapshots out of the box. To workaround that problem, this backend is able to use qcow2 internal snapshot capabilities.\n\nSame applies to clones. The backend uses the qcow2 base image feature to create clones.\n\nTable 4. Storage features for backend dir\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir vztmpl iso backup snippets\n\n\t\n\nraw qcow2 vmdk subvol\n\n\t\n\nno\n\n\t\n\nqcow2\n\n\t\n\nqcow2\n\n7.5.4. Examples\n\nPlease use the following command to allocate a 4GB image on storage local:\n\n# pvesm alloc local 100 vm-100-disk10.raw 4G\nFormatting '/var/lib/vz/images/100/vm-100-disk10.raw', fmt=raw size=4294967296\nsuccessfully created 'local:100/vm-100-disk10.raw'\n\tThe image name must conform to above naming conventions.\n\nThe real file system path is shown with:\n\n# pvesm path local:100/vm-100-disk10.raw\n/var/lib/vz/images/100/vm-100-disk10.raw\n\nAnd you can remove the image with:\n\n# pvesm free local:100/vm-100-disk10.raw\n7.6. NFS Backend\n\nStorage pool type: nfs\n\nThe NFS backend is based on the directory backend, so it shares most properties. The directory layout and the file naming conventions are the same. The main advantage is that you can directly configure the NFS server properties, so the backend can mount the share automatically. There is no need to modify /etc/fstab. The backend can also test if the server is online, and provides a method to query the server for exported shares.\n\n7.6.1. Configuration\n\nThe backend supports all common storage properties, except the shared flag, which is always set. Additionally, the following properties are used to configure the NFS server:\n\nserver\n\nServer IP or DNS name. To avoid DNS lookup delays, it is usually preferable to use an IP address instead of a DNS name - unless you have a very reliable DNS server, or list the server in the local /etc/hosts file.\n\nexport\n\nNFS export path (as listed by pvesm nfsscan).\n\nYou can also set NFS mount options:\n\npath\n\nThe local mount point (defaults to /mnt/pve/<STORAGE_ID>/).\n\ncontent-dirs\n\nOverrides for the default directory layout. Optional.\n\noptions\n\nNFS mount options (see man nfs).\n\nConfiguration Example (/etc/pve/storage.cfg)\nnfs: iso-templates\n        path /mnt/pve/iso-templates\n        server 10.0.0.10\n        export /space/iso-templates\n        options vers=3,soft\n        content iso,vztmpl\n\tAfter an NFS request times out, NFS request are retried indefinitely by default. This can lead to unexpected hangs on the client side. For read-only content, it is worth to consider the NFS soft option, which limits the number of retries to three.\n7.6.2. Storage Features\n\nNFS does not support snapshots, but the backend uses qcow2 features to implement snapshots and cloning.\n\nTable 5. Storage features for backend nfs\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir vztmpl iso backup snippets\n\n\t\n\nraw qcow2 vmdk\n\n\t\n\nyes\n\n\t\n\nqcow2\n\n\t\n\nqcow2\n\n7.6.3. Examples\n\nYou can get a list of exported NFS shares with:\n\n# pvesm nfsscan <server>\n7.7. CIFS Backend\n\nStorage pool type: cifs\n\nThe CIFS backend extends the directory backend, so that no manual setup of a CIFS mount is needed. Such a storage can be added directly through the Proxmox VE API or the web UI, with all our backend advantages, like server heartbeat check or comfortable selection of exported shares.\n\n7.7.1. Configuration\n\nThe backend supports all common storage properties, except the shared flag, which is always set. Additionally, the following CIFS special properties are available:\n\nserver\n\nServer IP or DNS name. Required.\n\n\tTo avoid DNS lookup delays, it is usually preferable to use an IP address instead of a DNS name - unless you have a very reliable DNS server, or list the server in the local /etc/hosts file.\nshare\n\nCIFS share to use (get available ones with pvesm scan cifs <address> or the web UI). Required.\n\nusername\n\nThe username for the CIFS storage. Optional, defaults to ‘guest’.\n\npassword\n\nThe user password. Optional. It will be saved in a file only readable by root (/etc/pve/priv/storage/<STORAGE-ID>.pw).\n\ndomain\n\nSets the user domain (workgroup) for this storage. Optional.\n\nsmbversion\n\nSMB protocol Version. Optional, default is 3. SMB1 is not supported due to security issues.\n\npath\n\nThe local mount point. Optional, defaults to /mnt/pve/<STORAGE_ID>/.\n\ncontent-dirs\n\nOverrides for the default directory layout. Optional.\n\noptions\n\nAdditional CIFS mount options (see man mount.cifs). Some options are set automatically and shouldn’t be set here. Proxmox VE will always set the option soft. Depending on the configuration, these options are set automatically: username, credentials, guest, domain, vers.\n\nsubdir\n\nThe subdirectory of the share to mount. Optional, defaults to the root directory of the share.\n\nConfiguration Example (/etc/pve/storage.cfg)\ncifs: backup\n        path /mnt/pve/backup\n        server 10.0.0.11\n        share VMData\n        content backup\n        options noserverino,echo_interval=30\n        username anna\n        smbversion 3\n        subdir /data\n7.7.2. Storage Features\n\nCIFS does not support snapshots on a storage level. But you may use qcow2 backing files if you still want to have snapshots and cloning features available.\n\nTable 6. Storage features for backend cifs\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir vztmpl iso backup snippets\n\n\t\n\nraw qcow2 vmdk\n\n\t\n\nyes\n\n\t\n\nqcow2\n\n\t\n\nqcow2\n\n7.7.3. Examples\n\nYou can get a list of exported CIFS shares with:\n\n# pvesm scan cifs <server> [--username <username>] [--password]\n\nThen you could add this share as a storage to the whole Proxmox VE cluster with:\n\n# pvesm add cifs <storagename> --server <server> --share <share> [--username <username>] [--password]\n7.8. Proxmox Backup Server\n\nStorage pool type: pbs\n\nThis backend allows direct integration of a Proxmox Backup Server into Proxmox VE like any other storage. A Proxmox Backup storage can be added directly through the Proxmox VE API, CLI or the web interface.\n\n7.8.1. Configuration\n\nThe backend supports all common storage properties, except the shared flag, which is always set. Additionally, the following special properties to Proxmox Backup Server are available:\n\nserver\n\nServer IP or DNS name. Required.\n\nport\n\nUse this port instead of the default one, i.e. 8007. Optional.\n\nusername\n\nThe username for the Proxmox Backup Server storage. Required.\n\n\tDo not forget to add the realm to the username. For example, root@pam or archiver@pbs.\npassword\n\nThe user password. The value will be saved in a file under /etc/pve/priv/storage/<STORAGE-ID>.pw with access restricted to the root user. Required.\n\ndatastore\n\nThe ID of the Proxmox Backup Server datastore to use. Required.\n\nfingerprint\n\nThe fingerprint of the Proxmox Backup Server API TLS certificate. You can get it in the Servers Dashboard or using the proxmox-backup-manager cert info command. Required for self-signed certificates or any other one where the host does not trusts the servers CA.\n\nencryption-key\n\nA key to encrypt the backup data from the client side. Currently only non-password protected (no key derive function (kdf)) are supported. Will be saved in a file under /etc/pve/priv/storage/<STORAGE-ID>.enc with access restricted to the root user. Use the magic value autogen to automatically generate a new one using proxmox-backup-client key create --kdf none <path>. Optional.\n\nmaster-pubkey\n\nA public RSA key used to encrypt the backup encryption key as part of the backup task. The encrypted copy will be appended to the backup and stored on the Proxmox Backup Server instance for recovery purposes. Optional, requires encryption-key.\n\nConfiguration Example (/etc/pve/storage.cfg)\npbs: backup\n        datastore main\n        server enya.proxmox.com\n        content backup\n        fingerprint 09:54:ef:..snip..:88:af:47:fe:4c:3b:cf:8b:26:88:0b:4e:3c:b2\n        prune-backups keep-all=1\n        username archiver@pbs\n7.8.2. Storage Features\n\nProxmox Backup Server only supports backups, they can be block-level or file-level based. Proxmox VE uses block-level for virtual machines and file-level for container.\n\nTable 7. Storage features for backend pbs\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nbackup\n\n\t\n\nn/a\n\n\t\n\nyes\n\n\t\n\nn/a\n\n\t\n\nn/a\n\n7.8.3. Encryption\n\nOptionally, you can configure client-side encryption with AES-256 in GCM mode. Encryption can be configured either via the web interface, or on the CLI with the encryption-key option (see above). The key will be saved in the file /etc/pve/priv/storage/<STORAGE-ID>.enc, which is only accessible by the root user.\n\n\tWithout their key, backups will be inaccessible. Thus, you should keep keys ordered and in a place that is separate from the contents being backed up. It can happen, for example, that you back up an entire system, using a key on that system. If the system then becomes inaccessible for any reason and needs to be restored, this will not be possible as the encryption key will be lost along with the broken system.\n\nIt is recommended that you keep your key safe, but easily accessible, in order for quick disaster recovery. For this reason, the best place to store it is in your password manager, where it is immediately recoverable. As a backup to this, you should also save the key to a USB flash drive and store that in a secure place. This way, it is detached from any system, but is still easy to recover from, in case of emergency. Finally, in preparation for the worst case scenario, you should also consider keeping a paper copy of your key locked away in a safe place. The paperkey subcommand can be used to create a QR encoded version of your key. The following command sends the output of the paperkey command to a text file, for easy printing.\n\n# proxmox-backup-client key paperkey /etc/pve/priv/storage/<STORAGE-ID>.enc --output-format text > qrkey.txt\n\nAdditionally, it is possible to use a single RSA master key pair for key recovery purposes: configure all clients doing encrypted backups to use a single public master key, and all subsequent encrypted backups will contain a RSA-encrypted copy of the used AES encryption key. The corresponding private master key allows recovering the AES key and decrypting the backup even if the client system is no longer available.\n\n\tThe same safe-keeping rules apply to the master key pair as to the regular encryption keys. Without a copy of the private key recovery is not possible! The paperkey command supports generating paper copies of private master keys for storage in a safe, physical location.\n\nBecause the encryption is managed on the client side, you can use the same datastore on the server for unencrypted backups and encrypted backups, even if they are encrypted with different keys. However, deduplication between backups with different keys is not possible, so it is often better to create separate datastores.\n\n\tDo not use encryption if there is no benefit from it, for example, when you are running the server locally in a trusted network. It is always easier to recover from unencrypted backups.\n7.8.4. Example: Add Storage over CLI\n\nThen you could add this share as a storage to the whole Proxmox VE cluster with:\n\n# pvesm add pbs <id> --server <server> --datastore <datastore> --username <username> --fingerprint 00:B4:... --password\n7.9. GlusterFS Backend\n\nStorage pool type: glusterfs\n\nGlusterFS is a scalable network file system. The system uses a modular design, runs on commodity hardware, and can provide a highly available enterprise storage at low costs. Such system is capable of scaling to several petabytes, and can handle thousands of clients.\n\n\tAfter a node/brick crash, GlusterFS does a full rsync to make sure data is consistent. This can take a very long time with large files, so this backend is not suitable to store large VM images.\n7.9.1. Configuration\n\nThe backend supports all common storage properties, and adds the following GlusterFS specific options:\n\nserver\n\nGlusterFS volfile server IP or DNS name.\n\nserver2\n\nBackup volfile server IP or DNS name.\n\nvolume\n\nGlusterFS Volume.\n\ntransport\n\nGlusterFS transport: tcp, unix or rdma\n\nConfiguration Example (/etc/pve/storage.cfg)\nglusterfs: Gluster\n        server 10.2.3.4\n        server2 10.2.3.5\n        volume glustervol\n        content images,iso\n7.9.2. File naming conventions\n\nThe directory layout and the file naming conventions are inherited from the dir backend.\n\n7.9.3. Storage Features\n\nThe storage provides a file level interface, but no native snapshot/clone implementation.\n\nTable 8. Storage features for backend glusterfs\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages vztmpl iso backup snippets\n\n\t\n\nraw qcow2 vmdk\n\n\t\n\nyes\n\n\t\n\nqcow2\n\n\t\n\nqcow2\n\n7.10. Local ZFS Pool Backend\n\nStorage pool type: zfspool\n\nThis backend allows you to access local ZFS pools (or ZFS file systems inside such pools).\n\n7.10.1. Configuration\n\nThe backend supports the common storage properties content, nodes, disable, and the following ZFS specific properties:\n\npool\n\nSelect the ZFS pool/filesystem. All allocations are done within that pool.\n\nblocksize\n\nSet ZFS blocksize parameter.\n\nsparse\n\nUse ZFS thin-provisioning. A sparse volume is a volume whose reservation is not equal to the volume size.\n\nmountpoint\n\nThe mount point of the ZFS pool/filesystem. Changing this does not affect the mountpoint property of the dataset seen by zfs. Defaults to /<pool>.\n\nConfiguration Example (/etc/pve/storage.cfg)\nzfspool: vmdata\n        pool tank/vmdata\n        content rootdir,images\n        sparse\n7.10.2. File naming conventions\n\nThe backend uses the following naming scheme for VM images:\n\nvm-<VMID>-<NAME>      // normal VM images\nbase-<VMID>-<NAME>    // template VM image (read-only)\nsubvol-<VMID>-<NAME>  // subvolumes (ZFS filesystem for containers)\n<VMID>\n\nThis specifies the owner VM.\n\n<NAME>\n\nThis can be an arbitrary name (ascii) without white space. The backend uses disk[N] as default, where [N] is replaced by an integer to make the name unique.\n\n7.10.3. Storage Features\n\nZFS is probably the most advanced storage type regarding snapshot and cloning. The backend uses ZFS datasets for both VM images (format raw) and container data (format subvol). ZFS properties are inherited from the parent dataset, so you can simply set defaults on the parent dataset.\n\nTable 9. Storage features for backend zfs\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir\n\n\t\n\nraw subvol\n\n\t\n\nno\n\n\t\n\nyes\n\n\t\n\nyes\n\n7.10.4. Examples\n\nIt is recommended to create an extra ZFS file system to store your VM images:\n\n# zfs create tank/vmdata\n\nTo enable compression on that newly allocated file system:\n\n# zfs set compression=on tank/vmdata\n\nYou can get a list of available ZFS filesystems with:\n\n# pvesm zfsscan\n7.11. LVM Backend\n\nStorage pool type: lvm\n\nLVM is a light software layer on top of hard disks and partitions. It can be used to split available disk space into smaller logical volumes. LVM is widely used on Linux and makes managing hard drives easier.\n\nAnother use case is to put LVM on top of a big iSCSI LUN. That way you can easily manage space on that iSCSI LUN, which would not be possible otherwise, because the iSCSI specification does not define a management interface for space allocation.\n\n7.11.1. Configuration\n\nThe LVM backend supports the common storage properties content, nodes, disable, and the following LVM specific properties:\n\nvgname\n\nLVM volume group name. This must point to an existing volume group.\n\nbase\n\nBase volume. This volume is automatically activated before accessing the storage. This is mostly useful when the LVM volume group resides on a remote iSCSI server.\n\nsaferemove\n\nCalled \"Wipe Removed Volumes\" in the web UI. Zero-out data when removing LVs. When removing a volume, this makes sure that all data gets erased and cannot be accessed by other LVs created later (which happen to be assigned the same physical extents). This is a costly operation, but may be required as a security measure in certain environments.\n\nsaferemove_throughput\n\nWipe throughput (cstream -t parameter value).\n\nConfiguration Example (/etc/pve/storage.cfg)\nlvm: myspace\n        vgname myspace\n        content rootdir,images\n7.11.2. File naming conventions\n\nThe backend use basically the same naming conventions as the ZFS pool backend.\n\nvm-<VMID>-<NAME>      // normal VM images\n7.11.3. Storage Features\n\nLVM is a typical block storage, but this backend does not support snapshots and clones. Unfortunately, normal LVM snapshots are quite inefficient, because they interfere with all writes on the entire volume group during snapshot time.\n\nOne big advantage is that you can use it on top of a shared storage, for example, an iSCSI LUN. The backend itself implements proper cluster-wide locking.\n\n\tThe newer LVM-thin backend allows snapshots and clones, but does not support shared storage.\nTable 10. Storage features for backend lvm\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir\n\n\t\n\nraw\n\n\t\n\npossible\n\n\t\n\nno\n\n\t\n\nno\n\n7.11.4. Examples\n\nList available volume groups:\n\n# pvesm lvmscan\n7.12. LVM thin Backend\n\nStorage pool type: lvmthin\n\nLVM normally allocates blocks when you create a volume. LVM thin pools instead allocates blocks when they are written. This behaviour is called thin-provisioning, because volumes can be much larger than physically available space.\n\nYou can use the normal LVM command-line tools to manage and create LVM thin pools (see man lvmthin for details). Assuming you already have a LVM volume group called pve, the following commands create a new LVM thin pool (size 100G) called data:\n\nlvcreate -L 100G -n data pve\nlvconvert --type thin-pool pve/data\n7.12.1. Configuration\n\nThe LVM thin backend supports the common storage properties content, nodes, disable, and the following LVM specific properties:\n\nvgname\n\nLVM volume group name. This must point to an existing volume group.\n\nthinpool\n\nThe name of the LVM thin pool.\n\nConfiguration Example (/etc/pve/storage.cfg)\nlvmthin: local-lvm\n        thinpool data\n        vgname pve\n        content rootdir,images\n7.12.2. File naming conventions\n\nThe backend use basically the same naming conventions as the ZFS pool backend.\n\nvm-<VMID>-<NAME>      // normal VM images\n7.12.3. Storage Features\n\nLVM thin is a block storage, but fully supports snapshots and clones efficiently. New volumes are automatically initialized with zero.\n\nIt must be mentioned that LVM thin pools cannot be shared across multiple nodes, so you can only use them as local storage.\n\nTable 11. Storage features for backend lvmthin\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir\n\n\t\n\nraw\n\n\t\n\nno\n\n\t\n\nyes\n\n\t\n\nyes\n\n7.12.4. Examples\n\nList available LVM thin pools on volume group pve:\n\n# pvesm lvmthinscan pve\n7.13. Open-iSCSI initiator\n\nStorage pool type: iscsi\n\niSCSI is a widely employed technology used to connect to storage servers. Almost all storage vendors support iSCSI. There are also open source iSCSI target solutions available, e.g. OpenMediaVault, which is based on Debian.\n\nTo use this backend, you need to install the Open-iSCSI (open-iscsi) package. This is a standard Debian package, but it is not installed by default to save resources.\n\n# apt-get install open-iscsi\n\nLow-level iscsi management task can be done using the iscsiadm tool.\n\n7.13.1. Configuration\n\nThe backend supports the common storage properties content, nodes, disable, and the following iSCSI specific properties:\n\nportal\n\niSCSI portal (IP or DNS name with optional port).\n\ntarget\n\niSCSI target.\n\nConfiguration Example (/etc/pve/storage.cfg)\niscsi: mynas\n     portal 10.10.10.1\n     target iqn.2006-01.openfiler.com:tsn.dcb5aaaddd\n     content none\n\tIf you want to use LVM on top of iSCSI, it make sense to set content none. That way it is not possible to create VMs using iSCSI LUNs directly.\n7.13.2. File naming conventions\n\nThe iSCSI protocol does not define an interface to allocate or delete data. Instead, that needs to be done on the target side and is vendor specific. The target simply exports them as numbered LUNs. So Proxmox VE iSCSI volume names just encodes some information about the LUN as seen by the linux kernel.\n\n7.13.3. Storage Features\n\niSCSI is a block level type storage, and provides no management interface. So it is usually best to export one big LUN, and setup LVM on top of that LUN. You can then use the LVM plugin to manage the storage on that iSCSI LUN.\n\nTable 12. Storage features for backend iscsi\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages none\n\n\t\n\nraw\n\n\t\n\nyes\n\n\t\n\nno\n\n\t\n\nno\n\n7.13.4. Examples\n\nScan a remote iSCSI portal, and returns a list of possible targets:\n\npvesm scan iscsi <HOST[:PORT]>\n7.14. User Mode iSCSI Backend\n\nStorage pool type: iscsidirect\n\nThis backend provides basically the same functionality as the Open-iSCSI backed, but uses a user-level library to implement it. You need to install the libiscsi-bin package in order to use this backend.\n\nIt should be noted that there are no kernel drivers involved, so this can be viewed as performance optimization. But this comes with the drawback that you cannot use LVM on top of such iSCSI LUN. So you need to manage all space allocations at the storage server side.\n\n7.14.1. Configuration\n\nThe user mode iSCSI backend uses the same configuration options as the Open-iSCSI backed.\n\nConfiguration Example (/etc/pve/storage.cfg)\niscsidirect: faststore\n     portal 10.10.10.1\n     target iqn.2006-01.openfiler.com:tsn.dcb5aaaddd\n7.14.2. Storage Features\n\tThis backend works with VMs only. Containers cannot use this driver.\nTable 13. Storage features for backend iscsidirect\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages\n\n\t\n\nraw\n\n\t\n\nyes\n\n\t\n\nno\n\n\t\n\nno\n\n7.15. Ceph RADOS Block Devices (RBD)\n\nStorage pool type: rbd\n\nCeph is a distributed object store and file system designed to provide excellent performance, reliability and scalability. RADOS block devices implement a feature rich block level storage, and you get the following advantages:\n\nthin provisioning\n\nresizable volumes\n\ndistributed and redundant (striped over multiple OSDs)\n\nfull snapshot and clone capabilities\n\nself healing\n\nno single point of failure\n\nscalable to the exabyte level\n\nkernel and user space implementation available\n\n\tFor smaller deployments, it is also possible to run Ceph services directly on your Proxmox VE nodes. Recent hardware has plenty of CPU power and RAM, so running storage services and VMs on same node is possible.\n7.15.1. Configuration\n\nThis backend supports the common storage properties nodes, disable, content, and the following rbd specific properties:\n\nmonhost\n\nList of monitor daemon IPs. Optional, only needed if Ceph is not running on the Proxmox VE cluster.\n\npool\n\nCeph pool name.\n\nusername\n\nRBD user ID. Optional, only needed if Ceph is not running on the Proxmox VE cluster. Note that only the user ID should be used. The \"client.\" type prefix must be left out.\n\nkrbd\n\nEnforce access to rados block devices through the krbd kernel module. Optional.\n\n\tContainers will use krbd independent of the option value.\nConfiguration Example for a external Ceph cluster (/etc/pve/storage.cfg)\nrbd: ceph-external\n        monhost 10.1.1.20 10.1.1.21 10.1.1.22\n        pool ceph-external\n        content images\n        username admin\n\tYou can use the rbd utility to do low-level management tasks.\n7.15.2. Authentication\n\tIf Ceph is installed locally on the Proxmox VE cluster, the following is done automatically when adding the storage.\n\nIf you use cephx authentication, which is enabled by default, you need to provide the keyring from the external Ceph cluster.\n\nTo configure the storage via the CLI, you first need to make the file containing the keyring available. One way is to copy the file from the external Ceph cluster directly to one of the Proxmox VE nodes. The following example will copy it to the /root directory of the node on which we run it:\n\n# scp <external cephserver>:/etc/ceph/ceph.client.admin.keyring /root/rbd.keyring\n\nThen use the pvesm CLI tool to configure the external RBD storage, use the --keyring parameter, which needs to be a path to the keyring file that you copied. For example:\n\n# pvesm add rbd <name> --monhost \"10.1.1.20 10.1.1.21 10.1.1.22\" --content images --keyring /root/rbd.keyring\n\nWhen configuring an external RBD storage via the GUI, you can copy and paste the keyring into the appropriate field.\n\nThe keyring will be stored at\n\n# /etc/pve/priv/ceph/<STORAGE_ID>.keyring\n\tCreating a keyring with only the needed capabilities is recommend when connecting to an external cluster. For further information on Ceph user management, see the Ceph docs.[12]\n7.15.3. Ceph client configuration (optional)\n\nConnecting to an external Ceph storage doesn’t always allow setting client-specific options in the config DB on the external cluster. You can add a ceph.conf beside the Ceph keyring to change the Ceph client configuration for the storage.\n\nThe ceph.conf needs to have the same name as the storage.\n\n# /etc/pve/priv/ceph/<STORAGE_ID>.conf\n\nSee the RBD configuration reference [13] for possible settings.\n\n\tDo not change these settings lightly. Proxmox VE is merging the <STORAGE_ID>.conf with the storage configuration.\n7.15.4. Storage Features\n\nThe rbd backend is a block level storage, and implements full snapshot and clone functionality.\n\nTable 14. Storage features for backend rbd\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages rootdir\n\n\t\n\nraw\n\n\t\n\nyes\n\n\t\n\nyes\n\n\t\n\nyes\n\n7.16. Ceph Filesystem (CephFS)\n\nStorage pool type: cephfs\n\nCephFS implements a POSIX-compliant filesystem, using a Ceph storage cluster to store its data. As CephFS builds upon Ceph, it shares most of its properties. This includes redundancy, scalability, self-healing, and high availability.\n\n\tProxmox VE can manage Ceph setups, which makes configuring a CephFS storage easier. As modern hardware offers a lot of processing power and RAM, running storage services and VMs on same node is possible without a significant performance impact.\n\nTo use the CephFS storage plugin, you must replace the stock Debian Ceph client, by adding our Ceph repository. Once added, run apt update, followed by apt dist-upgrade, in order to get the newest packages.\n\n\tPlease ensure that there are no other Ceph repositories configured. Otherwise the installation will fail or there will be mixed package versions on the node, leading to unexpected behavior.\n7.16.1. Configuration\n\nThis backend supports the common storage properties nodes, disable, content, as well as the following cephfs specific properties:\n\nfs-name\n\nName of the Ceph FS.\n\nmonhost\n\nList of monitor daemon addresses. Optional, only needed if Ceph is not running on the Proxmox VE cluster.\n\npath\n\nThe local mount point. Optional, defaults to /mnt/pve/<STORAGE_ID>/.\n\nusername\n\nCeph user id. Optional, only needed if Ceph is not running on the Proxmox VE cluster, where it defaults to admin.\n\nsubdir\n\nCephFS subdirectory to mount. Optional, defaults to /.\n\nfuse\n\nAccess CephFS through FUSE, instead of the kernel client. Optional, defaults to 0.\n\nConfiguration example for an external Ceph cluster (/etc/pve/storage.cfg)\ncephfs: cephfs-external\n        monhost 10.1.1.20 10.1.1.21 10.1.1.22\n        path /mnt/pve/cephfs-external\n        content backup\n        username admin\n        fs-name cephfs\n\tDon’t forget to set up the client’s secret key file, if cephx was not disabled.\n7.16.2. Authentication\n\tIf Ceph is installed locally on the Proxmox VE cluster, the following is done automatically when adding the storage.\n\nIf you use cephx authentication, which is enabled by default, you need to provide the secret from the external Ceph cluster.\n\nTo configure the storage via the CLI, you first need to make the file containing the secret available. One way is to copy the file from the external Ceph cluster directly to one of the Proxmox VE nodes. The following example will copy it to the /root directory of the node on which we run it:\n\n# scp <external cephserver>:/etc/ceph/cephfs.secret /root/cephfs.secret\n\nThen use the pvesm CLI tool to configure the external RBD storage, use the --keyring parameter, which needs to be a path to the secret file that you copied. For example:\n\n# pvesm add cephfs <name> --monhost \"10.1.1.20 10.1.1.21 10.1.1.22\" --content backup --keyring /root/cephfs.secret\n\nWhen configuring an external RBD storage via the GUI, you can copy and paste the secret into the appropriate field.\n\nThe secret is only the key itself, as opposed to the rbd backend which also contains a [client.userid] section.\n\nThe secret will be stored at\n\n# /etc/pve/priv/ceph/<STORAGE_ID>.secret\n\nA secret can be received from the Ceph cluster (as Ceph admin) by issuing the command below, where userid is the client ID that has been configured to access the cluster. For further information on Ceph user management, see the Ceph docs.[12]\n\n# ceph auth get-key client.userid > cephfs.secret\n7.16.3. Storage Features\n\nThe cephfs backend is a POSIX-compliant filesystem, on top of a Ceph cluster.\n\nTable 15. Storage features for backend cephfs\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nvztmpl iso backup snippets\n\n\t\n\nnone\n\n\t\n\nyes\n\n\t\n\nyes[1]\n\n\t\n\nno\n\n[1] While no known bugs exist, snapshots are not yet guaranteed to be stable, as they lack sufficient testing.\n\n7.17. BTRFS Backend\n\nStorage pool type: btrfs\n\nOn the surface, this storage type is very similar to the directory storage type, so see the directory backend section for a general overview.\n\nThe main difference is that with this storage type raw formatted disks will be placed in a subvolume, in order to allow taking snapshots and supporting offline storage migration with snapshots being preserved.\n\n\tBTRFS will honor the O_DIRECT flag when opening files, meaning VMs should not use cache mode none, otherwise there will be checksum errors.\n7.17.1. Configuration\n\nThis backend is configured similarly to the directory storage. Note that when adding a directory as a BTRFS storage, which is not itself also the mount point, it is highly recommended to specify the actual mount point via the is_mountpoint option.\n\nFor example, if a BTRFS file system is mounted at /mnt/data2 and its pve-storage/ subdirectory (which may be a snapshot, which is recommended) should be added as a storage pool called data2, you can use the following entry:\n\nbtrfs: data2\n        path /mnt/data2/pve-storage\n        content rootdir,images\n        is_mountpoint /mnt/data2\n7.17.2. Snapshots\n\nWhen taking a snapshot of a subvolume or raw file, the snapshot will be created as a read-only subvolume with the same path followed by an @ and the snapshot’s name.\n\n7.18. ZFS over ISCSI Backend\n\nStorage pool type: zfs\n\nThis backend accesses a remote machine having a ZFS pool as storage and an iSCSI target implementation via ssh. For each guest disk it creates a ZVOL and, exports it as iSCSI LUN. This LUN is used by Proxmox VE for the guest disk.\n\nThe following iSCSI target implementations are supported:\n\nLIO (Linux)\n\nIET (Linux)\n\nISTGT (FreeBSD)\n\nComstar (Solaris)\n\n\tThis plugin needs a ZFS capable remote storage appliance, you cannot use it to create a ZFS Pool on a regular Storage Appliance/SAN\n7.18.1. Configuration\n\nIn order to use the ZFS over iSCSI plugin you need to configure the remote machine (target) to accept ssh connections from the Proxmox VE node. Proxmox VE connects to the target for creating the ZVOLs and exporting them via iSCSI. Authentication is done through a ssh-key (without password protection) stored in /etc/pve/priv/zfs/<target_ip>_id_rsa\n\nThe following steps create a ssh-key and distribute it to the storage machine with IP 192.0.2.1:\n\nmkdir /etc/pve/priv/zfs\nssh-keygen -f /etc/pve/priv/zfs/192.0.2.1_id_rsa\nssh-copy-id -i /etc/pve/priv/zfs/192.0.2.1_id_rsa.pub root@192.0.2.1\nssh -i /etc/pve/priv/zfs/192.0.2.1_id_rsa root@192.0.2.1\n\nThe backend supports the common storage properties content, nodes, disable, and the following ZFS over ISCSI specific properties:\n\npool\n\nThe ZFS pool/filesystem on the iSCSI target. All allocations are done within that pool.\n\nportal\n\niSCSI portal (IP or DNS name with optional port).\n\ntarget\n\niSCSI target.\n\niscsiprovider\n\nThe iSCSI target implementation used on the remote machine\n\ncomstar_tg\n\ntarget group for comstar views.\n\ncomstar_hg\n\nhost group for comstar views.\n\nlio_tpg\n\ntarget portal group for Linux LIO targets\n\nnowritecache\n\ndisable write caching on the target\n\nblocksize\n\nSet ZFS blocksize parameter.\n\nsparse\n\nUse ZFS thin-provisioning. A sparse volume is a volume whose reservation is not equal to the volume size.\n\nConfiguration Examples (/etc/pve/storage.cfg)\nzfs: lio\n   blocksize 4k\n   iscsiprovider LIO\n   pool tank\n   portal 192.0.2.111\n   target iqn.2003-01.org.linux-iscsi.lio.x8664:sn.xxxxxxxxxxxx\n   content images\n   lio_tpg tpg1\n   sparse 1\n\nzfs: solaris\n   blocksize 4k\n   target iqn.2010-08.org.illumos:02:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx:tank1\n   pool tank\n   iscsiprovider comstar\n   portal 192.0.2.112\n   content images\n\nzfs: freebsd\n   blocksize 4k\n   target iqn.2007-09.jp.ne.peach.istgt:tank1\n   pool tank\n   iscsiprovider istgt\n   portal 192.0.2.113\n   content images\n\nzfs: iet\n   blocksize 4k\n   target iqn.2001-04.com.example:tank1\n   pool tank\n   iscsiprovider iet\n   portal 192.0.2.114\n   content images\n7.18.2. Storage Features\n\nThe ZFS over iSCSI plugin provides a shared storage, which is capable of snapshots. You need to make sure that the ZFS appliance does not become a single point of failure in your deployment.\n\nTable 16. Storage features for backend iscsi\nContent types\tImage formats\tShared\tSnapshots\tClones\n\n\nimages\n\n\t\n\nraw\n\n\t\n\nyes\n\n\t\n\nyes\n\n\t\n\nno\n\n8. Deploy Hyper-Converged Ceph Cluster\n8.1. Introduction\n\nProxmox VE unifies your compute and storage systems, that is, you can use the same physical nodes within a cluster for both computing (processing VMs and containers) and replicated storage. The traditional silos of compute and storage resources can be wrapped up into a single hyper-converged appliance. Separate storage networks (SANs) and connections via network attached storage (NAS) disappear. With the integration of Ceph, an open source software-defined storage platform, Proxmox VE has the ability to run and manage Ceph storage directly on the hypervisor nodes.\n\nCeph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\nSome advantages of Ceph on Proxmox VE are:\n\nEasy setup and management via CLI and GUI\n\nThin provisioning\n\nSnapshot support\n\nSelf healing\n\nScalable to the exabyte level\n\nProvides block, file system, and object storage\n\nSetup pools with different performance and redundancy characteristics\n\nData is replicated, making it fault tolerant\n\nRuns on commodity hardware\n\nNo need for hardware RAID controllers\n\nOpen source\n\nFor small to medium-sized deployments, it is possible to install a Ceph server for using RADOS Block Devices (RBD) or CephFS directly on your Proxmox VE cluster nodes (see Ceph RADOS Block Devices (RBD)). Recent hardware has a lot of CPU power and RAM, so running storage services and virtual guests on the same node is possible.\n\nTo simplify management, Proxmox VE provides you native integration to install and manage Ceph services on Proxmox VE nodes either via the built-in web interface, or using the pveceph command line tool.\n\n8.2. Terminology\nCeph consists of multiple Daemons, for use as an RBD storage:\n\nCeph Monitor (ceph-mon, or MON)\n\nCeph Manager (ceph-mgr, or MGS)\n\nCeph Metadata Service (ceph-mds, or MDS)\n\nCeph Object Storage Daemon (ceph-osd, or OSD)\n\n\tWe highly recommend to get familiar with Ceph [14], its architecture [15] and vocabulary [16].\n8.3. Recommendations for a Healthy Ceph Cluster\n\nTo build a hyper-converged Proxmox + Ceph Cluster, you must use at least three (preferably) identical servers for the setup.\n\nCheck also the recommendations from Ceph’s website.\n\n\tThe recommendations below should be seen as a rough guidance for choosing hardware. Therefore, it is still essential to adapt it to your specific needs. You should test your setup and monitor health and performance continuously.\nCPU\n\nCeph services can be classified into two categories: * Intensive CPU usage, benefiting from high CPU base frequencies and multiple cores. Members of that category are: Object Storage Daemon (OSD) services Meta Data Service (MDS) used for CephFS * Moderate CPU usage, not needing multiple CPU cores. These are: Monitor (MON) services Manager (MGR) services\n\nAs a simple rule of thumb, you should assign at least one CPU core (or thread) to each Ceph service to provide the minimum resources required for stable and durable Ceph performance.\n\nFor example, if you plan to run a Ceph monitor, a Ceph manager and 6 Ceph OSDs services on a node you should reserve 8 CPU cores purely for Ceph when targeting basic and stable performance.\n\nNote that OSDs CPU usage depend mostly from the disks performance. The higher the possible IOPS (IO Operations per Second) of a disk, the more CPU can be utilized by a OSD service. For modern enterprise SSD disks, like NVMe’s that can permanently sustain a high IOPS load over 100’000 with sub millisecond latency, each OSD can use multiple CPU threads, e.g., four to six CPU threads utilized per NVMe backed OSD is likely for very high performance disks.\n\nMemory\n\nEspecially in a hyper-converged setup, the memory consumption needs to be carefully planned out and monitored. In addition to the predicted memory usage of virtual machines and containers, you must also account for having enough memory available for Ceph to provide excellent and stable performance.\n\nAs a rule of thumb, for roughly 1 TiB of data, 1 GiB of memory will be used by an OSD. While the usage might be less under normal conditions, it will use most during critical operations like recovery, re-balancing or backfilling. That means that you should avoid maxing out your available memory already on normal operation, but rather leave some headroom to cope with outages.\n\nThe OSD service itself will use additional memory. The Ceph BlueStore backend of the daemon requires by default 3-5 GiB of memory (adjustable).\n\nNetwork\n\nWe recommend a network bandwidth of at least 10 Gbps, or more, to be used exclusively for Ceph traffic. A meshed network setup [17] is also an option for three to five node clusters, if there are no 10+ Gbps switches available.\n\n\tThe volume of traffic, especially during recovery, will interfere with other services on the same network, especially the latency sensitive Proxmox VE corosync cluster stack can be affected, resulting in possible loss of cluster quorum. Moving the Ceph traffic to dedicated and physical separated networks will avoid such interference, not only for corosync, but also for the networking services provided by any virtual guests.\n\nFor estimating your bandwidth needs, you need to take the performance of your disks into account.. While a single HDD might not saturate a 1 Gb link, multiple HDD OSDs per node can already saturate 10 Gbps too. If modern NVMe-attached SSDs are used, a single one can already saturate 10 Gbps of bandwidth, or more. For such high-performance setups we recommend at least a 25 Gpbs, while even 40 Gbps or 100+ Gbps might be required to utilize the full performance potential of the underlying disks.\n\nIf unsure, we recommend using three (physical) separate networks for high-performance setups: * one very high bandwidth (25+ Gbps) network for Ceph (internal) cluster traffic. * one high bandwidth (10+ Gpbs) network for Ceph (public) traffic between the ceph server and ceph client storage traffic. Depending on your needs this can also be used to host the virtual guest traffic and the VM live-migration traffic. * one medium bandwidth (1 Gbps) exclusive for the latency sensitive corosync cluster communication.\n\nDisks\n\nWhen planning the size of your Ceph cluster, it is important to take the recovery time into consideration. Especially with small clusters, recovery might take long. It is recommended that you use SSDs instead of HDDs in small setups to reduce recovery time, minimizing the likelihood of a subsequent failure event during recovery.\n\nIn general, SSDs will provide more IOPS than spinning disks. With this in mind, in addition to the higher cost, it may make sense to implement a class based separation of pools. Another way to speed up OSDs is to use a faster disk as a journal or DB/Write-Ahead-Log device, see creating Ceph OSDs. If a faster disk is used for multiple OSDs, a proper balance between OSD and WAL / DB (or journal) disk must be selected, otherwise the faster disk becomes the bottleneck for all linked OSDs.\n\nAside from the disk type, Ceph performs best with an evenly sized, and an evenly distributed amount of disks per node. For example, 4 x 500 GB disks within each node is better than a mixed setup with a single 1 TB and three 250 GB disk.\n\nYou also need to balance OSD count and single OSD capacity. More capacity allows you to increase storage density, but it also means that a single OSD failure forces Ceph to recover more data at once.\n\nAvoid RAID\n\nAs Ceph handles data object redundancy and multiple parallel writes to disks (OSDs) on its own, using a RAID controller normally doesn’t improve performance or availability. On the contrary, Ceph is designed to handle whole disks on it’s own, without any abstraction in between. RAID controllers are not designed for the Ceph workload and may complicate things and sometimes even reduce performance, as their write and caching algorithms may interfere with the ones from Ceph.\n\n\tAvoid RAID controllers. Use host bus adapter (HBA) instead.\n8.4. Initial Ceph Installation & Configuration\n8.4.1. Using the Web-based Wizard\n\nWith Proxmox VE you have the benefit of an easy to use installation wizard for Ceph. Click on one of your cluster nodes and navigate to the Ceph section in the menu tree. If Ceph is not already installed, you will see a prompt offering to do so.\n\nThe wizard is divided into multiple sections, where each needs to finish successfully, in order to use Ceph.\n\nFirst you need to chose which Ceph version you want to install. Prefer the one from your other nodes, or the newest if this is the first node you install Ceph.\n\nAfter starting the installation, the wizard will download and install all the required packages from Proxmox VE’s Ceph repository.\n\nAfter finishing the installation step, you will need to create a configuration. This step is only needed once per cluster, as this configuration is distributed automatically to all remaining cluster members through Proxmox VE’s clustered configuration file system (pmxcfs).\n\nThe configuration step includes the following settings:\n\nPublic Network: This network will be used for public storage communication (e.g., for virtual machines using a Ceph RBD backed disk, or a CephFS mount), and communication between the different Ceph services. This setting is required.\nSeparating your Ceph traffic from the Proxmox VE cluster communication (corosync), and possible the front-facing (public) networks of your virtual guests, is highly recommended. Otherwise, Ceph’s high-bandwidth IO-traffic could cause interference with other low-latency dependent services.\n\nCluster Network: Specify to separate the OSD replication and heartbeat traffic as well. This setting is optional.\nUsing a physically separated network is recommended, as it will relieve the Ceph public and the virtual guests network, while also providing a significant Ceph performance improvements.\nThe Ceph cluster network can be configured and moved to another physically separated network at a later time.\n\nYou have two more options which are considered advanced and therefore should only changed if you know what you are doing.\n\nNumber of replicas: Defines how often an object is replicated.\n\nMinimum replicas: Defines the minimum number of required replicas for I/O to be marked as complete.\n\nAdditionally, you need to choose your first monitor node. This step is required.\n\nThat’s it. You should now see a success page as the last step, with further instructions on how to proceed. Your system is now ready to start using Ceph. To get started, you will need to create some additional monitors, OSDs and at least one pool.\n\nThe rest of this chapter will guide you through getting the most out of your Proxmox VE based Ceph setup. This includes the aforementioned tips and more, such as CephFS, which is a helpful addition to your new Ceph cluster.\n\n8.4.2. CLI Installation of Ceph Packages\n\nAlternatively to the the recommended Proxmox VE Ceph installation wizard available in the web interface, you can use the following CLI command on each node:\n\npveceph install\n\nThis sets up an apt package repository in /etc/apt/sources.list.d/ceph.list and installs the required software.\n\n8.4.3. Initial Ceph configuration via CLI\n\nUse the Proxmox VE Ceph installation wizard (recommended) or run the following command on one node:\n\npveceph init --network 10.10.10.0/24\n\nThis creates an initial configuration at /etc/pve/ceph.conf with a dedicated network for Ceph. This file is automatically distributed to all Proxmox VE nodes, using pmxcfs. The command also creates a symbolic link at /etc/ceph/ceph.conf, which points to that file. Thus, you can simply run Ceph commands without the need to specify a configuration file.\n\n8.5. Ceph Monitor\n\nThe Ceph Monitor (MON) [18] maintains a master copy of the cluster map. For high availability, you need at least 3 monitors. One monitor will already be installed if you used the installation wizard. You won’t need more than 3 monitors, as long as your cluster is small to medium-sized. Only really large clusters will require more than this.\n\n8.5.1. Create Monitors\n\nOn each node where you want to place a monitor (three monitors are recommended), create one by using the Ceph → Monitor tab in the GUI or run:\n\npveceph mon create\n8.5.2. Destroy Monitors\n\nTo remove a Ceph Monitor via the GUI, first select a node in the tree view and go to the Ceph → Monitor panel. Select the MON and click the Destroy button.\n\nTo remove a Ceph Monitor via the CLI, first connect to the node on which the MON is running. Then execute the following command:\n\npveceph mon destroy\n\tAt least three Monitors are needed for quorum.\n8.6. Ceph Manager\n\nThe Manager daemon runs alongside the monitors. It provides an interface to monitor the cluster. Since the release of Ceph luminous, at least one ceph-mgr [19] daemon is required.\n\n8.6.1. Create Manager\n\nMultiple Managers can be installed, but only one Manager is active at any given time.\n\npveceph mgr create\n\tIt is recommended to install the Ceph Manager on the monitor nodes. For high availability install more then one manager.\n8.6.2. Destroy Manager\n\nTo remove a Ceph Manager via the GUI, first select a node in the tree view and go to the Ceph → Monitor panel. Select the Manager and click the Destroy button.\n\nTo remove a Ceph Monitor via the CLI, first connect to the node on which the Manager is running. Then execute the following command:\n\npveceph mgr destroy\n\tWhile a manager is not a hard-dependency, it is crucial for a Ceph cluster, as it handles important features like PG-autoscaling, device health monitoring, telemetry and more.\n8.7. Ceph OSDs\n\nCeph Object Storage Daemons store objects for Ceph over the network. It is recommended to use one OSD per physical disk.\n\n8.7.1. Create OSDs\n\nYou can create an OSD either via the Proxmox VE web interface or via the CLI using pveceph. For example:\n\npveceph osd create /dev/sd[X]\n\tWe recommend a Ceph cluster with at least three nodes and at least 12 OSDs, evenly distributed among the nodes.\n\nIf the disk was in use before (for example, for ZFS or as an OSD) you first need to zap all traces of that usage. To remove the partition table, boot sector and any other OSD leftover, you can use the following command:\n\nceph-volume lvm zap /dev/sd[X] --destroy\n\tThe above command will destroy all data on the disk!\nCeph Bluestore\n\nStarting with the Ceph Kraken release, a new Ceph OSD storage type was introduced called Bluestore [20]. This is the default when creating OSDs since Ceph Luminous.\n\npveceph osd create /dev/sd[X]\nBlock.db and block.wal\n\nIf you want to use a separate DB/WAL device for your OSDs, you can specify it through the -db_dev and -wal_dev options. The WAL is placed with the DB, if not specified separately.\n\npveceph osd create /dev/sd[X] -db_dev /dev/sd[Y] -wal_dev /dev/sd[Z]\n\nYou can directly choose the size of those with the -db_size and -wal_size parameters respectively. If they are not given, the following values (in order) will be used:\n\nbluestore_block_{db,wal}_size from Ceph configuration…\n\n… database, section osd\n\n… database, section global\n\n… file, section osd\n\n… file, section global\n\n10% (DB)/1% (WAL) of OSD size\n\n\tThe DB stores BlueStore’s internal metadata, and the WAL is BlueStore’s internal journal or write-ahead log. It is recommended to use a fast SSD or NVRAM for better performance.\nCeph Filestore\n\nBefore Ceph Luminous, Filestore was used as the default storage type for Ceph OSDs. Starting with Ceph Nautilus, Proxmox VE does not support creating such OSDs with pveceph anymore. If you still want to create filestore OSDs, use ceph-volume directly.\n\nceph-volume lvm create --filestore --data /dev/sd[X] --journal /dev/sd[Y]\n8.7.2. Destroy OSDs\n\nTo remove an OSD via the GUI, first select a Proxmox VE node in the tree view and go to the Ceph → OSD panel. Then select the OSD to destroy and click the OUT button. Once the OSD status has changed from in to out, click the STOP button. Finally, after the status has changed from up to down, select Destroy from the More drop-down menu.\n\nTo remove an OSD via the CLI run the following commands.\n\nceph osd out <ID>\nsystemctl stop ceph-osd@<ID>.service\n\tThe first command instructs Ceph not to include the OSD in the data distribution. The second command stops the OSD service. Until this time, no data is lost.\n\nThe following command destroys the OSD. Specify the -cleanup option to additionally destroy the partition table.\n\npveceph osd destroy <ID>\n\tThe above command will destroy all data on the disk!\n8.8. Ceph Pools\n\nA pool is a logical group for storing objects. It holds a collection of objects, known as Placement Groups (PG, pg_num).\n\n8.8.1. Create and Edit Pools\n\nYou can create and edit pools from the command line or the web interface of any Proxmox VE host under Ceph → Pools.\n\nWhen no options are given, we set a default of 128 PGs, a size of 3 replicas and a min_size of 2 replicas, to ensure no data loss occurs if any OSD fails.\n\n\tDo not set a min_size of 1. A replicated pool with min_size of 1 allows I/O on an object when it has only 1 replica, which could lead to data loss, incomplete PGs or unfound objects.\n\nIt is advised that you either enable the PG-Autoscaler or calculate the PG number based on your setup. You can find the formula and the PG calculator [21] online. From Ceph Nautilus onward, you can change the number of PGs [22] after the setup.\n\nThe PG autoscaler [23] can automatically scale the PG count for a pool in the background. Setting the Target Size or Target Ratio advanced parameters helps the PG-Autoscaler to make better decisions.\n\nExample for creating a pool over the CLI\npveceph pool create <pool-name> --add_storages\n\tIf you would also like to automatically define a storage for your pool, keep the ‘Add as Storage’ checkbox checked in the web interface, or use the command-line option --add_storages at pool creation.\nPool Options\n\nThe following options are available on pool creation, and partially also when editing a pool.\n\nName\n\nThe name of the pool. This must be unique and can’t be changed afterwards.\n\nSize\n\nThe number of replicas per object. Ceph always tries to have this many copies of an object. Default: 3.\n\nPG Autoscale Mode\n\nThe automatic PG scaling mode [23] of the pool. If set to warn, it produces a warning message when a pool has a non-optimal PG count. Default: warn.\n\nAdd as Storage\n\nConfigure a VM or container storage using the new pool. Default: true (only visible on creation).\n\nAdvanced Options\nMin. Size\n\nThe minimum number of replicas per object. Ceph will reject I/O on the pool if a PG has less than this many replicas. Default: 2.\n\nCrush Rule\n\nThe rule to use for mapping object placement in the cluster. These rules define how data is placed within the cluster. See Ceph CRUSH & device classes for information on device-based rules.\n\n# of PGs\n\nThe number of placement groups [22] that the pool should have at the beginning. Default: 128.\n\nTarget Ratio\n\nThe ratio of data that is expected in the pool. The PG autoscaler uses the ratio relative to other ratio sets. It takes precedence over the target size if both are set.\n\nTarget Size\n\nThe estimated amount of data expected in the pool. The PG autoscaler uses this size to estimate the optimal PG count.\n\nMin. # of PGs\n\nThe minimum number of placement groups. This setting is used to fine-tune the lower bound of the PG count for that pool. The PG autoscaler will not merge PGs below this threshold.\n\nFurther information on Ceph pool handling can be found in the Ceph pool operation [24] manual.\n\n8.8.2. Erasure Coded Pools\n\nErasure coding (EC) is a form of ‘forward error correction’ codes that allows to recover from a certain amount of data loss. Erasure coded pools can offer more usable space compared to replicated pools, but they do that for the price of performance.\n\nFor comparison: in classic, replicated pools, multiple replicas of the data are stored (size) while in erasure coded pool, data is split into k data chunks with additional m coding (checking) chunks. Those coding chunks can be used to recreate data should data chunks be missing.\n\nThe number of coding chunks, m, defines how many OSDs can be lost without losing any data. The total amount of objects stored is k + m.\n\nCreating EC Pools\n\nErasure coded (EC) pools can be created with the pveceph CLI tooling. Planning an EC pool needs to account for the fact, that they work differently than replicated pools.\n\nThe default min_size of an EC pool depends on the m parameter. If m = 1, the min_size of the EC pool will be k. The min_size will be k + 1 if m > 1. The Ceph documentation recommends a conservative min_size of k + 2 [25].\n\nIf there are less than min_size OSDs available, any IO to the pool will be blocked until there are enough OSDs available again.\n\n\tWhen planning an erasure coded pool, keep an eye on the min_size as it defines how many OSDs need to be available. Otherwise, IO will be blocked.\n\nFor example, an EC pool with k = 2 and m = 1 will have size = 3, min_size = 2 and will stay operational if one OSD fails. If the pool is configured with k = 2, m = 2, it will have a size = 4 and min_size = 3 and stay operational if one OSD is lost.\n\nTo create a new EC pool, run the following command:\n\npveceph pool create <pool-name> --erasure-coding k=2,m=1\n\nOptional parameters are failure-domain and device-class. If you need to change any EC profile settings used by the pool, you will have to create a new pool with a new profile.\n\nThis will create a new EC pool plus the needed replicated pool to store the RBD omap and other metadata. In the end, there will be a <pool name>-data and <pool name>-metada pool. The default behavior is to create a matching storage configuration as well. If that behavior is not wanted, you can disable it by providing the --add_storages 0 parameter. When configuring the storage configuration manually, keep in mind that the data-pool parameter needs to be set. Only then will the EC pool be used to store the data objects. For example:\n\n\tThe optional parameters --size, --min_size and --crush_rule will be used for the replicated metadata pool, but not for the erasure coded data pool. If you need to change the min_size on the data pool, you can do it later. The size and crush_rule parameters cannot be changed on erasure coded pools.\n\nIf there is a need to further customize the EC profile, you can do so by creating it with the Ceph tools directly [26], and specify the profile to use with the profile parameter.\n\nFor example:\n\npveceph pool create <pool-name> --erasure-coding profile=<profile-name>\nAdding EC Pools as Storage\n\nYou can add an already existing EC pool as storage to Proxmox VE. It works the same way as adding an RBD pool but requires the extra data-pool option.\n\npvesm add rbd <storage-name> --pool <replicated-pool> --data-pool <ec-pool>\n\tDo not forget to add the keyring and monhost option for any external Ceph clusters, not managed by the local Proxmox VE cluster.\n8.8.3. Destroy Pools\n\nTo destroy a pool via the GUI, select a node in the tree view and go to the Ceph → Pools panel. Select the pool to destroy and click the Destroy button. To confirm the destruction of the pool, you need to enter the pool name.\n\nRun the following command to destroy a pool. Specify the -remove_storages to also remove the associated storage.\n\npveceph pool destroy <name>\n\tPool deletion runs in the background and can take some time. You will notice the data usage in the cluster decreasing throughout this process.\n8.8.4. PG Autoscaler\n\nThe PG autoscaler allows the cluster to consider the amount of (expected) data stored in each pool and to choose the appropriate pg_num values automatically. It is available since Ceph Nautilus.\n\nYou may need to activate the PG autoscaler module before adjustments can take effect.\n\nceph mgr module enable pg_autoscaler\n\nThe autoscaler is configured on a per pool basis and has the following modes:\n\nwarn\n\t\n\nA health warning is issued if the suggested pg_num value differs too much from the current value.\n\n\non\n\t\n\nThe pg_num is adjusted automatically with no need for any manual interaction.\n\n\noff\n\t\n\nNo automatic pg_num adjustments are made, and no warning will be issued if the PG count is not optimal.\n\nThe scaling factor can be adjusted to facilitate future data storage with the target_size, target_size_ratio and the pg_num_min options.\n\n\tBy default, the autoscaler considers tuning the PG count of a pool if it is off by a factor of 3. This will lead to a considerable shift in data placement and might introduce a high load on the cluster.\n\nYou can find a more in-depth introduction to the PG autoscaler on Ceph’s Blog - New in Nautilus: PG merging and autotuning.\n\n8.9. Ceph CRUSH & device classes\n\nThe [27] (Controlled Replication Under Scalable Hashing) algorithm is at the foundation of Ceph.\n\nCRUSH calculates where to store and retrieve data from. This has the advantage that no central indexing service is needed. CRUSH works using a map of OSDs, buckets (device locations) and rulesets (data replication) for pools.\n\n\tFurther information can be found in the Ceph documentation, under the section CRUSH map [28].\n\nThis map can be altered to reflect different replication hierarchies. The object replicas can be separated (e.g., failure domains), while maintaining the desired distribution.\n\nA common configuration is to use different classes of disks for different Ceph pools. For this reason, Ceph introduced device classes with luminous, to accommodate the need for easy ruleset generation.\n\nThe device classes can be seen in the ceph osd tree output. These classes represent their own root bucket, which can be seen with the below command.\n\nceph osd crush tree --show-shadow\n\nExample output form the above command:\n\nID  CLASS WEIGHT  TYPE NAME\n-16  nvme 2.18307 root default~nvme\n-13  nvme 0.72769     host sumi1~nvme\n 12  nvme 0.72769         osd.12\n-14  nvme 0.72769     host sumi2~nvme\n 13  nvme 0.72769         osd.13\n-15  nvme 0.72769     host sumi3~nvme\n 14  nvme 0.72769         osd.14\n -1       7.70544 root default\n -3       2.56848     host sumi1\n 12  nvme 0.72769         osd.12\n -5       2.56848     host sumi2\n 13  nvme 0.72769         osd.13\n -7       2.56848     host sumi3\n 14  nvme 0.72769         osd.14\n\nTo instruct a pool to only distribute objects on a specific device class, you first need to create a ruleset for the device class:\n\nceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <class>\n\n<rule-name>\n\n\t\n\nname of the rule, to connect with a pool (seen in GUI & CLI)\n\n\n\n\n<root>\n\n\t\n\nwhich crush root it should belong to (default Ceph root \"default\")\n\n\n\n\n<failure-domain>\n\n\t\n\nat which failure-domain the objects should be distributed (usually host)\n\n\n\n\n<class>\n\n\t\n\nwhat type of OSD backing store to use (e.g., nvme, ssd, hdd)\n\nOnce the rule is in the CRUSH map, you can tell a pool to use the ruleset.\n\nceph osd pool set <pool-name> crush_rule <rule-name>\n\tIf the pool already contains objects, these must be moved accordingly. Depending on your setup, this may introduce a big performance impact on your cluster. As an alternative, you can create a new pool and move disks separately.\n8.10. Ceph Client\n\nFollowing the setup from the previous sections, you can configure Proxmox VE to use such pools to store VM and Container images. Simply use the GUI to add a new RBD storage (see section Ceph RADOS Block Devices (RBD)).\n\nYou also need to copy the keyring to a predefined location for an external Ceph cluster. If Ceph is installed on the Proxmox nodes itself, then this will be done automatically.\n\n\tThe filename needs to be <storage_id> + `.keyring, where <storage_id> is the expression after rbd: in /etc/pve/storage.cfg. In the following example, my-ceph-storage is the <storage_id>:\nmkdir /etc/pve/priv/ceph\ncp /etc/ceph/ceph.client.admin.keyring /etc/pve/priv/ceph/my-ceph-storage.keyring\n8.11. CephFS\n\nCeph also provides a filesystem, which runs on top of the same object storage as RADOS block devices do. A Metadata Server (MDS) is used to map the RADOS backed objects to files and directories, allowing Ceph to provide a POSIX-compliant, replicated filesystem. This allows you to easily configure a clustered, highly available, shared filesystem. Ceph’s Metadata Servers guarantee that files are evenly distributed over the entire Ceph cluster. As a result, even cases of high load will not overwhelm a single host, which can be an issue with traditional shared filesystem approaches, for example NFS.\n\nProxmox VE supports both creating a hyper-converged CephFS and using an existing CephFS as storage to save backups, ISO files, and container templates.\n\n8.11.1. Metadata Server (MDS)\n\nCephFS needs at least one Metadata Server to be configured and running, in order to function. You can create an MDS through the Proxmox VE web GUI’s Node -> CephFS panel or from the command line with:\n\npveceph mds create\n\nMultiple metadata servers can be created in a cluster, but with the default settings, only one can be active at a time. If an MDS or its node becomes unresponsive (or crashes), another standby MDS will get promoted to active. You can speed up the handover between the active and standby MDS by using the hotstandby parameter option on creation, or if you have already created it you may set/add:\n\nmds standby replay = true\n\nin the respective MDS section of /etc/pve/ceph.conf. With this enabled, the specified MDS will remain in a warm state, polling the active one, so that it can take over faster in case of any issues.\n\n\tThis active polling will have an additional performance impact on your system and the active MDS.\nMultiple Active MDS\n\nSince Luminous (12.2.x) you can have multiple active metadata servers running at once, but this is normally only useful if you have a high amount of clients running in parallel. Otherwise the MDS is rarely the bottleneck in a system. If you want to set this up, please refer to the Ceph documentation. [29]\n\n8.11.2. Create CephFS\n\nWith Proxmox VE’s integration of CephFS, you can easily create a CephFS using the web interface, CLI or an external API interface. Some prerequisites are required for this to work:\n\nPrerequisites for a successful CephFS setup:\n\nInstall Ceph packages - if this was already done some time ago, you may want to rerun it on an up-to-date system to ensure that all CephFS related packages get installed.\n\nSetup Monitors\n\nSetup your OSDs\n\nSetup at least one MDS\n\nAfter this is complete, you can simply create a CephFS through either the Web GUI’s Node -> CephFS panel or the command-line tool pveceph, for example:\n\npveceph fs create --pg_num 128 --add-storage\n\nThis creates a CephFS named cephfs, using a pool for its data named cephfs_data with 128 placement groups and a pool for its metadata named cephfs_metadata with one quarter of the data pool’s placement groups (32). Check the Proxmox VE managed Ceph pool chapter or visit the Ceph documentation for more information regarding an appropriate placement group number (pg_num) for your setup [22]. Additionally, the --add-storage parameter will add the CephFS to the Proxmox VE storage configuration after it has been created successfully.\n\n8.11.3. Destroy CephFS\n\tDestroying a CephFS will render all of its data unusable. This cannot be undone!\n\nTo completely and gracefully remove a CephFS, the following steps are necessary:\n\nDisconnect every non-Proxmox VE client (e.g. unmount the CephFS in guests).\n\nDisable all related CephFS Proxmox VE storage entries (to prevent it from being automatically mounted).\n\nRemove all used resources from guests (e.g. ISOs) that are on the CephFS you want to destroy.\n\nUnmount the CephFS storages on all cluster nodes manually with\n\numount /mnt/pve/<STORAGE-NAME>\n\nWhere <STORAGE-NAME> is the name of the CephFS storage in your Proxmox VE.\n\nNow make sure that no metadata server (MDS) is running for that CephFS, either by stopping or destroying them. This can be done through the web interface or via the command-line interface, for the latter you would issue the following command:\n\npveceph stop --service mds.NAME\n\nto stop them, or\n\npveceph mds destroy NAME\n\nto destroy them.\n\nNote that standby servers will automatically be promoted to active when an active MDS is stopped or removed, so it is best to first stop all standby servers.\n\nNow you can destroy the CephFS with\n\npveceph fs destroy NAME --remove-storages --remove-pools\n\nThis will automatically destroy the underlying Ceph pools as well as remove the storages from pve config.\n\nAfter these steps, the CephFS should be completely removed and if you have other CephFS instances, the stopped metadata servers can be started again to act as standbys.\n\n8.12. Ceph maintenance\n8.12.1. Replace OSDs\n\nOne of the most common maintenance tasks in Ceph is to replace the disk of an OSD. If a disk is already in a failed state, then you can go ahead and run through the steps in Destroy OSDs. Ceph will recreate those copies on the remaining OSDs if possible. This rebalancing will start as soon as an OSD failure is detected or an OSD was actively stopped.\n\n\tWith the default size/min_size (3/2) of a pool, recovery only starts when ‘size + 1` nodes are available. The reason for this is that the Ceph object balancer CRUSH defaults to a full node as `failure domain’.\n\nTo replace a functioning disk from the GUI, go through the steps in Destroy OSDs. The only addition is to wait until the cluster shows HEALTH_OK before stopping the OSD to destroy it.\n\nOn the command line, use the following commands:\n\nceph osd out osd.<id>\n\nYou can check with the command below if the OSD can be safely removed.\n\nceph osd safe-to-destroy osd.<id>\n\nOnce the above check tells you that it is safe to remove the OSD, you can continue with the following commands:\n\nsystemctl stop ceph-osd@<id>.service\npveceph osd destroy <id>\n\nReplace the old disk with the new one and use the same procedure as described in Create OSDs.\n\n8.12.2. Trim/Discard\n\nIt is good practice to run fstrim (discard) regularly on VMs and containers. This releases data blocks that the filesystem isn’t using anymore. It reduces data usage and resource load. Most modern operating systems issue such discard commands to their disks regularly. You only need to ensure that the Virtual Machines enable the disk discard option.\n\n8.12.3. Scrub & Deep Scrub\n\nCeph ensures data integrity by scrubbing placement groups. Ceph checks every object in a PG for its health. There are two forms of Scrubbing, daily cheap metadata checks and weekly deep data checks. The weekly deep scrub reads the objects and uses checksums to ensure data integrity. If a running scrub interferes with business (performance) needs, you can adjust the time when scrubs [30] are executed.\n\n8.13. Ceph Monitoring and Troubleshooting\n\nIt is important to continuously monitor the health of a Ceph deployment from the beginning, either by using the Ceph tools or by accessing the status through the Proxmox VE API.\n\nThe following Ceph commands can be used to see if the cluster is healthy (HEALTH_OK), if there are warnings (HEALTH_WARN), or even errors (HEALTH_ERR). If the cluster is in an unhealthy state, the status commands below will also give you an overview of the current events and actions to take.\n\n# single time output\npve# ceph -s\n# continuously output status changes (press CTRL+C to stop)\npve# ceph -w\n\nTo get a more detailed view, every Ceph service has a log file under /var/log/ceph/. If more detail is required, the log level can be adjusted [31].\n\nYou can find more information about troubleshooting [32] a Ceph cluster on the official website.\n\n9. Storage Replication\n\nThe pvesr command-line tool manages the Proxmox VE storage replication framework. Storage replication brings redundancy for guests using local storage and reduces migration time.\n\nIt replicates guest volumes to another node so that all data is available without using shared storage. Replication uses snapshots to minimize traffic sent over the network. Therefore, new data is sent only incrementally after the initial full sync. In the case of a node failure, your guest data is still available on the replicated node.\n\nThe replication is done automatically in configurable intervals. The minimum replication interval is one minute, and the maximal interval once a week. The format used to specify those intervals is a subset of systemd calendar events, see Schedule Format section:\n\nIt is possible to replicate a guest to multiple target nodes, but not twice to the same target node.\n\nEach replications bandwidth can be limited, to avoid overloading a storage or server.\n\nOnly changes since the last replication (so-called deltas) need to be transferred if the guest is migrated to a node to which it already is replicated. This reduces the time needed significantly. The replication direction automatically switches if you migrate a guest to the replication target node.\n\nFor example: VM100 is currently on nodeA and gets replicated to nodeB. You migrate it to nodeB, so now it gets automatically replicated back from nodeB to nodeA.\n\nIf you migrate to a node where the guest is not replicated, the whole disk data must send over. After the migration, the replication job continues to replicate this guest to the configured nodes.\n\n\t\n\nHigh-Availability is allowed in combination with storage replication, but there may be some data loss between the last synced time and the time a node failed.\n\n9.1. Supported Storage Types\nTable 17. Storage Types\nDescription\tPlugin type\tSnapshots\tStable\n\n\nZFS (local)\n\n\t\n\nzfspool\n\n\t\n\nyes\n\n\t\n\nyes\n\n9.2. Schedule Format\n\nReplication uses calendar events for configuring the schedule.\n\n9.3. Error Handling\n\nIf a replication job encounters problems, it is placed in an error state. In this state, the configured replication intervals get suspended temporarily. The failed replication is repeatedly tried again in a 30 minute interval. Once this succeeds, the original schedule gets activated again.\n\n9.3.1. Possible issues\n\nSome of the most common issues are in the following list. Depending on your setup there may be another cause.\n\nNetwork is not working.\n\nNo free space left on the replication target storage.\n\nStorage with same storage ID available on the target node\n\n\tYou can always use the replication log to find out what is causing the problem.\n9.3.2. Migrating a guest in case of Error\n\nIn the case of a grave error, a virtual guest may get stuck on a failed node. You then need to move it manually to a working node again.\n\n9.3.3. Example\n\nLet’s assume that you have two guests (VM 100 and CT 200) running on node A and replicate to node B. Node A failed and can not get back online. Now you have to migrate the guest to Node B manually.\n\nconnect to node B over ssh or open its shell via the web UI\n\ncheck if that the cluster is quorate\n\n# pvecm status\n\nIf you have no quorum, we strongly advise to fix this first and make the node operable again. Only if this is not possible at the moment, you may use the following command to enforce quorum on the current node:\n\n# pvecm expected 1\n\tAvoid changes which affect the cluster if expected votes are set (for example adding/removing nodes, storages, virtual guests) at all costs. Only use it to get vital guests up and running again or to resolve the quorum issue itself.\n\nmove both guest configuration files form the origin node A to node B:\n\n# mv /etc/pve/nodes/A/qemu-server/100.conf /etc/pve/nodes/B/qemu-server/100.conf\n# mv /etc/pve/nodes/A/lxc/200.conf /etc/pve/nodes/B/lxc/200.conf\n\nNow you can start the guests again:\n\n# qm start 100\n# pct start 200\n\nRemember to replace the VMIDs and node names with your respective values.\n\n9.4. Managing Jobs\n\nYou can use the web GUI to create, modify, and remove replication jobs easily. Additionally, the command-line interface (CLI) tool pvesr can be used to do this.\n\nYou can find the replication panel on all levels (datacenter, node, virtual guest) in the web GUI. They differ in which jobs get shown: all, node- or guest-specific jobs.\n\nWhen adding a new job, you need to specify the guest if not already selected as well as the target node. The replication schedule can be set if the default of all 15 minutes is not desired. You may impose a rate-limit on a replication job. The rate limit can help to keep the load on the storage acceptable.\n\nA replication job is identified by a cluster-wide unique ID. This ID is composed of the VMID in addition to a job number. This ID must only be specified manually if the CLI tool is used.\n\n9.5. Command-line Interface Examples\n\nCreate a replication job which runs every 5 minutes with a limited bandwidth of 10 Mbps (megabytes per second) for the guest with ID 100.\n\n# pvesr create-local-job 100-0 pve1 --schedule \"*/5\" --rate 10\n\nDisable an active job with ID 100-0.\n\n# pvesr disable 100-0\n\nEnable a deactivated job with ID 100-0.\n\n# pvesr enable 100-0\n\nChange the schedule interval of the job with ID 100-0 to once per hour.\n\n# pvesr update 100-0 --schedule '*/00'\n10. QEMU/KVM Virtual Machines\n\nQEMU (short form for Quick Emulator) is an open source hypervisor that emulates a physical computer. From the perspective of the host system where QEMU is running, QEMU is a user program which has access to a number of local resources like partitions, files, network cards which are then passed to an emulated computer which sees them as if they were real devices.\n\nA guest operating system running in the emulated computer accesses these devices, and runs as if it were running on real hardware. For instance, you can pass an ISO image as a parameter to QEMU, and the OS running in the emulated computer will see a real CD-ROM inserted into a CD drive.\n\nQEMU can emulate a great variety of hardware from ARM to Sparc, but Proxmox VE is only concerned with 32 and 64 bits PC clone emulation, since it represents the overwhelming majority of server hardware. The emulation of PC clones is also one of the fastest due to the availability of processor extensions which greatly speed up QEMU when the emulated architecture is the same as the host architecture.\n\n\tYou may sometimes encounter the term KVM (Kernel-based Virtual Machine). It means that QEMU is running with the support of the virtualization processor extensions, via the Linux KVM module. In the context of Proxmox VE QEMU and KVM can be used interchangeably, as QEMU in Proxmox VE will always try to load the KVM module.\n\nQEMU inside Proxmox VE runs as a root process, since this is required to access block and PCI devices.\n\n10.1. Emulated devices and paravirtualized devices\n\nThe PC hardware emulated by QEMU includes a motherboard, network controllers, SCSI, IDE and SATA controllers, serial ports (the complete list can be seen in the kvm(1) man page) all of them emulated in software. All these devices are the exact software equivalent of existing hardware devices, and if the OS running in the guest has the proper drivers it will use the devices as if it were running on real hardware. This allows QEMU to run unmodified operating systems.\n\nThis however has a performance cost, as running in software what was meant to run in hardware involves a lot of extra work for the host CPU. To mitigate this, QEMU can present to the guest operating system paravirtualized devices, where the guest OS recognizes it is running inside QEMU and cooperates with the hypervisor.\n\nQEMU relies on the virtio virtualization standard, and is thus able to present paravirtualized virtio devices, which includes a paravirtualized generic disk controller, a paravirtualized network card, a paravirtualized serial port, a paravirtualized SCSI controller, etc …\n\n\tIt is highly recommended to use the virtio devices whenever you can, as they provide a big performance improvement and are generally better maintained. Using the virtio generic disk controller versus an emulated IDE controller will double the sequential write throughput, as measured with bonnie++(8). Using the virtio network interface can deliver up to three times the throughput of an emulated Intel E1000 network card, as measured with iperf(1). [33]\n10.2. Virtual Machines Settings\n\nGenerally speaking Proxmox VE tries to choose sane defaults for virtual machines (VM). Make sure you understand the meaning of the settings you change, as it could incur a performance slowdown, or putting your data at risk.\n\n10.2.1. General Settings\n\nGeneral settings of a VM include\n\nthe Node : the physical server on which the VM will run\n\nthe VM ID: a unique number in this Proxmox VE installation used to identify your VM\n\nName: a free form text string you can use to describe the VM\n\nResource Pool: a logical group of VMs\n\n10.2.2. OS Settings\n\nWhen creating a virtual machine (VM), setting the proper Operating System(OS) allows Proxmox VE to optimize some low level parameters. For instance Windows OS expect the BIOS clock to use the local time, while Unix based OS expect the BIOS clock to have the UTC time.\n\n10.2.3. System Settings\n\nOn VM creation you can change some basic system components of the new VM. You can specify which display type you want to use.\n\nAdditionally, the SCSI controller can be changed. If you plan to install the QEMU Guest Agent, or if your selected ISO image already ships and installs it automatically, you may want to tick the QEMU Agent box, which lets Proxmox VE know that it can use its features to show some more information, and complete some actions (for example, shutdown or snapshots) more intelligently.\n\nProxmox VE allows to boot VMs with different firmware and machine types, namely SeaBIOS and OVMF. In most cases you want to switch from the default SeaBIOS to OVMF only if you plan to use PCIe passthrough.\n\nMachine Type\n\nA VM’s Machine Type defines the hardware layout of the VM’s virtual motherboard. You can choose between the default Intel 440FX or the Q35 chipset, which also provides a virtual PCIe bus, and thus may be desired if you want to pass through PCIe hardware. Additionally, you can select a vIOMMU implementation.\n\nMachine Version\n\nEach machine type is versioned in QEMU and a given QEMU binary supports many machine versions. New versions might bring support for new features, fixes or general improvements. However, they also change properties of the virtual hardware. To avoid sudden changes from the guest’s perspective and ensure compatibility of the VM state, live-migration and snapshots with RAM will keep using the same machine version in the new QEMU instance.\n\nFor Windows guests, the machine version is pinned during creation, because Windows is sensitive to changes in the virtual hardware - even between cold boots. For example, the enumeration of network devices might be different with different machine versions. Other OSes like Linux can usually deal with such changes just fine. For those, the Latest machine version is used by default. This means that after a fresh start, the newest machine version supported by the QEMU binary is used (e.g. the newest machine version QEMU 8.1 supports is version 8.1 for each machine type).\n\nUpdate to a Newer Machine Version\n\nVery old machine versions might become deprecated in QEMU. For example, this is the case for versions 1.4 to 1.7 for the i440fx machine type. It is expected that support for these machine versions will be dropped at some point. If you see a deprecation warning, you should change the machine version to a newer one. Be sure to have a working backup first and be prepared for changes to how the guest sees hardware. In some scenarios, re-installing certain drivers might be required. You should also check for snapshots with RAM that were taken with these machine versions (i.e. the runningmachine configuration entry). Unfortunately, there is no way to change the machine version of a snapshot, so you’d need to load the snapshot to salvage any data from it.\n\n10.2.4. Hard Disk\nBus/Controller\n\nQEMU can emulate a number of storage controllers:\n\n\tIt is highly recommended to use the VirtIO SCSI or VirtIO Block controller for performance reasons and because they are better maintained.\n\nthe IDE controller, has a design which goes back to the 1984 PC/AT disk controller. Even if this controller has been superseded by recent designs, each and every OS you can think of has support for it, making it a great choice if you want to run an OS released before 2003. You can connect up to 4 devices on this controller.\n\nthe SATA (Serial ATA) controller, dating from 2003, has a more modern design, allowing higher throughput and a greater number of devices to be connected. You can connect up to 6 devices on this controller.\n\nthe SCSI controller, designed in 1985, is commonly found on server grade hardware, and can connect up to 14 storage devices. Proxmox VE emulates by default a LSI 53C895A controller.\n\nA SCSI controller of type VirtIO SCSI single and enabling the IO Thread setting for the attached disks is recommended if you aim for performance. This is the default for newly created Linux VMs since Proxmox VE 7.3. Each disk will have its own VirtIO SCSI controller, and QEMU will handle the disks IO in a dedicated thread. Linux distributions have support for this controller since 2012, and FreeBSD since 2014. For Windows OSes, you need to provide an extra ISO containing the drivers during the installation.\n\nThe VirtIO Block controller, often just called VirtIO or virtio-blk, is an older type of paravirtualized controller. It has been superseded by the VirtIO SCSI Controller, in terms of features.\n\nImage Format\n\nOn each controller you attach a number of emulated hard disks, which are backed by a file or a block device residing in the configured storage. The choice of a storage type will determine the format of the hard disk image. Storages which present block devices (LVM, ZFS, Ceph) will require the raw disk image format, whereas files based storages (Ext4, NFS, CIFS, GlusterFS) will let you to choose either the raw disk image format or the QEMU image format.\n\nthe QEMU image format is a copy on write format which allows snapshots, and thin provisioning of the disk image.\n\nthe raw disk image is a bit-to-bit image of a hard disk, similar to what you would get when executing the dd command on a block device in Linux. This format does not support thin provisioning or snapshots by itself, requiring cooperation from the storage layer for these tasks. It may, however, be up to 10% faster than the QEMU image format. [34]\n\nthe VMware image format only makes sense if you intend to import/export the disk image to other hypervisors.\n\nCache Mode\n\nSetting the Cache mode of the hard drive will impact how the host system will notify the guest systems of block write completions. The No cache default means that the guest system will be notified that a write is complete when each block reaches the physical storage write queue, ignoring the host page cache. This provides a good balance between safety and speed.\n\nIf you want the Proxmox VE backup manager to skip a disk when doing a backup of a VM, you can set the No backup option on that disk.\n\nIf you want the Proxmox VE storage replication mechanism to skip a disk when starting a replication job, you can set the Skip replication option on that disk. As of Proxmox VE 5.0, replication requires the disk images to be on a storage of type zfspool, so adding a disk image to other storages when the VM has replication configured requires to skip replication for this disk image.\n\nTrim/Discard\n\nIf your storage supports thin provisioning (see the storage chapter in the Proxmox VE guide), you can activate the Discard option on a drive. With Discard set and a TRIM-enabled guest OS [35], when the VM’s filesystem marks blocks as unused after deleting files, the controller will relay this information to the storage, which will then shrink the disk image accordingly. For the guest to be able to issue TRIM commands, you must enable the Discard option on the drive. Some guest operating systems may also require the SSD Emulation flag to be set. Note that Discard on VirtIO Block drives is only supported on guests using Linux Kernel 5.0 or higher.\n\nIf you would like a drive to be presented to the guest as a solid-state drive rather than a rotational hard disk, you can set the SSD emulation option on that drive. There is no requirement that the underlying storage actually be backed by SSDs; this feature can be used with physical media of any type. Note that SSD emulation is not supported on VirtIO Block drives.\n\nIO Thread\n\nThe option IO Thread can only be used when using a disk with the VirtIO controller, or with the SCSI controller, when the emulated controller type is VirtIO SCSI single. With IO Thread enabled, QEMU creates one I/O thread per storage controller rather than handling all I/O in the main event loop or vCPU threads. One benefit is better work distribution and utilization of the underlying storage. Another benefit is reduced latency (hangs) in the guest for very I/O-intensive host workloads, since neither the main thread nor a vCPU thread can be blocked by disk I/O.\n\n10.2.5. CPU\n\nA CPU socket is a physical slot on a PC motherboard where you can plug a CPU. This CPU can then contain one or many cores, which are independent processing units. Whether you have a single CPU socket with 4 cores, or two CPU sockets with two cores is mostly irrelevant from a performance point of view. However some software licenses depend on the number of sockets a machine has, in that case it makes sense to set the number of sockets to what the license allows you.\n\nIncreasing the number of virtual CPUs (cores and sockets) will usually provide a performance improvement though that is heavily dependent on the use of the VM. Multi-threaded applications will of course benefit from a large number of virtual CPUs, as for each virtual cpu you add, QEMU will create a new thread of execution on the host system. If you’re not sure about the workload of your VM, it is usually a safe bet to set the number of Total cores to 2.\n\n\tIt is perfectly safe if the overall number of cores of all your VMs is greater than the number of cores on the server (for example, 4 VMs each with 4 cores (= total 16) on a machine with only 8 cores). In that case the host system will balance the QEMU execution threads between your server cores, just like if you were running a standard multi-threaded application. However, Proxmox VE will prevent you from starting VMs with more virtual CPU cores than physically available, as this will only bring the performance down due to the cost of context switches.\nResource Limits\n\ncpulimit\n\nIn addition to the number of virtual cores, the total available “Host CPU Time” for the VM can be set with the cpulimit option. It is a floating point value representing CPU time in percent, so 1.0 is equal to 100%, 2.5 to 250% and so on. If a single process would fully use one single core it would have 100% CPU Time usage. If a VM with four cores utilizes all its cores fully it would theoretically use 400%. In reality the usage may be even a bit higher as QEMU can have additional threads for VM peripherals besides the vCPU core ones.\n\nThis setting can be useful when a VM should have multiple vCPUs because it is running some processes in parallel, but the VM as a whole should not be able to run all vCPUs at 100% at the same time.\n\nFor example, suppose you have a virtual machine that would benefit from having 8 virtual CPUs, but you don’t want the VM to be able to max out all 8 cores running at full load - because that would overload the server and leave other virtual machines and containers with too little CPU time. To solve this, you could set cpulimit to 4.0 (=400%). This means that if the VM fully utilizes all 8 virtual CPUs by running 8 processes simultaneously, each vCPU will receive a maximum of 50% CPU time from the physical cores. However, if the VM workload only fully utilizes 4 virtual CPUs, it could still receive up to 100% CPU time from a physical core, for a total of 400%.\n\n\tVMs can, depending on their configuration, use additional threads, such as for networking or IO operations but also live migration. Thus a VM can show up to use more CPU time than just its virtual CPUs could use. To ensure that a VM never uses more CPU time than vCPUs assigned, set the cpulimit to the same value as the total core count.\n\ncpuuntis\n\nWith the cpuunits option, nowadays often called CPU shares or CPU weight, you can control how much CPU time a VM gets compared to other running VMs. It is a relative weight which defaults to 100 (or 1024 if the host uses legacy cgroup v1). If you increase this for a VM it will be prioritized by the scheduler in comparison to other VMs with lower weight.\n\nFor example, if VM 100 has set the default 100 and VM 200 was changed to 200, the latter VM 200 would receive twice the CPU bandwidth than the first VM 100.\n\nFor more information see man systemd.resource-control, here CPUQuota corresponds to cpulimit and CPUWeight to our cpuunits setting. Visit its Notes section for references and implementation details.\n\naffinity\n\nWith the affinity option, you can specify the physical CPU cores that are used to run the VM’s vCPUs. Peripheral VM processes, such as those for I/O, are not affected by this setting. Note that the CPU affinity is not a security feature.\n\nForcing a CPU affinity can make sense in certain cases but is accompanied by an increase in complexity and maintenance effort. For example, if you want to add more VMs later or migrate VMs to nodes with fewer CPU cores. It can also easily lead to asynchronous and therefore limited system performance if some CPUs are fully utilized while others are almost idle.\n\nThe affinity is set through the taskset CLI tool. It accepts the host CPU numbers (see lscpu) in the List Format from man cpuset. This ASCII decimal list can contain numbers but also number ranges. For example, the affinity 0-1,8-11 (expanded 0, 1, 8, 9, 10, 11) would allow the VM to run on only these six specific host cores.\n\nCPU Type\n\nQEMU can emulate a number different of CPU types from 486 to the latest Xeon processors. Each new processor generation adds new features, like hardware assisted 3d rendering, random number generation, memory protection, etc. Also, a current generation can be upgraded through microcode update with bug or security fixes.\n\nUsually you should select for your VM a processor type which closely matches the CPU of the host system, as it means that the host CPU features (also called CPU flags ) will be available in your VMs. If you want an exact match, you can set the CPU type to host in which case the VM will have exactly the same CPU flags as your host system.\n\nThis has a downside though. If you want to do a live migration of VMs between different hosts, your VM might end up on a new system with a different CPU type or a different microcode version. If the CPU flags passed to the guest are missing, the QEMU process will stop. To remedy this QEMU has also its own virtual CPU types, that Proxmox VE uses by default.\n\nThe backend default is kvm64 which works on essentially all x86_64 host CPUs and the UI default when creating a new VM is x86-64-v2-AES, which requires a host CPU starting from Westmere for Intel or at least a fourth generation Opteron for AMD.\n\nIn short:\n\nIf you don’t care about live migration or have a homogeneous cluster where all nodes have the same CPU and same microcode version, set the CPU type to host, as in theory this will give your guests maximum performance.\n\nIf you care about live migration and security, and you have only Intel CPUs or only AMD CPUs, choose the lowest generation CPU model of your cluster.\n\nIf you care about live migration without security, or have mixed Intel/AMD cluster, choose the lowest compatible virtual QEMU CPU type.\n\n\tLive migrations between Intel and AMD host CPUs have no guarantee to work.\n\nSee also List of AMD and Intel CPU Types as Defined in QEMU.\n\nQEMU CPU Types\n\nQEMU also provide virtual CPU types, compatible with both Intel and AMD host CPUs.\n\n\tTo mitigate the Spectre vulnerability for virtual CPU types, you need to add the relevant CPU flags, see Meltdown / Spectre related CPU flags.\n\nHistorically, Proxmox VE had the kvm64 CPU model, with CPU flags at the level of Pentium 4 enabled, so performance was not great for certain workloads.\n\nIn the summer of 2020, AMD, Intel, Red Hat, and SUSE collaborated to define three x86-64 microarchitecture levels on top of the x86-64 baseline, with modern flags enabled. For details, see the x86-64-ABI specification.\n\n\tSome newer distributions like CentOS 9 are now built with x86-64-v2 flags as a minimum requirement.\n\nkvm64 (x86-64-v1): Compatible with Intel CPU >= Pentium 4, AMD CPU >= Phenom.\n\nx86-64-v2: Compatible with Intel CPU >= Nehalem, AMD CPU >= Opteron_G3. Added CPU flags compared to x86-64-v1: +cx16, +lahf-lm, +popcnt, +pni, +sse4.1, +sse4.2, +ssse3.\n\nx86-64-v2-AES: Compatible with Intel CPU >= Westmere, AMD CPU >= Opteron_G4. Added CPU flags compared to x86-64-v2: +aes.\n\nx86-64-v3: Compatible with Intel CPU >= Broadwell, AMD CPU >= EPYC. Added CPU flags compared to x86-64-v2-AES: +avx, +avx2, +bmi1, +bmi2, +f16c, +fma, +movbe, +xsave.\n\nx86-64-v4: Compatible with Intel CPU >= Skylake, AMD CPU >= EPYC v4 Genoa. Added CPU flags compared to x86-64-v3: +avx512f, +avx512bw, +avx512cd, +avx512dq, +avx512vl.\n\nCustom CPU Types\n\nYou can specify custom CPU types with a configurable set of features. These are maintained in the configuration file /etc/pve/virtual-guest/cpu-models.conf by an administrator. See man cpu-models.conf for format details.\n\nSpecified custom types can be selected by any user with the Sys.Audit privilege on /nodes. When configuring a custom CPU type for a VM via the CLI or API, the name needs to be prefixed with custom-.\n\nMeltdown / Spectre related CPU flags\n\nThere are several CPU flags related to the Meltdown and Spectre vulnerabilities [36] which need to be set manually unless the selected CPU type of your VM already enables them by default.\n\nThere are two requirements that need to be fulfilled in order to use these CPU flags:\n\nThe host CPU(s) must support the feature and propagate it to the guest’s virtual CPU(s)\n\nThe guest operating system must be updated to a version which mitigates the attacks and is able to utilize the CPU feature\n\nOtherwise you need to set the desired CPU flag of the virtual CPU, either by editing the CPU options in the web UI, or by setting the flags property of the cpu option in the VM configuration file.\n\nFor Spectre v1,v2,v4 fixes, your CPU or system vendor also needs to provide a so-called “microcode update” for your CPU, see chapter Firmware Updates. Note that not all affected CPUs can be updated to support spec-ctrl.\n\nTo check if the Proxmox VE host is vulnerable, execute the following command as root:\n\nfor f in /sys/devices/system/cpu/vulnerabilities/*; do echo \"${f##*/} -\" $(cat \"$f\"); done\n\nA community script is also available to detect if the host is still vulnerable. [37]\n\nIntel processors\n\npcid\n\nThis reduces the performance impact of the Meltdown (CVE-2017-5754) mitigation called Kernel Page-Table Isolation (KPTI), which effectively hides the Kernel memory from the user space. Without PCID, KPTI is quite an expensive mechanism [38].\n\nTo check if the Proxmox VE host supports PCID, execute the following command as root:\n\n# grep ' pcid ' /proc/cpuinfo\n\nIf this does not return empty your host’s CPU has support for pcid.\n\nspec-ctrl\n\nRequired to enable the Spectre v1 (CVE-2017-5753) and Spectre v2 (CVE-2017-5715) fix, in cases where retpolines are not sufficient. Included by default in Intel CPU models with -IBRS suffix. Must be explicitly turned on for Intel CPU models without -IBRS suffix. Requires an updated host CPU microcode (intel-microcode >= 20180425).\n\nssbd\n\nRequired to enable the Spectre V4 (CVE-2018-3639) fix. Not included by default in any Intel CPU model. Must be explicitly turned on for all Intel CPU models. Requires an updated host CPU microcode(intel-microcode >= 20180703).\n\nAMD processors\n\nibpb\n\nRequired to enable the Spectre v1 (CVE-2017-5753) and Spectre v2 (CVE-2017-5715) fix, in cases where retpolines are not sufficient. Included by default in AMD CPU models with -IBPB suffix. Must be explicitly turned on for AMD CPU models without -IBPB suffix. Requires the host CPU microcode to support this feature before it can be used for guest CPUs.\n\nvirt-ssbd\n\nRequired to enable the Spectre v4 (CVE-2018-3639) fix. Not included by default in any AMD CPU model. Must be explicitly turned on for all AMD CPU models. This should be provided to guests, even if amd-ssbd is also provided, for maximum guest compatibility. Note that this must be explicitly enabled when when using the \"host\" cpu model, because this is a virtual feature which does not exist in the physical CPUs.\n\namd-ssbd\n\nRequired to enable the Spectre v4 (CVE-2018-3639) fix. Not included by default in any AMD CPU model. Must be explicitly turned on for all AMD CPU models. This provides higher performance than virt-ssbd, therefore a host supporting this should always expose this to guests if possible. virt-ssbd should none the less also be exposed for maximum guest compatibility as some kernels only know about virt-ssbd.\n\namd-no-ssb\n\nRecommended to indicate the host is not vulnerable to Spectre V4 (CVE-2018-3639). Not included by default in any AMD CPU model. Future hardware generations of CPU will not be vulnerable to CVE-2018-3639, and thus the guest should be told not to enable its mitigations, by exposing amd-no-ssb. This is mutually exclusive with virt-ssbd and amd-ssbd.\n\nNUMA\n\nYou can also optionally emulate a NUMA [39] architecture in your VMs. The basics of the NUMA architecture mean that instead of having a global memory pool available to all your cores, the memory is spread into local banks close to each socket. This can bring speed improvements as the memory bus is not a bottleneck anymore. If your system has a NUMA architecture [40] we recommend to activate the option, as this will allow proper distribution of the VM resources on the host system. This option is also required to hot-plug cores or RAM in a VM.\n\nIf the NUMA option is used, it is recommended to set the number of sockets to the number of nodes of the host system.\n\nvCPU hot-plug\n\nModern operating systems introduced the capability to hot-plug and, to a certain extent, hot-unplug CPUs in a running system. Virtualization allows us to avoid a lot of the (physical) problems real hardware can cause in such scenarios. Still, this is a rather new and complicated feature, so its use should be restricted to cases where its absolutely needed. Most of the functionality can be replicated with other, well tested and less complicated, features, see Resource Limits.\n\nIn Proxmox VE the maximal number of plugged CPUs is always cores * sockets. To start a VM with less than this total core count of CPUs you may use the vcpus setting, it denotes how many vCPUs should be plugged in at VM start.\n\nCurrently only this feature is only supported on Linux, a kernel newer than 3.10 is needed, a kernel newer than 4.7 is recommended.\n\nYou can use a udev rule as follow to automatically set new CPUs as online in the guest:\n\nSUBSYSTEM==\"cpu\", ACTION==\"add\", TEST==\"online\", ATTR{online}==\"0\", ATTR{online}=\"1\"\n\nSave this under /etc/udev/rules.d/ as a file ending in .rules.\n\nNote: CPU hot-remove is machine dependent and requires guest cooperation. The deletion command does not guarantee CPU removal to actually happen, typically it’s a request forwarded to guest OS using target dependent mechanism, such as ACPI on x86/amd64.\n\n10.2.6. Memory\n\nFor each VM you have the option to set a fixed size memory or asking Proxmox VE to dynamically allocate memory based on the current RAM usage of the host.\n\nFixed Memory Allocation\n\nWhen setting memory and minimum memory to the same amount Proxmox VE will simply allocate what you specify to your VM.\n\nEven when using a fixed memory size, the ballooning device gets added to the VM, because it delivers useful information such as how much memory the guest really uses. In general, you should leave ballooning enabled, but if you want to disable it (like for debugging purposes), simply uncheck Ballooning Device or set\n\nballoon: 0\n\nin the configuration.\n\nAutomatic Memory Allocation\n\nWhen setting the minimum memory lower than memory, Proxmox VE will make sure that the minimum amount you specified is always available to the VM, and if RAM usage on the host is below 80%, will dynamically add memory to the guest up to the maximum memory specified.\n\nWhen the host is running low on RAM, the VM will then release some memory back to the host, swapping running processes if needed and starting the oom killer in last resort. The passing around of memory between host and guest is done via a special balloon kernel driver running inside the guest, which will grab or release memory pages from the host. [41]\n\nWhen multiple VMs use the autoallocate facility, it is possible to set a Shares coefficient which indicates the relative amount of the free host memory that each VM should take. Suppose for instance you have four VMs, three of them running an HTTP server and the last one is a database server. To cache more database blocks in the database server RAM, you would like to prioritize the database VM when spare RAM is available. For this you assign a Shares property of 3000 to the database VM, leaving the other VMs to the Shares default setting of 1000. The host server has 32GB of RAM, and is currently using 16GB, leaving 32 * 80/100 - 16 = 9GB RAM to be allocated to the VMs on top of their configured minimum memory amount. The database VM will benefit from 9 * 3000 / (3000\n1000 + 1000 + 1000) = 4.5 GB extra RAM and each HTTP server from 1.5 GB.\n\nAll Linux distributions released after 2010 have the balloon kernel driver included. For Windows OSes, the balloon driver needs to be added manually and can incur a slowdown of the guest, so we don’t recommend using it on critical systems.\n\nWhen allocating RAM to your VMs, a good rule of thumb is always to leave 1GB of RAM available to the host.\n\n10.2.7. Network Device\n\nEach VM can have many Network interface controllers (NIC), of four different types:\n\nIntel E1000 is the default, and emulates an Intel Gigabit network card.\n\nthe VirtIO paravirtualized NIC should be used if you aim for maximum performance. Like all VirtIO devices, the guest OS should have the proper driver installed.\n\nthe Realtek 8139 emulates an older 100 MB/s network card, and should only be used when emulating older operating systems ( released before 2002 )\n\nthe vmxnet3 is another paravirtualized device, which should only be used when importing a VM from another hypervisor.\n\nProxmox VE will generate for each NIC a random MAC address, so that your VM is addressable on Ethernet networks.\n\nThe NIC you added to the VM can follow one of two different models:\n\nin the default Bridged mode each virtual NIC is backed on the host by a tap device, ( a software loopback device simulating an Ethernet NIC ). This tap device is added to a bridge, by default vmbr0 in Proxmox VE. In this mode, VMs have direct access to the Ethernet LAN on which the host is located.\n\nin the alternative NAT mode, each virtual NIC will only communicate with the QEMU user networking stack, where a built-in router and DHCP server can provide network access. This built-in DHCP will serve addresses in the private 10.0.2.0/24 range. The NAT mode is much slower than the bridged mode, and should only be used for testing. This mode is only available via CLI or the API, but not via the web UI.\n\nYou can also skip adding a network device when creating a VM by selecting No network device.\n\nYou can overwrite the MTU setting for each VM network device. The option mtu=1 represents a special case, in which the MTU value will be inherited from the underlying bridge. This option is only available for VirtIO network devices.\n\nMultiqueue\n\nIf you are using the VirtIO driver, you can optionally activate the Multiqueue option. This option allows the guest OS to process networking packets using multiple virtual CPUs, providing an increase in the total number of packets transferred.\n\nWhen using the VirtIO driver with Proxmox VE, each NIC network queue is passed to the host kernel, where the queue will be processed by a kernel thread spawned by the vhost driver. With this option activated, it is possible to pass multiple network queues to the host kernel for each NIC.\n\nWhen using Multiqueue, it is recommended to set it to a value equal to the number of vCPUs of your guest. Remember that the number of vCPUs is the number of sockets times the number of cores configured for the VM. You also need to set the number of multi-purpose channels on each VirtIO NIC in the VM with this ethtool command:\n\nethtool -L ens1 combined X\n\nwhere X is the number of the number of vCPUs of the VM.\n\nTo configure a Windows guest for Multiqueue install the Redhat VirtIO Ethernet Adapter drivers, then adapt the NIC’s configuration as follows. Open the device manager, right click the NIC under \"Network adapters\", and select \"Properties\". Then open the \"Advanced\" tab and select \"Receive Side Scaling\" from the list on the left. Make sure it is set to \"Enabled\". Next, navigate to \"Maximum number of RSS Queues\" in the list and set it to the number of vCPUs of your VM. Once you verified that the settings are correct, click \"OK\" to confirm them.\n\nYou should note that setting the Multiqueue parameter to a value greater than one will increase the CPU load on the host and guest systems as the traffic increases. We recommend to set this option only when the VM has to process a great number of incoming connections, such as when the VM is running as a router, reverse proxy or a busy HTTP server doing long polling.\n\n10.2.8. Display\n\nQEMU can virtualize a few types of VGA hardware. Some examples are:\n\nstd, the default, emulates a card with Bochs VBE extensions.\n\ncirrus, this was once the default, it emulates a very old hardware module with all its problems. This display type should only be used if really necessary [42], for example, if using Windows XP or earlier\n\nvmware, is a VMWare SVGA-II compatible adapter.\n\nqxl, is the QXL paravirtualized graphics card. Selecting this also enables SPICE (a remote viewer protocol) for the VM.\n\nvirtio-gl, often named VirGL is a virtual 3D GPU for use inside VMs that can offload workloads to the host GPU without requiring special (expensive) models and drivers and neither binding the host GPU completely, allowing reuse between multiple guests and or the host.\n\n\tVirGL support needs some extra libraries that aren’t installed by default due to being relatively big and also not available as open source for all GPU models/vendors. For most setups you’ll just need to do: apt install libgl1 libegl1\n\nYou can edit the amount of memory given to the virtual GPU, by setting the memory option. This can enable higher resolutions inside the VM, especially with SPICE/QXL.\n\nAs the memory is reserved by display device, selecting Multi-Monitor mode for SPICE (such as qxl2 for dual monitors) has some implications:\n\nWindows needs a device for each monitor, so if your ostype is some version of Windows, Proxmox VE gives the VM an extra device per monitor. Each device gets the specified amount of memory.\n\nLinux VMs, can always enable more virtual monitors, but selecting a Multi-Monitor mode multiplies the memory given to the device with the number of monitors.\n\nSelecting serialX as display type disables the VGA output, and redirects the Web Console to the selected serial port. A configured display memory setting will be ignored in that case.\n\nVNC clipboard\n\nYou can enable the VNC clipboard by setting clipboard to vnc.\n\n# qm set <vmid> -vga <displaytype>,clipboard=vnc\n\nIn order to use the clipboard feature, you must first install the SPICE guest tools. On Debian-based distributions, this can be achieved by installing spice-vdagent. For other Operating Systems search for it in the offical repositories or see: https://www.spice-space.org/download.html\n\nOnce you have installed the spice guest tools, you can use the VNC clipboard function (e.g. in the noVNC console panel). However, if you’re using SPICE, virtio or virgl, you’ll need to choose which clipboard to use. This is because the default SPICE clipboard will be replaced by the VNC clipboard, if clipboard is set to vnc.\n\n10.2.9. USB Passthrough\n\nThere are two different types of USB passthrough devices:\n\nHost USB passthrough\n\nSPICE USB passthrough\n\nHost USB passthrough works by giving a VM a USB device of the host. This can either be done via the vendor- and product-id, or via the host bus and port.\n\nThe vendor/product-id looks like this: 0123:abcd, where 0123 is the id of the vendor, and abcd is the id of the product, meaning two pieces of the same usb device have the same id.\n\nThe bus/port looks like this: 1-2.3.4, where 1 is the bus and 2.3.4 is the port path. This represents the physical ports of your host (depending of the internal order of the usb controllers).\n\nIf a device is present in a VM configuration when the VM starts up, but the device is not present in the host, the VM can boot without problems. As soon as the device/port is available in the host, it gets passed through.\n\n\tUsing this kind of USB passthrough means that you cannot move a VM online to another host, since the hardware is only available on the host the VM is currently residing.\n\nThe second type of passthrough is SPICE USB passthrough. If you add one or more SPICE USB ports to your VM, you can dynamically pass a local USB device from your SPICE client through to the VM. This can be useful to redirect an input device or hardware dongle temporarily.\n\nIt is also possible to map devices on a cluster level, so that they can be properly used with HA and hardware changes are detected and non root users can configure them. See Resource Mapping for details on that.\n\n10.2.10. BIOS and UEFI\n\nIn order to properly emulate a computer, QEMU needs to use a firmware. Which, on common PCs often known as BIOS or (U)EFI, is executed as one of the first steps when booting a VM. It is responsible for doing basic hardware initialization and for providing an interface to the firmware and hardware for the operating system. By default QEMU uses SeaBIOS for this, which is an open-source, x86 BIOS implementation. SeaBIOS is a good choice for most standard setups.\n\nSome operating systems (such as Windows 11) may require use of an UEFI compatible implementation. In such cases, you must use OVMF instead, which is an open-source UEFI implementation. [43]\n\nThere are other scenarios in which the SeaBIOS may not be the ideal firmware to boot from, for example if you want to do VGA passthrough. [44]\n\nIf you want to use OVMF, there are several things to consider:\n\nIn order to save things like the boot order, there needs to be an EFI Disk. This disk will be included in backups and snapshots, and there can only be one.\n\nYou can create such a disk with the following command:\n\n# qm set <vmid> -efidisk0 <storage>:1,format=<format>,efitype=4m,pre-enrolled-keys=1\n\nWhere <storage> is the storage where you want to have the disk, and <format> is a format which the storage supports. Alternatively, you can create such a disk through the web interface with Add → EFI Disk in the hardware section of a VM.\n\nThe efitype option specifies which version of the OVMF firmware should be used. For new VMs, this should always be 4m, as it supports Secure Boot and has more space allocated to support future development (this is the default in the GUI).\n\npre-enroll-keys specifies if the efidisk should come pre-loaded with distribution-specific and Microsoft Standard Secure Boot keys. It also enables Secure Boot by default (though it can still be disabled in the OVMF menu within the VM).\n\n\tIf you want to start using Secure Boot in an existing VM (that still uses a 2m efidisk), you need to recreate the efidisk. To do so, delete the old one (qm set <vmid> -delete efidisk0) and add a new one as described above. This will reset any custom configurations you have made in the OVMF menu!\n\nWhen using OVMF with a virtual display (without VGA passthrough), you need to set the client resolution in the OVMF menu (which you can reach with a press of the ESC button during boot), or you have to choose SPICE as the display type.\n\n10.2.11. Trusted Platform Module (TPM)\n\nA Trusted Platform Module is a device which stores secret data - such as encryption keys - securely and provides tamper-resistance functions for validating system boot.\n\nCertain operating systems (such as Windows 11) require such a device to be attached to a machine (be it physical or virtual).\n\nA TPM is added by specifying a tpmstate volume. This works similar to an efidisk, in that it cannot be changed (only removed) once created. You can add one via the following command:\n\n# qm set <vmid> -tpmstate0 <storage>:1,version=<version>\n\nWhere <storage> is the storage you want to put the state on, and <version> is either v1.2 or v2.0. You can also add one via the web interface, by choosing Add → TPM State in the hardware section of a VM.\n\nThe v2.0 TPM spec is newer and better supported, so unless you have a specific implementation that requires a v1.2 TPM, it should be preferred.\n\n\tCompared to a physical TPM, an emulated one does not provide any real security benefits. The point of a TPM is that the data on it cannot be modified easily, except via commands specified as part of the TPM spec. Since with an emulated device the data storage happens on a regular volume, it can potentially be edited by anyone with access to it.\n10.2.12. Inter-VM shared memory\n\nYou can add an Inter-VM shared memory device (ivshmem), which allows one to share memory between the host and a guest, or also between multiple guests.\n\nTo add such a device, you can use qm:\n\n# qm set <vmid> -ivshmem size=32,name=foo\n\nWhere the size is in MiB. The file will be located under /dev/shm/pve-shm-$name (the default name is the vmid).\n\n\tCurrently the device will get deleted as soon as any VM using it got shutdown or stopped. Open connections will still persist, but new connections to the exact same device cannot be made anymore.\n\nA use case for such a device is the Looking Glass [45] project, which enables high performance, low-latency display mirroring between host and guest.\n\n10.2.13. Audio Device\n\nTo add an audio device run the following command:\n\nqm set <vmid> -audio0 device=<device>\n\nSupported audio devices are:\n\nich9-intel-hda: Intel HD Audio Controller, emulates ICH9\n\nintel-hda: Intel HD Audio Controller, emulates ICH6\n\nAC97: Audio Codec '97, useful for older operating systems like Windows XP\n\nThere are two backends available:\n\nspice\n\nnone\n\nThe spice backend can be used in combination with SPICE while the none backend can be useful if an audio device is needed in the VM for some software to work. To use the physical audio device of the host use device passthrough (see PCI Passthrough and USB Passthrough). Remote protocols like Microsoft’s RDP have options to play sound.\n\n10.2.14. VirtIO RNG\n\nA RNG (Random Number Generator) is a device providing entropy (randomness) to a system. A virtual hardware-RNG can be used to provide such entropy from the host system to a guest VM. This helps to avoid entropy starvation problems in the guest (a situation where not enough entropy is available and the system may slow down or run into problems), especially during the guests boot process.\n\nTo add a VirtIO-based emulated RNG, run the following command:\n\nqm set <vmid> -rng0 source=<source>[,max_bytes=X,period=Y]\n\nsource specifies where entropy is read from on the host and has to be one of the following:\n\n/dev/urandom: Non-blocking kernel entropy pool (preferred)\n\n/dev/random: Blocking kernel pool (not recommended, can lead to entropy starvation on the host system)\n\n/dev/hwrng: To pass through a hardware RNG attached to the host (if multiple are available, the one selected in /sys/devices/virtual/misc/hw_random/rng_current will be used)\n\nA limit can be specified via the max_bytes and period parameters, they are read as max_bytes per period in milliseconds. However, it does not represent a linear relationship: 1024B/1000ms would mean that up to 1 KiB of data becomes available on a 1 second timer, not that 1 KiB is streamed to the guest over the course of one second. Reducing the period can thus be used to inject entropy into the guest at a faster rate.\n\nBy default, the limit is set to 1024 bytes per 1000 ms (1 KiB/s). It is recommended to always use a limiter to avoid guests using too many host resources. If desired, a value of 0 for max_bytes can be used to disable all limits.\n\n10.2.15. Device Boot Order\n\nQEMU can tell the guest which devices it should boot from, and in which order. This can be specified in the config via the boot property, for example:\n\nboot: order=scsi0;net0;hostpci0\n\nThis way, the guest would first attempt to boot from the disk scsi0, if that fails, it would go on to attempt network boot from net0, and in case that fails too, finally attempt to boot from a passed through PCIe device (seen as disk in case of NVMe, otherwise tries to launch into an option ROM).\n\nOn the GUI you can use a drag-and-drop editor to specify the boot order, and use the checkbox to enable or disable certain devices for booting altogether.\n\n\tIf your guest uses multiple disks to boot the OS or load the bootloader, all of them must be marked as bootable (that is, they must have the checkbox enabled or appear in the list in the config) for the guest to be able to boot. This is because recent SeaBIOS and OVMF versions only initialize disks if they are marked bootable.\n\nIn any case, even devices not appearing in the list or having the checkmark disabled will still be available to the guest, once it’s operating system has booted and initialized them. The bootable flag only affects the guest BIOS and bootloader.\n\n10.2.16. Automatic Start and Shutdown of Virtual Machines\n\nAfter creating your VMs, you probably want them to start automatically when the host system boots. For this you need to select the option Start at boot from the Options Tab of your VM in the web interface, or set it with the following command:\n\n# qm set <vmid> -onboot 1\nStart and Shutdown Order\n\nIn some case you want to be able to fine tune the boot order of your VMs, for instance if one of your VM is providing firewalling or DHCP to other guest systems. For this you can use the following parameters:\n\nStart/Shutdown order: Defines the start order priority. For example, set it to 1 if you want the VM to be the first to be started. (We use the reverse startup order for shutdown, so a machine with a start order of 1 would be the last to be shut down). If multiple VMs have the same order defined on a host, they will additionally be ordered by VMID in ascending order.\n\nStartup delay: Defines the interval between this VM start and subsequent VMs starts. For example, set it to 240 if you want to wait 240 seconds before starting other VMs.\n\nShutdown timeout: Defines the duration in seconds Proxmox VE should wait for the VM to be offline after issuing a shutdown command. By default this value is set to 180, which means that Proxmox VE will issue a shutdown request and wait 180 seconds for the machine to be offline. If the machine is still online after the timeout it will be stopped forcefully.\n\n\tVMs managed by the HA stack do not follow the start on boot and boot order options currently. Those VMs will be skipped by the startup and shutdown algorithm as the HA manager itself ensures that VMs get started and stopped.\n\nPlease note that machines without a Start/Shutdown order parameter will always start after those where the parameter is set. Further, this parameter can only be enforced between virtual machines running on the same host, not cluster-wide.\n\nIf you require a delay between the host boot and the booting of the first VM, see the section on Proxmox VE Node Management.\n\n10.2.17. QEMU Guest Agent\n\nThe QEMU Guest Agent is a service which runs inside the VM, providing a communication channel between the host and the guest. It is used to exchange information and allows the host to issue commands to the guest.\n\nFor example, the IP addresses in the VM summary panel are fetched via the guest agent.\n\nOr when starting a backup, the guest is told via the guest agent to sync outstanding writes via the fs-freeze and fs-thaw commands.\n\nFor the guest agent to work properly the following steps must be taken:\n\ninstall the agent in the guest and make sure it is running\n\nenable the communication via the agent in Proxmox VE\n\nInstall Guest Agent\n\nFor most Linux distributions, the guest agent is available. The package is usually named qemu-guest-agent.\n\nFor Windows, it can be installed from the Fedora VirtIO driver ISO.\n\nEnable Guest Agent Communication\n\nCommunication from Proxmox VE with the guest agent can be enabled in the VM’s Options panel. A fresh start of the VM is necessary for the changes to take effect.\n\nAutomatic TRIM Using QGA\n\nIt is possible to enable the Run guest-trim option. With this enabled, Proxmox VE will issue a trim command to the guest after the following operations that have the potential to write out zeros to the storage:\n\nmoving a disk to another storage\n\nlive migrating a VM to another node with local storage\n\nOn a thin provisioned storage, this can help to free up unused space.\n\n\tThere is a caveat with ext4 on Linux, because it uses an in-memory optimization to avoid issuing duplicate TRIM requests. Since the guest doesn’t know about the change in the underlying storage, only the first guest-trim will run as expected. Subsequent ones, until the next reboot, will only consider parts of the filesystem that changed since then.\nFilesystem Freeze & Thaw on Backup\n\nBy default, guest filesystems are synced via the fs-freeze QEMU Guest Agent Command when a backup is performed, to provide consistency.\n\nOn Windows guests, some applications might handle consistent backups themselves by hooking into the Windows VSS (Volume Shadow Copy Service) layer, a fs-freeze then might interfere with that. For example, it has been observed that calling fs-freeze with some SQL Servers triggers VSS to call the SQL Writer VSS module in a mode that breaks the SQL Server backup chain for differential backups.\n\nFor such setups you can configure Proxmox VE to not issue a freeze-and-thaw cycle on backup by setting the freeze-fs-on-backup QGA option to 0. This can also be done via the GUI with the Freeze/thaw guest filesystems on backup for consistency option.\n\n\tDisabling this option can potentially lead to backups with inconsistent filesystems and should therefore only be disabled if you know what you are doing.\nTroubleshooting\nVM does not shut down\n\nMake sure the guest agent is installed and running.\n\nOnce the guest agent is enabled, Proxmox VE will send power commands like shutdown via the guest agent. If the guest agent is not running, commands cannot get executed properly and the shutdown command will run into a timeout.\n\n10.2.18. SPICE Enhancements\n\nSPICE Enhancements are optional features that can improve the remote viewer experience.\n\nTo enable them via the GUI go to the Options panel of the virtual machine. Run the following command to enable them via the CLI:\n\nqm set <vmid> -spice_enhancements foldersharing=1,videostreaming=all\n\tTo use these features the Display of the virtual machine must be set to SPICE (qxl).\nFolder Sharing\n\nShare a local folder with the guest. The spice-webdavd daemon needs to be installed in the guest. It makes the shared folder available through a local WebDAV server located at http://localhost:9843.\n\nFor Windows guests the installer for the Spice WebDAV daemon can be downloaded from the official SPICE website.\n\nMost Linux distributions have a package called spice-webdavd that can be installed.\n\nTo share a folder in Virt-Viewer (Remote Viewer) go to File → Preferences. Select the folder to share and then enable the checkbox.\n\n\tFolder sharing currently only works in the Linux version of Virt-Viewer.\n\tExperimental! Currently this feature does not work reliably.\nVideo Streaming\n\nFast refreshing areas are encoded into a video stream. Two options exist:\n\nall: Any fast refreshing area will be encoded into a video stream.\n\nfilter: Additional filters are used to decide if video streaming should be used (currently only small window surfaces are skipped).\n\nA general recommendation if video streaming should be enabled and which option to choose from cannot be given. Your mileage may vary depending on the specific circumstances.\n\nTroubleshooting\nShared folder does not show up\n\nMake sure the WebDAV service is enabled and running in the guest. On Windows it is called Spice webdav proxy. In Linux the name is spice-webdavd but can be different depending on the distribution.\n\nIf the service is running, check the WebDAV server by opening http://localhost:9843 in a browser in the guest.\n\nIt can help to restart the SPICE session.\n\n10.3. Migration\n\nIf you have a cluster, you can migrate your VM to another host with\n\n# qm migrate <vmid> <target>\n\nThere are generally two mechanisms for this\n\nOnline Migration (aka Live Migration)\n\nOffline Migration\n\n10.3.1. Online Migration\n\nIf your VM is running and no locally bound resources are configured (such as devices that are passed through), you can initiate a live migration with the --online flag in the qm migration command evocation. The web interface defaults to live migration when the VM is running.\n\nHow it works\n\nOnline migration first starts a new QEMU process on the target host with the incoming flag, which performs only basic initialization with the guest vCPUs still paused and then waits for the guest memory and device state data streams of the source Virtual Machine. All other resources, such as disks, are either shared or got already sent before runtime state migration of the VMs begins; so only the memory content and device state remain to be transferred.\n\nOnce this connection is established, the source begins asynchronously sending the memory content to the target. If the guest memory on the source changes, those sections are marked dirty and another pass is made to send the guest memory data. This loop is repeated until the data difference between running source VM and incoming target VM is small enough to be sent in a few milliseconds, because then the source VM can be paused completely, without a user or program noticing the pause, so that the remaining data can be sent to the target, and then unpause the targets VM’s CPU to make it the new running VM in well under a second.\n\nRequirements\n\nFor Live Migration to work, there are some things required:\n\nThe VM has no local resources that cannot be migrated. For example, PCI or USB devices that are passed through currently block live-migration. Local Disks, on the other hand, can be migrated by sending them to the target just fine.\n\nThe hosts are located in the same Proxmox VE cluster.\n\nThe hosts have a working (and reliable) network connection between them.\n\nThe target host must have the same, or higher versions of the Proxmox VE packages. Although it can sometimes work the other way around, this cannot be guaranteed.\n\nThe hosts have CPUs from the same vendor with similar capabilities. Different vendor might work depending on the actual models and VMs CPU type configured, but it cannot be guaranteed - so please test before deploying such a setup in production.\n\n10.3.2. Offline Migration\n\nIf you have local resources, you can still migrate your VMs offline as long as all disk are on storage defined on both hosts. Migration then copies the disks to the target host over the network, as with online migration. Note that any hardware passthrough configuration may need to be adapted to the device location on the target host.\n\n10.4. Copies and Clones\n\nVM installation is usually done using an installation media (CD-ROM) from the operating system vendor. Depending on the OS, this can be a time consuming task one might want to avoid.\n\nAn easy way to deploy many VMs of the same type is to copy an existing VM. We use the term clone for such copies, and distinguish between linked and full clones.\n\nFull Clone\n\nThe result of such copy is an independent VM. The new VM does not share any storage resources with the original.\n\nIt is possible to select a Target Storage, so one can use this to migrate a VM to a totally different storage. You can also change the disk image Format if the storage driver supports several formats.\n\n\tA full clone needs to read and copy all VM image data. This is usually much slower than creating a linked clone.\n\nSome storage types allows to copy a specific Snapshot, which defaults to the current VM data. This also means that the final copy never includes any additional snapshots from the original VM.\n\nLinked Clone\n\nModern storage drivers support a way to generate fast linked clones. Such a clone is a writable copy whose initial contents are the same as the original data. Creating a linked clone is nearly instantaneous, and initially consumes no additional space.\n\nThey are called linked because the new image still refers to the original. Unmodified data blocks are read from the original image, but modification are written (and afterwards read) from a new location. This technique is called Copy-on-write.\n\nThis requires that the original volume is read-only. With Proxmox VE one can convert any VM into a read-only Template). Such templates can later be used to create linked clones efficiently.\n\n\tYou cannot delete an original template while linked clones exist.\n\nIt is not possible to change the Target storage for linked clones, because this is a storage internal feature.\n\nThe Target node option allows you to create the new VM on a different node. The only restriction is that the VM is on shared storage, and that storage is also available on the target node.\n\nTo avoid resource conflicts, all network interface MAC addresses get randomized, and we generate a new UUID for the VM BIOS (smbios1) setting.\n\n10.5. Virtual Machine Templates\n\nOne can convert a VM into a Template. Such templates are read-only, and you can use them to create linked clones.\n\n\tIt is not possible to start templates, because this would modify the disk images. If you want to change the template, create a linked clone and modify that.\n10.6. VM Generation ID\n\nProxmox VE supports Virtual Machine Generation ID (vmgenid) [46] for virtual machines. This can be used by the guest operating system to detect any event resulting in a time shift event, for example, restoring a backup or a snapshot rollback.\n\nWhen creating new VMs, a vmgenid will be automatically generated and saved in its configuration file.\n\nTo create and add a vmgenid to an already existing VM one can pass the special value ‘1’ to let Proxmox VE autogenerate one or manually set the UUID [47] by using it as value, for example:\n\n# qm set VMID -vmgenid 1\n# qm set VMID -vmgenid 00000000-0000-0000-0000-000000000000\n\tThe initial addition of a vmgenid device to an existing VM, may result in the same effects as a change on snapshot rollback, backup restore, etc., has as the VM can interpret this as generation change.\n\nIn the rare case the vmgenid mechanism is not wanted one can pass ‘0’ for its value on VM creation, or retroactively delete the property in the configuration with:\n\n# qm set VMID -delete vmgenid\n\nThe most prominent use case for vmgenid are newer Microsoft Windows operating systems, which use it to avoid problems in time sensitive or replicate services (such as databases or domain controller [48]) on snapshot rollback, backup restore or a whole VM clone operation.\n\n10.7. Importing Virtual Machines\n\nImporting existing virtual machines from foreign hypervisors or other Proxmox VE clusters can be achieved through various methods, the most common ones are:\n\nUsing the native import wizard, which utilizes the import content type, such as provided by the ESXi special storage.\n\nPerforming a backup on the source and then restoring on the target. This method works best when migrating from another Proxmox VE instance.\n\nusing the OVF-specific import command of the qm command-line tool.\n\nIf you import VMs to Proxmox VE from other hypervisors, it’s recommended to familiarize yourself with the concepts of Proxmox VE.\n\n10.7.1. Import Wizard\n\nProxmox VE provides an integrated VM importer using the storage plugin system for native integration into the API and web-based user interface. You can use this to import the VM as a whole, with most of its config mapped to Proxmox VE’s config model and reduced downtime.\n\n\tThe import wizard was added during the Proxmox VE 8.2 development cycle and is in tech preview state. While it’s already promising and working stable, it’s still under active development, focusing on adding other import-sources, like for example OVF/OVA files, in the future.\n\nTo use the import wizard you have to first set up a new storage for an import source, you can do so on the web-interface under Datacenter → Storage → Add.\n\nThen you can select the new storage in the resource tree and use the Virtual Guests content tab to see all available guests that can be imported.\n\nSelect one and use the Import button (or double-click) to open the import wizard. You can modify a subset of the available options here and then start the import. Please note that you can do more advanced modifications after the import finished.\n\n\tThe import wizard is currently (2024-03) available for ESXi and has been tested with ESXi versions 6.5 through 8.0. Note that guests using vSAN storage cannot be directly imported directly; their disks must first be moved to another storage. While it is possible to use a vCenter as the import source, performance is dramatically degraded (5 to 10 times slower).\n\nFor a step-by-step guide and tips for how to adapt the virtual guest to the new hyper-visor see our migrate to Proxmox VE wiki article.\n\n10.7.2. Import OVF/OVA Through CLI\n\nA VM export from a foreign hypervisor takes usually the form of one or more disk images, with a configuration file describing the settings of the VM (RAM, number of cores).\nThe disk images can be in the vmdk format, if the disks come from VMware or VirtualBox, or qcow2 if the disks come from a KVM hypervisor. The most popular configuration format for VM exports is the OVF standard, but in practice interoperation is limited because many settings are not implemented in the standard itself, and hypervisors export the supplementary information in non-standard extensions.\n\nBesides the problem of format, importing disk images from other hypervisors may fail if the emulated hardware changes too much from one hypervisor to another. Windows VMs are particularly concerned by this, as the OS is very picky about any changes of hardware. This problem may be solved by installing the MergeIDE.zip utility available from the Internet before exporting and choosing a hard disk type of IDE before booting the imported Windows VM.\n\nFinally there is the question of paravirtualized drivers, which improve the speed of the emulated system and are specific to the hypervisor. GNU/Linux and other free Unix OSes have all the necessary drivers installed by default and you can switch to the paravirtualized drivers right after importing the VM. For Windows VMs, you need to install the Windows paravirtualized drivers by yourself.\n\nGNU/Linux and other free Unix can usually be imported without hassle. Note that we cannot guarantee a successful import/export of Windows VMs in all cases due to the problems above.\n\nStep-by-step example of a Windows OVF import\n\nMicrosoft provides Virtual Machines downloads to get started with Windows development.We are going to use one of these to demonstrate the OVF import feature.\n\nDownload the Virtual Machine zip\n\nAfter getting informed about the user agreement, choose the Windows 10 Enterprise (Evaluation - Build) for the VMware platform, and download the zip.\n\nExtract the disk image from the zip\n\nUsing the unzip utility or any archiver of your choice, unpack the zip, and copy via ssh/scp the ovf and vmdk files to your Proxmox VE host.\n\nImport the Virtual Machine\n\nThis will create a new virtual machine, using cores, memory and VM name as read from the OVF manifest, and import the disks to the local-lvm storage. You have to configure the network manually.\n\n# qm importovf 999 WinDev1709Eval.ovf local-lvm\n\nThe VM is ready to be started.\n\nAdding an external disk image to a Virtual Machine\n\nYou can also add an existing disk image to a VM, either coming from a foreign hypervisor, or one that you created yourself.\n\nSuppose you created a Debian/Ubuntu disk image with the vmdebootstrap tool:\n\nvmdebootstrap --verbose \\\n --size 10GiB --serial-console \\\n --grub --no-extlinux \\\n --package openssh-server \\\n --package avahi-daemon \\\n --package qemu-guest-agent \\\n --hostname vm600 --enable-dhcp \\\n --customize=./copy_pub_ssh.sh \\\n --sparse --image vm600.raw\n\nYou can now create a new target VM, importing the image to the storage pvedir and attaching it to the VM’s SCSI controller:\n\n# qm create 600 --net0 virtio,bridge=vmbr0 --name vm600 --serial0 socket \\\n   --boot order=scsi0 --scsihw virtio-scsi-pci --ostype l26 \\\n   --scsi0 pvedir:0,import-from=/path/to/dir/vm600.raw\n\nThe VM is ready to be started.\n\n10.8. Cloud-Init Support\n\nCloud-Init is the de facto multi-distribution package that handles early initialization of a virtual machine instance. Using Cloud-Init, configuration of network devices and ssh keys on the hypervisor side is possible. When the VM starts for the first time, the Cloud-Init software inside the VM will apply those settings.\n\nMany Linux distributions provide ready-to-use Cloud-Init images, mostly designed for OpenStack. These images will also work with Proxmox VE. While it may seem convenient to get such ready-to-use images, we usually recommended to prepare the images by yourself. The advantage is that you will know exactly what you have installed, and this helps you later to easily customize the image for your needs.\n\nOnce you have created such a Cloud-Init image we recommend to convert it into a VM template. From a VM template you can quickly create linked clones, so this is a fast method to roll out new VM instances. You just need to configure the network (and maybe the ssh keys) before you start the new VM.\n\nWe recommend using SSH key-based authentication to login to the VMs provisioned by Cloud-Init. It is also possible to set a password, but this is not as safe as using SSH key-based authentication because Proxmox VE needs to store an encrypted version of that password inside the Cloud-Init data.\n\nProxmox VE generates an ISO image to pass the Cloud-Init data to the VM. For that purpose, all Cloud-Init VMs need to have an assigned CD-ROM drive. Usually, a serial console should be added and used as a display. Many Cloud-Init images rely on this, it is a requirement for OpenStack. However, other images might have problems with this configuration. Switch back to the default display configuration if using a serial console doesn’t work.\n\n10.8.1. Preparing Cloud-Init Templates\n\nThe first step is to prepare your VM. Basically you can use any VM. Simply install the Cloud-Init packages inside the VM that you want to prepare. On Debian/Ubuntu based systems this is as simple as:\n\napt-get install cloud-init\n\tThis command is not intended to be executed on the Proxmox VE host, but only inside the VM.\n\nAlready many distributions provide ready-to-use Cloud-Init images (provided as .qcow2 files), so alternatively you can simply download and import such images. For the following example, we will use the cloud image provided by Ubuntu at https://cloud-images.ubuntu.com.\n\n# download the image\nwget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img\n\n# create a new VM with VirtIO SCSI controller\nqm create 9000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci\n\n# import the downloaded disk to the local-lvm storage, attaching it as a SCSI drive\nqm set 9000 --scsi0 local-lvm:0,import-from=/path/to/bionic-server-cloudimg-amd64.img\n\tUbuntu Cloud-Init images require the virtio-scsi-pci controller type for SCSI drives.\nAdd Cloud-Init CD-ROM drive\n\nThe next step is to configure a CD-ROM drive, which will be used to pass the Cloud-Init data to the VM.\n\nqm set 9000 --ide2 local-lvm:cloudinit\n\nTo be able to boot directly from the Cloud-Init image, set the boot parameter to order=scsi0 to restrict BIOS to boot from this disk only. This will speed up booting, because VM BIOS skips the testing for a bootable CD-ROM.\n\nqm set 9000 --boot order=scsi0\n\nFor many Cloud-Init images, it is required to configure a serial console and use it as a display. If the configuration doesn’t work for a given image however, switch back to the default display instead.\n\nqm set 9000 --serial0 socket --vga serial0\n\nIn a last step, it is helpful to convert the VM into a template. From this template you can then quickly create linked clones. The deployment from VM templates is much faster than creating a full clone (copy).\n\nqm template 9000\n10.8.2. Deploying Cloud-Init Templates\n\nYou can easily deploy such a template by cloning:\n\nqm clone 9000 123 --name ubuntu2\n\nThen configure the SSH public key used for authentication, and configure the IP setup:\n\nqm set 123 --sshkey ~/.ssh/id_rsa.pub\nqm set 123 --ipconfig0 ip=10.0.10.123/24,gw=10.0.10.1\n\nYou can also configure all the Cloud-Init options using a single command only. We have simply split the above example to separate the commands for reducing the line length. Also make sure to adopt the IP setup for your specific environment.\n\n10.8.3. Custom Cloud-Init Configuration\n\nThe Cloud-Init integration also allows custom config files to be used instead of the automatically generated configs. This is done via the cicustom option on the command line:\n\nqm set 9000 --cicustom \"user=<volume>,network=<volume>,meta=<volume>\"\n\nThe custom config files have to be on a storage that supports snippets and have to be available on all nodes the VM is going to be migrated to. Otherwise the VM won’t be able to start. For example:\n\nqm set 9000 --cicustom \"user=local:snippets/userconfig.yaml\"\n\nThere are three kinds of configs for Cloud-Init. The first one is the user config as seen in the example above. The second is the network config and the third the meta config. They can all be specified together or mixed and matched however needed. The automatically generated config will be used for any that don’t have a custom config file specified.\n\nThe generated config can be dumped to serve as a base for custom configs:\n\nqm cloudinit dump 9000 user\n\nThe same command exists for network and meta.\n\n10.8.4. Cloud-Init specific Options\ncicustom: [meta=<volume>] [,network=<volume>] [,user=<volume>] [,vendor=<volume>]\n\nSpecify custom files to replace the automatically generated ones at start.\n\nmeta=<volume>\n\nSpecify a custom file containing all meta data passed to the VM via\" .\" cloud-init. This is provider specific meaning configdrive2 and nocloud differ.\n\nnetwork=<volume>\n\nTo pass a custom file containing all network data to the VM via cloud-init.\n\nuser=<volume>\n\nTo pass a custom file containing all user data to the VM via cloud-init.\n\nvendor=<volume>\n\nTo pass a custom file containing all vendor data to the VM via cloud-init.\n\ncipassword: <string>\n\nPassword to assign the user. Using this is generally not recommended. Use ssh keys instead. Also note that older cloud-init versions do not support hashed passwords.\n\ncitype: <configdrive2 | nocloud | opennebula>\n\nSpecifies the cloud-init configuration format. The default depends on the configured operating system type (ostype. We use the nocloud format for Linux, and configdrive2 for windows.\n\nciupgrade: <boolean> (default = 1)\n\ndo an automatic package upgrade after the first boot.\n\nciuser: <string>\n\nUser name to change ssh keys and password for instead of the image’s configured default user.\n\nipconfig[n]: [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]\n\nSpecify IP addresses and gateways for the corresponding interface.\n\nIP addresses use CIDR notation, gateways are optional but need an IP of the same type specified.\n\nThe special string dhcp can be used for IP addresses to use DHCP, in which case no explicit gateway should be provided. For IPv6 the special string auto can be used to use stateless autoconfiguration. This requires cloud-init 19.4 or newer.\n\nIf cloud-init is enabled and neither an IPv4 nor an IPv6 address is specified, it defaults to using dhcp on IPv4.\n\ngw=<GatewayIPv4>\n\nDefault gateway for IPv4 traffic.\n\n\tRequires option(s): ip\ngw6=<GatewayIPv6>\n\nDefault gateway for IPv6 traffic.\n\n\tRequires option(s): ip6\nip=<IPv4Format/CIDR> (default = dhcp)\n\nIPv4 address in CIDR format.\n\nip6=<IPv6Format/CIDR> (default = dhcp)\n\nIPv6 address in CIDR format.\n\nnameserver: <string>\n\nSets DNS server IP address for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\nsearchdomain: <string>\n\nSets DNS search domains for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\nsshkeys: <string>\n\nSetup public SSH keys (one key per line, OpenSSH format).\n\n10.9. PCI(e) Passthrough\n\nPCI(e) passthrough is a mechanism to give a virtual machine control over a PCI device from the host. This can have some advantages over using virtualized hardware, for example lower latency, higher performance, or more features (e.g., offloading).\n\nBut, if you pass through a device to a virtual machine, you cannot use that device anymore on the host or in any other VM.\n\nNote that, while PCI passthrough is available for i440fx and q35 machines, PCIe passthrough is only available on q35 machines. This does not mean that PCIe capable devices that are passed through as PCI devices will only run at PCI speeds. Passing through devices as PCIe just sets a flag for the guest to tell it that the device is a PCIe device instead of a \"really fast legacy PCI device\". Some guest applications benefit from this.\n\n10.9.1. General Requirements\n\nSince passthrough is performed on real hardware, it needs to fulfill some requirements. A brief overview of these requirements is given below, for more information on specific devices, see PCI Passthrough Examples.\n\nHardware\n\nYour hardware needs to support IOMMU (I/O Memory Management Unit) interrupt remapping, this includes the CPU and the motherboard.\n\nGenerally, Intel systems with VT-d and AMD systems with AMD-Vi support this. But it is not guaranteed that everything will work out of the box, due to bad hardware implementation and missing or low quality drivers.\n\nFurther, server grade hardware has often better support than consumer grade hardware, but even then, many modern system can support this.\n\nPlease refer to your hardware vendor to check if they support this feature under Linux for your specific setup.\n\nDetermining PCI Card Address\n\nThe easiest way is to use the GUI to add a device of type \"Host PCI\" in the VM’s hardware tab. Alternatively, you can use the command line.\n\nYou can locate your card using\n\n lspci\nConfiguration\n\nOnce you ensured that your hardware supports passthrough, you will need to do some configuration to enable PCI(e) passthrough.\n\nIOMMU\n\nFirst, you will have to enable IOMMU support in your BIOS/UEFI. Usually the corresponding setting is called IOMMU or VT-d, but you should find the exact option name in the manual of your motherboard.\n\nFor Intel CPUs, you also need to enable the IOMMU on the kernel command line kernels by adding:\n\n intel_iommu=on\n\nFor AMD CPUs it should be enabled automatically.\n\nIOMMU Passthrough Mode\n\nIf your hardware supports IOMMU passthrough mode, enabling this mode might increase performance. This is because VMs then bypass the (default) DMA translation normally performed by the hyper-visor and instead pass DMA requests directly to the hardware IOMMU. To enable these options, add:\n\n iommu=pt\n\nto the kernel commandline.\n\nKernel Modules\n\nYou have to make sure the following modules are loaded. This can be achieved by adding them to ‘/etc/modules’. In kernels newer than 6.2 (Proxmox VE 8 and onward) the vfio_virqfd module is part of the vfio module, therefore loading vfio_virqfd in Proxmox VE 8 and newer is not necessary.\n\n vfio\n vfio_iommu_type1\n vfio_pci\n vfio_virqfd #not needed if on kernel 6.2 or newer\n\nAfter changing anything modules related, you need to refresh your initramfs. On Proxmox VE this can be done by executing:\n\n# update-initramfs -u -k all\n\nTo check if the modules are being loaded, the output of\n\n# lsmod | grep vfio\n\nshould include the four modules from above.\n\nFinish Configuration\n\nFinally reboot to bring the changes into effect and check that it is indeed enabled.\n\n# dmesg | grep -e DMAR -e IOMMU -e AMD-Vi\n\nshould display that IOMMU, Directed I/O or Interrupt Remapping is enabled, depending on hardware and kernel the exact message can vary.\n\nFor notes on how to troubleshoot or verify if IOMMU is working as intended, please see the Verifying IOMMU Parameters section in our wiki.\n\nIt is also important that the device(s) you want to pass through are in a separate IOMMU group. This can be checked with a call to the Proxmox VE API:\n\n# pvesh get /nodes/{nodename}/hardware/pci --pci-class-blacklist \"\"\n\nIt is okay if the device is in an IOMMU group together with its functions (e.g. a GPU with the HDMI Audio device) or with its root port or PCI(e) bridge.\n\n\t\nPCI(e) slots\n\nSome platforms handle their physical PCI(e) slots differently. So, sometimes it can help to put the card in a another PCI(e) slot, if you do not get the desired IOMMU group separation.\n\n\t\nUnsafe interrupts\n\nFor some platforms, it may be necessary to allow unsafe interrupts. For this add the following line in a file ending with ‘.conf’ file in /etc/modprobe.d/:\n\n options vfio_iommu_type1 allow_unsafe_interrupts=1\n\nPlease be aware that this option can make your system unstable.\n\nGPU Passthrough Notes\n\nIt is not possible to display the frame buffer of the GPU via NoVNC or SPICE on the Proxmox VE web interface.\n\nWhen passing through a whole GPU or a vGPU and graphic output is wanted, one has to either physically connect a monitor to the card, or configure a remote desktop software (for example, VNC or RDP) inside the guest.\n\nIf you want to use the GPU as a hardware accelerator, for example, for programs using OpenCL or CUDA, this is not required.\n\n10.9.2. Host Device Passthrough\n\nThe most used variant of PCI(e) passthrough is to pass through a whole PCI(e) card, for example a GPU or a network card.\n\nHost Configuration\n\nProxmox VE tries to automatically make the PCI(e) device unavailable for the host. However, if this doesn’t work, there are two things that can be done:\n\npass the device IDs to the options of the vfio-pci modules by adding\n\n options vfio-pci ids=1234:5678,4321:8765\n\nto a .conf file in /etc/modprobe.d/ where 1234:5678 and 4321:8765 are the vendor and device IDs obtained by:\n\n# lspci -nn\n\nblacklist the driver on the host completely, ensuring that it is free to bind for passthrough, with\n\n blacklist DRIVERNAME\n\nin a .conf file in /etc/modprobe.d/.\n\nTo find the drivername, execute\n\n# lspci -k\n\nfor example:\n\n# lspci -k | grep -A 3 \"VGA\"\n\nwill output something similar to\n\n01:00.0 VGA compatible controller: NVIDIA Corporation GP108 [GeForce GT 1030] (rev a1)\n        Subsystem: Micro-Star International Co., Ltd. [MSI] GP108 [GeForce GT 1030]\n        Kernel driver in use: <some-module>\n        Kernel modules: <some-module>\n\nNow we can blacklist the drivers by writing them into a .conf file:\n\necho \"blacklist <some-module>\" >> /etc/modprobe.d/blacklist.conf\n\nFor both methods you need to update the initramfs again and reboot after that.\n\nShould this not work, you might need to set a soft dependency to load the gpu modules before loading vfio-pci. This can be done with the softdep flag, see also the manpages on modprobe.d for more information.\n\nFor example, if you are using drivers named <some-module>:\n\n# echo \"softdep <some-module> pre: vfio-pci\" >> /etc/modprobe.d/<some-module>.conf\nVerify Configuration\n\nTo check if your changes were successful, you can use\n\n# lspci -nnk\n\nand check your device entry. If it says\n\nKernel driver in use: vfio-pci\n\nor the in use line is missing entirely, the device is ready to be used for passthrough.\n\nVM Configuration\n\nWhen passing through a GPU, the best compatibility is reached when using q35 as machine type, OVMF (UEFI for VMs) instead of SeaBIOS and PCIe instead of PCI. Note that if you want to use OVMF for GPU passthrough, the GPU needs to have an UEFI capable ROM, otherwise use SeaBIOS instead. To check if the ROM is UEFI capable, see the PCI Passthrough Examples wiki.\n\nFurthermore, using OVMF, disabling vga arbitration may be possible, reducing the amount of legacy code needed to be run during boot. To disable vga arbitration:\n\n echo \"options vfio-pci ids=<vendor-id>,<device-id> disable_vga=1\" > /etc/modprobe.d/vfio.conf\n\nreplacing the <vendor-id> and <device-id> with the ones obtained from:\n\n# lspci -nn\n\nPCI devices can be added in the web interface in the hardware section of the VM. Alternatively, you can use the command line; set the hostpciX option in the VM configuration, for example by executing:\n\n# qm set VMID -hostpci0 00:02.0\n\nor by adding a line to the VM configuration file:\n\n hostpci0: 00:02.0\n\nIf your device has multiple functions (e.g., ‘00:02.0’ and ‘00:02.1’ ), you can pass them through all together with the shortened syntax ``00:02`. This is equivalent with checking the ``All Functions` checkbox in the web interface.\n\nThere are some options to which may be necessary, depending on the device and guest OS:\n\nx-vga=on|off marks the PCI(e) device as the primary GPU of the VM. With this enabled the vga configuration option will be ignored.\n\npcie=on|off tells Proxmox VE to use a PCIe or PCI port. Some guests/device combination require PCIe rather than PCI. PCIe is only available for q35 machine types.\n\nrombar=on|off makes the firmware ROM visible for the guest. Default is on. Some PCI(e) devices need this disabled.\n\nromfile=<path>, is an optional path to a ROM file for the device to use. This is a relative path under /usr/share/kvm/.\n\nExample\n\nAn example of PCIe passthrough with a GPU set to primary:\n\n# qm set VMID -hostpci0 02:00,pcie=on,x-vga=on\nPCI ID overrides\n\nYou can override the PCI vendor ID, device ID, and subsystem IDs that will be seen by the guest. This is useful if your device is a variant with an ID that your guest’s drivers don’t recognize, but you want to force those drivers to be loaded anyway (e.g. if you know your device shares the same chipset as a supported variant).\n\nThe available options are vendor-id, device-id, sub-vendor-id, and sub-device-id. You can set any or all of these to override your device’s default IDs.\n\nFor example:\n\n# qm set VMID -hostpci0 02:00,device-id=0x10f6,sub-vendor-id=0x0000\n10.9.3. SR-IOV\n\nAnother variant for passing through PCI(e) devices is to use the hardware virtualization features of your devices, if available.\n\n\t\nEnabling SR-IOV\n\nTo use SR-IOV, platform support is especially important. It may be necessary to enable this feature in the BIOS/UEFI first, or to use a specific PCI(e) port for it to work. In doubt, consult the manual of the platform or contact its vendor.\n\nSR-IOV (Single-Root Input/Output Virtualization) enables a single device to provide multiple VF (Virtual Functions) to the system. Each of those VF can be used in a different VM, with full hardware features and also better performance and lower latency than software virtualized devices.\n\nCurrently, the most common use case for this are NICs (Network Interface Card) with SR-IOV support, which can provide multiple VFs per physical port. This allows using features such as checksum offloading, etc. to be used inside a VM, reducing the (host) CPU overhead.\n\nHost Configuration\n\nGenerally, there are two methods for enabling virtual functions on a device.\n\nsometimes there is an option for the driver module e.g. for some Intel drivers\n\n max_vfs=4\n\nwhich could be put file with .conf ending under /etc/modprobe.d/. (Do not forget to update your initramfs after that)\n\nPlease refer to your driver module documentation for the exact parameters and options.\n\nThe second, more generic, approach is using the sysfs. If a device and driver supports this you can change the number of VFs on the fly. For example, to setup 4 VFs on device 0000:01:00.0 execute:\n\n# echo 4 > /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs\n\nTo make this change persistent you can use the ‘sysfsutils` Debian package. After installation configure it via /etc/sysfs.conf or a `FILE.conf’ in /etc/sysfs.d/.\n\nVM Configuration\n\nAfter creating VFs, you should see them as separate PCI(e) devices when outputting them with lspci. Get their ID and pass them through like a normal PCI(e) device.\n\n10.9.4. Mediated Devices (vGPU, GVT-g)\n\nMediated devices are another method to reuse features and performance from physical hardware for virtualized hardware. These are found most common in virtualized GPU setups such as Intel’s GVT-g and NVIDIA’s vGPUs used in their GRID technology.\n\nWith this, a physical Card is able to create virtual cards, similar to SR-IOV. The difference is that mediated devices do not appear as PCI(e) devices in the host, and are such only suited for using in virtual machines.\n\nHost Configuration\n\nIn general your card’s driver must support that feature, otherwise it will not work. So please refer to your vendor for compatible drivers and how to configure them.\n\nIntel’s drivers for GVT-g are integrated in the Kernel and should work with 5th, 6th and 7th generation Intel Core Processors, as well as E3 v4, E3 v5 and E3 v6 Xeon Processors.\n\nTo enable it for Intel Graphics, you have to make sure to load the module kvmgt (for example via /etc/modules) and to enable it on the Kernel commandline and add the following parameter:\n\n i915.enable_gvt=1\n\nAfter that remember to update the initramfs, and reboot your host.\n\nVM Configuration\n\nTo use a mediated device, simply specify the mdev property on a hostpciX VM configuration option.\n\nYou can get the supported devices via the sysfs. For example, to list the supported types for the device 0000:00:02.0 you would simply execute:\n\n# ls /sys/bus/pci/devices/0000:00:02.0/mdev_supported_types\n\nEach entry is a directory which contains the following important files:\n\navailable_instances contains the amount of still available instances of this type, each mdev use in a VM reduces this.\n\ndescription contains a short description about the capabilities of the type\n\ncreate is the endpoint to create such a device, Proxmox VE does this automatically for you, if a hostpciX option with mdev is configured.\n\nExample configuration with an Intel GVT-g vGPU (Intel Skylake 6700k):\n\n# qm set VMID -hostpci0 00:02.0,mdev=i915-GVTg_V5_4\n\nWith this set, Proxmox VE automatically creates such a device on VM start, and cleans it up again when the VM stops.\n\n10.9.5. Use in Clusters\n\nIt is also possible to map devices on a cluster level, so that they can be properly used with HA and hardware changes are detected and non root users can configure them. See Resource Mapping for details on that.\n\n10.9.6. vIOMMU (emulated IOMMU)\n\nvIOMMU is the emulation of a hardware IOMMU within a virtual machine, providing improved memory access control and security for virtualized I/O devices. Using the vIOMMU option also allows you to pass through PCI devices to level-2 VMs in level-1 VMs via Nested Virtualization. There are currently two vIOMMU implementations available: Intel and VirtIO.\n\nHost requirement:\n\nAdd intel_iommu=on or amd_iommu=on depending on your CPU to your kernel command line.\n\nIntel vIOMMU\n\nIntel vIOMMU specific VM requirements:\n\nWhether you are using an Intel or AMD CPU on your host, it is important to set intel_iommu=on in the VMs kernel parameters.\n\nTo use Intel vIOMMU you need to set q35 as the machine type.\n\nIf all requirements are met, you can add viommu=intel to the machine parameter in the configuration of the VM that should be able to pass through PCI devices.\n\n# qm set VMID -machine q35,viommu=intel\n\nQEMU documentation for VT-d\n\nVirtIO vIOMMU\n\nThis vIOMMU implementation is more recent and does not have as many limitations as Intel vIOMMU but is currently less used in production and less documentated.\n\nWith VirtIO vIOMMU there is no need to set any kernel parameters. It is also not necessary to use q35 as the machine type, but it is advisable if you want to use PCIe.\n\n# qm set VMID -machine q35,viommu=virtio\n\nBlog-Post by Michael Zhao explaining virtio-iommu\n\n10.10. Hookscripts\n\nYou can add a hook script to VMs with the config property hookscript.\n\n# qm set 100 --hookscript local:snippets/hookscript.pl\n\nIt will be called during various phases of the guests lifetime. For an example and documentation see the example script under /usr/share/pve-docs/examples/guest-example-hookscript.pl.\n\n10.11. Hibernation\n\nYou can suspend a VM to disk with the GUI option Hibernate or with\n\n# qm suspend ID --todisk\n\nThat means that the current content of the memory will be saved onto disk and the VM gets stopped. On the next start, the memory content will be loaded and the VM can continue where it was left off.\n\nState storage selection\n\nIf no target storage for the memory is given, it will be automatically chosen, the first of:\n\nThe storage vmstatestorage from the VM config.\n\nThe first shared storage from any VM disk.\n\nThe first non-shared storage from any VM disk.\n\nThe storage local as a fallback.\n\n10.12. Resource Mapping\n\nWhen using or referencing local resources (e.g. address of a pci device), using the raw address or id is sometimes problematic, for example:\n\nwhen using HA, a different device with the same id or path may exist on the target node, and if one is not careful when assigning such guests to HA groups, the wrong device could be used, breaking configurations.\n\nchanging hardware can change ids and paths, so one would have to check all assigned devices and see if the path or id is still correct.\n\nTo handle this better, one can define cluster wide resource mappings, such that a resource has a cluster unique, user selected identifier which can correspond to different devices on different hosts. With this, HA won’t start a guest with a wrong device, and hardware changes can be detected.\n\nCreating such a mapping can be done with the Proxmox VE web GUI under Datacenter in the relevant tab in the Resource Mappings category, or on the cli with\n\n# pvesh create /cluster/mapping/<type> <options>\n\nWhere <type> is the hardware type (currently either pci or usb) and <options> are the device mappings and other configuration parameters.\n\nNote that the options must include a map property with all identifying properties of that hardware, so that it’s possible to verify the hardware did not change and the correct device is passed through.\n\nFor example to add a PCI device as device1 with the path 0000:01:00.0 that has the device id 0001 and the vendor id 0002 on the node node1, and 0000:02:00.0 on node2 you can add it with:\n\n# pvesh create /cluster/mapping/pci --id device1 \\\n --map node=node1,path=0000:01:00.0,id=0002:0001 \\\n --map node=node2,path=0000:02:00.0,id=0002:0001\n\nYou must repeat the map parameter for each node where that device should have a mapping (note that you can currently only map one USB device per node per mapping).\n\nUsing the GUI makes this much easier, as the correct properties are automatically picked up and sent to the API.\n\nIt’s also possible for PCI devices to provide multiple devices per node with multiple map properties for the nodes. If such a device is assigned to a guest, the first free one will be used when the guest is started. The order of the paths given is also the order in which they are tried, so arbitrary allocation policies can be implemented.\n\nThis is useful for devices with SR-IOV, since some times it is not important which exact virtual function is passed through.\n\nYou can assign such a device to a guest either with the GUI or with\n\n# qm set ID -hostpci0 <name>\n\nfor PCI devices, or\n\n# qm set <vmid> -usb0 <name>\n\nfor USB devices.\n\nWhere <vmid> is the guests id and <name> is the chosen name for the created mapping. All usual options for passing through the devices are allowed, such as mdev.\n\nTo create mappings Mapping.Modify on /mapping/<type>/<name> is necessary (where <type> is the device type and <name> is the name of the mapping).\n\nTo use these mappings, Mapping.Use on /mapping/<type>/<name> is necessary (in addition to the normal guest privileges to edit the configuration).\n\n10.13. Managing Virtual Machines with qm\n\nqm is the tool to manage QEMU/KVM virtual machines on Proxmox VE. You can create and destroy virtual machines, and control execution (start/stop/suspend/resume). Besides that, you can use qm to set parameters in the associated config file. It is also possible to create and delete virtual disks.\n\n10.13.1. CLI Usage Examples\n\nUsing an iso file uploaded on the local storage, create a VM with a 4 GB IDE disk on the local-lvm storage\n\n# qm create 300 -ide0 local-lvm:4 -net0 e1000 -cdrom local:iso/proxmox-mailgateway_2.1.iso\n\nStart the new VM\n\n# qm start 300\n\nSend a shutdown request, then wait until the VM is stopped.\n\n# qm shutdown 300 && qm wait 300\n\nSame as above, but only wait for 40 seconds.\n\n# qm shutdown 300 && qm wait 300 -timeout 40\n\nIf the VM does not shut down, force-stop it and overrule any running shutdown tasks. As stopping VMs may incur data loss, use it with caution.\n\n# qm stop 300 -overrule-shutdown 1\n\nDestroying a VM always removes it from Access Control Lists and it always removes the firewall configuration of the VM. You have to activate --purge, if you want to additionally remove the VM from replication jobs, backup jobs and HA resource configurations.\n\n# qm destroy 300 --purge\n\nMove a disk image to a different storage.\n\n# qm move-disk 300 scsi0 other-storage\n\nReassign a disk image to a different VM. This will remove the disk scsi1 from the source VM and attaches it as scsi3 to the target VM. In the background the disk image is being renamed so that the name matches the new owner.\n\n# qm move-disk 300 scsi1 --target-vmid 400 --target-disk scsi3\n10.14. Configuration\n\nVM configuration files are stored inside the Proxmox cluster file system, and can be accessed at /etc/pve/qemu-server/<VMID>.conf. Like other files stored inside /etc/pve/, they get automatically replicated to all other cluster nodes.\n\n\tVMIDs < 100 are reserved for internal purposes, and VMIDs need to be unique cluster wide.\nExample VM Configuration\nboot: order=virtio0;net0\ncores: 1\nsockets: 1\nmemory: 512\nname: webmail\nostype: l26\nnet0: e1000=EE:D2:28:5F:B6:3E,bridge=vmbr0\nvirtio0: local:vm-100-disk-1,size=32G\n\nThose configuration files are simple text files, and you can edit them using a normal text editor (vi, nano, …). This is sometimes useful to do small corrections, but keep in mind that you need to restart the VM to apply such changes.\n\nFor that reason, it is usually better to use the qm command to generate and modify those files, or do the whole thing using the GUI. Our toolkit is smart enough to instantaneously apply most changes to running VM. This feature is called \"hot plug\", and there is no need to restart the VM in that case.\n\n10.14.1. File Format\n\nVM configuration files use a simple colon separated key/value format. Each line has the following format:\n\n# this is a comment\nOPTION: value\n\nBlank lines in those files are ignored, and lines starting with a # character are treated as comments and are also ignored.\n\n10.14.2. Snapshots\n\nWhen you create a snapshot, qm stores the configuration at snapshot time into a separate snapshot section within the same configuration file. For example, after creating a snapshot called “testsnapshot”, your configuration file will look like this:\n\nVM configuration with snapshot\nmemory: 512\nswap: 512\nparent: testsnaphot\n...\n\n[testsnaphot]\nmemory: 512\nswap: 512\nsnaptime: 1457170803\n...\n\nThere are a few snapshot related properties like parent and snaptime. The parent property is used to store the parent/child relationship between snapshots. snaptime is the snapshot creation time stamp (Unix epoch).\n\nYou can optionally save the memory of a running VM with the option vmstate. For details about how the target storage gets chosen for the VM state, see State storage selection in the chapter Hibernation.\n\n10.14.3. Options\nacpi: <boolean> (default = 1)\n\nEnable/disable ACPI.\n\naffinity: <string>\n\nList of host cores used to execute guest processes, for example: 0,5,8-11\n\nagent: [enabled=]<1|0> [,freeze-fs-on-backup=<1|0>] [,fstrim_cloned_disks=<1|0>] [,type=<virtio|isa>]\n\nEnable/disable communication with the QEMU Guest Agent and its properties.\n\nenabled=<boolean> (default = 0)\n\nEnable/disable communication with a QEMU Guest Agent (QGA) running in the VM.\n\nfreeze-fs-on-backup=<boolean> (default = 1)\n\nFreeze/thaw guest filesystems on backup for consistency.\n\nfstrim_cloned_disks=<boolean> (default = 0)\n\nRun fstrim after moving a disk or migrating the VM.\n\ntype=<isa | virtio> (default = virtio)\n\nSelect the agent type\n\narch: <aarch64 | x86_64>\n\nVirtual processor architecture. Defaults to the host.\n\nargs: <string>\n\nArbitrary arguments passed to kvm, for example:\n\nargs: -no-reboot -smbios type=0,vendor=FOO\n\n\tthis option is for experts only.\naudio0: device=<ich9-intel-hda|intel-hda|AC97> [,driver=<spice|none>]\n\nConfigure a audio device, useful in combination with QXL/Spice.\n\ndevice=<AC97 | ich9-intel-hda | intel-hda>\n\nConfigure an audio device.\n\ndriver=<none | spice> (default = spice)\n\nDriver backend for the audio device.\n\nautostart: <boolean> (default = 0)\n\nAutomatic restart after crash (currently ignored).\n\nballoon: <integer> (0 - N)\n\nAmount of target RAM for the VM in MiB. Using zero disables the ballon driver.\n\nbios: <ovmf | seabios> (default = seabios)\n\nSelect BIOS implementation.\n\nboot: [[legacy=]<[acdn]{1,4}>] [,order=<device[;device...]>]\n\nSpecify guest boot order. Use the order= sub-property as usage with no key or legacy= is deprecated.\n\nlegacy=<[acdn]{1,4}> (default = cdn)\n\nBoot on floppy (a), hard disk (c), CD-ROM (d), or network (n). Deprecated, use order= instead.\n\norder=<device[;device...]>\n\nThe guest will attempt to boot from devices in the order they appear here.\n\nDisks, optical drives and passed-through storage USB devices will be directly booted from, NICs will load PXE, and PCIe devices will either behave like disks (e.g. NVMe) or load an option ROM (e.g. RAID controller, hardware NIC).\n\nNote that only devices in this list will be marked as bootable and thus loaded by the guest firmware (BIOS/UEFI). If you require multiple disks for booting (e.g. software-raid), you need to specify all of them here.\n\nOverrides the deprecated legacy=[acdn]* value when given.\n\nbootdisk: (ide|sata|scsi|virtio)\\d+\n\nEnable booting from specified disk. Deprecated: Use boot: order=foo;bar instead.\n\ncdrom: <volume>\n\nThis is an alias for option -ide2\n\ncicustom: [meta=<volume>] [,network=<volume>] [,user=<volume>] [,vendor=<volume>]\n\ncloud-init: Specify custom files to replace the automatically generated ones at start.\n\nmeta=<volume>\n\nSpecify a custom file containing all meta data passed to the VM via\" .\" cloud-init. This is provider specific meaning configdrive2 and nocloud differ.\n\nnetwork=<volume>\n\nTo pass a custom file containing all network data to the VM via cloud-init.\n\nuser=<volume>\n\nTo pass a custom file containing all user data to the VM via cloud-init.\n\nvendor=<volume>\n\nTo pass a custom file containing all vendor data to the VM via cloud-init.\n\ncipassword: <string>\n\ncloud-init: Password to assign the user. Using this is generally not recommended. Use ssh keys instead. Also note that older cloud-init versions do not support hashed passwords.\n\ncitype: <configdrive2 | nocloud | opennebula>\n\nSpecifies the cloud-init configuration format. The default depends on the configured operating system type (ostype. We use the nocloud format for Linux, and configdrive2 for windows.\n\nciupgrade: <boolean> (default = 1)\n\ncloud-init: do an automatic package upgrade after the first boot.\n\nciuser: <string>\n\ncloud-init: User name to change ssh keys and password for instead of the image’s configured default user.\n\ncores: <integer> (1 - N) (default = 1)\n\nThe number of cores per socket.\n\ncpu: [[cputype=]<string>] [,flags=<+FLAG[;-FLAG...]>] [,hidden=<1|0>] [,hv-vendor-id=<vendor-id>] [,phys-bits=<8-64|host>] [,reported-model=<enum>]\n\nEmulated CPU type.\n\ncputype=<string> (default = kvm64)\n\nEmulated CPU type. Can be default or custom name (custom model names must be prefixed with custom-).\n\nflags=<+FLAG[;-FLAG...]>\n\nList of additional CPU flags separated by ;. Use +FLAG to enable, -FLAG to disable a flag. Custom CPU models can specify any flag supported by QEMU/KVM, VM-specific flags must be from the following set for security reasons: pcid, spec-ctrl, ibpb, ssbd, virt-ssbd, amd-ssbd, amd-no-ssb, pdpe1gb, md-clear, hv-tlbflush, hv-evmcs, aes\n\nhidden=<boolean> (default = 0)\n\nDo not identify as a KVM virtual machine.\n\nhv-vendor-id=<vendor-id>\n\nThe Hyper-V vendor ID. Some drivers or programs inside Windows guests need a specific ID.\n\nphys-bits=<8-64|host>\n\nThe physical memory address bits that are reported to the guest OS. Should be smaller or equal to the host’s. Set to host to use value from host CPU, but note that doing so will break live migration to CPUs with other values.\n\nreported-model=<486 | Broadwell | Broadwell-IBRS | Broadwell-noTSX | Broadwell-noTSX-IBRS | Cascadelake-Server | Cascadelake-Server-noTSX | Cascadelake-Server-v2 | Cascadelake-Server-v4 | Cascadelake-Server-v5 | Conroe | Cooperlake | Cooperlake-v2 | EPYC | EPYC-Genoa | EPYC-IBPB | EPYC-Milan | EPYC-Milan-v2 | EPYC-Rome | EPYC-Rome-v2 | EPYC-Rome-v3 | EPYC-Rome-v4 | EPYC-v3 | EPYC-v4 | GraniteRapids | Haswell | Haswell-IBRS | Haswell-noTSX | Haswell-noTSX-IBRS | Icelake-Client | Icelake-Client-noTSX | Icelake-Server | Icelake-Server-noTSX | Icelake-Server-v3 | Icelake-Server-v4 | Icelake-Server-v5 | Icelake-Server-v6 | IvyBridge | IvyBridge-IBRS | KnightsMill | Nehalem | Nehalem-IBRS | Opteron_G1 | Opteron_G2 | Opteron_G3 | Opteron_G4 | Opteron_G5 | Penryn | SandyBridge | SandyBridge-IBRS | SapphireRapids | SapphireRapids-v2 | Skylake-Client | Skylake-Client-IBRS | Skylake-Client-noTSX-IBRS | Skylake-Client-v4 | Skylake-Server | Skylake-Server-IBRS | Skylake-Server-noTSX-IBRS | Skylake-Server-v4 | Skylake-Server-v5 | Westmere | Westmere-IBRS | athlon | core2duo | coreduo | host | kvm32 | kvm64 | max | pentium | pentium2 | pentium3 | phenom | qemu32 | qemu64> (default = kvm64)\n\nCPU model and vendor to report to the guest. Must be a QEMU/KVM supported model. Only valid for custom CPU model definitions, default models will always report themselves to the guest OS.\n\ncpulimit: <number> (0 - 128) (default = 0)\n\nLimit of CPU usage.\n\n\tIf the computer has 2 CPUs, it has total of 2 CPU time. Value 0 indicates no CPU limit.\ncpuunits: <integer> (1 - 262144) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a VM. Argument is used in the kernel fair scheduler. The larger the number is, the more CPU time this VM gets. Number is relative to weights of all the other running VMs.\n\ndescription: <string>\n\nDescription for the VM. Shown in the web-interface VM’s summary. This is saved as comment inside the configuration file.\n\nefidisk0: [file=]<volume> [,efitype=<2m|4m>] [,format=<enum>] [,pre-enrolled-keys=<1|0>] [,size=<DiskSize>]\n\nConfigure a disk for storing EFI vars.\n\nefitype=<2m | 4m> (default = 2m)\n\nSize and type of the OVMF EFI vars. 4m is newer and recommended, and required for Secure Boot. For backwards compatibility, 2m is used if not otherwise specified. Ignored for VMs with arch=aarch64 (ARM).\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nformat=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>\n\nThe drive’s backing file’s data format.\n\npre-enrolled-keys=<boolean> (default = 0)\n\nUse am EFI vars template with distribution-specific and Microsoft Standard keys enrolled, if used with efitype=4m. Note that this will enable Secure Boot by default, though it can still be turned off from within the VM.\n\nsize=<DiskSize>\n\nDisk size. This is purely informational and has no effect.\n\nfreeze: <boolean>\n\nFreeze CPU at startup (use c monitor command to start execution).\n\nhookscript: <string>\n\nScript that will be executed during various steps in the vms lifetime.\n\nhostpci[n]: [[host=]<HOSTPCIID[;HOSTPCIID2...]>] [,device-id=<hex id>] [,legacy-igd=<1|0>] [,mapping=<mapping-id>] [,mdev=<string>] [,pcie=<1|0>] [,rombar=<1|0>] [,romfile=<string>] [,sub-device-id=<hex id>] [,sub-vendor-id=<hex id>] [,vendor-id=<hex id>] [,x-vga=<1|0>]\n\nMap host PCI devices into guest.\n\n\tThis option allows direct access to host hardware. So it is no longer possible to migrate such machines - use with special care.\n\tExperimental! User reported problems with this option.\ndevice-id=<hex id>\n\nOverride PCI device ID visible to guest\n\nhost=<HOSTPCIID[;HOSTPCIID2...]>\n\nHost PCI device pass through. The PCI ID of a host’s PCI device or a list of PCI virtual functions of the host. HOSTPCIID syntax is:\n\nbus:dev.func (hexadecimal numbers)\n\nYou can us the lspci command to list existing PCI devices.\n\nEither this or the mapping key must be set.\n\nlegacy-igd=<boolean> (default = 0)\n\nPass this device in legacy IGD mode, making it the primary and exclusive graphics device in the VM. Requires pc-i440fx machine type and VGA set to none.\n\nmapping=<mapping-id>\n\nThe ID of a cluster wide mapping. Either this or the default-key host must be set.\n\nmdev=<string>\n\nThe type of mediated device to use. An instance of this type will be created on startup of the VM and will be cleaned up when the VM stops.\n\npcie=<boolean> (default = 0)\n\nChoose the PCI-express bus (needs the q35 machine model).\n\nrombar=<boolean> (default = 1)\n\nSpecify whether or not the device’s ROM will be visible in the guest’s memory map.\n\nromfile=<string>\n\nCustom pci device rom filename (must be located in /usr/share/kvm/).\n\nsub-device-id=<hex id>\n\nOverride PCI subsystem device ID visible to guest\n\nsub-vendor-id=<hex id>\n\nOverride PCI subsystem vendor ID visible to guest\n\nvendor-id=<hex id>\n\nOverride PCI vendor ID visible to guest\n\nx-vga=<boolean> (default = 0)\n\nEnable vfio-vga device support.\n\nhotplug: <string> (default = network,disk,usb)\n\nSelectively enable hotplug features. This is a comma separated list of hotplug features: network, disk, cpu, memory, usb and cloudinit. Use 0 to disable hotplug completely. Using 1 as value is an alias for the default network,disk,usb. USB hotplugging is possible for guests with machine version >= 7.1 and ostype l26 or windows > 7.\n\nhugepages: <1024 | 2 | any>\n\nEnable/disable hugepages memory.\n\nide[n]: [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,model=<model>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as IDE hard disk or CD-ROM (n is 0 to 3).\n\naio=<io_uring | native | threads>\n\nAIO type to use.\n\nbackup=<boolean>\n\nWhether the drive should be included when making backups.\n\nbps=<bps>\n\nMaximum r/w speed in bytes per second.\n\nbps_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\nbps_rd=<bps>\n\nMaximum read speed in bytes per second.\n\nbps_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\nbps_wr=<bps>\n\nMaximum write speed in bytes per second.\n\nbps_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\ncache=<directsync | none | unsafe | writeback | writethrough>\n\nThe drive’s cache mode\n\ncyls=<integer>\n\nForce the drive’s physical geometry to have a specific cylinder count.\n\ndetect_zeroes=<boolean>\n\nControls whether to detect and try to optimize writes of zeroes.\n\ndiscard=<ignore | on>\n\nControls whether to pass discard/trim requests to the underlying storage.\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nformat=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>\n\nThe drive’s backing file’s data format.\n\nheads=<integer>\n\nForce the drive’s physical geometry to have a specific head count.\n\niops=<iops>\n\nMaximum r/w I/O in operations per second.\n\niops_max=<iops>\n\nMaximum unthrottled r/w I/O pool in operations per second.\n\niops_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\niops_rd=<iops>\n\nMaximum read I/O in operations per second.\n\niops_rd_max=<iops>\n\nMaximum unthrottled read I/O pool in operations per second.\n\niops_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\niops_wr=<iops>\n\nMaximum write I/O in operations per second.\n\niops_wr_max=<iops>\n\nMaximum unthrottled write I/O pool in operations per second.\n\niops_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\nmbps=<mbps>\n\nMaximum r/w speed in megabytes per second.\n\nmbps_max=<mbps>\n\nMaximum unthrottled r/w pool in megabytes per second.\n\nmbps_rd=<mbps>\n\nMaximum read speed in megabytes per second.\n\nmbps_rd_max=<mbps>\n\nMaximum unthrottled read pool in megabytes per second.\n\nmbps_wr=<mbps>\n\nMaximum write speed in megabytes per second.\n\nmbps_wr_max=<mbps>\n\nMaximum unthrottled write pool in megabytes per second.\n\nmedia=<cdrom | disk> (default = disk)\n\nThe drive’s media type.\n\nmodel=<model>\n\nThe drive’s reported model name, url-encoded, up to 40 bytes long.\n\nreplicate=<boolean> (default = 1)\n\nWhether the drive should considered for replication jobs.\n\nrerror=<ignore | report | stop>\n\nRead error action.\n\nsecs=<integer>\n\nForce the drive’s physical geometry to have a specific sector count.\n\nserial=<serial>\n\nThe drive’s reported serial number, url-encoded, up to 20 bytes long.\n\nshared=<boolean> (default = 0)\n\nMark this locally-managed volume as available on all nodes.\n\n\tThis option does not share the volume automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nDisk size. This is purely informational and has no effect.\n\nsnapshot=<boolean>\n\nControls qemu’s snapshot mode feature. If activated, changes made to the disk are temporary and will be discarded when the VM is shutdown.\n\nssd=<boolean>\n\nWhether to expose this drive as an SSD, rather than a rotational hard disk.\n\ntrans=<auto | lba | none>\n\nForce disk geometry bios translation mode.\n\nwerror=<enospc | ignore | report | stop>\n\nWrite error action.\n\nwwn=<wwn>\n\nThe drive’s worldwide name, encoded as 16 bytes hex string, prefixed by 0x.\n\nipconfig[n]: [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]\n\ncloud-init: Specify IP addresses and gateways for the corresponding interface.\n\nIP addresses use CIDR notation, gateways are optional but need an IP of the same type specified.\n\nThe special string dhcp can be used for IP addresses to use DHCP, in which case no explicit gateway should be provided. For IPv6 the special string auto can be used to use stateless autoconfiguration. This requires cloud-init 19.4 or newer.\n\nIf cloud-init is enabled and neither an IPv4 nor an IPv6 address is specified, it defaults to using dhcp on IPv4.\n\ngw=<GatewayIPv4>\n\nDefault gateway for IPv4 traffic.\n\n\tRequires option(s): ip\ngw6=<GatewayIPv6>\n\nDefault gateway for IPv6 traffic.\n\n\tRequires option(s): ip6\nip=<IPv4Format/CIDR> (default = dhcp)\n\nIPv4 address in CIDR format.\n\nip6=<IPv6Format/CIDR> (default = dhcp)\n\nIPv6 address in CIDR format.\n\nivshmem: size=<integer> [,name=<string>]\n\nInter-VM shared memory. Useful for direct communication between VMs, or to the host.\n\nname=<string>\n\nThe name of the file. Will be prefixed with pve-shm-. Default is the VMID. Will be deleted when the VM is stopped.\n\nsize=<integer> (1 - N)\n\nThe size of the file in MB.\n\nkeephugepages: <boolean> (default = 0)\n\nUse together with hugepages. If enabled, hugepages will not not be deleted after VM shutdown and can be used for subsequent starts.\n\nkeyboard: <da | de | de-ch | en-gb | en-us | es | fi | fr | fr-be | fr-ca | fr-ch | hu | is | it | ja | lt | mk | nl | no | pl | pt | pt-br | sl | sv | tr>\n\nKeyboard layout for VNC server. This option is generally not required and is often better handled from within the guest OS.\n\nkvm: <boolean> (default = 1)\n\nEnable/disable KVM hardware virtualization.\n\nlocaltime: <boolean>\n\nSet the real time clock (RTC) to local time. This is enabled by default if the ostype indicates a Microsoft Windows OS.\n\nlock: <backup | clone | create | migrate | rollback | snapshot | snapshot-delete | suspended | suspending>\n\nLock/unlock the VM.\n\nmachine: [[type=]<machine type>] [,viommu=<intel|virtio>]\n\nSpecify the QEMU machine.\n\ntype=<machine type>\n\nSpecifies the QEMU machine type.\n\nviommu=<intel | virtio>\n\nEnable and set guest vIOMMU variant (Intel vIOMMU needs q35 to be set as machine type).\n\nmemory: [current=]<integer>\n\nMemory properties.\n\ncurrent=<integer> (16 - N) (default = 512)\n\nCurrent amount of online RAM for the VM in MiB. This is the maximum available memory when you use the balloon device.\n\nmigrate_downtime: <number> (0 - N) (default = 0.1)\n\nSet maximum tolerated downtime (in seconds) for migrations.\n\nmigrate_speed: <integer> (0 - N) (default = 0)\n\nSet maximum speed (in MB/s) for migrations. Value 0 is no limit.\n\nname: <string>\n\nSet a name for the VM. Only used on the configuration web interface.\n\nnameserver: <string>\n\ncloud-init: Sets DNS server IP address for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\nnet[n]: [model=]<enum> [,bridge=<bridge>] [,firewall=<1|0>] [,link_down=<1|0>] [,macaddr=<XX:XX:XX:XX:XX:XX>] [,mtu=<integer>] [,queues=<integer>] [,rate=<number>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,<model>=<macaddr>]\n\nSpecify network devices.\n\nbridge=<bridge>\n\nBridge to attach the network device to. The Proxmox VE standard bridge is called vmbr0.\n\nIf you do not specify a bridge, we create a kvm user (NATed) network device, which provides DHCP and DNS services. The following addresses are used:\n\n10.0.2.2   Gateway\n10.0.2.3   DNS Server\n10.0.2.4   SMB Server\n\nThe DHCP server assign addresses to the guest starting from 10.0.2.15.\n\nfirewall=<boolean>\n\nWhether this interface should be protected by the firewall.\n\nlink_down=<boolean>\n\nWhether this interface should be disconnected (like pulling the plug).\n\nmacaddr=<XX:XX:XX:XX:XX:XX>\n\nA common MAC address with the I/G (Individual/Group) bit not set.\n\nmodel=<e1000 | e1000-82540em | e1000-82544gc | e1000-82545em | e1000e | i82551 | i82557b | i82559er | ne2k_isa | ne2k_pci | pcnet | rtl8139 | virtio | vmxnet3>\n\nNetwork Card Model. The virtio model provides the best performance with very low CPU overhead. If your guest does not support this driver, it is usually best to use e1000.\n\nmtu=<integer> (1 - 65520)\n\nForce MTU, for VirtIO only. Set to 1 to use the bridge MTU\n\nqueues=<integer> (0 - 64)\n\nNumber of packet queues to be used on the device.\n\nrate=<number> (0 - N)\n\nRate limit in mbps (megabytes per second) as floating point number.\n\ntag=<integer> (1 - 4094)\n\nVLAN tag to apply to packets on this interface.\n\ntrunks=<vlanid[;vlanid...]>\n\nVLAN trunks to pass through this interface.\n\nnuma: <boolean> (default = 0)\n\nEnable/disable NUMA.\n\nnuma[n]: cpus=<id[-id];...> [,hostnodes=<id[-id];...>] [,memory=<number>] [,policy=<preferred|bind|interleave>]\n\nNUMA topology.\n\ncpus=<id[-id];...>\n\nCPUs accessing this NUMA node.\n\nhostnodes=<id[-id];...>\n\nHost NUMA nodes to use.\n\nmemory=<number>\n\nAmount of memory this NUMA node provides.\n\npolicy=<bind | interleave | preferred>\n\nNUMA allocation policy.\n\nonboot: <boolean> (default = 0)\n\nSpecifies whether a VM will be started during system bootup.\n\nostype: <l24 | l26 | other | solaris | w2k | w2k3 | w2k8 | win10 | win11 | win7 | win8 | wvista | wxp>\n\nSpecify guest operating system. This is used to enable special optimization/features for specific operating systems:\n\nother\n\t\n\nunspecified OS\n\n\nwxp\n\t\n\nMicrosoft Windows XP\n\n\nw2k\n\t\n\nMicrosoft Windows 2000\n\n\nw2k3\n\t\n\nMicrosoft Windows 2003\n\n\nw2k8\n\t\n\nMicrosoft Windows 2008\n\n\nwvista\n\t\n\nMicrosoft Windows Vista\n\n\nwin7\n\t\n\nMicrosoft Windows 7\n\n\nwin8\n\t\n\nMicrosoft Windows 8/2012/2012r2\n\n\nwin10\n\t\n\nMicrosoft Windows 10/2016/2019\n\n\nwin11\n\t\n\nMicrosoft Windows 11/2022/2025\n\n\nl24\n\t\n\nLinux 2.4 Kernel\n\n\nl26\n\t\n\nLinux 2.6 - 6.X Kernel\n\n\nsolaris\n\t\n\nSolaris/OpenSolaris/OpenIndiania kernel\n\nparallel[n]: /dev/parport\\d+|/dev/usb/lp\\d+\n\nMap host parallel devices (n is 0 to 2).\n\n\tThis option allows direct access to host hardware. So it is no longer possible to migrate such machines - use with special care.\n\tExperimental! User reported problems with this option.\nprotection: <boolean> (default = 0)\n\nSets the protection flag of the VM. This will disable the remove VM and remove disk operations.\n\nreboot: <boolean> (default = 1)\n\nAllow reboot. If set to 0 the VM exit on reboot.\n\nrng0: [source=]</dev/urandom|/dev/random|/dev/hwrng> [,max_bytes=<integer>] [,period=<integer>]\n\nConfigure a VirtIO-based Random Number Generator.\n\nmax_bytes=<integer> (default = 1024)\n\nMaximum bytes of entropy allowed to get injected into the guest every period milliseconds. Prefer a lower value when using /dev/random as source. Use 0 to disable limiting (potentially dangerous!).\n\nperiod=<integer> (default = 1000)\n\nEvery period milliseconds the entropy-injection quota is reset, allowing the guest to retrieve another max_bytes of entropy.\n\nsource=</dev/hwrng | /dev/random | /dev/urandom>\n\nThe file on the host to gather entropy from. In most cases /dev/urandom should be preferred over /dev/random to avoid entropy-starvation issues on the host. Using urandom does not decrease security in any meaningful way, as it’s still seeded from real entropy, and the bytes provided will most likely be mixed with real entropy on the guest as well. /dev/hwrng can be used to pass through a hardware RNG from the host.\n\nsata[n]: [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SATA hard disk or CD-ROM (n is 0 to 5).\n\naio=<io_uring | native | threads>\n\nAIO type to use.\n\nbackup=<boolean>\n\nWhether the drive should be included when making backups.\n\nbps=<bps>\n\nMaximum r/w speed in bytes per second.\n\nbps_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\nbps_rd=<bps>\n\nMaximum read speed in bytes per second.\n\nbps_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\nbps_wr=<bps>\n\nMaximum write speed in bytes per second.\n\nbps_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\ncache=<directsync | none | unsafe | writeback | writethrough>\n\nThe drive’s cache mode\n\ncyls=<integer>\n\nForce the drive’s physical geometry to have a specific cylinder count.\n\ndetect_zeroes=<boolean>\n\nControls whether to detect and try to optimize writes of zeroes.\n\ndiscard=<ignore | on>\n\nControls whether to pass discard/trim requests to the underlying storage.\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nformat=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>\n\nThe drive’s backing file’s data format.\n\nheads=<integer>\n\nForce the drive’s physical geometry to have a specific head count.\n\niops=<iops>\n\nMaximum r/w I/O in operations per second.\n\niops_max=<iops>\n\nMaximum unthrottled r/w I/O pool in operations per second.\n\niops_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\niops_rd=<iops>\n\nMaximum read I/O in operations per second.\n\niops_rd_max=<iops>\n\nMaximum unthrottled read I/O pool in operations per second.\n\niops_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\niops_wr=<iops>\n\nMaximum write I/O in operations per second.\n\niops_wr_max=<iops>\n\nMaximum unthrottled write I/O pool in operations per second.\n\niops_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\nmbps=<mbps>\n\nMaximum r/w speed in megabytes per second.\n\nmbps_max=<mbps>\n\nMaximum unthrottled r/w pool in megabytes per second.\n\nmbps_rd=<mbps>\n\nMaximum read speed in megabytes per second.\n\nmbps_rd_max=<mbps>\n\nMaximum unthrottled read pool in megabytes per second.\n\nmbps_wr=<mbps>\n\nMaximum write speed in megabytes per second.\n\nmbps_wr_max=<mbps>\n\nMaximum unthrottled write pool in megabytes per second.\n\nmedia=<cdrom | disk> (default = disk)\n\nThe drive’s media type.\n\nreplicate=<boolean> (default = 1)\n\nWhether the drive should considered for replication jobs.\n\nrerror=<ignore | report | stop>\n\nRead error action.\n\nsecs=<integer>\n\nForce the drive’s physical geometry to have a specific sector count.\n\nserial=<serial>\n\nThe drive’s reported serial number, url-encoded, up to 20 bytes long.\n\nshared=<boolean> (default = 0)\n\nMark this locally-managed volume as available on all nodes.\n\n\tThis option does not share the volume automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nDisk size. This is purely informational and has no effect.\n\nsnapshot=<boolean>\n\nControls qemu’s snapshot mode feature. If activated, changes made to the disk are temporary and will be discarded when the VM is shutdown.\n\nssd=<boolean>\n\nWhether to expose this drive as an SSD, rather than a rotational hard disk.\n\ntrans=<auto | lba | none>\n\nForce disk geometry bios translation mode.\n\nwerror=<enospc | ignore | report | stop>\n\nWrite error action.\n\nwwn=<wwn>\n\nThe drive’s worldwide name, encoded as 16 bytes hex string, prefixed by 0x.\n\nscsi[n]: [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,product=<product>] [,queues=<integer>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,scsiblock=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,vendor=<vendor>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SCSI hard disk or CD-ROM (n is 0 to 30).\n\naio=<io_uring | native | threads>\n\nAIO type to use.\n\nbackup=<boolean>\n\nWhether the drive should be included when making backups.\n\nbps=<bps>\n\nMaximum r/w speed in bytes per second.\n\nbps_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\nbps_rd=<bps>\n\nMaximum read speed in bytes per second.\n\nbps_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\nbps_wr=<bps>\n\nMaximum write speed in bytes per second.\n\nbps_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\ncache=<directsync | none | unsafe | writeback | writethrough>\n\nThe drive’s cache mode\n\ncyls=<integer>\n\nForce the drive’s physical geometry to have a specific cylinder count.\n\ndetect_zeroes=<boolean>\n\nControls whether to detect and try to optimize writes of zeroes.\n\ndiscard=<ignore | on>\n\nControls whether to pass discard/trim requests to the underlying storage.\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nformat=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>\n\nThe drive’s backing file’s data format.\n\nheads=<integer>\n\nForce the drive’s physical geometry to have a specific head count.\n\niops=<iops>\n\nMaximum r/w I/O in operations per second.\n\niops_max=<iops>\n\nMaximum unthrottled r/w I/O pool in operations per second.\n\niops_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\niops_rd=<iops>\n\nMaximum read I/O in operations per second.\n\niops_rd_max=<iops>\n\nMaximum unthrottled read I/O pool in operations per second.\n\niops_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\niops_wr=<iops>\n\nMaximum write I/O in operations per second.\n\niops_wr_max=<iops>\n\nMaximum unthrottled write I/O pool in operations per second.\n\niops_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\niothread=<boolean>\n\nWhether to use iothreads for this drive\n\nmbps=<mbps>\n\nMaximum r/w speed in megabytes per second.\n\nmbps_max=<mbps>\n\nMaximum unthrottled r/w pool in megabytes per second.\n\nmbps_rd=<mbps>\n\nMaximum read speed in megabytes per second.\n\nmbps_rd_max=<mbps>\n\nMaximum unthrottled read pool in megabytes per second.\n\nmbps_wr=<mbps>\n\nMaximum write speed in megabytes per second.\n\nmbps_wr_max=<mbps>\n\nMaximum unthrottled write pool in megabytes per second.\n\nmedia=<cdrom | disk> (default = disk)\n\nThe drive’s media type.\n\nproduct=<product>\n\nThe drive’s product name, up to 16 bytes long.\n\nqueues=<integer> (2 - N)\n\nNumber of queues.\n\nreplicate=<boolean> (default = 1)\n\nWhether the drive should considered for replication jobs.\n\nrerror=<ignore | report | stop>\n\nRead error action.\n\nro=<boolean>\n\nWhether the drive is read-only.\n\nscsiblock=<boolean> (default = 0)\n\nwhether to use scsi-block for full passthrough of host block device\n\n\tcan lead to I/O errors in combination with low memory or high memory fragmentation on host\nsecs=<integer>\n\nForce the drive’s physical geometry to have a specific sector count.\n\nserial=<serial>\n\nThe drive’s reported serial number, url-encoded, up to 20 bytes long.\n\nshared=<boolean> (default = 0)\n\nMark this locally-managed volume as available on all nodes.\n\n\tThis option does not share the volume automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nDisk size. This is purely informational and has no effect.\n\nsnapshot=<boolean>\n\nControls qemu’s snapshot mode feature. If activated, changes made to the disk are temporary and will be discarded when the VM is shutdown.\n\nssd=<boolean>\n\nWhether to expose this drive as an SSD, rather than a rotational hard disk.\n\ntrans=<auto | lba | none>\n\nForce disk geometry bios translation mode.\n\nvendor=<vendor>\n\nThe drive’s vendor name, up to 8 bytes long.\n\nwerror=<enospc | ignore | report | stop>\n\nWrite error action.\n\nwwn=<wwn>\n\nThe drive’s worldwide name, encoded as 16 bytes hex string, prefixed by 0x.\n\nscsihw: <lsi | lsi53c810 | megasas | pvscsi | virtio-scsi-pci | virtio-scsi-single> (default = lsi)\n\nSCSI controller model\n\nsearchdomain: <string>\n\ncloud-init: Sets DNS search domains for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\nserial[n]: (/dev/.+|socket)\n\nCreate a serial device inside the VM (n is 0 to 3), and pass through a host serial device (i.e. /dev/ttyS0), or create a unix socket on the host side (use qm terminal to open a terminal connection).\n\n\tIf you pass through a host serial device, it is no longer possible to migrate such machines - use with special care.\n\tExperimental! User reported problems with this option.\nshares: <integer> (0 - 50000) (default = 1000)\n\nAmount of memory shares for auto-ballooning. The larger the number is, the more memory this VM gets. Number is relative to weights of all other running VMs. Using zero disables auto-ballooning. Auto-ballooning is done by pvestatd.\n\nsmbios1: [base64=<1|0>] [,family=<Base64 encoded string>] [,manufacturer=<Base64 encoded string>] [,product=<Base64 encoded string>] [,serial=<Base64 encoded string>] [,sku=<Base64 encoded string>] [,uuid=<UUID>] [,version=<Base64 encoded string>]\n\nSpecify SMBIOS type 1 fields.\n\nbase64=<boolean>\n\nFlag to indicate that the SMBIOS values are base64 encoded\n\nfamily=<Base64 encoded string>\n\nSet SMBIOS1 family string.\n\nmanufacturer=<Base64 encoded string>\n\nSet SMBIOS1 manufacturer.\n\nproduct=<Base64 encoded string>\n\nSet SMBIOS1 product ID.\n\nserial=<Base64 encoded string>\n\nSet SMBIOS1 serial number.\n\nsku=<Base64 encoded string>\n\nSet SMBIOS1 SKU string.\n\nuuid=<UUID>\n\nSet SMBIOS1 UUID.\n\nversion=<Base64 encoded string>\n\nSet SMBIOS1 version.\n\nsmp: <integer> (1 - N) (default = 1)\n\nThe number of CPUs. Please use option -sockets instead.\n\nsockets: <integer> (1 - N) (default = 1)\n\nThe number of CPU sockets.\n\nspice_enhancements: [foldersharing=<1|0>] [,videostreaming=<off|all|filter>]\n\nConfigure additional enhancements for SPICE.\n\nfoldersharing=<boolean> (default = 0)\n\nEnable folder sharing via SPICE. Needs Spice-WebDAV daemon installed in the VM.\n\nvideostreaming=<all | filter | off> (default = off)\n\nEnable video streaming. Uses compression for detected video streams.\n\nsshkeys: <string>\n\ncloud-init: Setup public SSH keys (one key per line, OpenSSH format).\n\nstartdate: (now | YYYY-MM-DD | YYYY-MM-DDTHH:MM:SS) (default = now)\n\nSet the initial date of the real time clock. Valid format for date are:'now' or 2006-06-17T16:01:21 or 2006-06-17.\n\nstartup: `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\ntablet: <boolean> (default = 1)\n\nEnable/disable the USB tablet device. This device is usually needed to allow absolute mouse positioning with VNC. Else the mouse runs out of sync with normal VNC clients. If you’re running lots of console-only guests on one host, you may consider disabling this to save some context switches. This is turned off by default if you use spice (qm set <vmid> --vga qxl).\n\ntags: <string>\n\nTags of the VM. This is only meta information.\n\ntdf: <boolean> (default = 0)\n\nEnable/disable time drift fix.\n\ntemplate: <boolean> (default = 0)\n\nEnable/disable Template.\n\ntpmstate0: [file=]<volume> [,size=<DiskSize>] [,version=<v1.2|v2.0>]\n\nConfigure a Disk for storing TPM state. The format is fixed to raw.\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nsize=<DiskSize>\n\nDisk size. This is purely informational and has no effect.\n\nversion=<v1.2 | v2.0> (default = v2.0)\n\nThe TPM interface version. v2.0 is newer and should be preferred. Note that this cannot be changed later on.\n\nunused[n]: [file=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nusb[n]: [[host=]<HOSTUSBDEVICE|spice>] [,mapping=<mapping-id>] [,usb3=<1|0>]\n\nConfigure an USB device (n is 0 to 4, for machine version >= 7.1 and ostype l26 or windows > 7, n can be up to 14).\n\nhost=<HOSTUSBDEVICE|spice>\n\nThe Host USB device or port or the value spice. HOSTUSBDEVICE syntax is:\n\n'bus-port(.port)*' (decimal numbers) or\n'vendor_id:product_id' (hexadeciaml numbers) or\n'spice'\n\nYou can use the lsusb -t command to list existing usb devices.\n\n\tThis option allows direct access to host hardware. So it is no longer possible to migrate such machines - use with special care.\n\nThe value spice can be used to add a usb redirection devices for spice.\n\nEither this or the mapping key must be set.\n\nmapping=<mapping-id>\n\nThe ID of a cluster wide mapping. Either this or the default-key host must be set.\n\nusb3=<boolean> (default = 0)\n\nSpecifies whether if given host option is a USB3 device or port. For modern guests (machine version >= 7.1 and ostype l26 and windows > 7), this flag is irrelevant (all devices are plugged into a xhci controller).\n\nvcpus: <integer> (1 - N) (default = 0)\n\nNumber of hotplugged vcpus.\n\nvga: [[type=]<enum>] [,clipboard=<vnc>] [,memory=<integer>]\n\nConfigure the VGA Hardware. If you want to use high resolution modes (>= 1280x1024x16) you may need to increase the vga memory option. Since QEMU 2.9 the default VGA display type is std for all OS types besides some Windows versions (XP and older) which use cirrus. The qxl option enables the SPICE display server. For win* OS you can select how many independent displays you want, Linux guests can add displays them self. You can also run without any graphic card, using a serial device as terminal.\n\nclipboard=<vnc>\n\nEnable a specific clipboard. If not set, depending on the display type the SPICE one will be added. Migration with VNC clipboard is not yet supported!\n\nmemory=<integer> (4 - 512)\n\nSets the VGA memory (in MiB). Has no effect with serial display.\n\ntype=<cirrus | none | qxl | qxl2 | qxl3 | qxl4 | serial0 | serial1 | serial2 | serial3 | std | virtio | virtio-gl | vmware> (default = std)\n\nSelect the VGA type.\n\nvirtio[n]: [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>]\n\nUse volume as VIRTIO hard disk (n is 0 to 15).\n\naio=<io_uring | native | threads>\n\nAIO type to use.\n\nbackup=<boolean>\n\nWhether the drive should be included when making backups.\n\nbps=<bps>\n\nMaximum r/w speed in bytes per second.\n\nbps_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\nbps_rd=<bps>\n\nMaximum read speed in bytes per second.\n\nbps_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\nbps_wr=<bps>\n\nMaximum write speed in bytes per second.\n\nbps_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\ncache=<directsync | none | unsafe | writeback | writethrough>\n\nThe drive’s cache mode\n\ncyls=<integer>\n\nForce the drive’s physical geometry to have a specific cylinder count.\n\ndetect_zeroes=<boolean>\n\nControls whether to detect and try to optimize writes of zeroes.\n\ndiscard=<ignore | on>\n\nControls whether to pass discard/trim requests to the underlying storage.\n\nfile=<volume>\n\nThe drive’s backing volume.\n\nformat=<cloop | cow | qcow | qcow2 | qed | raw | vmdk>\n\nThe drive’s backing file’s data format.\n\nheads=<integer>\n\nForce the drive’s physical geometry to have a specific head count.\n\niops=<iops>\n\nMaximum r/w I/O in operations per second.\n\niops_max=<iops>\n\nMaximum unthrottled r/w I/O pool in operations per second.\n\niops_max_length=<seconds>\n\nMaximum length of I/O bursts in seconds.\n\niops_rd=<iops>\n\nMaximum read I/O in operations per second.\n\niops_rd_max=<iops>\n\nMaximum unthrottled read I/O pool in operations per second.\n\niops_rd_max_length=<seconds>\n\nMaximum length of read I/O bursts in seconds.\n\niops_wr=<iops>\n\nMaximum write I/O in operations per second.\n\niops_wr_max=<iops>\n\nMaximum unthrottled write I/O pool in operations per second.\n\niops_wr_max_length=<seconds>\n\nMaximum length of write I/O bursts in seconds.\n\niothread=<boolean>\n\nWhether to use iothreads for this drive\n\nmbps=<mbps>\n\nMaximum r/w speed in megabytes per second.\n\nmbps_max=<mbps>\n\nMaximum unthrottled r/w pool in megabytes per second.\n\nmbps_rd=<mbps>\n\nMaximum read speed in megabytes per second.\n\nmbps_rd_max=<mbps>\n\nMaximum unthrottled read pool in megabytes per second.\n\nmbps_wr=<mbps>\n\nMaximum write speed in megabytes per second.\n\nmbps_wr_max=<mbps>\n\nMaximum unthrottled write pool in megabytes per second.\n\nmedia=<cdrom | disk> (default = disk)\n\nThe drive’s media type.\n\nreplicate=<boolean> (default = 1)\n\nWhether the drive should considered for replication jobs.\n\nrerror=<ignore | report | stop>\n\nRead error action.\n\nro=<boolean>\n\nWhether the drive is read-only.\n\nsecs=<integer>\n\nForce the drive’s physical geometry to have a specific sector count.\n\nserial=<serial>\n\nThe drive’s reported serial number, url-encoded, up to 20 bytes long.\n\nshared=<boolean> (default = 0)\n\nMark this locally-managed volume as available on all nodes.\n\n\tThis option does not share the volume automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nDisk size. This is purely informational and has no effect.\n\nsnapshot=<boolean>\n\nControls qemu’s snapshot mode feature. If activated, changes made to the disk are temporary and will be discarded when the VM is shutdown.\n\ntrans=<auto | lba | none>\n\nForce disk geometry bios translation mode.\n\nwerror=<enospc | ignore | report | stop>\n\nWrite error action.\n\nvmgenid: <UUID> (default = 1 (autogenerated))\n\nThe VM generation ID (vmgenid) device exposes a 128-bit integer value identifier to the guest OS. This allows to notify the guest operating system when the virtual machine is executed with a different configuration (e.g. snapshot execution or creation from a template). The guest operating system notices the change, and is then able to react as appropriate by marking its copies of distributed databases as dirty, re-initializing its random number generator, etc. Note that auto-creation only works when done through API/CLI create or update methods, but not when manually editing the config file.\n\nvmstatestorage: <storage ID>\n\nDefault storage for VM state volumes/files.\n\nwatchdog: [[model=]<i6300esb|ib700>] [,action=<enum>]\n\nCreate a virtual hardware watchdog device. Once enabled (by a guest action), the watchdog must be periodically polled by an agent inside the guest or else the watchdog will reset the guest (or execute the respective action specified)\n\naction=<debug | none | pause | poweroff | reset | shutdown>\n\nThe action to perform if after activation the guest fails to poll the watchdog in time.\n\nmodel=<i6300esb | ib700> (default = i6300esb)\n\nWatchdog type to emulate.\n\n10.15. Locks\n\nOnline migrations, snapshots and backups (vzdump) set a lock to prevent incompatible concurrent actions on the affected VMs. Sometimes you need to remove such a lock manually (for example after a power failure).\n\n# qm unlock <vmid>\n\tOnly do that if you are sure the action which set the lock is no longer running.\n11. Proxmox Container Toolkit\n\nContainers are a lightweight alternative to fully virtualized machines (VMs). They use the kernel of the host system that they run on, instead of emulating a full operating system (OS). This means that containers can access resources on the host system directly.\n\nThe runtime costs for containers is low, usually negligible. However, there are some drawbacks that need be considered:\n\nOnly Linux distributions can be run in Proxmox Containers. It is not possible to run other operating systems like, for example, FreeBSD or Microsoft Windows inside a container.\n\nFor security reasons, access to host resources needs to be restricted. Therefore, containers run in their own separate namespaces. Additionally some syscalls (user space requests to the Linux kernel) are not allowed within containers.\n\nProxmox VE uses Linux Containers (LXC) as its underlying container technology. The “Proxmox Container Toolkit” (pct) simplifies the usage and management of LXC, by providing an interface that abstracts complex tasks.\n\nContainers are tightly integrated with Proxmox VE. This means that they are aware of the cluster setup, and they can use the same network and storage resources as virtual machines. You can also use the Proxmox VE firewall, or manage containers using the HA framework.\n\nOur primary goal is to offer an environment that provides the benefits of using a VM, but without the additional overhead. This means that Proxmox Containers can be categorized as “System Containers”, rather than “Application Containers”.\n\n\tIf you want to run application containers, for example, Docker images, it is recommended that you run them inside a Proxmox QEMU VM. This will give you all the advantages of application containerization, while also providing the benefits that VMs offer, such as strong isolation from the host and the ability to live-migrate, which otherwise isn’t possible with containers.\n11.1. Technology Overview\n\nLXC (https://linuxcontainers.org/)\n\nIntegrated into Proxmox VE graphical web user interface (GUI)\n\nEasy to use command-line tool pct\n\nAccess via Proxmox VE REST API\n\nlxcfs to provide containerized /proc file system\n\nControl groups (cgroups) for resource isolation and limitation\n\nAppArmor and seccomp to improve security\n\nModern Linux kernels\n\nImage based deployment (templates)\n\nUses Proxmox VE storage library\n\nContainer setup from host (network, DNS, storage, etc.)\n\n11.2. Supported Distributions\n\nList of officially supported distributions can be found below.\n\nTemplates for the following distributions are available through our repositories. You can use pveam tool or the Graphical User Interface to download them.\n\n11.2.1. Alpine Linux\n\nAlpine Linux is a security-oriented, lightweight Linux distribution based on musl libc and busybox.\n\n— https://alpinelinux.org\n\nFor currently supported releases see:\n\nhttps://alpinelinux.org/releases/\n\n11.2.2. Arch Linux\n\nArch Linux, a lightweight and flexible Linux® distribution that tries to Keep It Simple.\n\n— https://archlinux.org/\n\nArch Linux is using a rolling-release model, see its wiki for more details:\n\nhttps://wiki.archlinux.org/title/Arch_Linux\n\n11.2.3. CentOS, Almalinux, Rocky Linux\nCentOS / CentOS Stream\n\nThe CentOS Linux distribution is a stable, predictable, manageable and reproducible platform derived from the sources of Red Hat Enterprise Linux (RHEL)\n\n— https://centos.org\n\nFor currently supported releases see:\n\nhttps://en.wikipedia.org/wiki/CentOS#End-of-support_schedule\n\nAlmalinux\n\nAn Open Source, community owned and governed, forever-free enterprise Linux distribution, focused on long-term stability, providing a robust production-grade platform. AlmaLinux OS is 1:1 binary compatible with RHEL® and pre-Stream CentOS.\n\n— https://almalinux.org\n\nFor currently supported releases see:\n\nhttps://en.wikipedia.org/wiki/AlmaLinux#Releases\n\nRocky Linux\n\nRocky Linux is a community enterprise operating system designed to be 100% bug-for-bug compatible with America’s top enterprise Linux distribution now that its downstream partner has shifted direction.\n\n— https://rockylinux.org\n\nFor currently supported releases see:\n\nhttps://en.wikipedia.org/wiki/Rocky_Linux#Releases\n\n11.2.4. Debian\n\nDebian is a free operating system, developed and maintained by the Debian project. A free Linux distribution with thousands of applications to meet our users' needs.\n\n— https://www.debian.org/intro/index#software\n\nFor currently supported releases see:\n\nhttps://www.debian.org/releases/stable/releasenotes\n\n11.2.5. Devuan\n\nDevuan GNU+Linux is a fork of Debian without systemd that allows users to reclaim control over their system by avoiding unnecessary entanglements and ensuring Init Freedom.\n\n— https://www.devuan.org\n\nFor currently supported releases see:\n\nhttps://www.devuan.org/os/releases\n\n11.2.6. Fedora\n\nFedora creates an innovative, free, and open source platform for hardware, clouds, and containers that enables software developers and community members to build tailored solutions for their users.\n\n— https://getfedora.org\n\nFor currently supported releases see:\n\nhttps://fedoraproject.org/wiki/Releases\n\n11.2.7. Gentoo\n\na highly flexible, source-based Linux distribution.\n\n— https://www.gentoo.org\n\nGentoo is using a rolling-release model.\n\n11.2.8. OpenSUSE\n\nThe makers' choice for sysadmins, developers and desktop users.\n\n— https://www.opensuse.org\n\nFor currently supported releases see:\n\nhttps://get.opensuse.org/leap/\n\n11.2.9. Ubuntu\n\nUbuntu is the modern, open source operating system on Linux for the enterprise server, desktop, cloud, and IoT.\n\n— https://ubuntu.com/\n\nFor currently supported releases see:\n\nhttps://wiki.ubuntu.com/Releases\n\n11.3. Container Images\n\nContainer images, sometimes also referred to as “templates” or “appliances”, are tar archives which contain everything to run a container.\n\nProxmox VE itself provides a variety of basic templates for the most common Linux distributions. They can be downloaded using the GUI or the pveam (short for Proxmox VE Appliance Manager) command-line utility. Additionally, TurnKey Linux container templates are also available to download.\n\nThe list of available templates is updated daily through the pve-daily-update timer. You can also trigger an update manually by executing:\n\n# pveam update\n\nTo view the list of available images run:\n\n# pveam available\n\nYou can restrict this large list by specifying the section you are interested in, for example basic system images:\n\nList available system images\n# pveam available --section system\nsystem          alpine-3.12-default_20200823_amd64.tar.xz\nsystem          alpine-3.13-default_20210419_amd64.tar.xz\nsystem          alpine-3.14-default_20210623_amd64.tar.xz\nsystem          archlinux-base_20210420-1_amd64.tar.gz\nsystem          centos-7-default_20190926_amd64.tar.xz\nsystem          centos-8-default_20201210_amd64.tar.xz\nsystem          debian-9.0-standard_9.7-1_amd64.tar.gz\nsystem          debian-10-standard_10.7-1_amd64.tar.gz\nsystem          devuan-3.0-standard_3.0_amd64.tar.gz\nsystem          fedora-33-default_20201115_amd64.tar.xz\nsystem          fedora-34-default_20210427_amd64.tar.xz\nsystem          gentoo-current-default_20200310_amd64.tar.xz\nsystem          opensuse-15.2-default_20200824_amd64.tar.xz\nsystem          ubuntu-16.04-standard_16.04.5-1_amd64.tar.gz\nsystem          ubuntu-18.04-standard_18.04.1-1_amd64.tar.gz\nsystem          ubuntu-20.04-standard_20.04-1_amd64.tar.gz\nsystem          ubuntu-20.10-standard_20.10-1_amd64.tar.gz\nsystem          ubuntu-21.04-standard_21.04-1_amd64.tar.gz\n\nBefore you can use such a template, you need to download them into one of your storages. If you’re unsure to which one, you can simply use the local named storage for that purpose. For clustered installations, it is preferred to use a shared storage so that all nodes can access those images.\n\n# pveam download local debian-10.0-standard_10.0-1_amd64.tar.gz\n\nYou are now ready to create containers using that image, and you can list all downloaded images on storage local with:\n\n# pveam list local\nlocal:vztmpl/debian-10.0-standard_10.0-1_amd64.tar.gz  219.95MB\n\tYou can also use the Proxmox VE web interface GUI to download, list and delete container templates.\n\npct uses them to create a new container, for example:\n\n# pct create 999 local:vztmpl/debian-10.0-standard_10.0-1_amd64.tar.gz\n\nThe above command shows you the full Proxmox VE volume identifiers. They include the storage name, and most other Proxmox VE commands can use them. For example you can delete that image later with:\n\n# pveam remove local:vztmpl/debian-10.0-standard_10.0-1_amd64.tar.gz\n11.4. Container Settings\n11.4.1. General Settings\n\nGeneral settings of a container include\n\nthe Node : the physical server on which the container will run\n\nthe CT ID: a unique number in this Proxmox VE installation used to identify your container\n\nHostname: the hostname of the container\n\nResource Pool: a logical group of containers and VMs\n\nPassword: the root password of the container\n\nSSH Public Key: a public key for connecting to the root account over SSH\n\nUnprivileged container: this option allows to choose at creation time if you want to create a privileged or unprivileged container.\n\nUnprivileged Containers\n\nUnprivileged containers use a new kernel feature called user namespaces. The root UID 0 inside the container is mapped to an unprivileged user outside the container. This means that most security issues (container escape, resource abuse, etc.) in these containers will affect a random unprivileged user, and would be a generic kernel security bug rather than an LXC issue. The LXC team thinks unprivileged containers are safe by design.\n\nThis is the default option when creating a new container.\n\n\tIf the container uses systemd as an init system, please be aware the systemd version running inside the container should be equal to or greater than 220.\nPrivileged Containers\n\nSecurity in containers is achieved by using mandatory access control AppArmor restrictions, seccomp filters and Linux kernel namespaces. The LXC team considers this kind of container as unsafe, and they will not consider new container escape exploits to be security issues worthy of a CVE and quick fix. That’s why privileged containers should only be used in trusted environments.\n\n11.4.2. CPU\n\nYou can restrict the number of visible CPUs inside the container using the cores option. This is implemented using the Linux cpuset cgroup (control group). A special task inside pvestatd tries to distribute running containers among available CPUs periodically. To view the assigned CPUs run the following command:\n\n# pct cpusets\n ---------------------\n 102:              6 7\n 105:      2 3 4 5\n 108:  0 1\n ---------------------\n\nContainers use the host kernel directly. All tasks inside a container are handled by the host CPU scheduler. Proxmox VE uses the Linux CFS (Completely Fair Scheduler) scheduler by default, which has additional bandwidth control options.\n\ncpulimit:\n\t\n\nYou can use this option to further limit assigned CPU time. Please note that this is a floating point number, so it is perfectly valid to assign two cores to a container, but restrict overall CPU consumption to half a core.\n\ncores: 2\ncpulimit: 0.5\n\ncpuunits:\n\t\n\nThis is a relative weight passed to the kernel scheduler. The larger the number is, the more CPU time this container gets. Number is relative to the weights of all the other running containers. The default is 100 (or 1024 if the host uses legacy cgroup v1). You can use this setting to prioritize some containers.\n\n11.4.3. Memory\n\nContainer memory is controlled using the cgroup memory controller.\n\nmemory:\n\t\n\nLimit overall memory usage. This corresponds to the memory.limit_in_bytes cgroup setting.\n\n\nswap:\n\t\n\nAllows the container to use additional swap memory from the host swap space. This corresponds to the memory.memsw.limit_in_bytes cgroup setting, which is set to the sum of both value (memory + swap).\n\n11.4.4. Mount Points\n\nThe root mount point is configured with the rootfs property. You can configure up to 256 additional mount points. The corresponding options are called mp0 to mp255. They can contain the following settings:\n\nrootfs: [volume=]<volume> [,acl=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container root. See below for a detailed description of all options.\n\nmp[n]: [volume=]<volume> ,mp=<Path> [,acl=<1|0>] [,backup=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container mount point. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume.\n\nacl=<boolean>\n\nExplicitly enable or disable ACL support.\n\nbackup=<boolean>\n\nWhether to include the mount point in backups (only used for volume mount points).\n\nmountoptions=<opt[;opt...]>\n\nExtra mount options for rootfs/mps.\n\nmp=<Path>\n\nPath to the mount point as seen from inside the container.\n\n\tMust not contain any symlinks for security reasons.\nquota=<boolean>\n\nEnable user quotas inside the container (not supported with zfs subvolumes)\n\nreplicate=<boolean> (default = 1)\n\nWill include this volume to a storage replica job.\n\nro=<boolean>\n\nRead-only mount point\n\nshared=<boolean> (default = 0)\n\nMark this non-volume mount point as available on all nodes.\n\n\tThis option does not share the mount point automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nVolume size (read only value).\n\nvolume=<volume>\n\nVolume, device or directory to mount into the container.\n\nCurrently there are three types of mount points: storage backed mount points, bind mounts, and device mounts.\n\nTypical container rootfs configuration\nrootfs: thin1:base-100-disk-1,size=8G\nStorage Backed Mount Points\n\nStorage backed mount points are managed by the Proxmox VE storage subsystem and come in three different flavors:\n\nImage based: these are raw images containing a single ext4 formatted file system.\n\nZFS subvolumes: these are technically bind mounts, but with managed storage, and thus allow resizing and snapshotting.\n\nDirectories: passing size=0 triggers a special case where instead of a raw image a directory is created.\n\n\tThe special option syntax STORAGE_ID:SIZE_IN_GB for storage backed mount point volumes will automatically allocate a volume of the specified size on the specified storage. For example, calling\npct set 100 -mp0 thin1:10,mp=/path/in/container\n\nwill allocate a 10GB volume on the storage thin1 and replace the volume ID place holder 10 with the allocated volume ID, and setup the moutpoint in the container at /path/in/container\n\nBind Mount Points\n\nBind mounts allow you to access arbitrary directories from your Proxmox VE host inside a container. Some potential use cases are:\n\nAccessing your home directory in the guest\n\nAccessing an USB device directory in the guest\n\nAccessing an NFS mount from the host in the guest\n\nBind mounts are considered to not be managed by the storage subsystem, so you cannot make snapshots or deal with quotas from inside the container. With unprivileged containers you might run into permission problems caused by the user mapping and cannot use ACLs.\n\n\tThe contents of bind mount points are not backed up when using vzdump.\n\tFor security reasons, bind mounts should only be established using source directories especially reserved for this purpose, e.g., a directory hierarchy under /mnt/bindmounts. Never bind mount system directories like /, /var or /etc into a container - this poses a great security risk.\n\tThe bind mount source path must not contain any symlinks.\n\nFor example, to make the directory /mnt/bindmounts/shared accessible in the container with ID 100 under the path /shared, add a configuration line such as:\n\nmp0: /mnt/bindmounts/shared,mp=/shared\n\ninto /etc/pve/lxc/100.conf.\n\nOr alternatively use the pct tool:\n\npct set 100 -mp0 /mnt/bindmounts/shared,mp=/shared\n\nto achieve the same result.\n\nDevice Mount Points\n\nDevice mount points allow to mount block devices of the host directly into the container. Similar to bind mounts, device mounts are not managed by Proxmox VE’s storage subsystem, but the quota and acl options will be honored.\n\n\tDevice mount points should only be used under special circumstances. In most cases a storage backed mount point offers the same performance and a lot more features.\n\tThe contents of device mount points are not backed up when using vzdump.\n11.4.5. Network\n\nYou can configure up to 10 network interfaces for a single container. The corresponding options are called net0 to net9, and they can contain the following setting:\n\nnet[n]: name=<string> [,bridge=<bridge>] [,firewall=<1|0>] [,gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,hwaddr=<XX:XX:XX:XX:XX:XX>] [,ip=<(IPv4/CIDR|dhcp|manual)>] [,ip6=<(IPv6/CIDR|auto|dhcp|manual)>] [,link_down=<1|0>] [,mtu=<integer>] [,rate=<mbps>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,type=<veth>]\n\nSpecifies network interfaces for the container.\n\nbridge=<bridge>\n\nBridge to attach the network device to.\n\nfirewall=<boolean>\n\nControls whether this interface’s firewall rules should be used.\n\ngw=<GatewayIPv4>\n\nDefault gateway for IPv4 traffic.\n\ngw6=<GatewayIPv6>\n\nDefault gateway for IPv6 traffic.\n\nhwaddr=<XX:XX:XX:XX:XX:XX>\n\nA common MAC address with the I/G (Individual/Group) bit not set.\n\nip=<(IPv4/CIDR|dhcp|manual)>\n\nIPv4 address in CIDR format.\n\nip6=<(IPv6/CIDR|auto|dhcp|manual)>\n\nIPv6 address in CIDR format.\n\nlink_down=<boolean>\n\nWhether this interface should be disconnected (like pulling the plug).\n\nmtu=<integer> (64 - 65535)\n\nMaximum transfer unit of the interface. (lxc.network.mtu)\n\nname=<string>\n\nName of the network device as seen from inside the container. (lxc.network.name)\n\nrate=<mbps>\n\nApply rate limiting to the interface\n\ntag=<integer> (1 - 4094)\n\nVLAN tag for this interface.\n\ntrunks=<vlanid[;vlanid...]>\n\nVLAN ids to pass through the interface\n\ntype=<veth>\n\nNetwork interface type.\n\n11.4.6. Automatic Start and Shutdown of Containers\n\nTo automatically start a container when the host system boots, select the option Start at boot in the Options panel of the container in the web interface or run the following command:\n\n# pct set CTID -onboot 1\nStart and Shutdown Order\n\nIf you want to fine tune the boot order of your containers, you can use the following parameters:\n\nStart/Shutdown order: Defines the start order priority. For example, set it to 1 if you want the CT to be the first to be started. (We use the reverse startup order for shutdown, so a container with a start order of 1 would be the last to be shut down)\n\nStartup delay: Defines the interval between this container start and subsequent containers starts. For example, set it to 240 if you want to wait 240 seconds before starting other containers.\n\nShutdown timeout: Defines the duration in seconds Proxmox VE should wait for the container to be offline after issuing a shutdown command. By default this value is set to 60, which means that Proxmox VE will issue a shutdown request, wait 60s for the machine to be offline, and if after 60s the machine is still online will notify that the shutdown action failed.\n\nPlease note that containers without a Start/Shutdown order parameter will always start after those where the parameter is set, and this parameter only makes sense between the machines running locally on a host, and not cluster-wide.\n\nIf you require a delay between the host boot and the booting of the first container, see the section on Proxmox VE Node Management.\n\n11.4.7. Hookscripts\n\nYou can add a hook script to CTs with the config property hookscript.\n\n# pct set 100 -hookscript local:snippets/hookscript.pl\n\nIt will be called during various phases of the guests lifetime. For an example and documentation see the example script under /usr/share/pve-docs/examples/guest-example-hookscript.pl.\n\n11.5. Security Considerations\n\nContainers use the kernel of the host system. This exposes an attack surface for malicious users. In general, full virtual machines provide better isolation. This should be considered if containers are provided to unknown or untrusted people.\n\nTo reduce the attack surface, LXC uses many security features like AppArmor, CGroups and kernel namespaces.\n\n11.5.1. AppArmor\n\nAppArmor profiles are used to restrict access to possibly dangerous actions. Some system calls, i.e. mount, are prohibited from execution.\n\nTo trace AppArmor activity, use:\n\n# dmesg | grep apparmor\n\nAlthough it is not recommended, AppArmor can be disabled for a container. This brings security risks with it. Some syscalls can lead to privilege escalation when executed within a container if the system is misconfigured or if a LXC or Linux Kernel vulnerability exists.\n\nTo disable AppArmor for a container, add the following line to the container configuration file located at /etc/pve/lxc/CTID.conf:\n\nlxc.apparmor.profile = unconfined\n\tPlease note that this is not recommended for production use.\n11.5.2. Control Groups (cgroup)\n\ncgroup is a kernel mechanism used to hierarchically organize processes and distribute system resources.\n\nThe main resources controlled via cgroups are CPU time, memory and swap limits, and access to device nodes. cgroups are also used to \"freeze\" a container before taking snapshots.\n\nThere are 2 versions of cgroups currently available, legacy and cgroupv2.\n\nSince Proxmox VE 7.0, the default is a pure cgroupv2 environment. Previously a \"hybrid\" setup was used, where resource control was mainly done in cgroupv1 with an additional cgroupv2 controller which could take over some subsystems via the cgroup_no_v1 kernel command-line parameter. (See the kernel parameter documentation for details.)\n\nCGroup Version Compatibility\n\nThe main difference between pure cgroupv2 and the old hybrid environments regarding Proxmox VE is that with cgroupv2 memory and swap are now controlled independently. The memory and swap settings for containers can map directly to these values, whereas previously only the memory limit and the limit of the sum of memory and swap could be limited.\n\nAnother important difference is that the devices controller is configured in a completely different way. Because of this, file system quotas are currently not supported in a pure cgroupv2 environment.\n\ncgroupv2 support by the container’s OS is needed to run in a pure cgroupv2 environment. Containers running systemd version 231 or newer support cgroupv2 [49], as do containers not using systemd as init system [50].\n\n\t\n\nCentOS 7 and Ubuntu 16.10 are two prominent Linux distributions releases, which have a systemd version that is too old to run in a cgroupv2 environment, you can either\n\nUpgrade the whole distribution to a newer release. For the examples above, that could be Ubuntu 18.04 or 20.04, and CentOS 8 (or RHEL/CentOS derivatives like AlmaLinux or Rocky Linux). This has the benefit to get the newest bug and security fixes, often also new features, and moving the EOL date in the future.\n\nUpgrade the Containers systemd version. If the distribution provides a backports repository this can be an easy and quick stop-gap measurement.\n\nMove the container, or its services, to a Virtual Machine. Virtual Machines have a much less interaction with the host, that’s why one can install decades old OS versions just fine there.\n\nSwitch back to the legacy cgroup controller. Note that while it can be a valid solution, it’s not a permanent one. Starting from Proxmox VE 9.0, the legacy controller will not be supported anymore.\n\nChanging CGroup Version\n\tIf file system quotas are not required and all containers support cgroupv2, it is recommended to stick to the new default.\n\nTo switch back to the previous version the following kernel command-line parameter can be used:\n\nsystemd.unified_cgroup_hierarchy=0\n\nSee this section on editing the kernel boot command line on where to add the parameter.\n\n11.6. Guest Operating System Configuration\n\nProxmox VE tries to detect the Linux distribution in the container, and modifies some files. Here is a short list of things done at container startup:\n\nset /etc/hostname\n\nto set the container name\n\nmodify /etc/hosts\n\nto allow lookup of the local hostname\n\nnetwork setup\n\npass the complete network setup to the container\n\nconfigure DNS\n\npass information about DNS servers\n\nadapt the init system\n\nfor example, fix the number of spawned getty processes\n\nset the root password\n\nwhen creating a new container\n\nrewrite ssh_host_keys\n\nso that each container has unique keys\n\nrandomize crontab\n\nso that cron does not start at the same time on all containers\n\nChanges made by Proxmox VE are enclosed by comment markers:\n\n# --- BEGIN PVE ---\n<data>\n# --- END PVE ---\n\nThose markers will be inserted at a reasonable location in the file. If such a section already exists, it will be updated in place and will not be moved.\n\nModification of a file can be prevented by adding a .pve-ignore. file for it. For instance, if the file /etc/.pve-ignore.hosts exists then the /etc/hosts file will not be touched. This can be a simple empty file created via:\n\n# touch /etc/.pve-ignore.hosts\n\nMost modifications are OS dependent, so they differ between different distributions and versions. You can completely disable modifications by manually setting the ostype to unmanaged.\n\nOS type detection is done by testing for certain files inside the container. Proxmox VE first checks the /etc/os-release file [51]. If that file is not present, or it does not contain a clearly recognizable distribution identifier the following distribution specific release files are checked.\n\nUbuntu\n\ninspect /etc/lsb-release (DISTRIB_ID=Ubuntu)\n\nDebian\n\ntest /etc/debian_version\n\nFedora\n\ntest /etc/fedora-release\n\nRedHat or CentOS\n\ntest /etc/redhat-release\n\nArchLinux\n\ntest /etc/arch-release\n\nAlpine\n\ntest /etc/alpine-release\n\nGentoo\n\ntest /etc/gentoo-release\n\n\tContainer start fails if the configured ostype differs from the auto detected type.\n11.7. Container Storage\n\nThe Proxmox VE LXC container storage model is more flexible than traditional container storage models. A container can have multiple mount points. This makes it possible to use the best suited storage for each application.\n\nFor example the root file system of the container can be on slow and cheap storage while the database can be on fast and distributed storage via a second mount point. See section Mount Points for further details.\n\nAny storage type supported by the Proxmox VE storage library can be used. This means that containers can be stored on local (for example lvm, zfs or directory), shared external (like iSCSI, NFS) or even distributed storage systems like Ceph. Advanced storage features like snapshots or clones can be used if the underlying storage supports them. The vzdump backup tool can use snapshots to provide consistent container backups.\n\nFurthermore, local devices or local directories can be mounted directly using bind mounts. This gives access to local resources inside a container with practically zero overhead. Bind mounts can be used as an easy way to share data between containers.\n\n11.7.1. FUSE Mounts\n\tBecause of existing issues in the Linux kernel’s freezer subsystem the usage of FUSE mounts inside a container is strongly advised against, as containers need to be frozen for suspend or snapshot mode backups.\n\nIf FUSE mounts cannot be replaced by other mounting mechanisms or storage technologies, it is possible to establish the FUSE mount on the Proxmox host and use a bind mount point to make it accessible inside the container.\n\n11.7.2. Using Quotas Inside Containers\n\nQuotas allow to set limits inside a container for the amount of disk space that each user can use.\n\n\tThis currently requires the use of legacy cgroups.\n\tThis only works on ext4 image based storage types and currently only works with privileged containers.\n\nActivating the quota option causes the following mount options to be used for a mount point: usrjquota=aquota.user,grpjquota=aquota.group,jqfmt=vfsv0\n\nThis allows quotas to be used like on any other system. You can initialize the /aquota.user and /aquota.group files by running:\n\n# quotacheck -cmug /\n# quotaon /\n\nThen edit the quotas using the edquota command. Refer to the documentation of the distribution running inside the container for details.\n\n\tYou need to run the above commands for every mount point by passing the mount point’s path instead of just /.\n11.7.3. Using ACLs Inside Containers\n\nThe standard Posix Access Control Lists are also available inside containers. ACLs allow you to set more detailed file ownership than the traditional user/group/others model.\n\n11.7.4. Backup of Container mount points\n\nTo include a mount point in backups, enable the backup option for it in the container configuration. For an existing mount point mp0\n\nmp0: guests:subvol-100-disk-1,mp=/root/files,size=8G\n\nadd backup=1 to enable it.\n\nmp0: guests:subvol-100-disk-1,mp=/root/files,size=8G,backup=1\n\tWhen creating a new mount point in the GUI, this option is enabled by default.\n\nTo disable backups for a mount point, add backup=0 in the way described above, or uncheck the Backup checkbox on the GUI.\n\n11.7.5. Replication of Containers mount points\n\nBy default, additional mount points are replicated when the Root Disk is replicated. If you want the Proxmox VE storage replication mechanism to skip a mount point, you can set the Skip replication option for that mount point. As of Proxmox VE 5.0, replication requires a storage of type zfspool. Adding a mount point to a different type of storage when the container has replication configured requires to have Skip replication enabled for that mount point.\n\n11.8. Backup and Restore\n11.8.1. Container Backup\n\nIt is possible to use the vzdump tool for container backup. Please refer to the vzdump manual page for details.\n\n11.8.2. Restoring Container Backups\n\nRestoring container backups made with vzdump is possible using the pct restore command. By default, pct restore will attempt to restore as much of the backed up container configuration as possible. It is possible to override the backed up configuration by manually setting container options on the command line (see the pct manual page for details).\n\n\tpvesm extractconfig can be used to view the backed up configuration contained in a vzdump archive.\n\nThere are two basic restore modes, only differing by their handling of mount points:\n\n“Simple” Restore Mode\n\nIf neither the rootfs parameter nor any of the optional mpX parameters are explicitly set, the mount point configuration from the backed up configuration file is restored using the following steps:\n\nExtract mount points and their options from backup\n\nCreate volumes for storage backed mount points on the storage provided with the storage parameter (default: local).\n\nExtract files from backup archive\n\nAdd bind and device mount points to restored configuration (limited to root user)\n\n\tSince bind and device mount points are never backed up, no files are restored in the last step, but only the configuration options. The assumption is that such mount points are either backed up with another mechanism (e.g., NFS space that is bind mounted into many containers), or not intended to be backed up at all.\n\nThis simple mode is also used by the container restore operations in the web interface.\n\n“Advanced” Restore Mode\n\nBy setting the rootfs parameter (and optionally, any combination of mpX parameters), the pct restore command is automatically switched into an advanced mode. This advanced mode completely ignores the rootfs and mpX configuration options contained in the backup archive, and instead only uses the options explicitly provided as parameters.\n\nThis mode allows flexible configuration of mount point settings at restore time, for example:\n\nSet target storages, volume sizes and other options for each mount point individually\n\nRedistribute backed up files according to new mount point scheme\n\nRestore to device and/or bind mount points (limited to root user)\n\n11.9. Managing Containers with pct\n\nThe “Proxmox Container Toolkit” (pct) is the command-line tool to manage Proxmox VE containers. It enables you to create or destroy containers, as well as control the container execution (start, stop, reboot, migrate, etc.). It can be used to set parameters in the config file of a container, for example the network configuration or memory limits.\n\n11.9.1. CLI Usage Examples\n\nCreate a container based on a Debian template (provided you have already downloaded the template via the web interface)\n\n# pct create 100 /var/lib/vz/template/cache/debian-10.0-standard_10.0-1_amd64.tar.gz\n\nStart container 100\n\n# pct start 100\n\nStart a login session via getty\n\n# pct console 100\n\nEnter the LXC namespace and run a shell as root user\n\n# pct enter 100\n\nDisplay the configuration\n\n# pct config 100\n\nAdd a network interface called eth0, bridged to the host bridge vmbr0, set the address and gateway, while it’s running\n\n# pct set 100 -net0 name=eth0,bridge=vmbr0,ip=192.168.15.147/24,gw=192.168.15.1\n\nReduce the memory of the container to 512MB\n\n# pct set 100 -memory 512\n\nDestroying a container always removes it from Access Control Lists and it always removes the firewall configuration of the container. You have to activate --purge, if you want to additionally remove the container from replication jobs, backup jobs and HA resource configurations.\n\n# pct destroy 100 --purge\n\nMove a mount point volume to a different storage.\n\n# pct move-volume 100 mp0 other-storage\n\nReassign a volume to a different CT. This will remove the volume mp0 from the source CT and attaches it as mp1 to the target CT. In the background the volume is being renamed so that the name matches the new owner.\n\n#  pct move-volume 100 mp0 --target-vmid 200 --target-volume mp1\n11.9.2. Obtaining Debugging Logs\n\nIn case pct start is unable to start a specific container, it might be helpful to collect debugging output by passing the --debug flag (replace CTID with the container’s CTID):\n\n# pct start CTID --debug\n\nAlternatively, you can use the following lxc-start command, which will save the debug log to the file specified by the -o output option:\n\n# lxc-start -n CTID -F -l DEBUG -o /tmp/lxc-CTID.log\n\nThis command will attempt to start the container in foreground mode, to stop the container run pct shutdown CTID or pct stop CTID in a second terminal.\n\nThe collected debug log is written to /tmp/lxc-CTID.log.\n\n\tIf you have changed the container’s configuration since the last start attempt with pct start, you need to run pct start at least once to also update the configuration used by lxc-start.\n11.10. Migration\n\nIf you have a cluster, you can migrate your Containers with\n\n# pct migrate <ctid> <target>\n\nThis works as long as your Container is offline. If it has local volumes or mount points defined, the migration will copy the content over the network to the target host if the same storage is defined there.\n\nRunning containers cannot live-migrated due to technical limitations. You can do a restart migration, which shuts down, moves and then starts a container again on the target node. As containers are very lightweight, this results normally only in a downtime of some hundreds of milliseconds.\n\nA restart migration can be done through the web interface or by using the --restart flag with the pct migrate command.\n\nA restart migration will shut down the Container and kill it after the specified timeout (the default is 180 seconds). Then it will migrate the Container like an offline migration and when finished, it starts the Container on the target node.\n\n11.11. Configuration\n\nThe /etc/pve/lxc/<CTID>.conf file stores container configuration, where <CTID> is the numeric ID of the given container. Like all other files stored inside /etc/pve/, they get automatically replicated to all other cluster nodes.\n\n\tCTIDs < 100 are reserved for internal purposes, and CTIDs need to be unique cluster wide.\nExample Container Configuration\nostype: debian\narch: amd64\nhostname: www\nmemory: 512\nswap: 512\nnet0: bridge=vmbr0,hwaddr=66:64:66:64:64:36,ip=dhcp,name=eth0,type=veth\nrootfs: local:107/vm-107-disk-1.raw,size=7G\n\nThe configuration files are simple text files. You can edit them using a normal text editor, for example, vi or nano. This is sometimes useful to do small corrections, but keep in mind that you need to restart the container to apply such changes.\n\nFor that reason, it is usually better to use the pct command to generate and modify those files, or do the whole thing using the GUI. Our toolkit is smart enough to instantaneously apply most changes to running containers. This feature is called “hot plug”, and there is no need to restart the container in that case.\n\nIn cases where a change cannot be hot-plugged, it will be registered as a pending change (shown in red color in the GUI). They will only be applied after rebooting the container.\n\n11.11.1. File Format\n\nThe container configuration file uses a simple colon separated key/value format. Each line has the following format:\n\n# this is a comment\nOPTION: value\n\nBlank lines in those files are ignored, and lines starting with a # character are treated as comments and are also ignored.\n\nIt is possible to add low-level, LXC style configuration directly, for example:\n\nlxc.init_cmd: /sbin/my_own_init\n\nor\n\nlxc.init_cmd = /sbin/my_own_init\n\nThe settings are passed directly to the LXC low-level tools.\n\n11.11.2. Snapshots\n\nWhen you create a snapshot, pct stores the configuration at snapshot time into a separate snapshot section within the same configuration file. For example, after creating a snapshot called “testsnapshot”, your configuration file will look like this:\n\nContainer configuration with snapshot\nmemory: 512\nswap: 512\nparent: testsnaphot\n...\n\n[testsnaphot]\nmemory: 512\nswap: 512\nsnaptime: 1457170803\n...\n\nThere are a few snapshot related properties like parent and snaptime. The parent property is used to store the parent/child relationship between snapshots. snaptime is the snapshot creation time stamp (Unix epoch).\n\n11.11.3. Options\narch: <amd64 | arm64 | armhf | i386 | riscv32 | riscv64> (default = amd64)\n\nOS architecture type.\n\ncmode: <console | shell | tty> (default = tty)\n\nConsole mode. By default, the console command tries to open a connection to one of the available tty devices. By setting cmode to console it tries to attach to /dev/console instead. If you set cmode to shell, it simply invokes a shell inside the container (no login).\n\nconsole: <boolean> (default = 1)\n\nAttach a console device (/dev/console) to the container.\n\ncores: <integer> (1 - 8192)\n\nThe number of cores assigned to the container. A container can use all available cores by default.\n\ncpulimit: <number> (0 - 8192) (default = 0)\n\nLimit of CPU usage.\n\n\tIf the computer has 2 CPUs, it has a total of 2 CPU time. Value 0 indicates no CPU limit.\ncpuunits: <integer> (0 - 500000) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a container. Argument is used in the kernel fair scheduler. The larger the number is, the more CPU time this container gets. Number is relative to the weights of all the other running guests.\n\ndebug: <boolean> (default = 0)\n\nTry to be more verbose. For now this only enables debug log-level on start.\n\ndescription: <string>\n\nDescription for the Container. Shown in the web-interface CT’s summary. This is saved as comment inside the configuration file.\n\ndev[n]: [[path=]<Path>] [,gid=<integer>] [,mode=<Octal access mode>] [,uid=<integer>]\n\nDevice to pass through to the container\n\ngid=<integer> (0 - N)\n\nGroup ID to be assigned to the device node\n\nmode=<Octal access mode>\n\nAccess mode to be set on the device node\n\npath=<Path>\n\nPath to the device to pass through to the container\n\nuid=<integer> (0 - N)\n\nUser ID to be assigned to the device node\n\nfeatures: [force_rw_sys=<1|0>] [,fuse=<1|0>] [,keyctl=<1|0>] [,mknod=<1|0>] [,mount=<fstype;fstype;...>] [,nesting=<1|0>]\n\nAllow containers access to advanced features.\n\nforce_rw_sys=<boolean> (default = 0)\n\nMount /sys in unprivileged containers as rw instead of mixed. This can break networking under newer (>= v245) systemd-network use.\n\nfuse=<boolean> (default = 0)\n\nAllow using fuse file systems in a container. Note that interactions between fuse and the freezer cgroup can potentially cause I/O deadlocks.\n\nkeyctl=<boolean> (default = 0)\n\nFor unprivileged containers only: Allow the use of the keyctl() system call. This is required to use docker inside a container. By default unprivileged containers will see this system call as non-existent. This is mostly a workaround for systemd-networkd, as it will treat it as a fatal error when some keyctl() operations are denied by the kernel due to lacking permissions. Essentially, you can choose between running systemd-networkd or docker.\n\nmknod=<boolean> (default = 0)\n\nAllow unprivileged containers to use mknod() to add certain device nodes. This requires a kernel with seccomp trap to user space support (5.3 or newer). This is experimental.\n\nmount=<fstype;fstype;...>\n\nAllow mounting file systems of specific types. This should be a list of file system types as used with the mount command. Note that this can have negative effects on the container’s security. With access to a loop device, mounting a file can circumvent the mknod permission of the devices cgroup, mounting an NFS file system can block the host’s I/O completely and prevent it from rebooting, etc.\n\nnesting=<boolean> (default = 0)\n\nAllow nesting. Best used with unprivileged containers with additional id mapping. Note that this will expose procfs and sysfs contents of the host to the guest.\n\nhookscript: <string>\n\nScript that will be exectued during various steps in the containers lifetime.\n\nhostname: <string>\n\nSet a host name for the container.\n\nlock: <backup | create | destroyed | disk | fstrim | migrate | mounted | rollback | snapshot | snapshot-delete>\n\nLock/unlock the container.\n\nmemory: <integer> (16 - N) (default = 512)\n\nAmount of RAM for the container in MB.\n\nmp[n]: [volume=]<volume> ,mp=<Path> [,acl=<1|0>] [,backup=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container mount point. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume.\n\nacl=<boolean>\n\nExplicitly enable or disable ACL support.\n\nbackup=<boolean>\n\nWhether to include the mount point in backups (only used for volume mount points).\n\nmountoptions=<opt[;opt...]>\n\nExtra mount options for rootfs/mps.\n\nmp=<Path>\n\nPath to the mount point as seen from inside the container.\n\n\tMust not contain any symlinks for security reasons.\nquota=<boolean>\n\nEnable user quotas inside the container (not supported with zfs subvolumes)\n\nreplicate=<boolean> (default = 1)\n\nWill include this volume to a storage replica job.\n\nro=<boolean>\n\nRead-only mount point\n\nshared=<boolean> (default = 0)\n\nMark this non-volume mount point as available on all nodes.\n\n\tThis option does not share the mount point automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nVolume size (read only value).\n\nvolume=<volume>\n\nVolume, device or directory to mount into the container.\n\nnameserver: <string>\n\nSets DNS server IP address for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\nnet[n]: name=<string> [,bridge=<bridge>] [,firewall=<1|0>] [,gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,hwaddr=<XX:XX:XX:XX:XX:XX>] [,ip=<(IPv4/CIDR|dhcp|manual)>] [,ip6=<(IPv6/CIDR|auto|dhcp|manual)>] [,link_down=<1|0>] [,mtu=<integer>] [,rate=<mbps>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,type=<veth>]\n\nSpecifies network interfaces for the container.\n\nbridge=<bridge>\n\nBridge to attach the network device to.\n\nfirewall=<boolean>\n\nControls whether this interface’s firewall rules should be used.\n\ngw=<GatewayIPv4>\n\nDefault gateway for IPv4 traffic.\n\ngw6=<GatewayIPv6>\n\nDefault gateway for IPv6 traffic.\n\nhwaddr=<XX:XX:XX:XX:XX:XX>\n\nA common MAC address with the I/G (Individual/Group) bit not set.\n\nip=<(IPv4/CIDR|dhcp|manual)>\n\nIPv4 address in CIDR format.\n\nip6=<(IPv6/CIDR|auto|dhcp|manual)>\n\nIPv6 address in CIDR format.\n\nlink_down=<boolean>\n\nWhether this interface should be disconnected (like pulling the plug).\n\nmtu=<integer> (64 - 65535)\n\nMaximum transfer unit of the interface. (lxc.network.mtu)\n\nname=<string>\n\nName of the network device as seen from inside the container. (lxc.network.name)\n\nrate=<mbps>\n\nApply rate limiting to the interface\n\ntag=<integer> (1 - 4094)\n\nVLAN tag for this interface.\n\ntrunks=<vlanid[;vlanid...]>\n\nVLAN ids to pass through the interface\n\ntype=<veth>\n\nNetwork interface type.\n\nonboot: <boolean> (default = 0)\n\nSpecifies whether a container will be started during system bootup.\n\nostype: <alpine | archlinux | centos | debian | devuan | fedora | gentoo | nixos | opensuse | ubuntu | unmanaged>\n\nOS type. This is used to setup configuration inside the container, and corresponds to lxc setup scripts in /usr/share/lxc/config/<ostype>.common.conf. Value unmanaged can be used to skip and OS specific setup.\n\nprotection: <boolean> (default = 0)\n\nSets the protection flag of the container. This will prevent the CT or CT’s disk remove/update operation.\n\nrootfs: [volume=]<volume> [,acl=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container root.\n\nacl=<boolean>\n\nExplicitly enable or disable ACL support.\n\nmountoptions=<opt[;opt...]>\n\nExtra mount options for rootfs/mps.\n\nquota=<boolean>\n\nEnable user quotas inside the container (not supported with zfs subvolumes)\n\nreplicate=<boolean> (default = 1)\n\nWill include this volume to a storage replica job.\n\nro=<boolean>\n\nRead-only mount point\n\nshared=<boolean> (default = 0)\n\nMark this non-volume mount point as available on all nodes.\n\n\tThis option does not share the mount point automatically, it assumes it is shared already!\nsize=<DiskSize>\n\nVolume size (read only value).\n\nvolume=<volume>\n\nVolume, device or directory to mount into the container.\n\nsearchdomain: <string>\n\nSets DNS search domains for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\nstartup: `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\nswap: <integer> (0 - N) (default = 512)\n\nAmount of SWAP for the container in MB.\n\ntags: <string>\n\nTags of the Container. This is only meta information.\n\ntemplate: <boolean> (default = 0)\n\nEnable/disable Template.\n\ntimezone: <string>\n\nTime zone to use in the container. If option isn’t set, then nothing will be done. Can be set to host to match the host time zone, or an arbitrary time zone option from /usr/share/zoneinfo/zone.tab\n\ntty: <integer> (0 - 6) (default = 2)\n\nSpecify the number of tty available to the container\n\nunprivileged: <boolean> (default = 0)\n\nMakes the container run as unprivileged user. (Should not be modified manually.)\n\nunused[n]: [volume=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\nvolume=<volume>\n\nThe volume that is not used currently.\n\n11.12. Locks\n\nContainer migrations, snapshots and backups (vzdump) set a lock to prevent incompatible concurrent actions on the affected container. Sometimes you need to remove such a lock manually (e.g., after a power failure).\n\n# pct unlock <CTID>\n\tOnly do this if you are sure the action which set the lock is no longer running.\n12. Software-Defined Network\n\nThe Software-Defined Network (SDN) feature in Proxmox VE enables the creation of virtual zones and networks (VNets). This functionality simplifies advanced networking configurations and multitenancy setup.\n\n12.1. Introduction\n\nThe Proxmox VE SDN allows for separation and fine-grained control of virtual guest networks, using flexible, software-controlled configurations.\n\nSeparation is managed through zones, virtual networks (VNets), and subnets. A zone is its own virtually separated network area. A VNet is a virtual network that belongs to a zone. A subnet is an IP range inside a VNet.\n\nDepending on the type of the zone, the network behaves differently and offers specific features, advantages, and limitations.\n\nUse cases for SDN range from an isolated private network on each individual node to complex overlay networks across multiple PVE clusters on different locations.\n\nAfter configuring an VNet in the cluster-wide datacenter SDN administration interface, it is available as a common Linux bridge, locally on each node, to be assigned to VMs and Containers.\n\n12.2. Support Status\n12.2.1. History\n\nThe Proxmox VE SDN stack has been available as an experimental feature since 2019 and has been continuously improved and tested by many developers and users. With its integration into the web interface in Proxmox VE 6.2, a significant milestone towards broader integration was achieved. During the Proxmox VE 7 release cycle, numerous improvements and features were added. Based on user feedback, it became apparent that the fundamental design choices and their implementation were quite sound and stable. Consequently, labeling it as ‘experimental’ did not do justice to the state of the SDN stack. For Proxmox VE 8, a decision was made to lay the groundwork for full integration of the SDN feature by elevating the management of networks and interfaces to a core component in the Proxmox VE access control stack. In Proxmox VE 8.1, two major milestones were achieved: firstly, DHCP integration was added to the IP address management (IPAM) feature, and secondly, the SDN integration is now installed by default.\n\n12.2.2. Current Status\n\nThe current support status for the various layers of our SDN installation is as follows:\n\nCore SDN, which includes VNet management and its integration with the Proxmox VE stack, is fully supported.\n\nIPAM, including DHCP management for virtual guests, is in tech preview.\n\nComplex routing via FRRouting and controller integration are in tech preview.\n\n12.3. Installation\n12.3.1. SDN Core\n\nSince Proxmox VE 8.1 the core Software-Defined Network (SDN) packages are installed by default.\n\nIf you upgrade from an older version, you need to install the libpve-network-perl package on every node:\n\napt update\napt install libpve-network-perl\n\tProxmox VE version 7.0 and above have the ifupdown2 package installed by default. If you originally installed your system with an older version, you need to explicitly install the ifupdown2 package.\n\nAfter installation, you need to ensure that the following line is present at the end of the /etc/network/interfaces configuration file on all nodes, so that the SDN configuration gets included and activated.\n\nsource /etc/network/interfaces.d/*\n12.3.2. DHCP IPAM\n\nThe DHCP integration into the built-in PVE IP Address Management stack currently uses dnsmasq for giving out DHCP leases. This is currently opt-in.\n\nTo use that feature you need to install the dnsmasq package on every node:\n\napt update\napt install dnsmasq\n# disable default instance\nsystemctl disable --now dnsmasq\n12.3.3. FRRouting\n\nThe Proxmox VE SDN stack uses the FRRouting project for advanced setups. This is currently opt-in.\n\nTo use the SDN routing integration you need to install the frr-pythontools package on all nodes:\n\napt update\napt install frr-pythontools\n12.4. Configuration Overview\n\nConfiguration is done at the web UI at datacenter level, separated into the following sections:\n\nSDN:: Here you get an overview of the current active SDN state, and you can apply all pending changes to the whole cluster.\n\nZones: Create and manage the virtually separated network zones\n\nVNets VNets: Create virtual network bridges and manage subnets\n\nThe Options category allows adding and managing additional services to be used in your SDN setup.\n\nControllers: For controlling layer 3 routing in complex setups\n\nDHCP: Define a DHCP server for a zone that automatically allocates IPs for guests in the IPAM and leases them to the guests via DHCP.\n\nIPAM: Enables external for IP address management for guests\n\nDNS: Define a DNS server integration for registering virtual guests' hostname and IP addresses\n\n12.5. Technology & Configuration\n\nThe Proxmox VE Software-Defined Network implementation uses standard Linux networking as much as possible. The reason for this is that modern Linux networking provides almost all needs for a feature full SDN implementation and avoids adding external dependencies and reduces the overall amount of components that can break.\n\nThe Proxmox VE SDN configurations are located in /etc/pve/sdn, which is shared with all other cluster nodes through the Proxmox VE configuration file system. Those configurations get translated to the respective configuration formats of the tools that manage the underlying network stack (for example ifupdown2 or frr).\n\nNew changes are not immediately applied but recorded as pending first. You can then apply a set of different changes all at once in the main SDN overview panel on the web interface. This system allows to roll-out various changes as single atomic one.\n\nThe SDN tracks the rolled-out state through the .running-config and .version files located in /etc/pve/sdn.\n\n12.6. Zones\n\nA zone defines a virtually separated network. Zones are restricted to specific nodes and assigned permissions, in order to restrict users to a certain zone and its contained VNets.\n\nDifferent technologies can be used for separation:\n\nSimple: Isolated Bridge. A simple layer 3 routing bridge (NAT)\n\nVLAN: Virtual LANs are the classic method of subdividing a LAN\n\nQinQ: Stacked VLAN (formally known as IEEE 802.1ad)\n\nVXLAN: Layer 2 VXLAN network via a UDP tunnel\n\nEVPN (BGP EVPN): VXLAN with BGP to establish Layer 3 routing\n\n12.6.1. Common Options\n\nThe following options are available for all zone types:\n\nNodes\n\nThe nodes which the zone and associated VNets should be deployed on.\n\nIPAM\n\nUse an IP Address Management (IPAM) tool to manage IPs in the zone. Optional, defaults to pve.\n\nDNS\n\nDNS API server. Optional.\n\nReverseDNS\n\nReverse DNS API server. Optional.\n\nDNSZone\n\nDNS domain name. Used to register hostnames, such as <hostname>.<domain>. The DNS zone must already exist on the DNS server. Optional.\n\n12.6.2. Simple Zones\n\nThis is the simplest plugin. It will create an isolated VNet bridge. This bridge is not linked to a physical interface, and VM traffic is only local on each the node. It can be used in NAT or routed setups.\n\n12.6.3. VLAN Zones\n\nThe VLAN plugin uses an existing local Linux or OVS bridge to connect to the node’s physical interface. It uses VLAN tagging defined in the VNet to isolate the network segments. This allows connectivity of VMs between different nodes.\n\nVLAN zone configuration options:\n\nBridge\n\nThe local bridge or OVS switch, already configured on each node that allows node-to-node connection.\n\n12.6.4. QinQ Zones\n\nQinQ also known as VLAN stacking, that uses multiple layers of VLAN tags for isolation. The QinQ zone defines the outer VLAN tag (the Service VLAN) whereas the inner VLAN tag is defined by the VNet.\n\n\tYour physical network switches must support stacked VLANs for this configuration.\n\nQinQ zone configuration options:\n\nBridge\n\nA local, VLAN-aware bridge that is already configured on each local node\n\nService VLAN\n\nThe main VLAN tag of this zone\n\nService VLAN Protocol\n\nAllows you to choose between an 802.1q (default) or 802.1ad service VLAN type.\n\nMTU\n\nDue to the double stacking of tags, you need 4 more bytes for QinQ VLANs. For example, you must reduce the MTU to 1496 if you physical interface MTU is 1500.\n\n12.6.5. VXLAN Zones\n\nThe VXLAN plugin establishes a tunnel (overlay) on top of an existing network (underlay). This encapsulates layer 2 Ethernet frames within layer 4 UDP datagrams using the default destination port 4789.\n\nYou have to configure the underlay network yourself to enable UDP connectivity between all peers.\n\nYou can, for example, create a VXLAN overlay network on top of public internet, appearing to the VMs as if they share the same local Layer 2 network.\n\n\tVXLAN on its own does does not provide any encryption. When joining multiple sites via VXLAN, make sure to establish a secure connection between the site, for example by using a site-to-site VPN.\n\nVXLAN zone configuration options:\n\nPeers Address List\n\nA list of IP addresses of each node in the VXLAN zone. This can be external nodes reachable at this IP address. All nodes in the cluster need to be mentioned here.\n\nMTU\n\nBecause VXLAN encapsulation uses 50 bytes, the MTU needs to be 50 bytes lower than the outgoing physical interface.\n\n12.6.6. EVPN Zones\n\nThe EVPN zone creates a routable Layer 3 network, capable of spanning across multiple clusters. This is achieved by establishing a VPN and utilizing BGP as the routing protocol.\n\nThe VNet of EVPN can have an anycast IP address and/or MAC address. The bridge IP is the same on each node, meaning a virtual guest can use this address as gateway.\n\nRouting can work across VNets from different zones through a VRF (Virtual Routing and Forwarding) interface.\n\nEVPN zone configuration options:\n\nVRF VXLAN ID\n\nA VXLAN-ID used for dedicated routing interconnect between VNets. It must be different than the VXLAN-ID of the VNets.\n\nController\n\nThe EVPN-controller to use for this zone. (See controller plugins section).\n\nVNet MAC Address\n\nAnycast MAC address that gets assigned to all VNets in this zone. Will be auto-generated if not defined.\n\nExit Nodes\n\nNodes that shall be configured as exit gateways from the EVPN network, through the real network. The configured nodes will announce a default route in the EVPN network. Optional.\n\nPrimary Exit Node\n\nIf you use multiple exit nodes, force traffic through this primary exit node, instead of load-balancing on all nodes. Optional but necessary if you want to use SNAT or if your upstream router doesn’t support ECMP.\n\nExit Nodes Local Routing\n\nThis is a special option if you need to reach a VM/CT service from an exit node. (By default, the exit nodes only allow forwarding traffic between real network and EVPN network). Optional.\n\nAdvertise Subnets\n\nAnnounce the full subnet in the EVPN network. If you have silent VMs/CTs (for example, if you have multiple IPs and the anycast gateway doesn’t see traffic from theses IPs, the IP addresses won’t be able to be reached inside the EVPN network). Optional.\n\nDisable ARP ND Suppression\n\nDon’t suppress ARP or ND (Neighbor Discovery) packets. This is required if you use floating IPs in your VMs (IP and MAC addresses are being moved between systems). Optional.\n\nRoute-target Import\n\nAllows you to import a list of external EVPN route targets. Used for cross-DC or different EVPN network interconnects. Optional.\n\nMTU\n\nBecause VXLAN encapsulation uses 50 bytes, the MTU needs to be 50 bytes less than the maximal MTU of the outgoing physical interface. Optional, defaults to 1450.\n\n12.7. VNets\n\nAfter creating a virtual network (VNet) through the SDN GUI, a local network interface with the same name is available on each node. To connect a guest to the VNet, assign the interface to the guest and set the IP address accordingly.\n\nDepending on the zone, these options have different meanings and are explained in the respective zone section in this document.\n\n\tIn the current state, some options may have no effect or won’t work in certain zones.\n\nVNet configuration options:\n\nID\n\nAn up to 8 character ID to identify a VNet\n\nComment\n\nMore descriptive identifier. Assigned as an alias on the interface. Optional\n\nZone\n\nThe associated zone for this VNet\n\nTag\n\nThe unique VLAN or VXLAN ID\n\nVLAN Aware\n\nEnables vlan-aware option on the interface, enabling configuration in the guest.\n\n12.8. Subnets\n\nA subnet define a specific IP range, described by the CIDR network address. Each VNet, can have one or more subnets.\n\nA subnet can be used to:\n\nRestrict the IP addresses you can define on a specific VNet\n\nAssign routes/gateways on a VNet in layer 3 zones\n\nEnable SNAT on a VNet in layer 3 zones\n\nAuto assign IPs on virtual guests (VM or CT) through IPAM plugins\n\nDNS registration through DNS plugins\n\nIf an IPAM server is associated with the subnet zone, the subnet prefix will be automatically registered in the IPAM.\n\nSubnet configuration options:\n\nID\n\nA CIDR network address, for example 10.0.0.0/8\n\nGateway\n\nThe IP address of the network’s default gateway. On layer 3 zones (Simple/EVPN plugins), it will be deployed on the VNet.\n\nSNAT\n\nEnable Source NAT which allows VMs from inside a VNet to connect to the outside network by forwarding the packets to the nodes outgoing interface. On EVPN zones, forwarding is done on EVPN gateway-nodes. Optional.\n\nDNS Zone Prefix\n\nAdd a prefix to the domain registration, like <hostname>.prefix.<domain> Optional.\n\n12.9. Controllers\n\nSome zones implement a separated control and data plane that require an external controller to manage the VNet’s control plane.\n\nCurrently, only the EVPN zone requires an external controller.\n\n12.9.1. EVPN Controller\n\nThe EVPN, zone requires an external controller to manage the control plane. The EVPN controller plugin configures the Free Range Routing (frr) router.\n\nTo enable the EVPN controller, you need to install frr on every node that shall participate in the EVPN zone.\n\napt install frr frr-pythontools\n\nEVPN controller configuration options:\n\nASN #\n\nA unique BGP ASN number. It’s highly recommended to use a private ASN number (64512 – 65534, 4200000000 – 4294967294), as otherwise you could end up breaking global routing by mistake.\n\nPeers\n\nAn IP list of all nodes that are part of the EVPN zone. (could also be external nodes or route reflector servers)\n\n12.9.2. BGP Controller\n\nThe BGP controller is not used directly by a zone. You can use it to configure FRR to manage BGP peers.\n\nFor BGP-EVPN, it can be used to define a different ASN by node, so doing EBGP. It can also be used to export EVPN routes to an external BGP peer.\n\n\tBy default, for a simple full mesh EVPN, you don’t need to define a BGP controller.\n\nBGP controller configuration options:\n\nNode\n\nThe node of this BGP controller\n\nASN #\n\nA unique BGP ASN number. It’s highly recommended to use a private ASN number in the range (64512 - 65534) or (4200000000 - 4294967294), as otherwise you could break global routing by mistake.\n\nPeer\n\nA list of peer IP addresses you want to communicate with using the underlying BGP network.\n\nEBGP\n\nIf your peer’s remote-AS is different, this enables EBGP.\n\nLoopback Interface\n\nUse a loopback or dummy interface as the source of the EVPN network (for multipath).\n\nebgp-mutltihop\n\nIncrease the number of hops to reach peers, in case they are not directly connected or they use loopback.\n\nbgp-multipath-as-path-relax\n\nAllow ECMP if your peers have different ASN.\n\n12.9.3. ISIS Controller\n\nThe ISIS controller is not used directly by a zone. You can use it to configure FRR to export EVPN routes to an ISIS domain.\n\nISIS controller configuration options:\n\nNode\n\nThe node of this ISIS controller.\n\nDomain\n\nA unique ISIS domain.\n\nNetwork Entity Title\n\nA Unique ISIS network address that identifies this node.\n\nInterfaces\n\nA list of physical interface(s) used by ISIS.\n\nLoopback\n\nUse a loopback or dummy interface as the source of the EVPN network (for multipath).\n\n12.10. IPAM\n\nIP Address Management (IPAM) tools manage the IP addresses of clients on the network. SDN in Proxmox VE uses IPAM for example to find free IP addresses for new guests.\n\nA single IPAM instance can be associated with one or more zones.\n\n12.10.1. PVE IPAM Plugin\n\nThe default built-in IPAM for your Proxmox VE cluster.\n\nYou can inspect the current status of the PVE IPAM Plugin via the IPAM panel in the SDN section of the datacenter configuration. This UI can be used to create, update and delete IP mappings. This is particularly convenient in conjunction with the DHCP feature.\n\nIf you are using DHCP, you can use the IPAM panel to create or edit leases for specific VMs, which enables you to change the IPs allocated via DHCP. When editing an IP of a VM that is using DHCP you must make sure to force the guest to acquire a new DHCP leases. This can usually be done by reloading the network stack of the guest or rebooting it.\n\n12.10.2. NetBox IPAM Plugin\n\nNetBox is an open-source IP Address Management (IPAM) and datacenter infrastructure management (DCIM) tool.\n\nTo integrate NetBox with Proxmox VE SDN, create an API token in NetBox as described here: https://docs.netbox.dev/en/stable/integrations/rest-api/#tokens\n\nThe NetBox configuration properties are:\n\nURL\n\nThe NetBox REST API endpoint: http://yournetbox.domain.com/api\n\nToken\n\nAn API access token\n\n12.10.3. phpIPAM Plugin\n\nIn phpIPAM you need to create an \"application\" and add an API token with admin privileges to the application.\n\nThe phpIPAM configuration properties are:\n\nURL\n\nThe REST-API endpoint: http://phpipam.domain.com/api/<appname>/\n\nToken\n\nAn API access token\n\nSection\n\nAn integer ID. Sections are a group of subnets in phpIPAM. Default installations use sectionid=1 for customers.\n\n12.11. DNS\n\nThe DNS plugin in Proxmox VE SDN is used to define a DNS API server for registration of your hostname and IP address. A DNS configuration is associated with one or more zones, to provide DNS registration for all the subnet IPs configured for a zone.\n\n12.11.1. PowerDNS Plugin\n\nhttps://doc.powerdns.com/authoritative/http-api/index.html\n\nYou need to enable the web server and the API in your PowerDNS config:\n\napi=yes\napi-key=arandomgeneratedstring\nwebserver=yes\nwebserver-port=8081\n\nThe PowerDNS configuration options are:\n\nurl\n\nThe REST API endpoint: http://yourpowerdnserver.domain.com:8081/api/v1/servers/localhost\n\nkey\n\nAn API access key\n\nttl\n\nThe default TTL for records\n\n12.12. DHCP\n\nThe DHCP plugin in Proxmox VE SDN can be used to automatically deploy a DHCP server for a Zone. It provides DHCP for all Subnets in a Zone that have a DHCP range configured. Currently the only available backend plugin for DHCP is the dnsmasq plugin.\n\nThe DHCP plugin works by allocating an IP in the IPAM plugin configured in the Zone when adding a new network interface to a VM/CT. You can find more information on how to configure an IPAM in the respective section of our documentation.\n\nWhen the VM starts, a mapping for the MAC address and IP gets created in the DHCP plugin of the zone. When the network interfaces is removed or the VM/CT are destroyed, then the entry in the IPAM and the DHCP server are deleted as well.\n\n\tSome features (adding/editing/removing IP mappings) are currently only available when using the PVE IPAM plugin.\n12.12.1. Configuration\n\nYou can enable automatic DHCP for a zone in the Web UI via the Zones panel and enabling DHCP in the advanced options of a zone.\n\n\tCurrently only Simple Zones have support for automatic DHCP\n\nAfter automatic DHCP has been enabled for a Zone, DHCP Ranges need to be configured for the subnets in a Zone. In order to that, go to the Vnets panel and select the Subnet for which you want to configure DHCP ranges. In the edit dialogue you can configure DHCP ranges in the respective Tab. Alternatively you can set DHCP ranges for a Subnet via the following CLI command:\n\npvesh set /cluster/sdn/vnets/<vnet>/subnets/<subnet>\n -dhcp-range start-address=10.0.1.100,end-address=10.0.1.200\n -dhcp-range start-address=10.0.2.100,end-address=10.0.2.200\n\nYou also need to have a gateway configured for the subnet - otherwise automatic DHCP will not work.\n\nThe DHCP plugin will then allocate IPs in the IPAM only in the configured ranges.\n\nDo not forget to follow the installation steps for the dnsmasq DHCP plugin as well.\n\n12.12.2. Plugins\nDnsmasq Plugin\n\nCurrently this is the only DHCP plugin and therefore the plugin that gets used when you enable DHCP for a zone.\n\nInstallation\n\nFor installation see the DHCP IPAM section.\n\nConfiguration\n\nThe plugin will create a new systemd service for each zone that dnsmasq gets deployed to. The name for the service is dnsmasq@<zone>. The lifecycle of this service is managed by the DHCP plugin.\n\nThe plugin automatically generates the following configuration files in the folder /etc/dnsmasq.d/<zone>:\n\n00-default.conf\n\nThis contains the default global configuration for a dnsmasq instance.\n\n10-<zone>-<subnet_cidr>.conf\n\nThis file configures specific options for a subnet, such as the DNS server that should get configured via DHCP.\n\n10-<zone>-<subnet_cidr>.ranges.conf\n\nThis file configures the DHCP ranges for the dnsmasq instance.\n\nethers\n\nThis file contains the MAC-address and IP mappings from the IPAM plugin. In order to override those mappings, please use the respective IPAM plugin rather than editing this file, as it will get overwritten by the dnsmasq plugin.\n\nYou must not edit any of the above files, since they are managed by the DHCP plugin. In order to customize the dnsmasq configuration you can create additional files (e.g. 90-custom.conf) in the configuration folder - they will not get changed by the dnsmasq DHCP plugin.\n\nConfiguration files are read in order, so you can control the order of the configuration directives by naming your custom configuration files appropriately.\n\nDHCP leases are stored in the file /var/lib/misc/dnsmasq.<zone>.leases.\n\nWhen using the PVE IPAM plugin, you can update, create and delete DHCP leases. For more information please consult the documentation of the PVE IPAM plugin. Changing DHCP leases is currently not supported for the other IPAM plugins.\n\n12.13. Examples\n\nThis section presents multiple configuration examples tailored for common SDN use cases. It aims to offer tangible implementations, providing additional details to enhance comprehension of the available configuration options.\n\n12.13.1. Simple Zone Example\n\nSimple zone networks create an isolated network for guests on a single host to connect to each other.\n\n\tconnection between guests are possible if all guests reside on a same host but cannot be reached on other nodes.\n\nCreate a simple zone named simple.\n\nAdd a VNet names vnet1.\n\nCreate a Subnet with a gateway and the SNAT option enabled.\n\nThis creates a network bridge vnet1 on the node. Assign this bridge to the guests that shall join the network and configure an IP address.\n\nThe network interface configuration in two VMs may look like this which allows them to communicate via the 10.0.1.0/24 network.\n\nallow-hotplug ens19\niface ens19 inet static\n        address 10.0.1.14/24\nallow-hotplug ens19\niface ens19 inet static\n        address 10.0.1.15/24\n12.13.2. Source NAT Example\n\nIf you want to allow outgoing connections for guests in the simple network zone the simple zone offers a Source NAT (SNAT) option.\n\nStarting from the configuration above, Add a Subnet to the VNet vnet1, set a gateway IP and enable the SNAT option.\n\nSubnet: 172.16.0.0/24\nGateway: 172.16.0.1\nSNAT: checked\n\nIn the guests configure the static IP address inside the subnet’s IP range.\n\nThe node itself will join this network with the Gateway IP 172.16.0.1 and function as the NAT gateway for guests within the subnet range.\n\n12.13.3. VLAN Setup Example\n\nWhen VMs on different nodes need to communicate through an isolated network, the VLAN zone allows network level isolation using VLAN tags.\n\nCreate a VLAN zone named myvlanzone:\n\nID: myvlanzone\nBridge: vmbr0\n\nCreate a VNet named myvnet1 with VLAN tag 10 and the previously created myvlanzone.\n\nID: myvnet1\nZone: myvlanzone\nTag: 10\n\nApply the configuration through the main SDN panel, to create VNets locally on each node.\n\nCreate a Debian-based virtual machine (vm1) on node1, with a vNIC on myvnet1.\n\nUse the following network configuration for this VM:\n\nauto eth0\niface eth0 inet static\n        address 10.0.3.100/24\n\nCreate a second virtual machine (vm2) on node2, with a vNIC on the same VNet myvnet1 as vm1.\n\nUse the following network configuration for this VM:\n\nauto eth0\niface eth0 inet static\n        address 10.0.3.101/24\n\nFollowing this, you should be able to ping between both VMs using that network.\n\n12.13.4. QinQ Setup Example\n\nThis example configures two QinQ zones and adds two VMs to each zone to demonstrate the additional layer of VLAN tags which allows the configuration of more isolated VLANs.\n\nA typical use case for this configuration is a hosting provider that provides an isolated network to customers for VM communication but isolates the VMs from other customers.\n\nCreate a QinQ zone named qinqzone1 with service VLAN 20\n\nID: qinqzone1\nBridge: vmbr0\nService VLAN: 20\n\nCreate another QinQ zone named qinqzone2 with service VLAN 30\n\nID: qinqzone2\nBridge: vmbr0\nService VLAN: 30\n\nCreate a VNet named myvnet1 with VLAN-ID 100 on the previously created qinqzone1 zone.\n\nID: qinqvnet1\nZone: qinqzone1\nTag: 100\n\nCreate a myvnet2 with VLAN-ID 100 on the qinqzone2 zone.\n\nID: qinqvnet2\nZone: qinqzone2\nTag: 100\n\nApply the configuration on the main SDN web interface panel to create VNets locally on each node.\n\nCreate four Debian-bases virtual machines (vm1, vm2, vm3, vm4) and add network interfaces to vm1 and vm2 with bridge qinqvnet1 and vm3 and vm4 with bridge qinqvnet2.\n\nInside the VM, configure the IP addresses of the interfaces, for example via /etc/network/interfaces:\n\nauto eth0\niface eth0 inet static\n        address 10.0.3.101/24\n\nConfigure all four VMs to have IP addresses from the 10.0.3.101 to 10.0.3.104 range.\n\nNow you should be able to ping between the VMs vm1 and vm2, as well as between vm3 and vm4. However, neither of VMs vm1 or vm2 can ping VMs vm3 or vm4, as they are on a different zone with a different service-VLAN.\n\n12.13.5. VXLAN Setup Example\n\nThe example assumes a cluster with three nodes, with the node IP addresses 192.168.0.1, 192.168.0.2 and 192.168.0.3.\n\nCreate a VXLAN zone named myvxlanzone and add all IPs from the nodes to the peer address list. Use the default MTU of 1450 or configure accordingly.\n\nID: myvxlanzone\nPeers Address List: 192.168.0.1,192.168.0.2,192.168.0.3\n\nCreate a VNet named vxvnet1 using the VXLAN zone myvxlanzone created previously.\n\nID: vxvnet1\nZone: myvxlanzone\nTag: 100000\n\nApply the configuration on the main SDN web interface panel to create VNets locally on each nodes.\n\nCreate a Debian-based virtual machine (vm1) on node1, with a vNIC on vxvnet1.\n\nUse the following network configuration for this VM (note the lower MTU).\n\nauto eth0\niface eth0 inet static\n        address 10.0.3.100/24\n        mtu 1450\n\nCreate a second virtual machine (vm2) on node3, with a vNIC on the same VNet vxvnet1 as vm1.\n\nUse the following network configuration for this VM:\n\nauto eth0\niface eth0 inet static\n        address 10.0.3.101/24\n        mtu 1450\n\nThen, you should be able to ping between between vm1 and vm2.\n\n12.13.6. EVPN Setup Example\n\nThe example assumes a cluster with three nodes (node1, node2, node3) with IP addresses 192.168.0.1, 192.168.0.2 and 192.168.0.3.\n\nCreate an EVPN controller, using a private ASN number and the above node addresses as peers.\n\nID: myevpnctl\nASN#: 65000\nPeers: 192.168.0.1,192.168.0.2,192.168.0.3\n\nCreate an EVPN zone named myevpnzone, assign the previously created EVPN-controller and define node1 and node2 as exit nodes.\n\nID: myevpnzone\nVRF VXLAN Tag: 10000\nController: myevpnctl\nMTU: 1450\nVNet MAC Address: 32:F4:05:FE:6C:0A\nExit Nodes: node1,node2\n\nCreate the first VNet named myvnet1 using the EVPN zone myevpnzone.\n\nID: myvnet1\nZone: myevpnzone\nTag: 11000\n\nCreate a subnet on myvnet1:\n\nSubnet: 10.0.1.0/24\nGateway: 10.0.1.1\n\nCreate the second VNet named myvnet2 using the same EVPN zone myevpnzone.\n\nID: myvnet2\nZone: myevpnzone\nTag: 12000\n\nCreate a different subnet on myvnet2`:\n\nSubnet: 10.0.2.0/24\nGateway: 10.0.2.1\n\nApply the configuration from the main SDN web interface panel to create VNets locally on each node and generate the FRR configuration.\n\nCreate a Debian-based virtual machine (vm1) on node1, with a vNIC on myvnet1.\n\nUse the following network configuration for vm1:\n\nauto eth0\niface eth0 inet static\n        address 10.0.1.100/24\n        gateway 10.0.1.1\n        mtu 1450\n\nCreate a second virtual machine (vm2) on node2, with a vNIC on the other VNet myvnet2.\n\nUse the following network configuration for vm2:\n\nauto eth0\niface eth0 inet static\n        address 10.0.2.100/24\n        gateway 10.0.2.1\n        mtu 1450\n\nNow you should be able to ping vm2 from vm1, and vm1 from vm2.\n\nIf you ping an external IP from vm2 on the non-gateway node3, the packet will go to the configured myvnet2 gateway, then will be routed to the exit nodes (node1 or node2) and from there it will leave those nodes over the default gateway configured on node1 or node2.\n\n\tYou need to add reverse routes for the 10.0.1.0/24 and 10.0.2.0/24 networks to node1 and node2 on your external gateway, so that the public network can reply back.\n\nIf you have configured an external BGP router, the BGP-EVPN routes (10.0.1.0/24 and 10.0.2.0/24 in this example), will be announced dynamically.\n\n12.14. Notes\n12.14.1. Multiple EVPN Exit Nodes\n\nIf you have multiple gateway nodes, you should disable the rp_filter (Strict Reverse Path Filter) option, because packets can arrive at one node but go out from another node.\n\nAdd the following to /etc/sysctl.conf:\n\nnet.ipv4.conf.default.rp_filter=0\nnet.ipv4.conf.all.rp_filter=0\n12.14.2. VXLAN IPSEC Encryption\n\nTo add IPSEC encryption on top of a VXLAN, this example shows how to use strongswan.\n\nYou`ll need to reduce the MTU by additional 60 bytes for IPv4 or 80 bytes for IPv6 to handle encryption.\n\nSo with default real 1500 MTU, you need to use a MTU of 1370 (1370 + 80 (IPSEC) + 50 (VXLAN) == 1500).\n\nInstall strongswan on the host.\n\napt install strongswan\n\nAdd configuration to /etc/ipsec.conf. We only need to encrypt traffic from the VXLAN UDP port 4789.\n\nconn %default\n    ike=aes256-sha1-modp1024!  # the fastest, but reasonably secure cipher on modern HW\n    esp=aes256-sha1!\n    leftfirewall=yes           # this is necessary when using Proxmox VE firewall rules\n\nconn output\n    rightsubnet=%dynamic[udp/4789]\n    right=%any\n    type=transport\n    authby=psk\n    auto=route\n\nconn input\n    leftsubnet=%dynamic[udp/4789]\n    type=transport\n    authby=psk\n    auto=route\n\nGenerate a pre-shared key with:\n\nopenssl rand -base64 128\n\nand add the key to /etc/ipsec.secrets, so that the file contents looks like:\n\n: PSK <generatedbase64key>\n\nCopy the PSK and the configuration to all nodes participating in the VXLAN network.\n\n13. Proxmox VE Firewall\n\nProxmox VE Firewall provides an easy way to protect your IT infrastructure. You can setup firewall rules for all hosts inside a cluster, or define rules for virtual machines and containers. Features like firewall macros, security groups, IP sets and aliases help to make that task easier.\n\nWhile all configuration is stored on the cluster file system, the iptables-based firewall service runs on each cluster node, and thus provides full isolation between virtual machines. The distributed nature of this system also provides much higher bandwidth than a central firewall solution.\n\nThe firewall has full support for IPv4 and IPv6. IPv6 support is fully transparent, and we filter traffic for both protocols by default. So there is no need to maintain a different set of rules for IPv6.\n\n13.1. Zones\n\nThe Proxmox VE firewall groups the network into the following logical zones:\n\nHost\n\nTraffic from/to a cluster node\n\nVM\n\nTraffic from/to a specific VM\n\nFor each zone, you can define firewall rules for incoming and/or outgoing traffic.\n\n13.2. Configuration Files\n\nAll firewall related configuration is stored on the proxmox cluster file system. So those files are automatically distributed to all cluster nodes, and the pve-firewall service updates the underlying iptables rules automatically on changes.\n\nYou can configure anything using the GUI (i.e. Datacenter → Firewall, or on a Node → Firewall), or you can edit the configuration files directly using your preferred editor.\n\nFirewall configuration files contain sections of key-value pairs. Lines beginning with a # and blank lines are considered comments. Sections start with a header line containing the section name enclosed in [ and ].\n\n13.2.1. Cluster Wide Setup\n\nThe cluster-wide firewall configuration is stored at:\n\n/etc/pve/firewall/cluster.fw\n\nThe configuration can contain the following sections:\n\n[OPTIONS]\n\nThis is used to set cluster-wide firewall options.\n\nebtables: <boolean> (default = 1)\n\nEnable ebtables rules cluster wide.\n\nenable: <integer> (0 - N)\n\nEnable or disable the firewall cluster wide.\n\nlog_ratelimit: [enable=]<1|0> [,burst=<integer>] [,rate=<rate>]\n\nLog ratelimiting settings\n\nburst=<integer> (0 - N) (default = 5)\n\nInitial burst of packages which will always get logged before the rate is applied\n\nenable=<boolean> (default = 1)\n\nEnable or disable log rate limiting\n\nrate=<rate> (default = 1/second)\n\nFrequency with which the burst bucket gets refilled\n\npolicy_in: <ACCEPT | DROP | REJECT>\n\nInput policy.\n\npolicy_out: <ACCEPT | DROP | REJECT>\n\nOutput policy.\n\n[RULES]\n\nThis sections contains cluster-wide firewall rules for all nodes.\n\n[IPSET <name>]\n\nCluster wide IP set definitions.\n\n[GROUP <name>]\n\nCluster wide security group definitions.\n\n[ALIASES]\n\nCluster wide Alias definitions.\n\nEnabling the Firewall\n\nThe firewall is completely disabled by default, so you need to set the enable option here:\n\n[OPTIONS]\n# enable firewall (cluster-wide setting, default is disabled)\nenable: 1\n\tIf you enable the firewall, traffic to all hosts is blocked by default. Only exceptions is WebGUI(8006) and ssh(22) from your local network.\n\nIf you want to administrate your Proxmox VE hosts from remote, you need to create rules to allow traffic from those remote IPs to the web GUI (port 8006). You may also want to allow ssh (port 22), and maybe SPICE (port 3128).\n\n\tPlease open a SSH connection to one of your Proxmox VE hosts before enabling the firewall. That way you still have access to the host if something goes wrong .\n\nTo simplify that task, you can instead create an IPSet called “management”, and add all remote IPs there. This creates all required firewall rules to access the GUI from remote.\n\n13.2.2. Host Specific Configuration\n\nHost related configuration is read from:\n\n/etc/pve/nodes/<nodename>/host.fw\n\nThis is useful if you want to overwrite rules from cluster.fw config. You can also increase log verbosity, and set netfilter related options. The configuration can contain the following sections:\n\n[OPTIONS]\n\nThis is used to set host related firewall options.\n\nenable: <boolean>\n\nEnable host firewall rules.\n\nlog_level_in: <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for incoming traffic.\n\nlog_level_out: <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for outgoing traffic.\n\nlog_nf_conntrack: <boolean> (default = 0)\n\nEnable logging of conntrack information.\n\nndp: <boolean> (default = 0)\n\nEnable NDP (Neighbor Discovery Protocol).\n\nnf_conntrack_allow_invalid: <boolean> (default = 0)\n\nAllow invalid packets on connection tracking.\n\nnf_conntrack_helpers: <string> (default = ``)\n\nEnable conntrack helpers for specific protocols. Supported protocols: amanda, ftp, irc, netbios-ns, pptp, sane, sip, snmp, tftp\n\nnf_conntrack_max: <integer> (32768 - N) (default = 262144)\n\nMaximum number of tracked connections.\n\nnf_conntrack_tcp_timeout_established: <integer> (7875 - N) (default = 432000)\n\nConntrack established timeout.\n\nnf_conntrack_tcp_timeout_syn_recv: <integer> (30 - 60) (default = 60)\n\nConntrack syn recv timeout.\n\nnftables: <boolean> (default = 0)\n\nEnable nftables based firewall (tech preview)\n\nnosmurfs: <boolean>\n\nEnable SMURFS filter.\n\nprotection_synflood: <boolean> (default = 0)\n\nEnable synflood protection\n\nprotection_synflood_burst: <integer> (default = 1000)\n\nSynflood protection rate burst by ip src.\n\nprotection_synflood_rate: <integer> (default = 200)\n\nSynflood protection rate syn/sec by ip src.\n\nsmurf_log_level: <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for SMURFS filter.\n\ntcp_flags_log_level: <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for illegal tcp flags filter.\n\ntcpflags: <boolean> (default = 0)\n\nFilter illegal combinations of TCP flags.\n\n[RULES]\n\nThis sections contains host specific firewall rules.\n\n13.2.3. VM/Container Configuration\n\nVM firewall configuration is read from:\n\n/etc/pve/firewall/<VMID>.fw\n\nand contains the following data:\n\n[OPTIONS]\n\nThis is used to set VM/Container related firewall options.\n\ndhcp: <boolean> (default = 0)\n\nEnable DHCP.\n\nenable: <boolean> (default = 0)\n\nEnable/disable firewall rules.\n\nipfilter: <boolean>\n\nEnable default IP filters. This is equivalent to adding an empty ipfilter-net<id> ipset for every interface. Such ipsets implicitly contain sane default restrictions such as restricting IPv6 link local addresses to the one derived from the interface’s MAC address. For containers the configured IP addresses will be implicitly added.\n\nlog_level_in: <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for incoming traffic.\n\nlog_level_out: <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for outgoing traffic.\n\nmacfilter: <boolean> (default = 1)\n\nEnable/disable MAC address filter.\n\nndp: <boolean> (default = 0)\n\nEnable NDP (Neighbor Discovery Protocol).\n\npolicy_in: <ACCEPT | DROP | REJECT>\n\nInput policy.\n\npolicy_out: <ACCEPT | DROP | REJECT>\n\nOutput policy.\n\nradv: <boolean>\n\nAllow sending Router Advertisement.\n\n[RULES]\n\nThis sections contains VM/Container firewall rules.\n\n[IPSET <name>]\n\nIP set definitions.\n\n[ALIASES]\n\nIP Alias definitions.\n\nEnabling the Firewall for VMs and Containers\n\nEach virtual network device has its own firewall enable flag. So you can selectively enable the firewall for each interface. This is required in addition to the general firewall enable option.\n\n13.3. Firewall Rules\n\nFirewall rules consists of a direction (IN or OUT) and an action (ACCEPT, DENY, REJECT). You can also specify a macro name. Macros contain predefined sets of rules and options. Rules can be disabled by prefixing them with |.\n\nFirewall rules syntax\n[RULES]\n\nDIRECTION ACTION [OPTIONS]\n|DIRECTION ACTION [OPTIONS] # disabled rule\n\nDIRECTION MACRO(ACTION) [OPTIONS] # use predefined macro\n\nThe following options can be used to refine rule matches.\n\n--dest <string>\n\nRestrict packet destination address. This can refer to a single IP address, an IP set (+ipsetname) or an IP alias definition. You can also specify an address range like 20.34.101.207-201.3.9.99, or a list of IP addresses and networks (entries are separated by comma). Please do not mix IPv4 and IPv6 addresses inside such lists.\n\n--dport <string>\n\nRestrict TCP/UDP destination port. You can use service names or simple numbers (0-65535), as defined in /etc/services. Port ranges can be specified with \\d+:\\d+, for example 80:85, and you can use comma separated list to match several ports or ranges.\n\n--icmp-type <string>\n\nSpecify icmp-type. Only valid if proto equals icmp or icmpv6/ipv6-icmp.\n\n--iface <string>\n\nNetwork interface name. You have to use network configuration key names for VMs and containers (net\\d+). Host related rules can use arbitrary strings.\n\n--log <alert | crit | debug | emerg | err | info | nolog | notice | warning>\n\nLog level for firewall rule.\n\n--proto <string>\n\nIP protocol. You can use protocol names (tcp/udp) or simple numbers, as defined in /etc/protocols.\n\n--source <string>\n\nRestrict packet source address. This can refer to a single IP address, an IP set (+ipsetname) or an IP alias definition. You can also specify an address range like 20.34.101.207-201.3.9.99, or a list of IP addresses and networks (entries are separated by comma). Please do not mix IPv4 and IPv6 addresses inside such lists.\n\n--sport <string>\n\nRestrict TCP/UDP source port. You can use service names or simple numbers (0-65535), as defined in /etc/services. Port ranges can be specified with \\d+:\\d+, for example 80:85, and you can use comma separated list to match several ports or ranges.\n\nHere are some examples:\n\n[RULES]\nIN SSH(ACCEPT) -i net0\nIN SSH(ACCEPT) -i net0 # a comment\nIN SSH(ACCEPT) -i net0 -source 192.168.2.192 # only allow SSH from 192.168.2.192\nIN SSH(ACCEPT) -i net0 -source 10.0.0.1-10.0.0.10 # accept SSH for IP range\nIN SSH(ACCEPT) -i net0 -source 10.0.0.1,10.0.0.2,10.0.0.3 #accept ssh for IP list\nIN SSH(ACCEPT) -i net0 -source +mynetgroup # accept ssh for ipset mynetgroup\nIN SSH(ACCEPT) -i net0 -source myserveralias #accept ssh for alias myserveralias\n\n|IN SSH(ACCEPT) -i net0 # disabled rule\n\nIN  DROP # drop all incoming packages\nOUT ACCEPT # accept all outgoing packages\n13.4. Security Groups\n\nA security group is a collection of rules, defined at cluster level, which can be used in all VMs' rules. For example you can define a group named “webserver” with rules to open the http and https ports.\n\n# /etc/pve/firewall/cluster.fw\n\n[group webserver]\nIN  ACCEPT -p tcp -dport 80\nIN  ACCEPT -p tcp -dport 443\n\nThen, you can add this group to a VM’s firewall\n\n# /etc/pve/firewall/<VMID>.fw\n\n[RULES]\nGROUP webserver\n13.5. IP Aliases\n\nIP Aliases allow you to associate IP addresses of networks with a name. You can then refer to those names:\n\ninside IP set definitions\n\nin source and dest properties of firewall rules\n\n13.5.1. Standard IP Alias local_network\n\nThis alias is automatically defined. Please use the following command to see assigned values:\n\n# pve-firewall localnet\nlocal hostname: example\nlocal IP address: 192.168.2.100\nnetwork auto detect: 192.168.0.0/20\nusing detected local_network: 192.168.0.0/20\n\nThe firewall automatically sets up rules to allow everything needed for cluster communication (corosync, API, SSH) using this alias.\n\nThe user can overwrite these values in the cluster.fw alias section. If you use a single host on a public network, it is better to explicitly assign the local IP address\n\n#  /etc/pve/firewall/cluster.fw\n[ALIASES]\nlocal_network 1.2.3.4 # use the single IP address\n13.6. IP Sets\n\nIP sets can be used to define groups of networks and hosts. You can refer to them with ‘+name` in the firewall rules’ source and dest properties.\n\nThe following example allows HTTP traffic from the management IP set.\n\nIN HTTP(ACCEPT) -source +management\n13.6.1. Standard IP set management\n\nThis IP set applies only to host firewalls (not VM firewalls). Those IPs are allowed to do normal management tasks (Proxmox VE GUI, VNC, SPICE, SSH).\n\nThe local cluster network is automatically added to this IP set (alias cluster_network), to enable inter-host cluster communication. (multicast,ssh,…)\n\n# /etc/pve/firewall/cluster.fw\n\n[IPSET management]\n192.168.2.10\n192.168.2.10/24\n13.6.2. Standard IP set blacklist\n\nTraffic from these IPs is dropped by every host’s and VM’s firewall.\n\n# /etc/pve/firewall/cluster.fw\n\n[IPSET blacklist]\n77.240.159.182\n213.87.123.0/24\n13.6.3. Standard IP set ipfilter-net*\n\nThese filters belong to a VM’s network interface and are mainly used to prevent IP spoofing. If such a set exists for an interface then any outgoing traffic with a source IP not matching its interface’s corresponding ipfilter set will be dropped.\n\nFor containers with configured IP addresses these sets, if they exist (or are activated via the general IP Filter option in the VM’s firewall’s options tab), implicitly contain the associated IP addresses.\n\nFor both virtual machines and containers they also implicitly contain the standard MAC-derived IPv6 link-local address in order to allow the neighbor discovery protocol to work.\n\n/etc/pve/firewall/<VMID>.fw\n\n[IPSET ipfilter-net0] # only allow specified IPs on net0\n192.168.2.10\n13.7. Services and Commands\n\nThe firewall runs two service daemons on each node:\n\npvefw-logger: NFLOG daemon (ulogd replacement).\n\npve-firewall: updates iptables rules\n\nThere is also a CLI command named pve-firewall, which can be used to start and stop the firewall service:\n\n# pve-firewall start\n# pve-firewall stop\n\nTo get the status use:\n\n# pve-firewall status\n\nThe above command reads and compiles all firewall rules, so you will see warnings if your firewall configuration contains any errors.\n\nIf you want to see the generated iptables rules you can use:\n\n# iptables-save\n13.8. Default firewall rules\n\nThe following traffic is filtered by the default firewall configuration:\n\n13.8.1. Datacenter incoming/outgoing DROP/REJECT\n\nIf the input or output policy for the firewall is set to DROP or REJECT, the following traffic is still allowed for all Proxmox VE hosts in the cluster:\n\ntraffic over the loopback interface\n\nalready established connections\n\ntraffic using the IGMP protocol\n\nTCP traffic from management hosts to port 8006 in order to allow access to the web interface\n\nTCP traffic from management hosts to the port range 5900 to 5999 allowing traffic for the VNC web console\n\nTCP traffic from management hosts to port 3128 for connections to the SPICE proxy\n\nTCP traffic from management hosts to port 22 to allow ssh access\n\nUDP traffic in the cluster network to ports 5405-5412 for corosync\n\nUDP multicast traffic in the cluster network\n\nICMP traffic type 3 (Destination Unreachable), 4 (congestion control) or 11 (Time Exceeded)\n\nThe following traffic is dropped, but not logged even with logging enabled:\n\nTCP connections with invalid connection state\n\nBroadcast, multicast and anycast traffic not related to corosync, i.e., not coming through ports 5405-5412\n\nTCP traffic to port 43\n\nUDP traffic to ports 135 and 445\n\nUDP traffic to the port range 137 to 139\n\nUDP traffic form source port 137 to port range 1024 to 65535\n\nUDP traffic to port 1900\n\nTCP traffic to port 135, 139 and 445\n\nUDP traffic originating from source port 53\n\nThe rest of the traffic is dropped or rejected, respectively, and also logged. This may vary depending on the additional options enabled in Firewall → Options, such as NDP, SMURFS and TCP flag filtering.\n\nPlease inspect the output of the\n\n # iptables-save\n\nsystem command to see the firewall chains and rules active on your system. This output is also included in a System Report, accessible over a node’s subscription tab in the web GUI, or through the pvereport command-line tool.\n\n13.8.2. VM/CT incoming/outgoing DROP/REJECT\n\nThis drops or rejects all the traffic to the VMs, with some exceptions for DHCP, NDP, Router Advertisement, MAC and IP filtering depending on the set configuration. The same rules for dropping/rejecting packets are inherited from the datacenter, while the exceptions for accepted incoming/outgoing traffic of the host do not apply.\n\nAgain, you can use iptables-save (see above) to inspect all rules and chains applied.\n\n13.9. Logging of firewall rules\n\nBy default, all logging of traffic filtered by the firewall rules is disabled. To enable logging, the loglevel for incoming and/or outgoing traffic has to be set in Firewall → Options. This can be done for the host as well as for the VM/CT firewall individually. By this, logging of Proxmox VE’s standard firewall rules is enabled and the output can be observed in Firewall → Log. Further, only some dropped or rejected packets are logged for the standard rules (see default firewall rules).\n\nloglevel does not affect how much of the filtered traffic is logged. It changes a LOGID appended as prefix to the log output for easier filtering and post-processing.\n\nloglevel is one of the following flags:\n\nloglevel\tLOGID\n\n\nnolog\n\n\t\n\n — \n\n\n\n\nemerg\n\n\t\n\n0\n\n\n\n\nalert\n\n\t\n\n1\n\n\n\n\ncrit\n\n\t\n\n2\n\n\n\n\nerr\n\n\t\n\n3\n\n\n\n\nwarning\n\n\t\n\n4\n\n\n\n\nnotice\n\n\t\n\n5\n\n\n\n\ninfo\n\n\t\n\n6\n\n\n\n\ndebug\n\n\t\n\n7\n\nA typical firewall log output looks like this:\n\nVMID LOGID CHAIN TIMESTAMP POLICY: PACKET_DETAILS\n\nIn case of the host firewall, VMID is equal to 0.\n\n13.9.1. Logging of user defined firewall rules\n\nIn order to log packets filtered by user-defined firewall rules, it is possible to set a log-level parameter for each rule individually. This allows to log in a fine grained manner and independent of the log-level defined for the standard rules in Firewall → Options.\n\nWhile the loglevel for each individual rule can be defined or changed easily in the web UI during creation or modification of the rule, it is possible to set this also via the corresponding pvesh API calls.\n\nFurther, the log-level can also be set via the firewall configuration file by appending a -log <loglevel> to the selected rule (see possible log-levels).\n\nFor example, the following two are identical:\n\nIN REJECT -p icmp -log nolog\nIN REJECT -p icmp\n\nwhereas\n\nIN REJECT -p icmp -log debug\n\nproduces a log output flagged with the debug level.\n\n13.10. Tips and Tricks\n13.10.1. How to allow FTP\n\nFTP is an old style protocol which uses port 21 and several other dynamic ports. So you need a rule to accept port 21. In addition, you need to load the ip_conntrack_ftp module. So please run:\n\nmodprobe ip_conntrack_ftp\n\nand add ip_conntrack_ftp to /etc/modules (so that it works after a reboot).\n\n13.10.2. Suricata IPS integration\n\nIf you want to use the Suricata IPS (Intrusion Prevention System), it’s possible.\n\nPackets will be forwarded to the IPS only after the firewall ACCEPTed them.\n\nRejected/Dropped firewall packets don’t go to the IPS.\n\nInstall suricata on proxmox host:\n\n# apt-get install suricata\n# modprobe nfnetlink_queue\n\nDon’t forget to add nfnetlink_queue to /etc/modules for next reboot.\n\nThen, enable IPS for a specific VM with:\n\n# /etc/pve/firewall/<VMID>.fw\n\n[OPTIONS]\nips: 1\nips_queues: 0\n\nips_queues will bind a specific cpu queue for this VM.\n\nAvailable queues are defined in\n\n# /etc/default/suricata\nNFQUEUE=0\n13.11. Notes on IPv6\n\nThe firewall contains a few IPv6 specific options. One thing to note is that IPv6 does not use the ARP protocol anymore, and instead uses NDP (Neighbor Discovery Protocol) which works on IP level and thus needs IP addresses to succeed. For this purpose link-local addresses derived from the interface’s MAC address are used. By default the NDP option is enabled on both host and VM level to allow neighbor discovery (NDP) packets to be sent and received.\n\nBeside neighbor discovery NDP is also used for a couple of other things, like auto-configuration and advertising routers.\n\nBy default VMs are allowed to send out router solicitation messages (to query for a router), and to receive router advertisement packets. This allows them to use stateless auto configuration. On the other hand VMs cannot advertise themselves as routers unless the “Allow Router Advertisement” (radv: 1) option is set.\n\nAs for the link local addresses required for NDP, there’s also an “IP Filter” (ipfilter: 1) option which can be enabled which has the same effect as adding an ipfilter-net* ipset for each of the VM’s network interfaces containing the corresponding link local addresses. (See the Standard IP set ipfilter-net* section for details.)\n\n13.12. Ports used by Proxmox VE\n\nWeb interface: 8006 (TCP, HTTP/1.1 over TLS)\n\nVNC Web console: 5900-5999 (TCP, WebSocket)\n\nSPICE proxy: 3128 (TCP)\n\nsshd (used for cluster actions): 22 (TCP)\n\nrpcbind: 111 (UDP)\n\nsendmail: 25 (TCP, outgoing)\n\ncorosync cluster traffic: 5405-5412 UDP\n\nlive migration (VM memory and local-disk data): 60000-60050 (TCP)\n\n13.13. nftables\n\nAs an alternative to pve-firewall we offer proxmox-firewall, which is an implementation of the Proxmox VE firewall based on the newer nftables rather than iptables.\n\n\tproxmox-firewall is currently in tech preview. There might be bugs or incompatibilies with the original firewall. It is currently not suited for production use.\n\nThis implementation uses the same configuration files and configuration format, so you can use your old configuration when switching. It provides the exact same functionality with a few exceptions:\n\nREJECT is currently not possible for guest traffic (traffic will instead be dropped).\n\nUsing the NDP, Router Advertisement or DHCP options will always create firewall rules, irregardless of your default policy.\n\nfirewall rules for guests are evaluated even for connections that have conntrack table entries.\n\n13.13.1. Installation and Usage\n\nInstall the proxmox-firewall package:\n\napt install proxmox-firewall\n\nEnable the nftables backend via the Web UI on your hosts (Host > Firewall > Options > nftables), or by enabling it in the configuration file for your hosts (/etc/pve/nodes/<node_name>/host.fw):\n\n[OPTIONS]\n\nnftables: 1\n\tAfter enabling/disabling proxmox-firewall, all running VMs and containers need to be restarted for the old/new firewall to work properly.\n\nAfter setting the nftables configuration key, the new proxmox-firewall service will take over. You can check if the new service is working by checking the systemctl status of proxmox-firewall:\n\nsystemctl status proxmox-firewall\n\nYou can also examine the generated ruleset. You can find more information about this in the section Helpful Commands. You should also check whether pve-firewall is no longer generating iptables rules, you can find the respective commands in the Services and Commands section.\n\nSwitching back to the old firewall can be done by simply setting the configuration value back to 0 / No.\n\n13.13.2. Usage\n\nproxmox-firewall will create two tables that are managed by the proxmox-firewall service: proxmox-firewall and proxmox-firewall-guests. If you want to create custom rules that live outside the Proxmox VE firewall configuration you can create your own tables to manage your custom firewall rules. proxmox-firewall will only touch the tables it generates, so you can easily extend and modify the behavior of the proxmox-firewall by adding your own tables.\n\nInstead of using the pve-firewall command, the nftables-based firewall uses proxmox-firewall. It is a systemd service, so you can start and stop it via systemctl:\n\nsystemctl start proxmox-firewall\nsystemctl stop proxmox-firewall\n\nStopping the firewall service will remove all generated rules.\n\nTo query the status of the firewall, you can query the status of the systemctl service:\n\nsystemctl status proxmox-firewall\n13.13.3. Helpful Commands\n\nYou can check the generated ruleset via the following command:\n\nnft list ruleset\n\nIf you want to debug proxmox-firewall you can simply run the daemon in foreground with the RUST_LOG environment variable set to trace. This should provide you with detailed debugging output:\n\nRUST_LOG=trace /usr/libexec/proxmox/proxmox-firewall\n\nYou can also edit the systemctl service if you want to have detailed output for your firewall daemon:\n\nsystemctl edit proxmox-firewall\n\nThen you need to add the override for the RUST_LOG environment variable:\n\n[Service]\nEnvironment=\"RUST_LOG=trace\"\n\nThis will generate a large amount of logs very quickly, so only use this for debugging purposes. Other, less verbose, log levels are info and debug.\n\nRunning in foreground writes the log output to STDERR, so you can redirect it with the following command (e.g. for submitting logs to the community forum):\n\nRUST_LOG=trace /usr/libexec/proxmox/proxmox-firewall 2> firewall_log_$(hostname).txt\n\nIt can be helpful to trace packet flow through the different chains in order to debug firewall rules. This can be achieved by setting nftrace to 1 for packets that you want to track. It is advisable that you do not set this flag for all packets, in the example below we only examine ICMP packets.\n\n#!/usr/sbin/nft -f\ntable bridge tracebridge\ndelete table bridge tracebridge\n\ntable bridge tracebridge {\n    chain trace {\n        meta l4proto icmp meta nftrace set 1\n    }\n\n    chain prerouting {\n        type filter hook prerouting priority -350; policy accept;\n        jump trace\n    }\n\n    chain postrouting {\n        type filter hook postrouting priority -350; policy accept;\n        jump trace\n    }\n}\n\nSaving this file, making it executable, and then running it once will create the respective tracing chains. You can then inspect the tracing output via the Proxmox VE Web UI (Firewall > Log) or via nft monitor trace.\n\nThe above example traces traffic on all bridges, which is usually where guest traffic flows through. If you want to examine host traffic, create those chains in the inet table instead of the bridge table.\n\n\tBe aware that this can generate a lot of log spam and slow down the performance of your networking stack significantly.\n\nYou can remove the tracing rules via running the following command:\n\nnft delete table bridge tracebridge\n14. User Management\n\nProxmox VE supports multiple authentication sources, for example Linux PAM, an integrated Proxmox VE authentication server, LDAP, Microsoft Active Directory and OpenID Connect.\n\nBy using role-based user and permission management for all objects (VMs, Storage, nodes, etc.), granular access can be defined.\n\n14.1. Users\n\nProxmox VE stores user attributes in /etc/pve/user.cfg. Passwords are not stored here; users are instead associated with the authentication realms described below. Therefore, a user is often internally identified by their username and realm in the form <userid>@<realm>.\n\nEach user entry in this file contains the following information:\n\nFirst name\n\nLast name\n\nE-mail address\n\nGroup memberships\n\nAn optional expiration date\n\nA comment or note about this user\n\nWhether this user is enabled or disabled\n\nOptional two-factor authentication keys\n\n\tWhen you disable or delete a user, or if the expiry date set is in the past, this user will not be able to log in to new sessions or start new tasks. All tasks which have already been started by this user (for example, terminal sessions) will not be terminated automatically by any such event.\n14.1.1. System administrator\n\nThe system’s root user can always log in via the Linux PAM realm and is an unconfined administrator. This user cannot be deleted, but attributes can still be changed. System mails will be sent to the email address assigned to this user.\n\n14.2. Groups\n\nEach user can be a member of several groups. Groups are the preferred way to organize access permissions. You should always grant permissions to groups instead of individual users. That way you will get a much more maintainable access control list.\n\n14.3. API Tokens\n\nAPI tokens allow stateless access to most parts of the REST API from another system, software or API client. Tokens can be generated for individual users and can be given separate permissions and expiration dates to limit the scope and duration of the access. Should the API token get compromised, it can be revoked without disabling the user itself.\n\nAPI tokens come in two basic types:\n\nSeparated privileges: The token needs to be given explicit access with ACLs. Its effective permissions are calculated by intersecting user and token permissions.\n\nFull privileges: The token’s permissions are identical to that of the associated user.\n\n\tThe token value is only displayed/returned once when the token is generated. It cannot be retrieved again over the API at a later time!\n\nTo use an API token, set the HTTP header Authorization to the displayed value of the form PVEAPIToken=USER@REALM!TOKENID=UUID when making API requests, or refer to your API client’s documentation.\n\n14.4. Resource Pools\n\nA resource pool is a set of virtual machines, containers, and storage devices. It is useful for permission handling in cases where certain users should have controlled access to a specific set of resources, as it allows for a single permission to be applied to a set of elements, rather than having to manage this on a per-resource basis. Resource pools are often used in tandem with groups, so that the members of a group have permissions on a set of machines and storage.\n\n14.5. Authentication Realms\n\nAs Proxmox VE users are just counterparts for users existing on some external realm, the realms have to be configured in /etc/pve/domains.cfg. The following realms (authentication methods) are available:\n\nLinux PAM Standard Authentication\n\nLinux PAM is a framework for system-wide user authentication. These users are created on the host system with commands such as adduser. If PAM users exist on the Proxmox VE host system, corresponding entries can be added to Proxmox VE, to allow these users to log in via their system username and password.\n\nProxmox VE Authentication Server\n\nThis is a Unix-like password store, which stores hashed passwords in /etc/pve/priv/shadow.cfg. Passwords are hashed using the SHA-256 hashing algorithm. This is the most convenient realm for small-scale (or even mid-scale) installations, where users do not need access to anything outside of Proxmox VE. In this case, users are fully managed by Proxmox VE and are able to change their own passwords via the GUI.\n\nLDAP\n\nLDAP (Lightweight Directory Access Protocol) is an open, cross-platform protocol for authentication using directory services. OpenLDAP is a popular open-source implementations of the LDAP protocol.\n\nMicrosoft Active Directory (AD)\n\nMicrosoft Active Directory (AD) is a directory service for Windows domain networks and is supported as an authentication realm for Proxmox VE. It supports LDAP as an authentication protocol.\n\nOpenID Connect\n\nOpenID Connect is implemented as an identity layer on top of the OATH 2.0 protocol. It allows clients to verify the identity of the user, based on authentication performed by an external authorization server.\n\n14.5.1. Linux PAM Standard Authentication\n\nAs Linux PAM corresponds to host system users, a system user must exist on each node which the user is allowed to log in on. The user authenticates with their usual system password. This realm is added by default and can’t be removed. In terms of configurability, an administrator can choose to require two-factor authentication with logins from the realm and to set the realm as the default authentication realm.\n\n14.5.2. Proxmox VE Authentication Server\n\nThe Proxmox VE authentication server realm is a simple Unix-like password store. The realm is created by default, and as with Linux PAM, the only configuration items available are the ability to require two-factor authentication for users of the realm, and to set it as the default realm for login.\n\nUnlike the other Proxmox VE realm types, users are created and authenticated entirely through Proxmox VE, rather than authenticating against another system. Hence, you are required to set a password for this type of user upon creation.\n\n14.5.3. LDAP\n\nYou can also use an external LDAP server for user authentication (for examle, OpenLDAP). In this realm type, users are searched under a Base Domain Name (base_dn), using the username attribute specified in the User Attribute Name (user_attr) field.\n\nA server and optional fallback server can be configured, and the connection can be encrypted via SSL. Furthermore, filters can be configured for directories and groups. Filters allow you to further limit the scope of the realm.\n\nFor instance, if a user is represented via the following LDIF dataset:\n\n# user1 of People at ldap-test.com\ndn: uid=user1,ou=People,dc=ldap-test,dc=com\nobjectClass: top\nobjectClass: person\nobjectClass: organizationalPerson\nobjectClass: inetOrgPerson\nuid: user1\ncn: Test User 1\nsn: Testers\ndescription: This is the first test user.\n\nThe Base Domain Name would be ou=People,dc=ldap-test,dc=com and the user attribute would be uid.\n\nIf Proxmox VE needs to authenticate (bind) to the LDAP server before being able to query and authenticate users, a bind domain name can be configured via the bind_dn property in /etc/pve/domains.cfg. Its password then has to be stored in /etc/pve/priv/ldap/<realmname>.pw (for example, /etc/pve/priv/ldap/my-ldap.pw). This file should contain a single line with the raw password.\n\nTo verify certificates, you need to set capath. You can set it either directly to the CA certificate of your LDAP server, or to the system path containing all trusted CA certificates (/etc/ssl/certs). Additionally, you need to set the verify option, which can also be done over the web interface.\n\nThe main configuration options for an LDAP server realm are as follows:\n\nRealm (realm): The realm identifier for Proxmox VE users\n\nBase Domain Name (base_dn): The directory which users are searched under\n\nUser Attribute Name (user_attr): The LDAP attribute containing the username that users will log in with\n\nServer (server1): The server hosting the LDAP directory\n\nFallback Server (server2): An optional fallback server address, in case the primary server is unreachable\n\nPort (port): The port that the LDAP server listens on\n\n\tIn order to allow a particular user to authenticate using the LDAP server, you must also add them as a user of that realm from the Proxmox VE server. This can be carried out automatically with syncing.\n14.5.4. Microsoft Active Directory (AD)\n\nTo set up Microsoft AD as a realm, a server address and authentication domain need to be specified. Active Directory supports most of the same properties as LDAP, such as an optional fallback server, port, and SSL encryption. Furthermore, users can be added to Proxmox VE automatically via sync operations, after configuration.\n\nAs with LDAP, if Proxmox VE needs to authenticate before it binds to the AD server, you must configure the Bind User (bind_dn) property. This property is typically required by default for Microsoft AD.\n\nThe main configuration settings for Microsoft Active Directory are:\n\nRealm (realm): The realm identifier for Proxmox VE users\n\nDomain (domain): The AD domain of the server\n\nServer (server1): The FQDN or IP address of the server\n\nFallback Server (server2): An optional fallback server address, in case the primary server is unreachable\n\nPort (port): The port that the Microsoft AD server listens on\n\n\tMicrosoft AD normally checks values like usernames without case sensitivity. To make Proxmox VE do the same, you can disable the default case-sensitive option by editing the realm in the web UI, or using the CLI (change the ID with the realm ID): pveum realm modify ID --case-sensitive 0\n14.5.5. Syncing LDAP-Based Realms\n\nIt’s possible to automatically sync users and groups for LDAP-based realms (LDAP & Microsoft Active Directory), rather than having to add them to Proxmox VE manually. You can access the sync options from the Add/Edit window of the web interface’s Authentication panel or via the pveum realm add/modify commands. You can then carry out the sync operation from the Authentication panel of the GUI or using the following command:\n\npveum realm sync <realm>\n\nUsers and groups are synced to the cluster-wide configuration file, /etc/pve/user.cfg.\n\nAttributes to Properties\n\nIf the sync response includes user attributes, they will be synced into the matching user property in the user.cfg. For example: firstname or lastname.\n\nIf the names of the attributes are not matching the Proxmox VE properties, you can set a custom field-to-field map in the config by using the sync_attributes option.\n\nHow such properties are handled if anything vanishes can be controlled via the sync options, see below.\n\nSync Configuration\n\nThe configuration options for syncing LDAP-based realms can be found in the Sync Options tab of the Add/Edit window.\n\nThe configuration options are as follows:\n\nBind User (bind_dn): Refers to the LDAP account used to query users and groups. This account needs access to all desired entries. If it’s set, the search will be carried out via binding; otherwise, the search will be carried out anonymously. The user must be a complete LDAP formatted distinguished name (DN), for example, cn=admin,dc=example,dc=com.\n\nGroupname attr. (group_name_attr): Represents the users' groups. Only entries which adhere to the usual character limitations of the user.cfg are synced. Groups are synced with -$realm attached to the name, in order to avoid naming conflicts. Please ensure that a sync does not overwrite manually created groups.\n\nUser classes (user_classes): Objects classes associated with users.\n\nGroup classes (group_classes): Objects classes associated with groups.\n\nE-Mail attribute: If the LDAP-based server specifies user email addresses, these can also be included in the sync by setting the associated attribute here. From the command line, this is achievable through the --sync_attributes parameter.\n\nUser Filter (filter): For further filter options to target specific users.\n\nGroup Filter (group_filter): For further filter options to target specific groups.\n\n\tFilters allow you to create a set of additional match criteria, to narrow down the scope of a sync. Information on available LDAP filter types and their usage can be found at ldap.com.\nSync Options\n\nIn addition to the options specified in the previous section, you can also configure further options that describe the behavior of the sync operation.\n\nThese options are either set as parameters before the sync, or as defaults via the realm option sync-defaults-options.\n\nThe main options for syncing are:\n\nScope (scope): The scope of what to sync. It can be either users, groups or both.\n\nEnable new (enable-new): If set, the newly synced users are enabled and can log in. The default is true.\n\nRemove Vanished (remove-vanished): This is a list of options which, when activated, determine if they are removed when they are not returned from the sync response. The options are:\n\nACL (acl): Remove ACLs of users and groups which were not returned returned in the sync response. This most often makes sense together with Entry.\n\nEntry (entry): Removes entries (i.e. users and groups) when they are not returned in the sync response.\n\nProperties (properties): Removes properties of entries where the user in the sync response did not contain those attributes. This includes all properties, even those never set by a sync. Exceptions are tokens and the enable flag, these will be retained even with this option enabled.\n\nPreview (dry-run): No data is written to the config. This is useful if you want to see which users and groups would get synced to the user.cfg.\n\nReserved characters\n\nCertain characters are reserved (see RFC2253) and cannot be easily used in attribute values in DNs without being escaped properly.\n\nFollowing characters need escaping:\n\nSpace ( ) at the beginning or end\n\nNumber sign (#) at the beginning\n\nComma (,)\n\nPlus sign (+)\n\nDouble quote (\")\n\nForward slashes (/)\n\nAngle brackets (<>)\n\nSemicolon (;)\n\nEquals sign (=)\n\nTo use such characters in DNs, surround the attribute value in double quotes. For example, to bind with a user with the CN (Common Name) Example, User, use CN=\"Example, User\",OU=people,DC=example,DC=com as value for bind_dn.\n\nThis applies to the base_dn, bind_dn, and group_dn attributes.\n\n\tUsers with colons and forward slashes cannot be synced since these are reserved characters in usernames.\n14.5.6. OpenID Connect\n\nThe main OpenID Connect configuration options are:\n\nIssuer URL (issuer-url): This is the URL of the authorization server. Proxmox uses the OpenID Connect Discovery protocol to automatically configure further details.\n\nWhile it is possible to use unencrypted http:// URLs, we strongly recommend to use encrypted https:// connections.\n\nRealm (realm): The realm identifier for Proxmox VE users\n\nClient ID (client-id): OpenID Client ID.\n\nClient Key (client-key): Optional OpenID Client Key.\n\nAutocreate Users (autocreate): Automatically create users if they do not exist. While authentication is done at the OpenID server, all users still need an entry in the Proxmox VE user configuration. You can either add them manually, or use the autocreate option to automatically add new users.\n\nUsername Claim (username-claim): OpenID claim used to generate the unique username (subject, username or email).\n\nUsername mapping\n\nThe OpenID Connect specification defines a single unique attribute (claim in OpenID terms) named subject. By default, we use the value of this attribute to generate Proxmox VE usernames, by simple adding @ and the realm name: ${subject}@${realm}.\n\nUnfortunately, most OpenID servers use random strings for subject, like DGH76OKH34BNG3245SB, so a typical username would look like DGH76OKH34BNG3245SB@yourrealm. While unique, it is difficult for humans to remember such random strings, making it quite impossible to associate real users with this.\n\nThe username-claim setting allows you to use other attributes for the username mapping. Setting it to username is preferred if the OpenID Connect server provides that attribute and guarantees its uniqueness.\n\nAnother option is to use email, which also yields human readable usernames. Again, only use this setting if the server guarantees the uniqueness of this attribute.\n\nExamples\n\nHere is an example of creating an OpenID realm using Google. You need to replace --client-id and --client-key with the values from your Google OpenID settings.\n\npveum realm add myrealm1 --type openid --issuer-url  https://accounts.google.com --client-id XXXX --client-key YYYY --username-claim email\n\nThe above command uses --username-claim email, so that the usernames on the Proxmox VE side look like example.user@google.com@myrealm1.\n\nKeycloak (https://www.keycloak.org/) is a popular open source Identity and Access Management tool, which supports OpenID Connect. In the following example, you need to replace the --issuer-url and --client-id with your information:\n\npveum realm add myrealm2 --type openid --issuer-url  https://your.server:8080/realms/your-realm --client-id XXX --username-claim username\n\nUsing --username-claim username enables simple usernames on the Proxmox VE side, like example.user@myrealm2.\n\n\tYou need to ensure that the user is not allowed to edit the username setting themselves (on the Keycloak server).\n14.6. Two-Factor Authentication\n\nThere are two ways to use two-factor authentication:\n\nIt can be required by the authentication realm, either via TOTP (Time-based One-Time Password) or YubiKey OTP. In this case, a newly created user needs to have their keys added immediately, as there is no way to log in without the second factor. In the case of TOTP, users can also change the TOTP later on, provided they can log in first.\n\nAlternatively, users can choose to opt-in to two-factor authentication later on, even if the realm does not enforce it.\n\n14.6.1. Available Second Factors\n\nYou can set up multiple second factors, in order to avoid a situation in which losing your smartphone or security key locks you out of your account permanently.\n\nThe following two-factor authentication methods are available in addition to realm-enforced TOTP and YubiKey OTP:\n\nUser configured TOTP (Time-based One-Time Password). A short code derived from a shared secret and the current time, it changes every 30 seconds.\n\nWebAuthn (Web Authentication). A general standard for authentication. It is implemented by various security devices, like hardware keys or trusted platform modules (TPM) from a computer or smart phone.\n\nSingle use Recovery Keys. A list of keys which should either be printed out and locked in a secure place or saved digitally in an electronic vault. Each key can be used only once. These are perfect for ensuring that you are not locked out, even if all of your other second factors are lost or corrupt.\n\nBefore WebAuthn was supported, U2F could be setup by the user. Existing U2F factors can still be used, but it is recommended to switch to WebAuthn, once it is configured on the server.\n\n14.6.2. Realm Enforced Two-Factor Authentication\n\nThis can be done by selecting one of the available methods via the TFA dropdown box when adding or editing an Authentication Realm. When a realm has TFA enabled, it becomes a requirement, and only users with configured TFA will be able to log in.\n\nCurrently there are two methods available:\n\nTime-based OATH (TOTP)\n\nThis uses the standard HMAC-SHA1 algorithm, where the current time is hashed with the user’s configured key. The time step and password length parameters are configurable.\n\nA user can have multiple keys configured (separated by spaces), and the keys can be specified in Base32 (RFC3548) or hexadecimal notation.\n\nProxmox VE provides a key generation tool (oathkeygen) which prints out a random key in Base32 notation, that can be used directly with various OTP tools, such as the oathtool command-line tool, or on Android Google Authenticator, FreeOTP, andOTP or similar applications.\n\nYubiKey OTP\n\nFor authenticating via a YubiKey a Yubico API ID, API KEY and validation server URL must be configured, and users must have a YubiKey available. In order to get the key ID from a YubiKey, you can trigger the YubiKey once after connecting it via USB, and copy the first 12 characters of the typed password into the user’s Key IDs field.\n\nPlease refer to the YubiKey OTP documentation for how to use the YubiCloud or host your own verification server.\n\n14.6.3. Limits and Lockout of Two-Factor Authentication\n\nA second factor is meant to protect users if their password is somehow leaked or guessed. However, some factors could still be broken by brute force. For this reason, users will be locked out after too many failed 2nd factor login attempts.\n\nFor TOTP, 8 failed attempts will disable the user’s TOTP factors. They are unlocked when logging in with a recovery key. If TOTP was the only available factor, admin intervention is required, and it is highly recommended to require the user to change their password immediately.\n\nSince FIDO2/Webauthn and recovery keys are less susceptible to brute force attacks, the limit there is higher (100 tries), but all second factors are blocked for an hour when exceeded.\n\nAn admin can unlock a user’s Two-Factor Authentication at any time via the user list in the UI or the command line:\n\n pveum user tfa unlock joe@pve\n14.6.4. User Configured TOTP Authentication\n\nUsers can choose to enable TOTP or WebAuthn as a second factor on login, via the TFA button in the user list (unless the realm enforces YubiKey OTP).\n\nUsers can always add and use one time Recovery Keys.\n\nAfter opening the TFA window, the user is presented with a dialog to set up TOTP authentication. The Secret field contains the key, which can be randomly generated via the Randomize button. An optional Issuer Name can be added to provide information to the TOTP app about what the key belongs to. Most TOTP apps will show the issuer name together with the corresponding OTP values. The username is also included in the QR code for the TOTP app.\n\nAfter generating a key, a QR code will be displayed, which can be used with most OTP apps such as FreeOTP. The user then needs to verify the current user password (unless logged in as root), as well as the ability to correctly use the TOTP key, by typing the current OTP value into the Verification Code field and pressing the Apply button.\n\n14.6.5. TOTP\n\nThere is no server setup required. Simply install a TOTP app on your smartphone (for example, FreeOTP) and use the Proxmox Backup Server web interface to add a TOTP factor.\n\n14.6.6. WebAuthn\n\nFor WebAuthn to work, you need to have two things:\n\nA trusted HTTPS certificate (for example, by using Let’s Encrypt). While it probably works with an untrusted certificate, some browsers may warn or refuse WebAuthn operations if it is not trusted.\n\nSetup the WebAuthn configuration (see Datacenter → Options → WebAuthn Settings in the Proxmox VE web interface). This can be auto-filled in most setups.\n\nOnce you have fulfilled both of these requirements, you can add a WebAuthn configuration in the Two Factor panel under Datacenter → Permissions → Two Factor.\n\n14.6.7. Recovery Keys\n\nRecovery key codes do not need any preparation; you can simply create a set of recovery keys in the Two Factor panel under Datacenter → Permissions → Two Factor.\n\n\tThere can only be one set of single-use recovery keys per user at any time.\n14.6.8. Server Side Webauthn Configuration\n\nTo allow users to use WebAuthn authentication, it is necessaary to use a valid domain with a valid SSL certificate, otherwise some browsers may warn or refuse to authenticate altogether.\n\n\tChanging the WebAuthn configuration may render all existing WebAuthn registrations unusable!\n\nThis is done via /etc/pve/datacenter.cfg. For instance:\n\nwebauthn: rp=mypve.example.com,origin=https://mypve.example.com:8006,id=mypve.example.com\n14.6.9. Server Side U2F Configuration\n\tIt is recommended to use WebAuthn instead.\n\nTo allow users to use U2F authentication, it may be necessary to use a valid domain with a valid SSL certificate, otherwise, some browsers may print a warning or reject U2F usage altogether. Initially, an AppId [52] needs to be configured.\n\n\tChanging the AppId will render all existing U2F registrations unusable!\n\nThis is done via /etc/pve/datacenter.cfg. For instance:\n\nu2f: appid=https://mypve.example.com:8006\n\nFor a single node, the AppId can simply be the address of the web interface, exactly as it is used in the browser, including the https:// and the port, as shown above. Please note that some browsers may be more strict than others when matching AppIds.\n\nWhen using multiple nodes, it is best to have a separate https server providing an appid.json [53] file, as it seems to be compatible with most browsers. If all nodes use subdomains of the same top level domain, it may be enough to use the TLD as AppId. It should however be noted that some browsers may not accept this.\n\n\tA bad AppId will usually produce an error, but we have encountered situations when this does not happen, particularly when using a top level domain AppId for a node that is accessed via a subdomain in Chromium. For this reason it is recommended to test the configuration with multiple browsers, as changing the AppId later will render existing U2F registrations unusable.\n14.6.10. Activating U2F as a User\n\nTo enable U2F authentication, open the TFA window’s U2F tab, type in the current password (unless logged in as root), and press the Register button. If the server is set up correctly and the browser accepts the server’s provided AppId, a message will appear prompting the user to press the button on the U2F device (if it is a YubiKey, the button light should be toggling on and off steadily, roughly twice per second).\n\nFirefox users may need to enable security.webauth.u2f via about:config before they can use a U2F token.\n\n14.7. Permission Management\n\nIn order for a user to perform an action (such as listing, modifying or deleting parts of a VM’s configuration), the user needs to have the appropriate permissions.\n\nProxmox VE uses a role and path based permission management system. An entry in the permissions table allows a user, group or token to take on a specific role when accessing an object or path. This means that such an access rule can be represented as a triple of (path, user, role), (path, group, role) or (path, token, role), with the role containing a set of allowed actions, and the path representing the target of these actions.\n\n14.7.1. Roles\n\nA role is simply a list of privileges. Proxmox VE comes with a number of predefined roles, which satisfy most requirements.\n\nAdministrator: has full privileges\n\nNoAccess: has no privileges (used to forbid access)\n\nPVEAdmin: can do most tasks, but has no rights to modify system settings (Sys.PowerMgmt, Sys.Modify, Realm.Allocate) or permissions (Permissions.Modify)\n\nPVEAuditor: has read only access\n\nPVEDatastoreAdmin: create and allocate backup space and templates\n\nPVEDatastoreUser: allocate backup space and view storage\n\nPVEMappingAdmin: manage resource mappings\n\nPVEMappingUser: view and use resource mappings\n\nPVEPoolAdmin: allocate pools\n\nPVEPoolUser: view pools\n\nPVESDNAdmin: manage SDN configuration\n\nPVESDNUser: access to bridges/vnets\n\nPVESysAdmin: audit, system console and system logs\n\nPVETemplateUser: view and clone templates\n\nPVEUserAdmin: manage users\n\nPVEVMAdmin: fully administer VMs\n\nPVEVMUser: view, backup, configure CD-ROM, VM console, VM power management\n\nYou can see the whole set of predefined roles in the GUI.\n\nYou can add new roles via the GUI or the command line.\n\nFrom the GUI, navigate to the Permissions → Roles tab from Datacenter and click on the Create button. There you can set a role name and select any desired privileges from the Privileges drop-down menu.\n\nTo add a role through the command line, you can use the pveum CLI tool, for example:\n\npveum role add VM_Power-only --privs \"VM.PowerMgmt VM.Console\"\npveum role add Sys_Power-only --privs \"Sys.PowerMgmt Sys.Console\"\n\tRoles starting with PVE are always builtin, custom roles are not allowed use this reserved prefix.\n14.7.2. Privileges\n\nA privilege is the right to perform a specific action. To simplify management, lists of privileges are grouped into roles, which can then be used in the permission table. Note that privileges cannot be directly assigned to users and paths without being part of a role.\n\nWe currently support the following privileges:\n\nNode / System related privileges\n\nGroup.Allocate: create/modify/remove groups\n\nMapping.Audit: view resource mappings\n\nMapping.Modify: manage resource mappings\n\nMapping.Use: use resource mappings\n\nPermissions.Modify: modify access permissions\n\nPool.Allocate: create/modify/remove a pool\n\nPool.Audit: view a pool\n\nRealm.AllocateUser: assign user to a realm\n\nRealm.Allocate: create/modify/remove authentication realms\n\nSDN.Allocate: manage SDN configuration\n\nSDN.Audit: view SDN configuration\n\nSys.Audit: view node status/config, Corosync cluster config, and HA config\n\nSys.Console: console access to node\n\nSys.Incoming: allow incoming data streams from other clusters (experimental)\n\nSys.Modify: create/modify/remove node network parameters\n\nSys.PowerMgmt: node power management (start, stop, reset, shutdown, …)\n\nSys.Syslog: view syslog\n\nUser.Modify: create/modify/remove user access and details.\n\nVirtual machine related privileges\n\nSDN.Use: access SDN vnets and local network bridges\n\nVM.Allocate: create/remove VM on a server\n\nVM.Audit: view VM config\n\nVM.Backup: backup/restore VMs\n\nVM.Clone: clone/copy a VM\n\nVM.Config.CDROM: eject/change CD-ROM\n\nVM.Config.CPU: modify CPU settings\n\nVM.Config.Cloudinit: modify Cloud-init parameters\n\nVM.Config.Disk: add/modify/remove disks\n\nVM.Config.HWType: modify emulated hardware types\n\nVM.Config.Memory: modify memory settings\n\nVM.Config.Network: add/modify/remove network devices\n\nVM.Config.Options: modify any other VM configuration\n\nVM.Console: console access to VM\n\nVM.Migrate: migrate VM to alternate server on cluster\n\nVM.Monitor: access to VM monitor (kvm)\n\nVM.PowerMgmt: power management (start, stop, reset, shutdown, …)\n\nVM.Snapshot.Rollback: rollback VM to one of its snapshots\n\nVM.Snapshot: create/delete VM snapshots\n\nStorage related privileges\n\nDatastore.Allocate: create/modify/remove a datastore and delete volumes\n\nDatastore.AllocateSpace: allocate space on a datastore\n\nDatastore.AllocateTemplate: allocate/upload templates and ISO images\n\nDatastore.Audit: view/browse a datastore\n\n\tBoth Permissions.Modify and Sys.Modify should be handled with care, as they allow modifying aspects of the system and its configuration that are dangerous or sensitive.\n\tCarefully read the section about inheritance below to understand how assigned roles (and their privileges) are propagated along the ACL tree.\n14.7.3. Objects and Paths\n\nAccess permissions are assigned to objects, such as virtual machines, storages or resource pools. We use file system like paths to address these objects. These paths form a natural tree, and permissions of higher levels (shorter paths) can optionally be propagated down within this hierarchy.\n\nPaths can be templated. When an API call requires permissions on a templated path, the path may contain references to parameters of the API call. These references are specified in curly braces. Some parameters are implicitly taken from the API call’s URI. For instance, the permission path /nodes/{node} when calling /nodes/mynode/status requires permissions on /nodes/mynode, while the path {path} in a PUT request to /access/acl refers to the method’s path parameter.\n\nSome examples are:\n\n/nodes/{node}: Access to Proxmox VE server machines\n\n/vms: Covers all VMs\n\n/vms/{vmid}: Access to specific VMs\n\n/storage/{storeid}: Access to a specific storage\n\n/pool/{poolname}: Access to resources contained in a specific pool\n\n/access/groups: Group administration\n\n/access/realms/{realmid}: Administrative access to realms\n\nInheritance\n\nAs mentioned earlier, object paths form a file system like tree, and permissions can be inherited by objects down that tree (the propagate flag is set by default). We use the following inheritance rules:\n\nPermissions for individual users always replace group permissions.\n\nPermissions for groups apply when the user is member of that group.\n\nPermissions on deeper levels replace those inherited from an upper level.\n\nNoAccess cancels all other roles on a given path.\n\nAdditionally, privilege separated tokens can never have permissions on any given path that their associated user does not have.\n\n14.7.4. Pools\n\nPools can be used to group a set of virtual machines and datastores. You can then simply set permissions on pools (/pool/{poolid}), which are inherited by all pool members. This is a great way to simplify access control.\n\n14.7.5. Which Permissions Do I Need?\n\nThe required API permissions are documented for each individual method, and can be found at https://pve.proxmox.com/pve-docs/api-viewer/.\n\nThe permissions are specified as a list, which can be interpreted as a tree of logic and access-check functions:\n\n[\"and\", <subtests>...] and [\"or\", <subtests>...]\n\nEach(and) or any(or) further element in the current list has to be true.\n\n[\"perm\", <path>, [ <privileges>... ], <options>...]\n\nThe path is a templated parameter (see Objects and Paths). All (or, if the any option is used, any) of the listed privileges must be allowed on the specified path. If a require-param option is specified, then its specified parameter is required even if the API call’s schema otherwise lists it as being optional.\n\n[\"userid-group\", [ <privileges>... ], <options>...]\n\nThe caller must have any of the listed privileges on /access/groups. In addition, there are two possible checks, depending on whether the groups_param option is set:\n\ngroups_param is set: The API call has a non-optional groups parameter and the caller must have any of the listed privileges on all of the listed groups.\n\ngroups_param is not set: The user passed via the userid parameter must exist and be part of a group on which the caller has any of the listed privileges (via the /access/groups/<group> path).\n\n[\"userid-param\", \"self\"]\n\nThe value provided for the API call’s userid parameter must refer to the user performing the action (usually in conjunction with or, to allow users to perform an action on themselves, even if they don’t have elevated privileges).\n\n[\"userid-param\", \"Realm.AllocateUser\"]\n\nThe user needs Realm.AllocateUser access to /access/realm/<realm>, with <realm> referring to the realm of the user passed via the userid parameter. Note that the user does not need to exist in order to be associated with a realm, since user IDs are passed in the form of <username>@<realm>.\n\n[\"perm-modify\", <path>]\n\nThe path is a templated parameter (see Objects and Paths). The user needs either the Permissions.Modify privilege or, depending on the path, the following privileges as a possible substitute:\n\n/storage/...: requires 'Datastore.Allocate`\n\n/vms/...: requires 'VM.Allocate`\n\n/pool/...: requires 'Pool.Allocate`\n\nIf the path is empty, Permissions.Modify on /access is required.\n\nIf the user does not have the Permissions.Modify privilege, they can only delegate subsets of their own privileges on the given path (e.g., a user with PVEVMAdmin could assign PVEVMUser, but not PVEAdmin).\n\n14.8. Command-line Tool\n\nMost users will simply use the GUI to manage users. But there is also a fully featured command-line tool called pveum (short for “Proxmox VE User Manager”). Please note that all Proxmox VE command-line tools are wrappers around the API, so you can also access those functions through the REST API.\n\nHere are some simple usage examples. To show help, type:\n\n pveum\n\nor (to show detailed help about a specific command)\n\n pveum help user add\n\nCreate a new user:\n\n pveum user add testuser@pve -comment \"Just a test\"\n\nSet or change the password (not all realms support this):\n\n pveum passwd testuser@pve\n\nDisable a user:\n\n pveum user modify testuser@pve -enable 0\n\nCreate a new group:\n\n pveum group add testgroup\n\nCreate a new role:\n\n pveum role add PVE_Power-only -privs \"VM.PowerMgmt VM.Console\"\n14.9. Real World Examples\n14.9.1. Administrator Group\n\nIt is possible that an administrator would want to create a group of users with full administrator rights (without using the root account).\n\nTo do this, first define the group:\n\n pveum group add admin -comment \"System Administrators\"\n\nThen assign the role:\n\n pveum acl modify / -group admin -role Administrator\n\nFinally, you can add users to the new admin group:\n\n pveum user modify testuser@pve -group admin\n14.9.2. Auditors\n\nYou can give read only access to users by assigning the PVEAuditor role to users or groups.\n\nExample 1: Allow user joe@pve to see everything\n\n pveum acl modify / -user joe@pve -role PVEAuditor\n\nExample 2: Allow user joe@pve to see all virtual machines\n\n pveum acl modify /vms -user joe@pve -role PVEAuditor\n14.9.3. Delegate User Management\n\nIf you want to delegate user management to user joe@pve, you can do that with:\n\n pveum acl modify /access -user joe@pve -role PVEUserAdmin\n\nUser joe@pve can now add and remove users, and change other user attributes, such as passwords. This is a very powerful role, and you most likely want to limit it to selected realms and groups. The following example allows joe@pve to modify users within the realm pve, if they are members of group customers:\n\n pveum acl modify /access/realm/pve -user joe@pve -role PVEUserAdmin\n pveum acl modify /access/groups/customers -user joe@pve -role PVEUserAdmin\n\tThe user is able to add other users, but only if they are members of the group customers and within the realm pve.\n14.9.4. Limited API Token for Monitoring\n\nPermissions on API tokens are always a subset of those of their corresponding user, meaning that an API token can’t be used to carry out a task that the backing user has no permission to do. This section will demonstrate how you can use an API token with separate privileges, to limit the token owner’s permissions further.\n\nGive the user joe@pve the role PVEVMAdmin on all VMs:\n\n pveum acl modify /vms -user joe@pve -role PVEVMAdmin\n\nAdd a new API token with separate privileges, which is only allowed to view VM information (for example, for monitoring purposes):\n\n pveum user token add joe@pve monitoring -privsep 1\n pveum acl modify /vms -token 'joe@pve!monitoring' -role PVEAuditor\n\nVerify the permissions of the user and token:\n\n pveum user permissions joe@pve\n pveum user token permissions joe@pve monitoring\n14.9.5. Resource Pools\n\nAn enterprise is usually structured into several smaller departments, and it is common that you want to assign resources and delegate management tasks to each of these. Let’s assume that you want to set up a pool for a software development department. First, create a group:\n\n pveum group add developers -comment \"Our software developers\"\n\nNow we create a new user which is a member of that group:\n\n pveum user add developer1@pve -group developers -password\n\tThe \"-password\" parameter will prompt you for a password\n\nThen we create a resource pool for our development department to use:\n\n pveum pool add dev-pool --comment \"IT development pool\"\n\nFinally, we can assign permissions to that pool:\n\n pveum acl modify /pool/dev-pool/ -group developers -role PVEAdmin\n\nOur software developers can now administer the resources assigned to that pool.\n\n15. High Availability\n\nOur modern society depends heavily on information provided by computers over the network. Mobile devices amplified that dependency, because people can access the network any time from anywhere. If you provide such services, it is very important that they are available most of the time.\n\nWe can mathematically define the availability as the ratio of (A), the total time a service is capable of being used during a given interval to (B), the length of the interval. It is normally expressed as a percentage of uptime in a given year.\n\nTable 18. Availability - Downtime per Year\nAvailability %\tDowntime per year\n\n\n99\n\n\t\n\n3.65 days\n\n\n\n\n99.9\n\n\t\n\n8.76 hours\n\n\n\n\n99.99\n\n\t\n\n52.56 minutes\n\n\n\n\n99.999\n\n\t\n\n5.26 minutes\n\n\n\n\n99.9999\n\n\t\n\n31.5 seconds\n\n\n\n\n99.99999\n\n\t\n\n3.15 seconds\n\nThere are several ways to increase availability. The most elegant solution is to rewrite your software, so that you can run it on several hosts at the same time. The software itself needs to have a way to detect errors and do failover. If you only want to serve read-only web pages, then this is relatively simple. However, this is generally complex and sometimes impossible, because you cannot modify the software yourself. The following solutions works without modifying the software:\n\nUse reliable “server” components\n\n\tComputer components with the same functionality can have varying reliability numbers, depending on the component quality. Most vendors sell components with higher reliability as “server” components - usually at higher price.\n\nEliminate single point of failure (redundant components)\n\nuse an uninterruptible power supply (UPS)\n\nuse redundant power supplies in your servers\n\nuse ECC-RAM\n\nuse redundant network hardware\n\nuse RAID for local storage\n\nuse distributed, redundant storage for VM data\n\nReduce downtime\n\nrapidly accessible administrators (24/7)\n\navailability of spare parts (other nodes in a Proxmox VE cluster)\n\nautomatic error detection (provided by ha-manager)\n\nautomatic failover (provided by ha-manager)\n\nVirtualization environments like Proxmox VE make it much easier to reach high availability because they remove the “hardware” dependency. They also support the setup and use of redundant storage and network devices, so if one host fails, you can simply start those services on another host within your cluster.\n\nBetter still, Proxmox VE provides a software stack called ha-manager, which can do that automatically for you. It is able to automatically detect errors and do automatic failover.\n\nProxmox VE ha-manager works like an “automated” administrator. First, you configure what resources (VMs, containers, …) it should manage. Then, ha-manager observes the correct functionality, and handles service failover to another node in case of errors. ha-manager can also handle normal user requests which may start, stop, relocate and migrate a service.\n\nBut high availability comes at a price. High quality components are more expensive, and making them redundant doubles the costs at least. Additional spare parts increase costs further. So you should carefully calculate the benefits, and compare with those additional costs.\n\n\tIncreasing availability from 99% to 99.9% is relatively simple. But increasing availability from 99.9999% to 99.99999% is very hard and costly. ha-manager has typical error detection and failover times of about 2 minutes, so you can get no more than 99.999% availability.\n15.1. Requirements\n\nYou must meet the following requirements before you start with HA:\n\nat least three cluster nodes (to get reliable quorum)\n\nshared storage for VMs and containers\n\nhardware redundancy (everywhere)\n\nuse reliable “server” components\n\nhardware watchdog - if not available we fall back to the linux kernel software watchdog (softdog)\n\noptional hardware fencing devices\n\n15.2. Resources\n\nWe call the primary management unit handled by ha-manager a resource. A resource (also called “service”) is uniquely identified by a service ID (SID), which consists of the resource type and a type specific ID, for example vm:100. That example would be a resource of type vm (virtual machine) with the ID 100.\n\nFor now we have two important resources types - virtual machines and containers. One basic idea here is that we can bundle related software into such a VM or container, so there is no need to compose one big service from other services, as was done with rgmanager. In general, a HA managed resource should not depend on other resources.\n\n15.3. Management Tasks\n\nThis section provides a short overview of common management tasks. The first step is to enable HA for a resource. This is done by adding the resource to the HA resource configuration. You can do this using the GUI, or simply use the command-line tool, for example:\n\n# ha-manager add vm:100\n\nThe HA stack now tries to start the resources and keep them running. Please note that you can configure the “requested” resources state. For example you may want the HA stack to stop the resource:\n\n# ha-manager set vm:100 --state stopped\n\nand start it again later:\n\n# ha-manager set vm:100 --state started\n\nYou can also use the normal VM and container management commands. They automatically forward the commands to the HA stack, so\n\n# qm start 100\n\nsimply sets the requested state to started. The same applies to qm stop, which sets the requested state to stopped.\n\n\tThe HA stack works fully asynchronous and needs to communicate with other cluster members. Therefore, it takes some seconds until you see the result of such actions.\n\nTo view the current HA resource configuration use:\n\n# ha-manager config\nvm:100\n        state stopped\n\nAnd you can view the actual HA manager and resource state with:\n\n# ha-manager status\nquorum OK\nmaster node1 (active, Wed Nov 23 11:07:23 2016)\nlrm elsa (active, Wed Nov 23 11:07:19 2016)\nservice vm:100 (node1, started)\n\nYou can also initiate resource migration to other nodes:\n\n# ha-manager migrate vm:100 node2\n\nThis uses online migration and tries to keep the VM running. Online migration needs to transfer all used memory over the network, so it is sometimes faster to stop the VM, then restart it on the new node. This can be done using the relocate command:\n\n# ha-manager relocate vm:100 node2\n\nFinally, you can remove the resource from the HA configuration using the following command:\n\n# ha-manager remove vm:100\n\tThis does not start or stop the resource.\n\nBut all HA related tasks can be done in the GUI, so there is no need to use the command line at all.\n\n15.4. How It Works\n\nThis section provides a detailed description of the Proxmox VE HA manager internals. It describes all involved daemons and how they work together. To provide HA, two daemons run on each node:\n\npve-ha-lrm\n\nThe local resource manager (LRM), which controls the services running on the local node. It reads the requested states for its services from the current manager status file and executes the respective commands.\n\npve-ha-crm\n\nThe cluster resource manager (CRM), which makes the cluster-wide decisions. It sends commands to the LRM, processes the results, and moves resources to other nodes if something fails. The CRM also handles node fencing.\n\n\t\nLocks in the LRM & CRM\nLocks are provided by our distributed configuration file system (pmxcfs). They are used to guarantee that each LRM is active once and working. As an LRM only executes actions when it holds its lock, we can mark a failed node as fenced if we can acquire its lock. This then lets us recover any failed HA services securely without any interference from the now unknown failed node. This all gets supervised by the CRM which currently holds the manager master lock.\n15.4.1. Service States\n\nThe CRM uses a service state enumeration to record the current service state. This state is displayed on the GUI and can be queried using the ha-manager command-line tool:\n\n# ha-manager status\nquorum OK\nmaster elsa (active, Mon Nov 21 07:23:29 2016)\nlrm elsa (active, Mon Nov 21 07:23:22 2016)\nservice ct:100 (elsa, stopped)\nservice ct:102 (elsa, started)\nservice vm:501 (elsa, started)\n\nHere is the list of possible states:\n\nstopped\n\nService is stopped (confirmed by LRM). If the LRM detects a stopped service is still running, it will stop it again.\n\nrequest_stop\n\nService should be stopped. The CRM waits for confirmation from the LRM.\n\nstopping\n\nPending stop request. But the CRM did not get the request so far.\n\nstarted\n\nService is active an LRM should start it ASAP if not already running. If the Service fails and is detected to be not running the LRM restarts it (see Start Failure Policy).\n\nstarting\n\nPending start request. But the CRM has not got any confirmation from the LRM that the service is running.\n\nfence\n\nWait for node fencing as the service node is not inside the quorate cluster partition (see Fencing). As soon as node gets fenced successfully the service will be placed into the recovery state.\n\nrecovery\n\nWait for recovery of the service. The HA manager tries to find a new node where the service can run on. This search depends not only on the list of online and quorate nodes, but also if the service is a group member and how such a group is limited. As soon as a new available node is found, the service will be moved there and initially placed into stopped state. If it’s configured to run the new node will do so.\n\nfreeze\n\nDo not touch the service state. We use this state while we reboot a node, or when we restart the LRM daemon (see Package Updates).\n\nignored\n\nAct as if the service were not managed by HA at all. Useful, when full control over the service is desired temporarily, without removing it from the HA configuration.\n\nmigrate\n\nMigrate service (live) to other node.\n\nerror\n\nService is disabled because of LRM errors. Needs manual intervention (see Error Recovery).\n\nqueued\n\nService is newly added, and the CRM has not seen it so far.\n\ndisabled\n\nService is stopped and marked as disabled\n\n15.4.2. Local Resource Manager\n\nThe local resource manager (pve-ha-lrm) is started as a daemon on boot and waits until the HA cluster is quorate and thus cluster-wide locks are working.\n\nIt can be in three states:\n\nwait for agent lock\n\nThe LRM waits for our exclusive lock. This is also used as idle state if no service is configured.\n\nactive\n\nThe LRM holds its exclusive lock and has services configured.\n\nlost agent lock\n\nThe LRM lost its lock, this means a failure happened and quorum was lost.\n\nAfter the LRM gets in the active state it reads the manager status file in /etc/pve/ha/manager_status and determines the commands it has to execute for the services it owns. For each command a worker gets started, these workers are running in parallel and are limited to at most 4 by default. This default setting may be changed through the datacenter configuration key max_worker. When finished the worker process gets collected and its result saved for the CRM.\n\n\t\nMaximum Concurrent Worker Adjustment Tips\nThe default value of at most 4 concurrent workers may be unsuited for a specific setup. For example, 4 live migrations may occur at the same time, which can lead to network congestions with slower networks and/or big (memory wise) services. Also, ensure that in the worst case, congestion is at a minimum, even if this means lowering the max_worker value. On the contrary, if you have a particularly powerful, high-end setup you may also want to increase it.\n\nEach command requested by the CRM is uniquely identifiable by a UID. When the worker finishes, its result will be processed and written in the LRM status file /etc/pve/nodes/<nodename>/lrm_status. There the CRM may collect it and let its state machine - respective to the commands output - act on it.\n\nThe actions on each service between CRM and LRM are normally always synced. This means that the CRM requests a state uniquely marked by a UID, the LRM then executes this action one time and writes back the result, which is also identifiable by the same UID. This is needed so that the LRM does not execute an outdated command. The only exceptions to this behaviour are the stop and error commands; these two do not depend on the result produced and are executed always in the case of the stopped state and once in the case of the error state.\n\n\t\nRead the Logs\nThe HA Stack logs every action it makes. This helps to understand what and also why something happens in the cluster. Here its important to see what both daemons, the LRM and the CRM, did. You may use journalctl -u pve-ha-lrm on the node(s) where the service is and the same command for the pve-ha-crm on the node which is the current master.\n15.4.3. Cluster Resource Manager\n\nThe cluster resource manager (pve-ha-crm) starts on each node and waits there for the manager lock, which can only be held by one node at a time. The node which successfully acquires the manager lock gets promoted to the CRM master.\n\nIt can be in three states:\n\nwait for agent lock\n\nThe CRM waits for our exclusive lock. This is also used as idle state if no service is configured\n\nactive\n\nThe CRM holds its exclusive lock and has services configured\n\nlost agent lock\n\nThe CRM lost its lock, this means a failure happened and quorum was lost.\n\nIts main task is to manage the services which are configured to be highly available and try to always enforce the requested state. For example, a service with the requested state started will be started if its not already running. If it crashes it will be automatically started again. Thus the CRM dictates the actions the LRM needs to execute.\n\nWhen a node leaves the cluster quorum, its state changes to unknown. If the current CRM can then secure the failed node’s lock, the services will be stolen and restarted on another node.\n\nWhen a cluster member determines that it is no longer in the cluster quorum, the LRM waits for a new quorum to form. As long as there is no quorum the node cannot reset the watchdog. This will trigger a reboot after the watchdog times out (this happens after 60 seconds).\n\n15.5. HA Simulator\n\nBy using the HA simulator you can test and learn all functionalities of the Proxmox VE HA solutions.\n\nBy default, the simulator allows you to watch and test the behaviour of a real-world 3 node cluster with 6 VMs. You can also add or remove additional VMs or Container.\n\nYou do not have to setup or configure a real cluster, the HA simulator runs out of the box.\n\nInstall with apt:\n\napt install pve-ha-simulator\n\nYou can even install the package on any Debian-based system without any other Proxmox VE packages. For that you will need to download the package and copy it to the system you want to run it on for installation. When you install the package with apt from the local file system it will also resolve the required dependencies for you.\n\nTo start the simulator on a remote machine you must have an X11 redirection to your current system.\n\nIf you are on a Linux machine you can use:\n\nssh root@<IPofPVE> -Y\n\nOn Windows it works with mobaxterm.\n\nAfter connecting to an existing Proxmox VE with the simulator installed or installing it on your local Debian-based system manually, you can try it out as follows.\n\nFirst you need to create a working directory where the simulator saves its current state and writes its default config:\n\nmkdir working\n\nThen, simply pass the created directory as a parameter to pve-ha-simulator:\n\npve-ha-simulator working/\n\nYou can then start, stop, migrate the simulated HA services, or even check out what happens on a node failure.\n\n15.6. Configuration\n\nThe HA stack is well integrated into the Proxmox VE API. So, for example, HA can be configured via the ha-manager command-line interface, or the Proxmox VE web interface - both interfaces provide an easy way to manage HA. Automation tools can use the API directly.\n\nAll HA configuration files are within /etc/pve/ha/, so they get automatically distributed to the cluster nodes, and all nodes share the same HA configuration.\n\n15.6.1. Resources\n\nThe resource configuration file /etc/pve/ha/resources.cfg stores the list of resources managed by ha-manager. A resource configuration inside that list looks like this:\n\n<type>: <name>\n        <property> <value>\n        ...\n\nIt starts with a resource type followed by a resource specific name, separated with colon. Together this forms the HA resource ID, which is used by all ha-manager commands to uniquely identify a resource (example: vm:100 or ct:101). The next lines contain additional properties:\n\ncomment: <string>\n\nDescription.\n\ngroup: <string>\n\nThe HA group identifier.\n\nmax_relocate: <integer> (0 - N) (default = 1)\n\nMaximal number of service relocate tries when a service failes to start.\n\nmax_restart: <integer> (0 - N) (default = 1)\n\nMaximal number of tries to restart the service on a node after its start failed.\n\nstate: <disabled | enabled | ignored | started | stopped> (default = started)\n\nRequested resource state. The CRM reads this state and acts accordingly. Please note that enabled is just an alias for started.\n\nstarted\n\nThe CRM tries to start the resource. Service state is set to started after successful start. On node failures, or when start fails, it tries to recover the resource. If everything fails, service state it set to error.\n\nstopped\n\nThe CRM tries to keep the resource in stopped state, but it still tries to relocate the resources on node failures.\n\ndisabled\n\nThe CRM tries to put the resource in stopped state, but does not try to relocate the resources on node failures. The main purpose of this state is error recovery, because it is the only way to move a resource out of the error state.\n\nignored\n\nThe resource gets removed from the manager status and so the CRM and the LRM do not touch the resource anymore. All {pve} API calls affecting this resource will be executed, directly bypassing the HA stack. CRM commands will be thrown away while there source is in this state. The resource will not get relocated on node failures.\n\nHere is a real world example with one VM and one container. As you see, the syntax of those files is really simple, so it is even possible to read or edit those files using your favorite editor:\n\nConfiguration Example (/etc/pve/ha/resources.cfg)\nvm: 501\n    state started\n    max_relocate 2\n\nct: 102\n    # Note: use default settings for everything\n\nThe above config was generated using the ha-manager command-line tool:\n\n# ha-manager add vm:501 --state started --max_relocate 2\n# ha-manager add ct:102\n15.6.2. Groups\n\nThe HA group configuration file /etc/pve/ha/groups.cfg is used to define groups of cluster nodes. A resource can be restricted to run only on the members of such group. A group configuration look like this:\n\ngroup: <group>\n       nodes <node_list>\n       <property> <value>\n       ...\ncomment: <string>\n\nDescription.\n\nnodes: <node>[:<pri>]{,<node>[:<pri>]}*\n\nList of cluster node members, where a priority can be given to each node. A resource bound to a group will run on the available nodes with the highest priority. If there are more nodes in the highest priority class, the services will get distributed to those nodes. The priorities have a relative meaning only.\n\nnofailback: <boolean> (default = 0)\n\nThe CRM tries to run services on the node with the highest priority. If a node with higher priority comes online, the CRM migrates the service to that node. Enabling nofailback prevents that behavior.\n\nrestricted: <boolean> (default = 0)\n\nResources bound to restricted groups may only run on nodes defined by the group. The resource will be placed in the stopped state if no group node member is online. Resources on unrestricted groups may run on any cluster node if all group members are offline, but they will migrate back as soon as a group member comes online. One can implement a preferred node behavior using an unrestricted group with only one member.\n\nA common requirement is that a resource should run on a specific node. Usually the resource is able to run on other nodes, so you can define an unrestricted group with a single member:\n\n# ha-manager groupadd prefer_node1 --nodes node1\n\nFor bigger clusters, it makes sense to define a more detailed failover behavior. For example, you may want to run a set of services on node1 if possible. If node1 is not available, you want to run them equally split on node2 and node3. If those nodes also fail, the services should run on node4. To achieve this you could set the node list to:\n\n# ha-manager groupadd mygroup1 -nodes \"node1:2,node2:1,node3:1,node4\"\n\nAnother use case is if a resource uses other resources only available on specific nodes, lets say node1 and node2. We need to make sure that HA manager does not use other nodes, so we need to create a restricted group with said nodes:\n\n# ha-manager groupadd mygroup2 -nodes \"node1,node2\" -restricted\n\nThe above commands created the following group configuration file:\n\nConfiguration Example (/etc/pve/ha/groups.cfg)\ngroup: prefer_node1\n       nodes node1\n\ngroup: mygroup1\n       nodes node2:1,node4,node1:2,node3:1\n\ngroup: mygroup2\n       nodes node2,node1\n       restricted 1\n\nThe nofailback options is mostly useful to avoid unwanted resource movements during administration tasks. For example, if you need to migrate a service to a node which doesn’t have the highest priority in the group, you need to tell the HA manager not to instantly move this service back by setting the nofailback option.\n\nAnother scenario is when a service was fenced and it got recovered to another node. The admin tries to repair the fenced node and brings it up online again to investigate the cause of failure and check if it runs stably again. Setting the nofailback flag prevents the recovered services from moving straight back to the fenced node.\n\n15.7. Fencing\n\nOn node failures, fencing ensures that the erroneous node is guaranteed to be offline. This is required to make sure that no resource runs twice when it gets recovered on another node. This is a really important task, because without this, it would not be possible to recover a resource on another node.\n\nIf a node did not get fenced, it would be in an unknown state where it may have still access to shared resources. This is really dangerous! Imagine that every network but the storage one broke. Now, while not reachable from the public network, the VM still runs and writes to the shared storage.\n\nIf we then simply start up this VM on another node, we would get a dangerous race condition, because we write from both nodes. Such conditions can destroy all VM data and the whole VM could be rendered unusable. The recovery could also fail if the storage protects against multiple mounts.\n\n15.7.1. How Proxmox VE Fences\n\nThere are different methods to fence a node, for example, fence devices which cut off the power from the node or disable their communication completely. Those are often quite expensive and bring additional critical components into a system, because if they fail you cannot recover any service.\n\nWe thus wanted to integrate a simpler fencing method, which does not require additional external hardware. This can be done using watchdog timers.\n\nPossible Fencing Methods\n\nexternal power switches\n\nisolate nodes by disabling complete network traffic on the switch\n\nself fencing using watchdog timers\n\nWatchdog timers have been widely used in critical and dependable systems since the beginning of microcontrollers. They are often simple, independent integrated circuits which are used to detect and recover from computer malfunctions.\n\nDuring normal operation, ha-manager regularly resets the watchdog timer to prevent it from elapsing. If, due to a hardware fault or program error, the computer fails to reset the watchdog, the timer will elapse and trigger a reset of the whole server (reboot).\n\nRecent server motherboards often include such hardware watchdogs, but these need to be configured. If no watchdog is available or configured, we fall back to the Linux Kernel softdog. While still reliable, it is not independent of the servers hardware, and thus has a lower reliability than a hardware watchdog.\n\n15.7.2. Configure Hardware Watchdog\n\nBy default, all hardware watchdog modules are blocked for security reasons. They are like a loaded gun if not correctly initialized. To enable a hardware watchdog, you need to specify the module to load in /etc/default/pve-ha-manager, for example:\n\n# select watchdog module (default is softdog)\nWATCHDOG_MODULE=iTCO_wdt\n\nThis configuration is read by the watchdog-mux service, which loads the specified module at startup.\n\n15.7.3. Recover Fenced Services\n\nAfter a node failed and its fencing was successful, the CRM tries to move services from the failed node to nodes which are still online.\n\nThe selection of nodes, on which those services gets recovered, is influenced by the resource group settings, the list of currently active nodes, and their respective active service count.\n\nThe CRM first builds a set out of the intersection between user selected nodes (from group setting) and available nodes. It then choose the subset of nodes with the highest priority, and finally select the node with the lowest active service count. This minimizes the possibility of an overloaded node.\n\n\tOn node failure, the CRM distributes services to the remaining nodes. This increases the service count on those nodes, and can lead to high load, especially on small clusters. Please design your cluster so that it can handle such worst case scenarios.\n15.8. Start Failure Policy\n\nThe start failure policy comes into effect if a service failed to start on a node one or more times. It can be used to configure how often a restart should be triggered on the same node and how often a service should be relocated, so that it has an attempt to be started on another node. The aim of this policy is to circumvent temporary unavailability of shared resources on a specific node. For example, if a shared storage isn’t available on a quorate node anymore, for instance due to network problems, but is still available on other nodes, the relocate policy allows the service to start nonetheless.\n\nThere are two service start recover policy settings which can be configured specific for each resource.\n\nmax_restart\n\nMaximum number of attempts to restart a failed service on the actual node. The default is set to one.\n\nmax_relocate\n\nMaximum number of attempts to relocate the service to a different node. A relocate only happens after the max_restart value is exceeded on the actual node. The default is set to one.\n\n\tThe relocate count state will only reset to zero when the service had at least one successful start. That means if a service is re-started without fixing the error only the restart policy gets repeated.\n15.9. Error Recovery\n\nIf, after all attempts, the service state could not be recovered, it gets placed in an error state. In this state, the service won’t get touched by the HA stack anymore. The only way out is disabling a service:\n\n# ha-manager set vm:100 --state disabled\n\nThis can also be done in the web interface.\n\nTo recover from the error state you should do the following:\n\nbring the resource back into a safe and consistent state (e.g.: kill its process if the service could not be stopped)\n\ndisable the resource to remove the error flag\n\nfix the error which led to this failures\n\nafter you fixed all errors you may request that the service starts again\n\n15.10. Package Updates\n\nWhen updating the ha-manager, you should do one node after the other, never all at once for various reasons. First, while we test our software thoroughly, a bug affecting your specific setup cannot totally be ruled out. Updating one node after the other and checking the functionality of each node after finishing the update helps to recover from eventual problems, while updating all at once could result in a broken cluster and is generally not good practice.\n\nAlso, the Proxmox VE HA stack uses a request acknowledge protocol to perform actions between the cluster and the local resource manager. For restarting, the LRM makes a request to the CRM to freeze all its services. This prevents them from getting touched by the Cluster during the short time the LRM is restarting. After that, the LRM may safely close the watchdog during a restart. Such a restart happens normally during a package update and, as already stated, an active master CRM is needed to acknowledge the requests from the LRM. If this is not the case the update process can take too long which, in the worst case, may result in a reset triggered by the watchdog.\n\n15.11. Node Maintenance\n\nSometimes it is necessary to perform maintenance on a node, such as replacing hardware or simply installing a new kernel image. This also applies while the HA stack is in use.\n\nThe HA stack can support you mainly in two types of maintenance:\n\nfor general shutdowns or reboots, the behavior can be configured, see Shutdown Policy.\n\nfor maintenance that does not require a shutdown or reboot, or that should not be switched off automatically after only one reboot, you can enable the manual maintenance mode.\n\n15.11.1. Maintenance Mode\n\nYou can use the manual maintenance mode to mark the node as unavailable for HA operation, prompting all services managed by HA to migrate to other nodes.\n\nThe target nodes for these migrations are selected from the other currently available nodes, and determined by the HA group configuration and the configured cluster resource scheduler (CRS) mode. During each migration, the original node will be recorded in the HA managers' state, so that the service can be moved back again automatically once the maintenance mode is disabled and the node is back online.\n\nCurrently you can enabled or disable the maintenance mode using the ha-manager CLI tool.\n\nEnabling maintenance mode for a node\n# ha-manager crm-command node-maintenance enable NODENAME\n\nThis will queue a CRM command, when the manager processes this command it will record the request for maintenance-mode in the manager status. This allows you to submit the command on any node, not just on the one you want to place in, or out of the maintenance mode.\n\nOnce the LRM on the respective node picks the command up it will mark itself as unavailable, but still process all migration commands. This means that the LRM self-fencing watchdog will stay active until all active services got moved, and all running workers finished.\n\nNote that the LRM status will read maintenance mode as soon as the LRM picked the requested state up, not only when all services got moved away, this user experience is planned to be improved in the future. For now, you can check for any active HA service left on the node, or watching out for a log line like: pve-ha-lrm[PID]: watchdog closed (disabled) to know when the node finished its transition into the maintenance mode.\n\n\tThe manual maintenance mode is not automatically deleted on node reboot, but only if it is either manually deactivated using the ha-manager CLI or if the manager-status is manually cleared.\nDisabling maintenance mode for a node\n# ha-manager crm-command node-maintenance disable NODENAME\n\nThe process of disabling the manual maintenance mode is similar to enabling it. Using the ha-manager CLI command shown above will queue a CRM command that, once processed, marks the respective LRM node as available again.\n\nIf you deactivate the maintenance mode, all services that were on the node when the maintenance mode was activated will be moved back.\n\n15.11.2. Shutdown Policy\n\nBelow you will find a description of the different HA policies for a node shutdown. Currently Conditional is the default due to backward compatibility. Some users may find that Migrate behaves more as expected.\n\nThe shutdown policy can be configured in the Web UI (Datacenter → Options → HA Settings), or directly in datacenter.cfg:\n\nha: shutdown_policy=<value>\nMigrate\n\nOnce the Local Resource manager (LRM) gets a shutdown request and this policy is enabled, it will mark itself as unavailable for the current HA manager. This triggers a migration of all HA Services currently located on this node. The LRM will try to delay the shutdown process, until all running services get moved away. But, this expects that the running services can be migrated to another node. In other words, the service must not be locally bound, for example by using hardware passthrough. As non-group member nodes are considered as runnable target if no group member is available, this policy can still be used when making use of HA groups with only some nodes selected. But, marking a group as restricted tells the HA manager that the service cannot run outside of the chosen set of nodes. If all of those nodes are unavailable, the shutdown will hang until you manually intervene. Once the shut down node comes back online again, the previously displaced services will be moved back, if they were not already manually migrated in-between.\n\n\tThe watchdog is still active during the migration process on shutdown. If the node loses quorum it will be fenced and the services will be recovered.\n\nIf you start a (previously stopped) service on a node which is currently being maintained, the node needs to be fenced to ensure that the service can be moved and started on another available node.\n\nFailover\n\nThis mode ensures that all services get stopped, but that they will also be recovered, if the current node is not online soon. It can be useful when doing maintenance on a cluster scale, where live-migrating VMs may not be possible if too many nodes are powered off at a time, but you still want to ensure HA services get recovered and started again as soon as possible.\n\nFreeze\n\nThis mode ensures that all services get stopped and frozen, so that they won’t get recovered until the current node is online again.\n\nConditional\n\nThe Conditional shutdown policy automatically detects if a shutdown or a reboot is requested, and changes behaviour accordingly.\n\nShutdown\n\nA shutdown (poweroff) is usually done if it is planned for the node to stay down for some time. The LRM stops all managed services in this case. This means that other nodes will take over those services afterwards.\n\n\tRecent hardware has large amounts of memory (RAM). So we stop all resources, then restart them to avoid online migration of all that RAM. If you want to use online migration, you need to invoke that manually before you shutdown the node.\nReboot\n\nNode reboots are initiated with the reboot command. This is usually done after installing a new kernel. Please note that this is different from “shutdown”, because the node immediately starts again.\n\nThe LRM tells the CRM that it wants to restart, and waits until the CRM puts all resources into the freeze state (same mechanism is used for Package Updates). This prevents those resources from being moved to other nodes. Instead, the CRM starts the resources after the reboot on the same node.\n\nManual Resource Movement\n\nLast but not least, you can also manually move resources to other nodes, before you shutdown or restart a node. The advantage is that you have full control, and you can decide if you want to use online migration or not.\n\n\tPlease do not kill services like pve-ha-crm, pve-ha-lrm or watchdog-mux. They manage and use the watchdog, so this can result in an immediate node reboot or even reset.\n15.12. Cluster Resource Scheduling\n\nThe cluster resource scheduler (CRS) mode controls how HA selects nodes for the recovery of a service as well as for migrations that are triggered by a shutdown policy. The default mode is basic, you can change it in the Web UI (Datacenter → Options), or directly in datacenter.cfg:\n\ncrs: ha=static\n\nThe change will be in effect starting with the next manager round (after a few seconds).\n\nFor each service that needs to be recovered or migrated, the scheduler iteratively chooses the best node among the nodes with the highest priority in the service’s group.\n\n\tThere are plans to add modes for (static and dynamic) load-balancing in the future.\n15.12.1. Basic Scheduler\n\nThe number of active HA services on each node is used to choose a recovery node. Non-HA-managed services are currently not counted.\n\n15.12.2. Static-Load Scheduler\n\tThe static mode is still a technology preview.\n\nStatic usage information from HA services on each node is used to choose a recovery node. Usage of non-HA-managed services is currently not considered.\n\nFor this selection, each node in turn is considered as if the service was already running on it, using CPU and memory usage from the associated guest configuration. Then for each such alternative, CPU and memory usage of all nodes are considered, with memory being weighted much more, because it’s a truly limited resource. For both, CPU and memory, highest usage among nodes (weighted more, as ideally no node should be overcommitted) and average usage of all nodes (to still be able to distinguish in case there already is a more highly committed node) are considered.\n\n\tThe more services the more possible combinations there are, so it’s currently not recommended to use it if you have thousands of HA managed services.\n15.12.3. CRS Scheduling Points\n\nThe CRS algorithm is not applied for every service in every round, since this would mean a large number of constant migrations. Depending on the workload, this could put more strain on the cluster than could be avoided by constant balancing. That’s why the Proxmox VE HA manager favors keeping services on their current node.\n\nThe CRS is currently used at the following scheduling points:\n\nService recovery (always active). When a node with active HA services fails, all its services need to be recovered to other nodes. The CRS algorithm will be used here to balance that recovery over the remaining nodes.\n\nHA group config changes (always active). If a node is removed from a group, or its priority is reduced, the HA stack will use the CRS algorithm to find a new target node for the HA services in that group, matching the adapted priority constraints.\n\nHA service stopped → start transtion (opt-in). Requesting that a stopped service should be started is an good opportunity to check for the best suited node as per the CRS algorithm, as moving stopped services is cheaper to do than moving them started, especially if their disk volumes reside on shared storage. You can enable this by setting the ha-rebalance-on-start CRS option in the datacenter config. You can change that option also in the Web UI, under Datacenter → Options → Cluster Resource Scheduling.\n\n16. Backup and Restore\n\nBackups are a requirement for any sensible IT deployment, and Proxmox VE provides a fully integrated solution, using the capabilities of each storage and each guest system type. This allows the system administrator to fine tune via the mode option between consistency of the backups and downtime of the guest system.\n\nProxmox VE backups are always full backups - containing the VM/CT configuration and all data. Backups can be started via the GUI or via the vzdump command-line tool.\n\nBackup Storage\n\nBefore a backup can run, a backup storage must be defined. Refer to the storage documentation on how to add a storage. It can either be a Proxmox Backup Server storage, where backups are stored as de-duplicated chunks and metadata, or a file-level storage, where backups are stored as regular files. Using Proxmox Backup Server on a dedicated host is recommended, because of its advanced features. Using an NFS server is a good alternative. In both cases, you might want to save those backups later to a tape drive, for off-site archiving.\n\nScheduled Backup\n\nBackup jobs can be scheduled so that they are executed automatically on specific days and times, for selectable nodes and guest systems. See the Backup Jobs section for more.\n\n16.1. Backup Modes\n\nThere are several ways to provide consistency (option mode), depending on the guest type.\n\nBackup modes for VMs:\nstop mode\n\nThis mode provides the highest consistency of the backup, at the cost of a short downtime in the VM operation. It works by executing an orderly shutdown of the VM, and then runs a background QEMU process to backup the VM data. After the backup is started, the VM goes to full operation mode if it was previously running. Consistency is guaranteed by using the live backup feature.\n\nsuspend mode\n\nThis mode is provided for compatibility reason, and suspends the VM before calling the snapshot mode. Since suspending the VM results in a longer downtime and does not necessarily improve the data consistency, the use of the snapshot mode is recommended instead.\n\nsnapshot mode\n\nThis mode provides the lowest operation downtime, at the cost of a small inconsistency risk. It works by performing a Proxmox VE live backup, in which data blocks are copied while the VM is running. If the guest agent is enabled (agent: 1) and running, it calls guest-fsfreeze-freeze and guest-fsfreeze-thaw to improve consistency.\n\nA technical overview of the Proxmox VE live backup for QemuServer can be found online here.\n\n\tProxmox VE live backup provides snapshot-like semantics on any storage type. It does not require that the underlying storage supports snapshots. Also please note that since the backups are done via a background QEMU process, a stopped VM will appear as running for a short amount of time while the VM disks are being read by QEMU. However the VM itself is not booted, only its disk(s) are read.\nBackup modes for Containers:\nstop mode\n\nStop the container for the duration of the backup. This potentially results in a very long downtime.\n\nsuspend mode\n\nThis mode uses rsync to copy the container data to a temporary location (see option --tmpdir). Then the container is suspended and a second rsync copies changed files. After that, the container is started (resumed) again. This results in minimal downtime, but needs additional space to hold the container copy.\n\nWhen the container is on a local file system and the target storage of the backup is an NFS/CIFS server, you should set --tmpdir to reside on a local file system too, as this will result in a many fold performance improvement. Use of a local tmpdir is also required if you want to backup a local container using ACLs in suspend mode if the backup storage is an NFS server.\n\nsnapshot mode\n\nThis mode uses the snapshotting facilities of the underlying storage. First, the container will be suspended to ensure data consistency. A temporary snapshot of the container’s volumes will be made and the snapshot content will be archived in a tar file. Finally, the temporary snapshot is deleted again.\n\n\tsnapshot mode requires that all backed up volumes are on a storage that supports snapshots. Using the backup=no mount point option individual volumes can be excluded from the backup (and thus this requirement).\n\tBy default additional mount points besides the Root Disk mount point are not included in backups. For volume mount points you can set the Backup option to include the mount point in the backup. Device and bind mounts are never backed up as their content is managed outside the Proxmox VE storage library.\n16.1.1. VM Backup Fleecing\n\nWhen a backup for a VM is started, QEMU will install a \"copy-before-write\" filter in its block layer. This filter ensures that upon new guest writes, old data still needed for the backup is sent to the backup target first. The guest write blocks until this operation is finished so guest IO to not-yet-backed-up sectors will be limited by the speed of the backup target.\n\nWith backup fleecing, such old data is cached in a fleecing image rather than sent directly to the backup target. This can help guest IO performance and even prevent hangs in certain scenarios, at the cost of requiring more storage space. Use e.g. vzdump 123 --fleecing enabled=1,storage=local-lvm to enable backup fleecing, with fleecing images created on the storage local-lvm.\n\nThe fleecing storage should be a fast local storage, with thin provisioning and discard support. Examples are LVM-thin, RBD, ZFS with sparse 1 in the storage configuration, many file-based storages. Ideally, the fleecing storage is a dedicated storage, so it running full will not affect other guests and just fail the backup. Parts of the fleecing image that have been backed up will be discarded to try and keep the space usage low.\n\nFor file-based storages that do not support discard (e.g. NFS before version 4.2), you should set preallocation off in the storage configuration. In combination with qcow2 (used automatically as the format for the fleecing image when the storage supports it), this has the advantage that already allocated parts of the image can be re-used later, which can still help save quite a bit of space.\n\n\tOn a storage that’s not thinly provisioned, e.g. LVM or ZFS without the sparse option, the full size of the original disk needs to be reserved for the fleecing image up-front. On a thinly provisioned storage, the fleecing image can grow to the same size as the original image only if the guest re-writes a whole disk while the backup is busy with another disk.\n16.2. Backup File Names\n\nNewer versions of vzdump encode the guest type and the backup time into the filename, for example\n\nvzdump-lxc-105-2009_10_09-11_04_43.tar\n\nThat way it is possible to store several backup in the same directory. You can limit the number of backups that are kept with various retention options, see the Backup Retention section below.\n\n16.3. Backup File Compression\n\nThe backup file can be compressed with one of the following algorithms: lzo [54], gzip [55] or zstd [56].\n\nCurrently, Zstandard (zstd) is the fastest of these three algorithms. Multi-threading is another advantage of zstd over lzo and gzip. Lzo and gzip are more widely used and often installed by default.\n\nYou can install pigz [57] as a drop-in replacement for gzip to provide better performance due to multi-threading. For pigz & zstd, the amount of threads/cores can be adjusted. See the configuration options below.\n\nThe extension of the backup file name can usually be used to determine which compression algorithm has been used to create the backup.\n\n.zst\n\n\t\n\nZstandard (zstd) compression\n\n\n\n\n.gz or .tgz\n\n\t\n\ngzip compression\n\n\n\n\n.lzo\n\n\t\n\nlzo compression\n\nIf the backup file name doesn’t end with one of the above file extensions, then it was not compressed by vzdump.\n\n16.4. Backup Encryption\n\nFor Proxmox Backup Server storages, you can optionally set up client-side encryption of backups, see the corresponding section.\n\n16.5. Backup Jobs\n\nBesides triggering a backup manually, you can also setup periodic jobs that backup all, or a selection of virtual guest to a storage. You can manage the jobs in the UI under Datacenter → Backup or via the /cluster/backup API endpoint. Both will generate job entries in /etc/pve/jobs.cfg, which are parsed and executed by the pvescheduler daemon.\n\nA job is either configured for all cluster nodes or a specific node, and is executed according to a given schedule. The format for the schedule is very similar to systemd calendar events, see the calendar events section for details. The Schedule field in the UI can be freely edited, and it contains several examples that can be used as a starting point in its drop-down list.\n\nYou can configure job-specific retention options overriding those from the storage or node configuration, as well as a template for notes for additional information to be saved together with the backup.\n\nSince scheduled backups miss their execution when the host was offline or the pvescheduler was disabled during the scheduled time, it is possible to configure the behaviour for catching up. By enabling the Repeat missed option (in the Advanced tab in the UI, repeat-missed in the config), you can tell the scheduler that it should run missed jobs as soon as possible.\n\nThere are a few settings for tuning backup performance (some of which are exposed in the Advanced tab in the UI). The most notable is bwlimit for limiting IO bandwidth. The amount of threads used for the compressor can be controlled with the pigz (replacing gzip), respectively, zstd setting. Furthermore, there are ionice (when the BFQ scheduler is used) and, as part of the performance setting, max-workers (affects VM backups only) and pbs-entries-max (affects container backups only). See the configuration options for details.\n\n16.6. Backup Retention\n\nWith the prune-backups option you can specify which backups you want to keep in a flexible manner.\n\nThe following retention options are available:\n\nkeep-all <boolean>\n\nKeep all backups. If this is true, no other options can be set.\n\nkeep-last <N>\n\nKeep the last <N> backups.\n\nkeep-hourly <N>\n\nKeep backups for the last <N> hours. If there is more than one backup for a single hour, only the latest is kept.\n\nkeep-daily <N>\n\nKeep backups for the last <N> days. If there is more than one backup for a single day, only the latest is kept.\n\nkeep-weekly <N>\n\nKeep backups for the last <N> weeks. If there is more than one backup for a single week, only the latest is kept.\n\n\tWeeks start on Monday and end on Sunday. The software uses the ISO week date-system and handles weeks at the end of the year correctly.\nkeep-monthly <N>\n\nKeep backups for the last <N> months. If there is more than one backup for a single month, only the latest is kept.\n\nkeep-yearly <N>\n\nKeep backups for the last <N> years. If there is more than one backup for a single year, only the latest is kept.\n\nThe retention options are processed in the order given above. Each option only covers backups within its time period. The next option does not take care of already covered backups. It will only consider older backups.\n\nSpecify the retention options you want to use as a comma-separated list, for example:\n\n# vzdump 777 --prune-backups keep-last=3,keep-daily=13,keep-yearly=9\n\nWhile you can pass prune-backups directly to vzdump, it is often more sensible to configure the setting on the storage level, which can be done via the web interface.\n\n\tThe old maxfiles option is deprecated and should be replaced either by keep-last or, in case maxfiles was 0 for unlimited retention, by keep-all.\n16.6.1. Prune Simulator\n\nYou can use the prune simulator of the Proxmox Backup Server documentation to explore the effect of different retention options with various backup schedules.\n\n16.6.2. Retention Settings Example\n\nThe backup frequency and retention of old backups may depend on how often data changes, and how important an older state may be, in a specific work load. When backups act as a company’s document archive, there may also be legal requirements for how long backups must be kept.\n\nFor this example, we assume that you are doing daily backups, have a retention period of 10 years, and the period between backups stored gradually grows.\n\nkeep-last=3 - even if only daily backups are taken, an admin may want to create an extra one just before or after a big upgrade. Setting keep-last ensures this.\n\nkeep-hourly is not set - for daily backups this is not relevant. You cover extra manual backups already, with keep-last.\n\nkeep-daily=13 - together with keep-last, which covers at least one day, this ensures that you have at least two weeks of backups.\n\nkeep-weekly=8 - ensures that you have at least two full months of weekly backups.\n\nkeep-monthly=11 - together with the previous keep settings, this ensures that you have at least a year of monthly backups.\n\nkeep-yearly=9 - this is for the long term archive. As you covered the current year with the previous options, you would set this to nine for the remaining ones, giving you a total of at least 10 years of coverage.\n\nWe recommend that you use a higher retention period than is minimally required by your environment; you can always reduce it if you find it is unnecessarily high, but you cannot recreate backups once they have been removed.\n\n16.7. Backup Protection\n\nYou can mark a backup as protected to prevent its removal. Attempting to remove a protected backup via Proxmox VE’s UI, CLI or API will fail. However, this is enforced by Proxmox VE and not the file-system, that means that a manual removal of a backup file itself is still possible for anyone with write access to the underlying backup storage.\n\n\tProtected backups are ignored by pruning and do not count towards the retention settings.\n\nFor filesystem-based storages, the protection is implemented via a sentinel file <backup-name>.protected. For Proxmox Backup Server, it is handled on the server side (available since Proxmox Backup Server version 2.1).\n\nUse the storage option max-protected-backups to control how many protected backups per guest are allowed on the storage. Use -1 for unlimited. The default is unlimited for users with Datastore.Allocate privilege and 5 for other users.\n\n16.8. Backup Notes\n\nYou can add notes to backups using the Edit Notes button in the UI or via the storage content API.\n\nIt is also possible to specify a template for generating notes dynamically for a backup job and for manual backup. The template string can contain variables, surrounded by two curly braces, which will be replaced by the corresponding value when the backup is executed.\n\nCurrently supported are:\n\n{{cluster}} the cluster name, if any\n\n{{guestname}} the virtual guest’s assigned name\n\n{{node}} the host name of the node the backup is being created\n\n{{vmid}} the numerical VMID of the guest\n\nWhen specified via API or CLI, it needs to be a single line, where newline and backslash need to be escaped as literal \\n and \\\\ respectively.\n\n16.9. Restore\n\nA backup archive can be restored through the Proxmox VE web GUI or through the following CLI tools:\n\npct restore\n\nContainer restore utility\n\nqmrestore\n\nVirtual Machine restore utility\n\nFor details see the corresponding manual pages.\n\n16.9.1. Bandwidth Limit\n\nRestoring one or more big backups may need a lot of resources, especially storage bandwidth for both reading from the backup storage and writing to the target storage. This can negatively affect other virtual guests as access to storage can get congested.\n\nTo avoid this you can set bandwidth limits for a backup job. Proxmox VE implements two kinds of limits for restoring and archive:\n\nper-restore limit: denotes the maximal amount of bandwidth for reading from a backup archive\n\nper-storage write limit: denotes the maximal amount of bandwidth used for writing to a specific storage\n\nThe read limit indirectly affects the write limit, as we cannot write more than we read. A smaller per-job limit will overwrite a bigger per-storage limit. A bigger per-job limit will only overwrite the per-storage limit if you have ‘Data.Allocate’ permissions on the affected storage.\n\nYou can use the ‘--bwlimit <integer>` option from the restore CLI commands to set up a restore job specific bandwidth limit. KiB/s is used as unit for the limit, this means passing `10240’ will limit the read speed of the backup to 10 MiB/s, ensuring that the rest of the possible storage bandwidth is available for the already running virtual guests, and thus the backup does not impact their operations.\n\n\tYou can use ‘0` for the bwlimit parameter to disable all limits for a specific restore job. This can be helpful if you need to restore a very important virtual guest as fast as possible. (Needs `Data.Allocate’ permissions on storage)\n\nMost times your storage’s generally available bandwidth stays the same over time, thus we implemented the possibility to set a default bandwidth limit per configured storage, this can be done with:\n\n# pvesm set STORAGEID --bwlimit restore=KIBs\n16.9.2. Live-Restore\n\nRestoring a large backup can take a long time, in which a guest is still unavailable. For VM backups stored on a Proxmox Backup Server, this wait time can be mitigated using the live-restore option.\n\nEnabling live-restore via either the checkbox in the GUI or the --live-restore argument of qmrestore causes the VM to start as soon as the restore begins. Data is copied in the background, prioritizing chunks that the VM is actively accessing.\n\nNote that this comes with two caveats:\n\nDuring live-restore, the VM will operate with limited disk read speeds, as data has to be loaded from the backup server (once loaded, it is immediately available on the destination storage however, so accessing data twice only incurs the penalty the first time). Write speeds are largely unaffected.\n\nIf the live-restore fails for any reason, the VM will be left in an undefined state - that is, not all data might have been copied from the backup, and it is most likely not possible to keep any data that was written during the failed restore operation.\n\nThis mode of operation is especially useful for large VMs, where only a small amount of data is required for initial operation, e.g. web servers - once the OS and necessary services have been started, the VM is operational, while the background task continues copying seldom used data.\n\n16.9.3. Single File Restore\n\nThe File Restore button in the Backups tab of the storage GUI can be used to open a file browser directly on the data contained in a backup. This feature is only available for backups on a Proxmox Backup Server.\n\nFor containers, the first layer of the file tree shows all included pxar archives, which can be opened and browsed freely. For VMs, the first layer shows contained drive images, which can be opened to reveal a list of supported storage technologies found on the drive. In the most basic case, this will be an entry called part, representing a partition table, which contains entries for each partition found on the drive. Note that for VMs, not all data might be accessible (unsupported guest file systems, storage technologies, etc…).\n\nFiles and directories can be downloaded using the Download button, the latter being compressed into a zip archive on the fly.\n\nTo enable secure access to VM images, which might contain untrusted data, a temporary VM (not visible as a guest) is started. This does not mean that data downloaded from such an archive is inherently safe, but it avoids exposing the hypervisor system to danger. The VM will stop itself after a timeout. This entire process happens transparently from a user’s point of view.\n\n\tFor troubleshooting purposes, each temporary VM instance generates a log file in /var/log/proxmox-backup/file-restore/. The log file might contain additional information in case an attempt to restore individual files or accessing file systems contained in a backup archive fails.\n16.10. Configuration\n\nGlobal configuration is stored in /etc/vzdump.conf. The file uses a simple colon separated key/value format. Each line has the following format:\n\nOPTION: value\n\nBlank lines in the file are ignored, and lines starting with a # character are treated as comments and are also ignored. Values from this file are used as default, and can be overwritten on the command line.\n\nWe currently support the following options:\n\nbwlimit: <integer> (0 - N) (default = 0)\n\nLimit I/O bandwidth (in KiB/s).\n\ncompress: <0 | 1 | gzip | lzo | zstd> (default = 0)\n\nCompress dump file.\n\ndumpdir: <string>\n\nStore resulting files to specified directory.\n\nexclude-path: <array>\n\nExclude certain files/directories (shell globs). Paths starting with / are anchored to the container’s root, other paths match relative to each subdirectory.\n\nfleecing: [[enabled=]<1|0>] [,storage=<storage ID>]\n\nOptions for backup fleecing (VM only).\n\nenabled=<boolean> (default = 0)\n\nEnable backup fleecing. Cache backup data from blocks where new guest writes happen on specified storage instead of copying them directly to the backup target. This can help guest IO performance and even prevent hangs, at the cost of requiring more storage space.\n\nstorage=<storage ID>\n\nUse this storage to storage fleecing images. For efficient space usage, it’s best to use a local storage that supports discard and either thin provisioning or sparse files.\n\nionice: <integer> (0 - 8) (default = 7)\n\nSet IO priority when using the BFQ scheduler. For snapshot and suspend mode backups of VMs, this only affects the compressor. A value of 8 means the idle priority is used, otherwise the best-effort priority is used with the specified value.\n\nlockwait: <integer> (0 - N) (default = 180)\n\nMaximal time to wait for the global lock (minutes).\n\nmailnotification: <always | failure> (default = always)\n\nDeprecated: use notification targets/matchers instead. Specify when to send a notification mail\n\nmailto: <string>\n\nDeprecated: Use notification targets/matchers instead. Comma-separated list of email addresses or users that should receive email notifications.\n\nmaxfiles: <integer> (1 - N)\n\nDeprecated: use prune-backups instead. Maximal number of backup files per guest system.\n\nmode: <snapshot | stop | suspend> (default = snapshot)\n\nBackup mode.\n\nnotes-template: <string>\n\nTemplate string for generating notes for the backup(s). It can contain variables which will be replaced by their values. Currently supported are {\\{\\cluster}}, {\\{\\guestname}}, {\\{\\node}}, and {\\{\\vmid}}, but more might be added in the future. Needs to be a single line, newline and backslash need to be escaped as \\n and \\\\ respectively.\n\n\tRequires option(s): storage\nnotification-mode: <auto | legacy-sendmail | notification-system> (default = auto)\n\nDetermine which notification system to use. If set to legacy-sendmail, vzdump will consider the mailto/mailnotification parameters and send emails to the specified address(es) via the sendmail command. If set to notification-system, a notification will be sent via PVE’s notification system, and the mailto and mailnotification will be ignored. If set to auto (default setting), an email will be sent if mailto is set, and the notification system will be used if not.\n\nnotification-policy: <always | failure | never> (default = always)\n\nDeprecated: Do not use\n\nnotification-target: <string>\n\nDeprecated: Do not use\n\nperformance: [max-workers=<integer>] [,pbs-entries-max=<integer>]\n\nOther performance-related settings.\n\nmax-workers=<integer> (1 - 256) (default = 16)\n\nApplies to VMs. Allow up to this many IO workers at the same time.\n\npbs-entries-max=<integer> (1 - N) (default = 1048576)\n\nApplies to container backups sent to PBS. Limits the number of entries allowed in memory at a given time to avoid unintended OOM situations. Increase it to enable backups of containers with a large amount of files.\n\npigz: <integer> (default = 0)\n\nUse pigz instead of gzip when N>0. N=1 uses half of cores, N>1 uses N as thread count.\n\npool: <string>\n\nBackup all known guest systems included in the specified pool.\n\nprotected: <boolean>\n\nIf true, mark backup(s) as protected.\n\n\tRequires option(s): storage\nprune-backups: [keep-all=<1|0>] [,keep-daily=<N>] [,keep-hourly=<N>] [,keep-last=<N>] [,keep-monthly=<N>] [,keep-weekly=<N>] [,keep-yearly=<N>] (default = keep-all=1)\n\nUse these retention options instead of those from the storage configuration.\n\nkeep-all=<boolean>\n\nKeep all backups. Conflicts with the other options when true.\n\nkeep-daily=<N>\n\nKeep backups for the last <N> different days. If there is morethan one backup for a single day, only the latest one is kept.\n\nkeep-hourly=<N>\n\nKeep backups for the last <N> different hours. If there is morethan one backup for a single hour, only the latest one is kept.\n\nkeep-last=<N>\n\nKeep the last <N> backups.\n\nkeep-monthly=<N>\n\nKeep backups for the last <N> different months. If there is morethan one backup for a single month, only the latest one is kept.\n\nkeep-weekly=<N>\n\nKeep backups for the last <N> different weeks. If there is morethan one backup for a single week, only the latest one is kept.\n\nkeep-yearly=<N>\n\nKeep backups for the last <N> different years. If there is morethan one backup for a single year, only the latest one is kept.\n\nremove: <boolean> (default = 1)\n\nPrune older backups according to prune-backups.\n\nscript: <string>\n\nUse specified hook script.\n\nstdexcludes: <boolean> (default = 1)\n\nExclude temporary files and logs.\n\nstopwait: <integer> (0 - N) (default = 10)\n\nMaximal time to wait until a guest system is stopped (minutes).\n\nstorage: <storage ID>\n\nStore resulting file to this storage.\n\ntmpdir: <string>\n\nStore temporary files to specified directory.\n\nzstd: <integer> (default = 1)\n\nZstd threads. N=0 uses half of the available cores, if N is set to a value bigger than 0, N is used as thread count.\n\nExample vzdump.conf Configuration\ntmpdir: /mnt/fast_local_disk\nstorage: my_backup_storage\nmode: snapshot\nbwlimit: 10000\n16.11. Hook Scripts\n\nYou can specify a hook script with option --script. This script is called at various phases of the backup process, with parameters accordingly set. You can find an example in the documentation directory (vzdump-hook-script.pl).\n\n16.12. File Exclusions\n\tthis option is only available for container backups.\n\nvzdump skips the following files by default (disable with the option --stdexcludes 0)\n\n/tmp/?*\n/var/tmp/?*\n/var/run/?*pid\n\nYou can also manually specify (additional) exclude paths, for example:\n\n# vzdump 777 --exclude-path /tmp/ --exclude-path '/var/foo*'\n\nexcludes the directory /tmp/ and any file or directory named /var/foo, /var/foobar, and so on.\n\nPaths that do not start with a / are not anchored to the container’s root, but will match relative to any subdirectory. For example:\n\n# vzdump 777 --exclude-path bar\n\nexcludes any file or directory named /bar, /var/bar, /var/foo/bar, and so on, but not /bar2.\n\nConfiguration files are also stored inside the backup archive (in ./etc/vzdump/) and will be correctly restored.\n\n16.13. Examples\n\nSimply dump guest 777 - no snapshot, just archive the guest private area and configuration files to the default dump directory (usually /var/lib/vz/dump/).\n\n# vzdump 777\n\nUse rsync and suspend/resume to create a snapshot (minimal downtime).\n\n# vzdump 777 --mode suspend\n\nBackup all guest systems and send notification mails to root and admin. Due to mailto being set and notification-mode being set to auto by default, the notification mails are sent via the system’s sendmail command instead of the notification system.\n\n# vzdump --all --mode suspend --mailto root --mailto admin\n\nUse snapshot mode (no downtime) and non-default dump directory.\n\n# vzdump 777 --dumpdir /mnt/backup --mode snapshot\n\nBackup more than one guest (selectively)\n\n# vzdump 101 102 103 --mailto root\n\nBackup all guests excluding 101 and 102\n\n# vzdump --mode suspend --exclude 101,102\n\nRestore a container to a new CT 600\n\n# pct restore 600 /mnt/backup/vzdump-lxc-777.tar\n\nRestore a QemuServer VM to VM 601\n\n# qmrestore /mnt/backup/vzdump-qemu-888.vma 601\n\nClone an existing container 101 to a new container 300 with a 4GB root file system, using pipes\n\n# vzdump 101 --stdout | pct restore --rootfs 4 300 -\n17. Notifications\n17.1. Overview\n\nProxmox VE will send notifications if case of noteworthy events in the system.\n\nThere are a number of different notification events, each with their own set of metadata fields that can be used in notification matchers.\n\nA notification matcher determines which notifications shall be sent where. A matcher has match rules, that can be used to match on certain notification properties (e.g. timestamp, severity, metadata fields). If a matcher matches a notification, the notification will be routed to a configurable set of notification targets.\n\nA notification target is an abstraction for a destination where a notification should be sent to - for instance, a Gotify server instance, or a set of email addresses. There are multiple types of notification targets, including sendmail, which uses the system’s sendmail command to send emails, or gotify, which sends a notification to a Gotify instance.\n\nThe notification system can be configured in the GUI under Datacenter → Notifications. The configuration is stored in /etc/pve/notifications.cfg and /etc/pve/priv/notifications.cfg - the latter contains sensitive configuration options such as passwords or authentication tokens for notification targets.\n\n17.2. Notification Targets\n17.2.1. Sendmail\n\nThe sendmail binary is a program commonly found on Unix-like operating systems that handles the sending of email messages. It is a command-line utility that allows users and applications to send emails directly from the command line or from within scripts.\n\nThe sendmail notification target uses the sendmail binary to send emails.\n\n\tIn standard Proxmox VE installations, the sendmail binary is provided by Postfix. For this type of target to work correctly, it might be necessary to change Postfix’s configuration so that it can correctly deliver emails. For cluster setups it is necessary to have a working Postfix configuration on every single cluster node.\n\nThe configuration for Sendmail target plugins has the following options:\n\nmailto: E-Mail address to which the notification shall be sent to. Can be set multiple times to accomodate multiple recipients.\n\nmailto-user: Users to which emails shall be sent to. The user’s email address will be looked up in users.cfg. Can be set multiple times to accomodate multiple recipients.\n\nauthor: Sets the author of the E-Mail. Defaults to Proxmox VE.\n\nfrom-address: Sets the from address of the E-Mail. If the parameter is not set, the plugin will fall back to the email_from setting from datacenter.cfg. If that is also not set, the plugin will default to root@$hostname, where $hostname is the hostname of the node.\n\ncomment: Comment for this target The From header in the email will be set to $author <$from-address>.\n\nExample configuration (/etc/pve/notifications.cfg):\n\nsendmail: example\n        mailto-user root@pam\n        mailto-user admin@pve\n        mailto max@example.com\n        from-address pve1@example.com\n        comment Send to multiple users/addresses\n17.2.2. SMTP\n\nSMTP notification targets can send emails directly to an SMTP mail relay.\n\nThe configuration for SMTP target plugins has the following options:\n\nmailto: E-Mail address to which the notification shall be sent to. Can be set multiple times to accomodate multiple recipients.\n\nmailto-user: Users to which emails shall be sent to. The user’s email address will be looked up in users.cfg. Can be set multiple times to accomodate multiple recipients.\n\nauthor: Sets the author of the E-Mail. Defaults to Proxmox VE.\n\nfrom-address: Sets the From-addresss of the email. SMTP relays might require that this address is owned by the user in order to avoid spoofing. The From header in the email will be set to $author <$from-address>.\n\nusername: Username to use during authentication. If no username is set, no authentication will be performed. The PLAIN and LOGIN authentication methods are supported.\n\npassword: Password to use when authenticating.\n\nmode: Sets the encryption mode (insecure, starttls or tls). Defaults to tls.\n\nserver: Address/IP of the SMTP relay\n\nport: The port to connect to. If not set, the used port defaults to 25 (insecure), 465 (tls) or 587 (starttls), depending on the value of mode.\n\ncomment: Comment for this target\n\nExample configuration (/etc/pve/notifications.cfg):\n\nsmtp: example\n        mailto-user root@pam\n        mailto-user admin@pve\n        mailto max@example.com\n        from-address pve1@example.com\n        username pve1\n        server mail.example.com\n        mode starttls\n\nThe matching entry in /etc/pve/priv/notifications.cfg, containing the secret token:\n\nsmtp: example\n        password somepassword\n17.2.3. Gotify\n\nGotify is an open-source self-hosted notification server that allows you to send and receive push notifications to various devices and applications. It provides a simple API and web interface, making it easy to integrate with different platforms and services.\n\nThe configuration for Gotify target plugins has the following options:\n\nserver: The base URL of the Gotify server, e.g. http://<ip>:8888\n\ntoken: The authentication token. Tokens can be generated within the Gotify web interface.\n\ncomment: Comment for this target\n\n\tThe Gotify target plugin will respect the HTTP proxy settings from the datacenter configuration\n\nExample configuration (/etc/pve/notifications.cfg):\n\ngotify: example\n        server http://gotify.example.com:8888\n        comment Send to multiple users/addresses\n\nThe matching entry in /etc/pve/priv/notifications.cfg, containing the secret token:\n\ngotify: example\n        token somesecrettoken\n17.3. Notification Matchers\n\nNotification matchers route notifications to notification targets based on their matching rules. These rules can match certain properties of a notification, such as the timestamp (match-calendar), the severity of the notification (match-severity) or metadata fields (match-field). If a notification is matched by a matcher, all targets configured for the matcher will receive the notification.\n\nAn arbitrary number of matchers can be created, each with with their own matching rules and targets to notify. Every target is notified at most once for every notification, even if the target is used in multiple matchers.\n\nA matcher without any matching rules is always true; the configured targets will always be notified.\n\nmatcher: always-matches\n        target admin\n        comment This matcher always matches\n17.3.1. Matcher Options\n\ntarget: Determine which target should be notified if the matcher matches. can be used multiple times to notify multiple targets.\n\ninvert-match: Inverts the result of the whole matcher\n\nmode: Determines how the individual match rules are evaluated to compute the result for the whole matcher. If set to all, all matching rules must match. If set to any, at least one rule must match. a matcher must be true. Defaults to all.\n\nmatch-calendar: Match the notification’s timestamp against a schedule\n\nmatch-field: Match the notification’s metadata fields\n\nmatch-severity: Match the notification’s severity\n\ncomment: Comment for this matcher\n\n17.3.2. Calendar Matching Rules\n\nA calendar matcher matches the time when a notification is sent agaist a configurable schedule.\n\nmatch-calendar 8-12\n\nmatch-calendar 8:00-15:30\n\nmatch-calendar mon-fri 9:00-17:00\n\nmatch-calendar sun,tue-wed,fri 9-17\n\n17.3.3. Field Matching Rules\n\nNotifications have a selection of metadata fields that can be matched.\n\nmatch-field exact:type=vzdump Only match notifications about backups.\n\nmatch-field regex:hostname=^.+\\.example\\.com$ Match the hostname of the node.\n\nIf a matched metadata field does not exist, the notification will not be matched. For instance, a match-field regex:hostname=.* directive will only match notifications that have an arbitraty hostname metadata field, but will not match if the field does not exist.\n\n17.3.4. Severity Matching Rules\n\nA notification has a associated severity that can be matched.\n\nmatch-severity error: Only match errors\n\nmatch-severity warning,error: Match warnings and error\n\nThe following severities are in use: info, notice, warning, error, unknown.\n\n17.3.5. Examples\nmatcher: workday\n        match-calendar mon-fri 9-17\n        target admin\n        comment Notify admins during working hours\n\nmatcher: night-and-weekend\n        match-calendar mon-fri 9-17\n        invert-match true\n        target on-call-admins\n        comment Separate target for non-working hours\nmatcher: backup-failures\n        match-field exact:type=vzdump\n        match-severity error\n        target backup-admins\n        comment Send notifications about backup failures to one group of admins\n\nmatcher: cluster-failures\n        match-field exact:type=replication\n        match-field exact:type=fencing\n        mode any\n        target cluster-admins\n        comment Send cluster-related notifications to other group of admins\n\nThe last matcher could also be rewritten using a field matcher with a regular expression:\n\nmatcher: cluster-failures\n        match-field regex:type=^(replication|fencing)$\n        target cluster-admins\n        comment Send cluster-related notifications to other group of admins\n17.4. Notification Events\nEvent\ttype\tSeverity\tMetadata fields (in addition to type)\n\n\nSystem updates available\n\n\t\n\npackage-updates\n\n\t\n\ninfo\n\n\t\n\nhostname\n\n\n\n\nCluster node fenced\n\n\t\n\nfencing\n\n\t\n\nerror\n\n\t\n\nhostname\n\n\n\n\nStorage replication failed\n\n\t\n\nreplication\n\n\t\n\nerror\n\n\t\n\n-\n\n\n\n\nBackup finished\n\n\t\n\nvzdump\n\n\t\n\ninfo (error on failure)\n\n\t\n\nhostname\n\n\n\n\nMail for root\n\n\t\n\nsystem-mail\n\n\t\n\nunknown\n\n\t\n\n-\n\nField name\tDescription\n\n\ntype\n\n\t\n\nType of the notifcation\n\n\n\n\nhostname\n\n\t\n\nHostname, including domain (e.g. pve1.example.com)\n\n17.5. System Mail Forwarding\n\nCertain local system daemons, such as smartd, generate notification emails that are initially directed to the local root user. Proxmox VE will feed these mails into the notification system as a notification of type system-mail and with severity unknown.\n\nWhen the forwarding process involves an email-based target (like sendmail or smtp), the email is forwarded exactly as received, with all original mail headers remaining intact. For all other targets, the system tries to extract both a subject line and the main text body from the email content. In instances where emails solely consist of HTML content, they will be transformed into plain text format during this process.\n\n17.6. Permissions\n\nIn order to modify/view the configuration for notification targets, the Mapping.Modify/Mapping.Audit permissions are required for the /mapping/notifications ACL node.\n\nTesting a target requires Mapping.Use, Mapping.Audit or Mapping.Modify permissions on /mapping/notifications\n\n18. Important Service Daemons\n18.1. pvedaemon - Proxmox VE API Daemon\n\nThis daemon exposes the whole Proxmox VE API on 127.0.0.1:85. It runs as root and has permission to do all privileged operations.\n\n\tThe daemon listens to a local address only, so you cannot access it from outside. The pveproxy daemon exposes the API to the outside world.\n18.2. pveproxy - Proxmox VE API Proxy Daemon\n\nThis daemon exposes the whole Proxmox VE API on TCP port 8006 using HTTPS. It runs as user www-data and has very limited permissions. Operation requiring more permissions are forwarded to the local pvedaemon.\n\nRequests targeted for other nodes are automatically forwarded to those nodes. This means that you can manage your whole cluster by connecting to a single Proxmox VE node.\n\n18.2.1. Host based Access Control\n\nIt is possible to configure “apache2”-like access control lists. Values are read from file /etc/default/pveproxy. For example:\n\nALLOW_FROM=\"10.0.0.1-10.0.0.5,192.168.0.0/22\"\nDENY_FROM=\"all\"\nPOLICY=\"allow\"\n\nIP addresses can be specified using any syntax understood by Net::IP. The name all is an alias for 0/0 and ::/0 (meaning all IPv4 and IPv6 addresses).\n\nThe default policy is allow.\n\nMatch\tPOLICY=deny\tPOLICY=allow\n\n\nMatch Allow only\n\n\t\n\nallow\n\n\t\n\nallow\n\n\n\n\nMatch Deny only\n\n\t\n\ndeny\n\n\t\n\ndeny\n\n\n\n\nNo match\n\n\t\n\ndeny\n\n\t\n\nallow\n\n\n\n\nMatch Both Allow & Deny\n\n\t\n\ndeny\n\n\t\n\nallow\n\n18.2.2. Listening IP Address\n\nBy default the pveproxy and spiceproxy daemons listen on the wildcard address and accept connections from both IPv4 and IPv6 clients.\n\nBy setting LISTEN_IP in /etc/default/pveproxy you can control to which IP address the pveproxy and spiceproxy daemons bind. The IP-address needs to be configured on the system.\n\nSetting the sysctl net.ipv6.bindv6only to the non-default 1 will cause the daemons to only accept connection from IPv6 clients, while usually also causing lots of other issues. If you set this configuration we recommend to either remove the sysctl setting, or set the LISTEN_IP to 0.0.0.0 (which will only allow IPv4 clients).\n\nLISTEN_IP can be used to only to restricting the socket to an internal interface and thus have less exposure to the public internet, for example:\n\nLISTEN_IP=\"192.0.2.1\"\n\nSimilarly, you can also set an IPv6 address:\n\nLISTEN_IP=\"2001:db8:85a3::1\"\n\nNote that if you want to specify a link-local IPv6 address, you need to provide the interface name itself. For example:\n\nLISTEN_IP=\"fe80::c463:8cff:feb9:6a4e%vmbr0\"\n\tThe nodes in a cluster need access to pveproxy for communication, possibly on different sub-nets. It is not recommended to set LISTEN_IP on clustered systems.\n\nTo apply the change you need to either reboot your node or fully restart the pveproxy and spiceproxy service:\n\nsystemctl restart pveproxy.service spiceproxy.service\n\tUnlike reload, a restart of the pveproxy service can interrupt some long-running worker processes, for example a running console or shell from a virtual guest. So, please use a maintenance window to bring this change in effect.\n18.2.3. SSL Cipher Suite\n\nYou can define the cipher list in /etc/default/pveproxy via the CIPHERS (TLS ⇐ 1.2) and CIPHERSUITES (TLS >= 1.3) keys. For example\n\nCIPHERS=\"ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256\"\nCIPHERSUITES=\"TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_128_GCM_SHA256\"\n\nAbove is the default. See the ciphers(1) man page from the openssl package for a list of all available options.\n\nAdditionally, you can set the client to choose the cipher used in /etc/default/pveproxy (default is the first cipher in the list available to both client and pveproxy):\n\nHONOR_CIPHER_ORDER=0\n18.2.4. Supported TLS versions\n\nThe insecure SSL versions 2 and 3 are unconditionally disabled for pveproxy. TLS versions below 1.1 are disabled by default on recent OpenSSL versions, which is honored by pveproxy (see /etc/ssl/openssl.cnf).\n\nTo disable TLS version 1.2 or 1.3, set the following in /etc/default/pveproxy:\n\nDISABLE_TLS_1_2=1\n\nor, respectively:\n\nDISABLE_TLS_1_3=1\n\tUnless there is a specific reason to do so, it is not recommended to manually adjust the supported TLS versions.\n18.2.5. Diffie-Hellman Parameters\n\nYou can define the used Diffie-Hellman parameters in /etc/default/pveproxy by setting DHPARAMS to the path of a file containing DH parameters in PEM format, for example\n\nDHPARAMS=\"/path/to/dhparams.pem\"\n\nIf this option is not set, the built-in skip2048 parameters will be used.\n\n\tDH parameters are only used if a cipher suite utilizing the DH key exchange algorithm is negotiated.\n18.2.6. Alternative HTTPS certificate\n\nYou can change the certificate used to an external one or to one obtained via ACME.\n\npveproxy uses /etc/pve/local/pveproxy-ssl.pem and /etc/pve/local/pveproxy-ssl.key, if present, and falls back to /etc/pve/local/pve-ssl.pem and /etc/pve/local/pve-ssl.key. The private key may not use a passphrase.\n\nIt is possible to override the location of the certificate private key /etc/pve/local/pveproxy-ssl.key by setting TLS_KEY_FILE in /etc/default/pveproxy, for example:\n\nTLS_KEY_FILE=\"/secrets/pveproxy.key\"\n\tThe included ACME integration does not honor this setting.\n\nSee the Host System Administration chapter of the documentation for details.\n\n18.2.7. Response Compression\n\nBy default pveproxy uses gzip HTTP-level compression for compressible content, if the client supports it. This can disabled in /etc/default/pveproxy\n\nCOMPRESSION=0\n18.3. pvestatd - Proxmox VE Status Daemon\n\nThis daemon queries the status of VMs, storages and containers at regular intervals. The result is sent to all nodes in the cluster.\n\n18.4. spiceproxy - SPICE Proxy Service\n\nSPICE (the Simple Protocol for Independent Computing Environments) is an open remote computing solution, providing client access to remote displays and devices (e.g. keyboard, mouse, audio). The main use case is to get remote access to virtual machines and container.\n\nThis daemon listens on TCP port 3128, and implements an HTTP proxy to forward CONNECT request from the SPICE client to the correct Proxmox VE VM. It runs as user www-data and has very limited permissions.\n\n18.4.1. Host based Access Control\n\nIt is possible to configure \"apache2\" like access control lists. Values are read from file /etc/default/pveproxy. See pveproxy documentation for details.\n\n18.5. pvescheduler - Proxmox VE Scheduler Daemon\n\nThis deamon is responsible for starting jobs according to the schedule, such as replication and vzdump jobs.\n\nFor vzdump jobs, it gets its configuration from the file /etc/pve/jobs.cfg\n\n19. Useful Command-line Tools\n19.1. pvesubscription - Subscription Management\n\nThis tool is used to handle Proxmox VE subscriptions.\n\n19.2. pveperf - Proxmox VE Benchmark Script\n\nTries to gather some CPU/hard disk performance data on the hard disk mounted at PATH (/ is used as default):\n\nCPU BOGOMIPS\n\nbogomips sum of all CPUs\n\nREGEX/SECOND\n\nregular expressions per second (perl performance test), should be above 300000\n\nHD SIZE\n\nhard disk size\n\nBUFFERED READS\n\nsimple HD read test. Modern HDs should reach at least 40 MB/sec\n\nAVERAGE SEEK TIME\n\ntests average seek time. Fast SCSI HDs reach values < 8 milliseconds. Common IDE/SATA disks get values from 15 to 20 ms.\n\nFSYNCS/SECOND\n\nvalue should be greater than 200 (you should enable write back cache mode on you RAID controller - needs a battery backed cache (BBWC)).\n\nDNS EXT\n\naverage time to resolve an external DNS name\n\nDNS INT\n\naverage time to resolve a local DNS name\n\n19.3. Shell interface for the Proxmox VE API\n\nThe Proxmox VE management tool (pvesh) allows to directly invoke API function, without using the REST/HTTPS server.\n\n\tOnly root is allowed to do that.\n19.3.1. EXAMPLES\n\nGet the list of nodes in my cluster\n\n# pvesh get /nodes\n\nGet a list of available options for the datacenter\n\n# pvesh usage cluster/options -v\n\nSet the HTMl5 NoVNC console as the default console for the datacenter\n\n# pvesh set cluster/options -console html5\n20. Frequently Asked Questions\n\tNew FAQs are appended to the bottom of this section.\n\nWhat distribution is Proxmox VE based on?\n\nProxmox VE is based on Debian GNU/Linux\n\nWhat license does the Proxmox VE project use?\n\nProxmox VE code is licensed under the GNU Affero General Public License, version 3.\n\nWill Proxmox VE run on a 32bit processor?\n\nProxmox VE works only on 64-bit CPUs (AMD or Intel). There is no plan for 32-bit for the platform.\n\n\tVMs and Containers can be both 32-bit and 64-bit.\n\nDoes my CPU support virtualization?\n\nTo check if your CPU is virtualization compatible, check for the vmx or svm tag in this command output:\n\negrep '(vmx|svm)' /proc/cpuinfo\n\nSupported Intel CPUs\n\n64-bit processors with Intel Virtualization Technology (Intel VT-x) support. (List of processors with Intel VT and 64-bit)\n\nSupported AMD CPUs\n\n64-bit processors with AMD Virtualization Technology (AMD-V) support.\n\nWhat is a container/virtual environment (VE)/virtual private server (VPS)?\n\nIn the context of containers, these terms all refer to the concept of operating-system-level virtualization. Operating-system-level virtualization is a method of virtualization, in which the kernel of an operating system allows for multiple isolated instances, that all share the kernel. When referring to LXC, we call such instances containers. Because containers use the host’s kernel rather than emulating a full operating system, they require less overhead, but are limited to Linux guests.\n\nWhat is a QEMU/KVM guest (or VM)?\n\nA QEMU/KVM guest (or VM) is a guest system running virtualized under Proxmox VE using QEMU and the Linux KVM kernel module.\n\nWhat is QEMU?\n\nQEMU is a generic and open source machine emulator and virtualizer. QEMU uses the Linux KVM kernel module to achieve near native performance by executing the guest code directly on the host CPU. It is not limited to Linux guests but allows arbitrary operating systems to run.\n\nHow long will my Proxmox VE version be supported?\n\nProxmox VE versions are supported at least as long as the corresponding Debian Version is oldstable. Proxmox VE uses a rolling release model and using the latest stable version is always recommended.\n\nProxmox VE Version\tDebian Version\tFirst Release\tDebian EOL\tProxmox EOL\n\n\nProxmox VE 8\n\n\t\n\nDebian 12 (Bookworm)\n\n\t\n\n2023-06\n\n\t\n\ntba\n\n\t\n\ntba\n\n\n\n\nProxmox VE 7\n\n\t\n\nDebian 11 (Bullseye)\n\n\t\n\n2021-07\n\n\t\n\n2024-07\n\n\t\n\n2024-07\n\n\n\n\nProxmox VE 6\n\n\t\n\nDebian 10 (Buster)\n\n\t\n\n2019-07\n\n\t\n\n2022-09\n\n\t\n\n2022-09\n\n\n\n\nProxmox VE 5\n\n\t\n\nDebian 9 (Stretch)\n\n\t\n\n2017-07\n\n\t\n\n2020-07\n\n\t\n\n2020-07\n\n\n\n\nProxmox VE 4\n\n\t\n\nDebian 8 (Jessie)\n\n\t\n\n2015-10\n\n\t\n\n2018-06\n\n\t\n\n2018-06\n\n\n\n\nProxmox VE 3\n\n\t\n\nDebian 7 (Wheezy)\n\n\t\n\n2013-05\n\n\t\n\n2016-04\n\n\t\n\n2017-02\n\n\n\n\nProxmox VE 2\n\n\t\n\nDebian 6 (Squeeze)\n\n\t\n\n2012-04\n\n\t\n\n2014-05\n\n\t\n\n2014-05\n\n\n\n\nProxmox VE 1\n\n\t\n\nDebian 5 (Lenny)\n\n\t\n\n2008-10\n\n\t\n\n2012-03\n\n\t\n\n2013-01\n\nHow can I upgrade Proxmox VE to the next point release?\n\nMinor version upgrades, for example upgrading from Proxmox VE in version 7.1 to 7.2 or 7.3, can be done just like any normal update. But you should still check the release notes for any relevant noteable, or breaking change.\n\nFor the update itself use either the Web UI Node → Updates panel or through the CLI with:\n\napt update\napt full-upgrade\n\tAlways ensure you correctly setup the package repositories and only continue with the actual upgrade if apt update did not hit any error.\n\nHow can I upgrade Proxmox VE to the next major release?\n\nMajor version upgrades, for example going from Proxmox VE 4.4 to 5.0, are also supported. They must be carefully planned and tested and should never be started without having a current backup ready.\n\nAlthough the specific upgrade steps depend on your respective setup, we provide general instructions and advice of how a upgrade should be performed:\n\nUpgrade from Proxmox VE 7 to 8\n\nUpgrade from Proxmox VE 6 to 7\n\nUpgrade from Proxmox VE 5 to 6\n\nUpgrade from Proxmox VE 4 to 5\n\nUpgrade from Proxmox VE 3 to 4\n\nLXC vs LXD vs Proxmox Containers vs Docker\n\nLXC is a userspace interface for the Linux kernel containment features. Through a powerful API and simple tools, it lets Linux users easily create and manage system containers. LXC, as well as the former OpenVZ, aims at system virtualization. Thus, it allows you to run a complete OS inside a container, where you log in using ssh, add users, run apache, etc…\n\nLXD is built on top of LXC to provide a new, better user experience. Under the hood, LXD uses LXC through liblxc and its Go binding to create and manage the containers. It’s basically an alternative to LXC’s tools and distribution template system with the added features that come from being controllable over the network.\n\nProxmox Containers are how we refer to containers that are created and managed using the Proxmox Container Toolkit (pct). They also target system virtualization and use LXC as the basis of the container offering. The Proxmox Container Toolkit (pct) is tightly coupled with Proxmox VE. This means that it is aware of cluster setups, and it can use the same network and storage resources as QEMU virtual machines (VMs). You can even use the Proxmox VE firewall, create and restore backups, or manage containers using the HA framework. Everything can be controlled over the network using the Proxmox VE API.\n\nDocker aims at running a single application in an isolated, self-contained environment. These are generally referred to as “Application Containers”, rather than “System Containers”. You manage a Docker instance from the host, using the Docker Engine command-line interface. It is not recommended to run docker directly on your Proxmox VE host.\n\n\tIf you want to run application containers, for example, Docker images, it is best to run them inside a Proxmox QEMU VM.\n21. Bibliography\nBooks about Proxmox VE\n\n[Ahmed16] Wasim Ahmed. Mastering Proxmox - Third Edition. Packt Publishing, 2017. ISBN 978-1788397605\n\n[Ahmed15] Wasim Ahmed. Proxmox Cookbook. Packt Publishing, 2015. ISBN 978-1783980901\n\n[Cheng14] Simon M.C. Cheng. Proxmox High Availability. Packt Publishing, 2014. ISBN 978-1783980888\n\n[Goldman16] Rik Goldman. Learning Proxmox VE. Packt Publishing, 2016. ISBN 978-1783981786\n\n[Surber16]] Lee R. Surber. Virtualization Complete: Business Basic Edition. Linux Solutions (LRS-TEK), 2016. ASIN B01BBVQZT6\n\nBooks about related technology\n\n[Hertzog13] Raphaël Hertzog, Roland Mas., Freexian SARL The Debian Administrator's Handbook: Debian Bullseye from Discovery to Mastery, Freexian, 2021. ISBN 979-10-91414-20-3\n\n[Bir96] Kenneth P. Birman. Building Secure and Reliable Network Applications. Manning Publications Co, 1996. ISBN 978-1884777295\n\n[Walsh10] Norman Walsh. DocBook 5: The Definitive Guide. O’Reilly & Associates, 2010. ISBN 978-0596805029\n\n[Richardson07] Leonard Richardson & Sam Ruby. RESTful Web Services. O’Reilly Media, 2007. ISBN 978-0596529260\n\n[Singh15] Karan Singh. Learning Ceph. Packt Publishing, 2015. ISBN 978-1783985623\n\n[Singh16] Karan Signh. Ceph Cookbook Packt Publishing, 2016. ISBN 978-1784393502\n\n[Mauerer08] Wolfgang Mauerer. Professional Linux Kernel Architecture. John Wiley & Sons, 2008. ISBN 978-0470343432\n\n[Loshin03] Pete Loshin, IPv6: Theory, Protocol, and Practice, 2nd Edition. Morgan Kaufmann, 2003. ISBN 978-1558608108\n\n[Loeliger12] Jon Loeliger & Matthew McCullough. Version Control with Git: Powerful tools and techniques for collaborative software development. O’Reilly and Associates, 2012. ISBN 978-1449316389\n\n[Kreibich10] Jay A. Kreibich. Using SQLite, O’Reilly and Associates, 2010. ISBN 978-0596521189\n\nBooks about related topics\n\n[Bessen09] James Bessen & Michael J. Meurer, Patent Failure: How Judges, Bureaucrats, and Lawyers Put Innovators at Risk. Princeton Univ Press, 2009. ISBN 978-0691143217\n\n22. Appendix A: Command-line Interface\n22.1. Output format options [FORMAT_OPTIONS]\n\nIt is possible to specify the output format using the --output-format parameter. The default format text uses ASCII-art to draw nice borders around tables. It additionally transforms some values into human-readable text, for example:\n\nUnix epoch is displayed as ISO 8601 date string.\n\nDurations are displayed as week/day/hour/minute/second count, i.e 1d 5h.\n\nByte sizes value include units (B, KiB, MiB, GiB, TiB, PiB).\n\nFractions are display as percentage, i.e. 1.0 is displayed as 100%.\n\nYou can also completely suppress output using option --quiet.\n\n--human-readable <boolean> (default = 1)\n\nCall output rendering functions to produce human readable text.\n\n--noborder <boolean> (default = 0)\n\nDo not draw borders (for text format).\n\n--noheader <boolean> (default = 0)\n\nDo not show column headers (for text format).\n\n--output-format <json | json-pretty | text | yaml> (default = text)\n\nOutput format.\n\n--quiet <boolean>\n\nSuppress printing results.\n\n22.2. pvesm - Proxmox VE Storage Manager\n\npvesm <COMMAND> [ARGS] [OPTIONS]\n\npvesm add <type> <storage> [OPTIONS]\n\nCreate a new storage.\n\n<type>: <btrfs | cephfs | cifs | dir | esxi | glusterfs | iscsi | iscsidirect | lvm | lvmthin | nfs | pbs | rbd | zfs | zfspool>\n\nStorage type.\n\n<storage>: <storage ID>\n\nThe storage identifier.\n\n--authsupported <string>\n\nAuthsupported.\n\n--base <string>\n\nBase volume. This volume is automatically activated.\n\n--blocksize <string>\n\nblock size\n\n--bwlimit [clone=<LIMIT>] [,default=<LIMIT>] [,migration=<LIMIT>] [,move=<LIMIT>] [,restore=<LIMIT>]\n\nSet I/O bandwidth limit for various operations (in KiB/s).\n\n--comstar_hg <string>\n\nhost group for comstar views\n\n--comstar_tg <string>\n\ntarget group for comstar views\n\n--content <string>\n\nAllowed content types.\n\n\tthe value rootdir is used for Containers, and value images for VMs.\n--content-dirs <string>\n\nOverrides for default content type directories.\n\n--create-base-path <boolean> (default = yes)\n\nCreate the base directory if it doesn’t exist.\n\n--create-subdirs <boolean> (default = yes)\n\nPopulate the directory with the default structure.\n\n--data-pool <string>\n\nData Pool (for erasure coding only)\n\n--datastore <string>\n\nProxmox Backup Server datastore name.\n\n--disable <boolean>\n\nFlag to disable the storage.\n\n--domain <string>\n\nCIFS domain.\n\n--encryption-key a file containing an encryption key, or the special value \"autogen\"\n\nEncryption key. Use autogen to generate one automatically without passphrase.\n\n--export <string>\n\nNFS export path.\n\n--fingerprint ([A-Fa-f0-9]{2}:){31}[A-Fa-f0-9]{2}\n\nCertificate SHA 256 fingerprint.\n\n--format <string>\n\nDefault image format.\n\n--fs-name <string>\n\nThe Ceph filesystem name.\n\n--fuse <boolean>\n\nMount CephFS through FUSE.\n\n--is_mountpoint <string> (default = no)\n\nAssume the given path is an externally managed mountpoint and consider the storage offline if it is not mounted. Using a boolean (yes/no) value serves as a shortcut to using the target path in this field.\n\n--iscsiprovider <string>\n\niscsi provider\n\n--keyring file containing the keyring to authenticate in the Ceph cluster\n\nClient keyring contents (for external clusters).\n\n--krbd <boolean>\n\nAlways access rbd through krbd kernel module.\n\n--lio_tpg <string>\n\ntarget portal group for Linux LIO targets\n\n--master-pubkey a file containing a PEM-formatted master public key\n\nBase64-encoded, PEM-formatted public RSA key. Used to encrypt a copy of the encryption-key which will be added to each encrypted backup.\n\n--max-protected-backups <integer> (-1 - N) (default = Unlimited for users with Datastore.Allocate privilege, 5 for other users)\n\nMaximal number of protected backups per guest. Use -1 for unlimited.\n\n--maxfiles <integer> (0 - N)\n\nDeprecated: use prune-backups instead. Maximal number of backup files per VM. Use 0 for unlimited.\n\n--mkdir <boolean> (default = yes)\n\nCreate the directory if it doesn’t exist and populate it with default sub-dirs. NOTE: Deprecated, use the create-base-path and create-subdirs options instead.\n\n--monhost <string>\n\nIP addresses of monitors (for external clusters).\n\n--mountpoint <string>\n\nmount point\n\n--namespace <string>\n\nNamespace.\n\n--nocow <boolean> (default = 0)\n\nSet the NOCOW flag on files. Disables data checksumming and causes data errors to be unrecoverable from while allowing direct I/O. Only use this if data does not need to be any more safe than on a single ext4 formatted disk with no underlying raid system.\n\n--nodes <string>\n\nList of nodes for which the storage configuration applies.\n\n--nowritecache <boolean>\n\ndisable write caching on the target\n\n--options <string>\n\nNFS/CIFS mount options (see man nfs or man mount.cifs)\n\n--password <password>\n\nPassword for accessing the share/datastore.\n\n--path <string>\n\nFile system path.\n\n--pool <string>\n\nPool.\n\n--port <integer> (1 - 65535)\n\nFor non default port.\n\n--portal <string>\n\niSCSI portal (IP or DNS name with optional port).\n\n--preallocation <falloc | full | metadata | off> (default = metadata)\n\nPreallocation mode for raw and qcow2 images. Using metadata on raw images results in preallocation=off.\n\n--prune-backups [keep-all=<1|0>] [,keep-daily=<N>] [,keep-hourly=<N>] [,keep-last=<N>] [,keep-monthly=<N>] [,keep-weekly=<N>] [,keep-yearly=<N>]\n\nThe retention options with shorter intervals are processed first with --keep-last being the very first one. Each option covers a specific period of time. We say that backups within this period are covered by this option. The next option does not take care of already covered backups and only considers older backups.\n\n--saferemove <boolean>\n\nZero-out data when removing LVs.\n\n--saferemove_throughput <string>\n\nWipe throughput (cstream -t parameter value).\n\n--server <string>\n\nServer IP or DNS name.\n\n--server2 <string>\n\nBackup volfile server IP or DNS name.\n\n\tRequires option(s): server\n--share <string>\n\nCIFS share.\n\n--shared <boolean>\n\nIndicate that this is a single storage with the same contents on all nodes (or all listed in the nodes option). It will not make the contents of a local storage automatically accessible to other nodes, it just marks an already shared storage as such!\n\n--skip-cert-verification <boolean> (default = false)\n\nDisable TLS certificate verification, only enable on fully trusted networks!\n\n--smbversion <2.0 | 2.1 | 3 | 3.0 | 3.11 | default> (default = default)\n\nSMB protocol version. default if not set, negotiates the highest SMB2+ version supported by both the client and server.\n\n--sparse <boolean>\n\nuse sparse volumes\n\n--subdir <string>\n\nSubdir to mount.\n\n--tagged_only <boolean>\n\nOnly use logical volumes tagged with pve-vm-ID.\n\n--target <string>\n\niSCSI target.\n\n--thinpool <string>\n\nLVM thin pool LV name.\n\n--transport <rdma | tcp | unix>\n\nGluster transport: tcp or rdma\n\n--username <string>\n\nRBD Id.\n\n--vgname <string>\n\nVolume group name.\n\n--volume <string>\n\nGlusterfs Volume.\n\npvesm alloc <storage> <vmid> <filename> <size> [OPTIONS]\n\nAllocate disk images.\n\n<storage>: <storage ID>\n\nThe storage identifier.\n\n<vmid>: <integer> (100 - 999999999)\n\nSpecify owner VM\n\n<filename>: <string>\n\nThe name of the file to create.\n\n<size>: \\d+[MG]?\n\nSize in kilobyte (1024 bytes). Optional suffixes M (megabyte, 1024K) and G (gigabyte, 1024M)\n\n--format <qcow2 | raw | subvol>\n\nno description available\n\n\tRequires option(s): size\n\npvesm apiinfo\n\nReturns APIVER and APIAGE.\n\npvesm cifsscan\n\nAn alias for pvesm scan cifs.\n\npvesm export <volume> <format> <filename> [OPTIONS]\n\nUsed internally to export a volume.\n\n<volume>: <string>\n\nVolume identifier\n\n<format>: <btrfs | qcow2+size | raw+size | tar+size | vmdk+size | zfs>\n\nExport stream format\n\n<filename>: <string>\n\nDestination file name\n\n--base (?^i:[a-z0-9_\\-]{1,40})\n\nSnapshot to start an incremental stream from\n\n--snapshot (?^i:[a-z0-9_\\-]{1,40})\n\nSnapshot to export\n\n--snapshot-list <string>\n\nOrdered list of snapshots to transfer\n\n--with-snapshots <boolean> (default = 0)\n\nWhether to include intermediate snapshots in the stream\n\npvesm extractconfig <volume>\n\nExtract configuration from vzdump backup archive.\n\n<volume>: <string>\n\nVolume identifier\n\npvesm free <volume> [OPTIONS]\n\nDelete volume\n\n<volume>: <string>\n\nVolume identifier\n\n--delay <integer> (1 - 30)\n\nTime to wait for the task to finish. We return null if the task finish within that time.\n\n--storage <storage ID>\n\nThe storage identifier.\n\npvesm glusterfsscan\n\nAn alias for pvesm scan glusterfs.\n\npvesm help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvesm import <volume> <format> <filename> [OPTIONS]\n\nUsed internally to import a volume.\n\n<volume>: <string>\n\nVolume identifier\n\n<format>: <btrfs | qcow2+size | raw+size | tar+size | vmdk+size | zfs>\n\nImport stream format\n\n<filename>: <string>\n\nSource file name. For - stdin is used, the tcp://<IP-or-CIDR> format allows to use a TCP connection, the unix://PATH-TO-SOCKET format a UNIX socket as input.Else, the file is treated as common file.\n\n--allow-rename <boolean> (default = 0)\n\nChoose a new volume ID if the requested volume ID already exists, instead of throwing an error.\n\n--base (?^i:[a-z0-9_\\-]{1,40})\n\nBase snapshot of an incremental stream\n\n--delete-snapshot (?^i:[a-z0-9_\\-]{1,80})\n\nA snapshot to delete on success\n\n--snapshot (?^i:[a-z0-9_\\-]{1,40})\n\nThe current-state snapshot if the stream contains snapshots\n\n--with-snapshots <boolean> (default = 0)\n\nWhether the stream includes intermediate snapshots\n\npvesm iscsiscan\n\nAn alias for pvesm scan iscsi.\n\npvesm list <storage> [OPTIONS]\n\nList storage content.\n\n<storage>: <storage ID>\n\nThe storage identifier.\n\n--content <string>\n\nOnly list content of this type.\n\n--vmid <integer> (100 - 999999999)\n\nOnly list images for this VM\n\npvesm lvmscan\n\nAn alias for pvesm scan lvm.\n\npvesm lvmthinscan\n\nAn alias for pvesm scan lvmthin.\n\npvesm nfsscan\n\nAn alias for pvesm scan nfs.\n\npvesm path <volume>\n\nGet filesystem path for specified volume\n\n<volume>: <string>\n\nVolume identifier\n\npvesm prune-backups <storage> [OPTIONS]\n\nPrune backups. Only those using the standard naming scheme are considered. If no keep options are specified, those from the storage configuration are used.\n\n<storage>: <storage ID>\n\nThe storage identifier.\n\n--dry-run <boolean>\n\nOnly show what would be pruned, don’t delete anything.\n\n--keep-all <boolean>\n\nKeep all backups. Conflicts with the other options when true.\n\n--keep-daily <N>\n\nKeep backups for the last <N> different days. If there is morethan one backup for a single day, only the latest one is kept.\n\n--keep-hourly <N>\n\nKeep backups for the last <N> different hours. If there is morethan one backup for a single hour, only the latest one is kept.\n\n--keep-last <N>\n\nKeep the last <N> backups.\n\n--keep-monthly <N>\n\nKeep backups for the last <N> different months. If there is morethan one backup for a single month, only the latest one is kept.\n\n--keep-weekly <N>\n\nKeep backups for the last <N> different weeks. If there is morethan one backup for a single week, only the latest one is kept.\n\n--keep-yearly <N>\n\nKeep backups for the last <N> different years. If there is morethan one backup for a single year, only the latest one is kept.\n\n--type <lxc | qemu>\n\nEither qemu or lxc. Only consider backups for guests of this type.\n\n--vmid <integer> (100 - 999999999)\n\nOnly consider backups for this guest.\n\npvesm remove <storage>\n\nDelete storage configuration.\n\n<storage>: <storage ID>\n\nThe storage identifier.\n\npvesm scan cifs <server> [OPTIONS]\n\nScan remote CIFS server.\n\n<server>: <string>\n\nThe server address (name or IP).\n\n--domain <string>\n\nSMB domain (Workgroup).\n\n--password <password>\n\nUser password.\n\n--username <string>\n\nUser name.\n\npvesm scan glusterfs <server>\n\nScan remote GlusterFS server.\n\n<server>: <string>\n\nThe server address (name or IP).\n\npvesm scan iscsi <portal>\n\nScan remote iSCSI server.\n\n<portal>: <string>\n\nThe iSCSI portal (IP or DNS name with optional port).\n\npvesm scan lvm\n\nList local LVM volume groups.\n\npvesm scan lvmthin <vg>\n\nList local LVM Thin Pools.\n\n<vg>: [a-zA-Z0-9\\.\\+\\_][a-zA-Z0-9\\.\\+\\_\\-]+\n\nno description available\n\npvesm scan nfs <server>\n\nScan remote NFS server.\n\n<server>: <string>\n\nThe server address (name or IP).\n\npvesm scan pbs <server> <username> --password <string> [OPTIONS] [FORMAT_OPTIONS]\n\nScan remote Proxmox Backup Server.\n\n<server>: <string>\n\nThe server address (name or IP).\n\n<username>: <string>\n\nUser-name or API token-ID.\n\n--fingerprint ([A-Fa-f0-9]{2}:){31}[A-Fa-f0-9]{2}\n\nCertificate SHA 256 fingerprint.\n\n--password <string>\n\nUser password or API token secret.\n\n--port <integer> (1 - 65535) (default = 8007)\n\nOptional port.\n\npvesm scan zfs\n\nScan zfs pool list on local node.\n\npvesm set <storage> [OPTIONS]\n\nUpdate storage configuration.\n\n<storage>: <storage ID>\n\nThe storage identifier.\n\n--blocksize <string>\n\nblock size\n\n--bwlimit [clone=<LIMIT>] [,default=<LIMIT>] [,migration=<LIMIT>] [,move=<LIMIT>] [,restore=<LIMIT>]\n\nSet I/O bandwidth limit for various operations (in KiB/s).\n\n--comstar_hg <string>\n\nhost group for comstar views\n\n--comstar_tg <string>\n\ntarget group for comstar views\n\n--content <string>\n\nAllowed content types.\n\n\tthe value rootdir is used for Containers, and value images for VMs.\n--content-dirs <string>\n\nOverrides for default content type directories.\n\n--create-base-path <boolean> (default = yes)\n\nCreate the base directory if it doesn’t exist.\n\n--create-subdirs <boolean> (default = yes)\n\nPopulate the directory with the default structure.\n\n--data-pool <string>\n\nData Pool (for erasure coding only)\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--digest <string>\n\nPrevent changes if current configuration file has a different digest. This can be used to prevent concurrent modifications.\n\n--disable <boolean>\n\nFlag to disable the storage.\n\n--domain <string>\n\nCIFS domain.\n\n--encryption-key a file containing an encryption key, or the special value \"autogen\"\n\nEncryption key. Use autogen to generate one automatically without passphrase.\n\n--fingerprint ([A-Fa-f0-9]{2}:){31}[A-Fa-f0-9]{2}\n\nCertificate SHA 256 fingerprint.\n\n--format <string>\n\nDefault image format.\n\n--fs-name <string>\n\nThe Ceph filesystem name.\n\n--fuse <boolean>\n\nMount CephFS through FUSE.\n\n--is_mountpoint <string> (default = no)\n\nAssume the given path is an externally managed mountpoint and consider the storage offline if it is not mounted. Using a boolean (yes/no) value serves as a shortcut to using the target path in this field.\n\n--keyring file containing the keyring to authenticate in the Ceph cluster\n\nClient keyring contents (for external clusters).\n\n--krbd <boolean>\n\nAlways access rbd through krbd kernel module.\n\n--lio_tpg <string>\n\ntarget portal group for Linux LIO targets\n\n--master-pubkey a file containing a PEM-formatted master public key\n\nBase64-encoded, PEM-formatted public RSA key. Used to encrypt a copy of the encryption-key which will be added to each encrypted backup.\n\n--max-protected-backups <integer> (-1 - N) (default = Unlimited for users with Datastore.Allocate privilege, 5 for other users)\n\nMaximal number of protected backups per guest. Use -1 for unlimited.\n\n--maxfiles <integer> (0 - N)\n\nDeprecated: use prune-backups instead. Maximal number of backup files per VM. Use 0 for unlimited.\n\n--mkdir <boolean> (default = yes)\n\nCreate the directory if it doesn’t exist and populate it with default sub-dirs. NOTE: Deprecated, use the create-base-path and create-subdirs options instead.\n\n--monhost <string>\n\nIP addresses of monitors (for external clusters).\n\n--mountpoint <string>\n\nmount point\n\n--namespace <string>\n\nNamespace.\n\n--nocow <boolean> (default = 0)\n\nSet the NOCOW flag on files. Disables data checksumming and causes data errors to be unrecoverable from while allowing direct I/O. Only use this if data does not need to be any more safe than on a single ext4 formatted disk with no underlying raid system.\n\n--nodes <string>\n\nList of nodes for which the storage configuration applies.\n\n--nowritecache <boolean>\n\ndisable write caching on the target\n\n--options <string>\n\nNFS/CIFS mount options (see man nfs or man mount.cifs)\n\n--password <password>\n\nPassword for accessing the share/datastore.\n\n--pool <string>\n\nPool.\n\n--port <integer> (1 - 65535)\n\nFor non default port.\n\n--preallocation <falloc | full | metadata | off> (default = metadata)\n\nPreallocation mode for raw and qcow2 images. Using metadata on raw images results in preallocation=off.\n\n--prune-backups [keep-all=<1|0>] [,keep-daily=<N>] [,keep-hourly=<N>] [,keep-last=<N>] [,keep-monthly=<N>] [,keep-weekly=<N>] [,keep-yearly=<N>]\n\nThe retention options with shorter intervals are processed first with --keep-last being the very first one. Each option covers a specific period of time. We say that backups within this period are covered by this option. The next option does not take care of already covered backups and only considers older backups.\n\n--saferemove <boolean>\n\nZero-out data when removing LVs.\n\n--saferemove_throughput <string>\n\nWipe throughput (cstream -t parameter value).\n\n--server <string>\n\nServer IP or DNS name.\n\n--server2 <string>\n\nBackup volfile server IP or DNS name.\n\n\tRequires option(s): server\n--shared <boolean>\n\nIndicate that this is a single storage with the same contents on all nodes (or all listed in the nodes option). It will not make the contents of a local storage automatically accessible to other nodes, it just marks an already shared storage as such!\n\n--skip-cert-verification <boolean> (default = false)\n\nDisable TLS certificate verification, only enable on fully trusted networks!\n\n--smbversion <2.0 | 2.1 | 3 | 3.0 | 3.11 | default> (default = default)\n\nSMB protocol version. default if not set, negotiates the highest SMB2+ version supported by both the client and server.\n\n--sparse <boolean>\n\nuse sparse volumes\n\n--subdir <string>\n\nSubdir to mount.\n\n--tagged_only <boolean>\n\nOnly use logical volumes tagged with pve-vm-ID.\n\n--transport <rdma | tcp | unix>\n\nGluster transport: tcp or rdma\n\n--username <string>\n\nRBD Id.\n\npvesm status [OPTIONS]\n\nGet status for all datastores.\n\n--content <string>\n\nOnly list stores which support this content type.\n\n--enabled <boolean> (default = 0)\n\nOnly list stores which are enabled (not disabled in config).\n\n--format <boolean> (default = 0)\n\nInclude information about formats\n\n--storage <storage ID>\n\nOnly list status for specified storage\n\n--target <string>\n\nIf target is different to node, we only lists shared storages which content is accessible on this node and the specified target node.\n\npvesm zfsscan\n\nAn alias for pvesm scan zfs.\n\n22.3. pvesubscription - Proxmox VE Subscription Manager\n\npvesubscription <COMMAND> [ARGS] [OPTIONS]\n\npvesubscription delete\n\nDelete subscription key of this node.\n\npvesubscription get\n\nRead subscription info.\n\npvesubscription help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvesubscription set <key>\n\nSet subscription key.\n\n<key>: \\s*pve([1248])([cbsp])-[0-9a-f]{10}\\s*\n\nProxmox VE subscription key\n\npvesubscription set-offline-key <data>\n\nInternal use only! To set an offline key, use the package proxmox-offline-mirror-helper instead.\n\n<data>: <string>\n\nA signed subscription info blob\n\npvesubscription update [OPTIONS]\n\nUpdate subscription info.\n\n--force <boolean> (default = 0)\n\nAlways connect to server, even if local cache is still valid.\n\n22.4. pveperf - Proxmox VE Benchmark Script\n\npveperf [PATH]\n\n22.5. pveceph - Manage CEPH Services on Proxmox VE Nodes\n\npveceph <COMMAND> [ARGS] [OPTIONS]\n\npveceph createmgr\n\nAn alias for pveceph mgr create.\n\npveceph createmon\n\nAn alias for pveceph mon create.\n\npveceph createosd\n\nAn alias for pveceph osd create.\n\npveceph createpool\n\nAn alias for pveceph pool create.\n\npveceph destroymgr\n\nAn alias for pveceph mgr destroy.\n\npveceph destroymon\n\nAn alias for pveceph mon destroy.\n\npveceph destroyosd\n\nAn alias for pveceph osd destroy.\n\npveceph destroypool\n\nAn alias for pveceph pool destroy.\n\npveceph fs create [OPTIONS]\n\nCreate a Ceph filesystem\n\n--add-storage <boolean> (default = 0)\n\nConfigure the created CephFS as storage for this cluster.\n\n--name <string> (default = cephfs)\n\nThe ceph filesystem name.\n\n--pg_num <integer> (8 - 32768) (default = 128)\n\nNumber of placement groups for the backing data pool. The metadata pool will use a quarter of this.\n\npveceph fs destroy <name> [OPTIONS]\n\nDestroy a Ceph filesystem\n\n<name>: <string>\n\nThe ceph filesystem name.\n\n--remove-pools <boolean> (default = 0)\n\nRemove data and metadata pools configured for this fs.\n\n--remove-storages <boolean> (default = 0)\n\nRemove all pveceph-managed storages configured for this fs.\n\npveceph help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npveceph init [OPTIONS]\n\nCreate initial ceph default configuration and setup symlinks.\n\n--cluster-network <string>\n\nDeclare a separate cluster network, OSDs will routeheartbeat, object replication and recovery traffic over it\n\n\tRequires option(s): network\n--disable_cephx <boolean> (default = 0)\n\nDisable cephx authentication.\n\n\tcephx is a security feature protecting against man-in-the-middle attacks. Only consider disabling cephx if your network is private!\n--min_size <integer> (1 - 7) (default = 2)\n\nMinimum number of available replicas per object to allow I/O\n\n--network <string>\n\nUse specific network for all ceph related traffic\n\n--pg_bits <integer> (6 - 14) (default = 6)\n\nPlacement group bits, used to specify the default number of placement groups.\n\nDepreacted. This setting was deprecated in recent Ceph versions.\n\n--size <integer> (1 - 7) (default = 3)\n\nTargeted number of replicas per object\n\npveceph install [OPTIONS]\n\nInstall ceph related packages.\n\n--allow-experimental <boolean> (default = 0)\n\nAllow experimental versions. Use with care!\n\n--repository <enterprise | no-subscription | test> (default = enterprise)\n\nCeph repository to use.\n\n--version <quincy | reef> (default = quincy)\n\nCeph version to install.\n\npveceph lspools\n\nAn alias for pveceph pool ls.\n\npveceph mds create [OPTIONS]\n\nCreate Ceph Metadata Server (MDS)\n\n--hotstandby <boolean> (default = 0)\n\nDetermines whether a ceph-mds daemon should poll and replay the log of an active MDS. Faster switch on MDS failure, but needs more idle resources.\n\n--name [a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])? (default = nodename)\n\nThe ID for the mds, when omitted the same as the nodename\n\npveceph mds destroy <name>\n\nDestroy Ceph Metadata Server\n\n<name>: [a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\n\nThe name (ID) of the mds\n\npveceph mgr create [OPTIONS]\n\nCreate Ceph Manager\n\n--id [a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\n\nThe ID for the manager, when omitted the same as the nodename\n\npveceph mgr destroy <id>\n\nDestroy Ceph Manager.\n\n<id>: [a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\n\nThe ID of the manager\n\npveceph mon create [OPTIONS]\n\nCreate Ceph Monitor and Manager\n\n--mon-address <string>\n\nOverwrites autodetected monitor IP address(es). Must be in the public network(s) of Ceph.\n\n--monid [a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\n\nThe ID for the monitor, when omitted the same as the nodename\n\npveceph mon destroy <monid>\n\nDestroy Ceph Monitor and Manager.\n\n<monid>: [a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\n\nMonitor ID\n\npveceph osd create <dev> [OPTIONS]\n\nCreate OSD\n\n<dev>: <string>\n\nBlock device name.\n\n--crush-device-class <string>\n\nSet the device class of the OSD in crush.\n\n--db_dev <string>\n\nBlock device name for block.db.\n\n--db_dev_size <number> (1 - N) (default = bluestore_block_db_size or 10% of OSD size)\n\nSize in GiB for block.db.\n\n\tRequires option(s): db_dev\n--encrypted <boolean> (default = 0)\n\nEnables encryption of the OSD.\n\n--osds-per-device <integer> (1 - N)\n\nOSD services per physical device. Only useful for fast NVMe devices\" .\" to utilize their performance better.\n\n--wal_dev <string>\n\nBlock device name for block.wal.\n\n--wal_dev_size <number> (0.5 - N) (default = bluestore_block_wal_size or 1% of OSD size)\n\nSize in GiB for block.wal.\n\n\tRequires option(s): wal_dev\n\npveceph osd destroy <osdid> [OPTIONS]\n\nDestroy OSD\n\n<osdid>: <integer>\n\nOSD ID\n\n--cleanup <boolean> (default = 0)\n\nIf set, we remove partition table entries.\n\npveceph osd details <osdid> [OPTIONS] [FORMAT_OPTIONS]\n\nGet OSD details.\n\n<osdid>: <string>\n\nID of the OSD\n\n--verbose <boolean> (default = 0)\n\nPrint verbose information, same as json-pretty output format.\n\npveceph pool create <name> [OPTIONS]\n\nCreate Ceph pool\n\n<name>: <string>\n\nThe name of the pool. It must be unique.\n\n--add_storages <boolean> (default = 0; for erasure coded pools: 1)\n\nConfigure VM and CT storage using the new pool.\n\n--application <cephfs | rbd | rgw> (default = rbd)\n\nThe application of the pool.\n\n--crush_rule <string>\n\nThe rule to use for mapping object placement in the cluster.\n\n--erasure-coding k=<integer> ,m=<integer> [,device-class=<class>] [,failure-domain=<domain>] [,profile=<profile>]\n\nCreate an erasure coded pool for RBD with an accompaning replicated pool for metadata storage. With EC, the common ceph options size, min_size and crush_rule parameters will be applied to the metadata pool.\n\n--min_size <integer> (1 - 7) (default = 2)\n\nMinimum number of replicas per object\n\n--pg_autoscale_mode <off | on | warn> (default = warn)\n\nThe automatic PG scaling mode of the pool.\n\n--pg_num <integer> (1 - 32768) (default = 128)\n\nNumber of placement groups.\n\n--pg_num_min <integer> (-N - 32768)\n\nMinimal number of placement groups.\n\n--size <integer> (1 - 7) (default = 3)\n\nNumber of replicas per object\n\n--target_size ^(\\d+(\\.\\d+)?)([KMGT])?$\n\nThe estimated target size of the pool for the PG autoscaler.\n\n--target_size_ratio <number>\n\nThe estimated target ratio of the pool for the PG autoscaler.\n\npveceph pool destroy <name> [OPTIONS]\n\nDestroy pool\n\n<name>: <string>\n\nThe name of the pool. It must be unique.\n\n--force <boolean> (default = 0)\n\nIf true, destroys pool even if in use\n\n--remove_ecprofile <boolean> (default = 1)\n\nRemove the erasure code profile. Defaults to true, if applicable.\n\n--remove_storages <boolean> (default = 0)\n\nRemove all pveceph-managed storages configured for this pool\n\npveceph pool get <name> [OPTIONS] [FORMAT_OPTIONS]\n\nShow the current pool status.\n\n<name>: <string>\n\nThe name of the pool. It must be unique.\n\n--verbose <boolean> (default = 0)\n\nIf enabled, will display additional data(eg. statistics).\n\npveceph pool ls [FORMAT_OPTIONS]\n\nList all pools and their settings (which are settable by the POST/PUT endpoints).\n\npveceph pool set <name> [OPTIONS]\n\nChange POOL settings\n\n<name>: <string>\n\nThe name of the pool. It must be unique.\n\n--application <cephfs | rbd | rgw>\n\nThe application of the pool.\n\n--crush_rule <string>\n\nThe rule to use for mapping object placement in the cluster.\n\n--min_size <integer> (1 - 7)\n\nMinimum number of replicas per object\n\n--pg_autoscale_mode <off | on | warn>\n\nThe automatic PG scaling mode of the pool.\n\n--pg_num <integer> (1 - 32768)\n\nNumber of placement groups.\n\n--pg_num_min <integer> (-N - 32768)\n\nMinimal number of placement groups.\n\n--size <integer> (1 - 7)\n\nNumber of replicas per object\n\n--target_size ^(\\d+(\\.\\d+)?)([KMGT])?$\n\nThe estimated target size of the pool for the PG autoscaler.\n\n--target_size_ratio <number>\n\nThe estimated target ratio of the pool for the PG autoscaler.\n\npveceph purge [OPTIONS]\n\nDestroy ceph related data and configuration files.\n\n--crash <boolean>\n\nAdditionally purge Ceph crash logs, /var/lib/ceph/crash.\n\n--logs <boolean>\n\nAdditionally purge Ceph logs, /var/log/ceph.\n\npveceph start [OPTIONS]\n\nStart ceph services.\n\n--service (ceph|mon|mds|osd|mgr)(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?)? (default = ceph.target)\n\nCeph service name.\n\npveceph status\n\nGet Ceph Status.\n\npveceph stop [OPTIONS]\n\nStop ceph services.\n\n--service (ceph|mon|mds|osd|mgr)(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?)? (default = ceph.target)\n\nCeph service name.\n\n22.6. pvenode - Proxmox VE Node Management\n\npvenode <COMMAND> [ARGS] [OPTIONS]\n\npvenode acme account deactivate [<name>]\n\nDeactivate existing ACME account at CA.\n\n<name>: <name> (default = default)\n\nACME account config file name.\n\npvenode acme account info [<name>] [FORMAT_OPTIONS]\n\nReturn existing ACME account information.\n\n<name>: <name> (default = default)\n\nACME account config file name.\n\npvenode acme account list\n\nACMEAccount index.\n\npvenode acme account register [<name>] {<contact>} [OPTIONS]\n\nRegister a new ACME account with a compatible CA.\n\n<name>: <name> (default = default)\n\nACME account config file name.\n\n<contact>: <string>\n\nContact email addresses.\n\n--directory ^https?://.*\n\nURL of ACME CA directory endpoint.\n\npvenode acme account update [<name>] [OPTIONS]\n\nUpdate existing ACME account information with CA. Note: not specifying any new account information triggers a refresh.\n\n<name>: <name> (default = default)\n\nACME account config file name.\n\n--contact <string>\n\nContact email addresses.\n\npvenode acme cert order [OPTIONS]\n\nOrder a new certificate from ACME-compatible CA.\n\n--force <boolean> (default = 0)\n\nOverwrite existing custom certificate.\n\npvenode acme cert renew [OPTIONS]\n\nRenew existing certificate from CA.\n\n--force <boolean> (default = 0)\n\nForce renewal even if expiry is more than 30 days away.\n\npvenode acme cert revoke\n\nRevoke existing certificate from CA.\n\npvenode acme plugin add <type> <id> [OPTIONS]\n\nAdd ACME plugin configuration.\n\n<type>: <dns | standalone>\n\nACME challenge type.\n\n<id>: <string>\n\nACME Plugin ID name\n\n--api <1984hosting | acmedns | acmeproxy | active24 | ad | ali | anx | artfiles | arvan | aurora | autodns | aws | azion | azure | bookmyname | bunny | cf | clouddns | cloudns | cn | conoha | constellix | cpanel | curanet | cyon | da | ddnss | desec | df | dgon | dnsexit | dnshome | dnsimple | dnsservices | do | doapi | domeneshop | dp | dpi | dreamhost | duckdns | durabledns | dyn | dynu | dynv6 | easydns | edgedns | euserv | exoscale | fornex | freedns | gandi_livedns | gcloud | gcore | gd | geoscaling | googledomains | he | hetzner | hexonet | hostingde | huaweicloud | infoblox | infomaniak | internetbs | inwx | ionos | ipv64 | ispconfig | jd | joker | kappernet | kas | kinghost | knot | la | leaseweb | lexicon | linode | linode_v4 | loopia | lua | maradns | me | miab | misaka | myapi | mydevil | mydnsjp | mythic_beasts | namecheap | namecom | namesilo | nanelo | nederhost | neodigit | netcup | netlify | nic | njalla | nm | nsd | nsone | nsupdate | nw | oci | one | online | openprovider | openstack | opnsense | ovh | pdns | pleskxml | pointhq | porkbun | rackcorp | rackspace | rage4 | rcode0 | regru | scaleway | schlundtech | selectel | selfhost | servercow | simply | tele3 | tencent | transip | udr | ultra | unoeuro | variomedia | veesp | vercel | vscale | vultr | websupport | world4you | yandex | yc | zilore | zone | zonomi>\n\nAPI plugin name\n\n--data File with one key-value pair per line, will be base64url encode for storage in plugin config.\n\nDNS plugin data. (base64 encoded)\n\n--disable <boolean>\n\nFlag to disable the config.\n\n--nodes <string>\n\nList of cluster node names.\n\n--validation-delay <integer> (0 - 172800) (default = 30)\n\nExtra delay in seconds to wait before requesting validation. Allows to cope with a long TTL of DNS records.\n\npvenode acme plugin config <id> [FORMAT_OPTIONS]\n\nGet ACME plugin configuration.\n\n<id>: <string>\n\nUnique identifier for ACME plugin instance.\n\npvenode acme plugin list [OPTIONS] [FORMAT_OPTIONS]\n\nACME plugin index.\n\n--type <dns | standalone>\n\nOnly list ACME plugins of a specific type\n\npvenode acme plugin remove <id>\n\nDelete ACME plugin configuration.\n\n<id>: <string>\n\nUnique identifier for ACME plugin instance.\n\npvenode acme plugin set <id> [OPTIONS]\n\nUpdate ACME plugin configuration.\n\n<id>: <string>\n\nACME Plugin ID name\n\n--api <1984hosting | acmedns | acmeproxy | active24 | ad | ali | anx | artfiles | arvan | aurora | autodns | aws | azion | azure | bookmyname | bunny | cf | clouddns | cloudns | cn | conoha | constellix | cpanel | curanet | cyon | da | ddnss | desec | df | dgon | dnsexit | dnshome | dnsimple | dnsservices | do | doapi | domeneshop | dp | dpi | dreamhost | duckdns | durabledns | dyn | dynu | dynv6 | easydns | edgedns | euserv | exoscale | fornex | freedns | gandi_livedns | gcloud | gcore | gd | geoscaling | googledomains | he | hetzner | hexonet | hostingde | huaweicloud | infoblox | infomaniak | internetbs | inwx | ionos | ipv64 | ispconfig | jd | joker | kappernet | kas | kinghost | knot | la | leaseweb | lexicon | linode | linode_v4 | loopia | lua | maradns | me | miab | misaka | myapi | mydevil | mydnsjp | mythic_beasts | namecheap | namecom | namesilo | nanelo | nederhost | neodigit | netcup | netlify | nic | njalla | nm | nsd | nsone | nsupdate | nw | oci | one | online | openprovider | openstack | opnsense | ovh | pdns | pleskxml | pointhq | porkbun | rackcorp | rackspace | rage4 | rcode0 | regru | scaleway | schlundtech | selectel | selfhost | servercow | simply | tele3 | tencent | transip | udr | ultra | unoeuro | variomedia | veesp | vercel | vscale | vultr | websupport | world4you | yandex | yc | zilore | zone | zonomi>\n\nAPI plugin name\n\n--data File with one key-value pair per line, will be base64url encode for storage in plugin config.\n\nDNS plugin data. (base64 encoded)\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--digest <string>\n\nPrevent changes if current configuration file has a different digest. This can be used to prevent concurrent modifications.\n\n--disable <boolean>\n\nFlag to disable the config.\n\n--nodes <string>\n\nList of cluster node names.\n\n--validation-delay <integer> (0 - 172800) (default = 30)\n\nExtra delay in seconds to wait before requesting validation. Allows to cope with a long TTL of DNS records.\n\npvenode cert delete [<restart>]\n\nDELETE custom certificate chain and key.\n\n<restart>: <boolean> (default = 0)\n\nRestart pveproxy.\n\npvenode cert info [FORMAT_OPTIONS]\n\nGet information about node’s certificates.\n\npvenode cert set <certificates> [<key>] [OPTIONS] [FORMAT_OPTIONS]\n\nUpload or update custom certificate chain and key.\n\n<certificates>: <string>\n\nPEM encoded certificate (chain).\n\n<key>: <string>\n\nPEM encoded private key.\n\n--force <boolean> (default = 0)\n\nOverwrite existing custom or ACME certificate files.\n\n--restart <boolean> (default = 0)\n\nRestart pveproxy.\n\npvenode config get [OPTIONS]\n\nGet node configuration options.\n\n--property <acme | acmedomain0 | acmedomain1 | acmedomain2 | acmedomain3 | acmedomain4 | acmedomain5 | description | startall-onboot-delay | wakeonlan> (default = all)\n\nReturn only a specific property from the node configuration.\n\npvenode config set [OPTIONS]\n\nSet node configuration options.\n\n--acme [account=<name>] [,domains=<domain[;domain;...]>]\n\nNode specific ACME settings.\n\n--acmedomain[n] [domain=]<domain> [,alias=<domain>] [,plugin=<name of the plugin configuration>]\n\nACME domain and validation plugin\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--description <string>\n\nDescription for the Node. Shown in the web-interface node notes panel. This is saved as comment inside the configuration file.\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1 digest. This can be used to prevent concurrent modifications.\n\n--startall-onboot-delay <integer> (0 - 300) (default = 0)\n\nInitial delay in seconds, before starting all the Virtual Guests with on-boot enabled.\n\n--wakeonlan [mac=]<MAC address> [,bind-interface=<bind interface>] [,broadcast-address=<IPv4 broadcast address>]\n\nNode specific wake on LAN settings.\n\npvenode help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvenode migrateall <target> [OPTIONS]\n\nMigrate all VMs and Containers.\n\n<target>: <string>\n\nTarget node.\n\n--maxworkers <integer> (1 - N)\n\nMaximal number of parallel migration job. If not set, uses’max_workers' from datacenter.cfg. One of both must be set!\n\n--vms <string>\n\nOnly consider Guests with these IDs.\n\n--with-local-disks <boolean>\n\nEnable live storage migration for local disk\n\npvenode startall [OPTIONS]\n\nStart all VMs and containers located on this node (by default only those with onboot=1).\n\n--force <boolean> (default = off)\n\nIssue start command even if virtual guest have onboot not set or set to off.\n\n--vms <string>\n\nOnly consider guests from this comma separated list of VMIDs.\n\npvenode stopall [OPTIONS]\n\nStop all VMs and Containers.\n\n--force-stop <boolean> (default = 1)\n\nForce a hard-stop after the timeout.\n\n--timeout <integer> (0 - 7200) (default = 180)\n\nTimeout for each guest shutdown task. Depending on force-stop, the shutdown gets then simply aborted or a hard-stop is forced.\n\n--vms <string>\n\nOnly consider Guests with these IDs.\n\npvenode task list [OPTIONS] [FORMAT_OPTIONS]\n\nRead task list for one node (finished tasks).\n\n--errors <boolean> (default = 0)\n\nOnly list tasks with a status of ERROR.\n\n--limit <integer> (0 - N) (default = 50)\n\nOnly list this amount of tasks.\n\n--since <integer>\n\nOnly list tasks since this UNIX epoch.\n\n--source <active | all | archive> (default = archive)\n\nList archived, active or all tasks.\n\n--start <integer> (0 - N) (default = 0)\n\nList tasks beginning from this offset.\n\n--statusfilter <string>\n\nList of Task States that should be returned.\n\n--typefilter <string>\n\nOnly list tasks of this type (e.g., vzstart, vzdump).\n\n--until <integer>\n\nOnly list tasks until this UNIX epoch.\n\n--userfilter <string>\n\nOnly list tasks from this user.\n\n--vmid <integer> (100 - 999999999)\n\nOnly list tasks for this VM.\n\npvenode task log <upid> [OPTIONS]\n\nRead task log.\n\n<upid>: <string>\n\nThe task’s unique ID.\n\n--download <boolean>\n\nWhether the tasklog file should be downloaded. This parameter can’t be used in conjunction with other parameters\n\n--start <integer> (0 - N) (default = 0)\n\nStart at this line when reading the tasklog\n\npvenode task status <upid> [FORMAT_OPTIONS]\n\nRead task status.\n\n<upid>: <string>\n\nThe task’s unique ID.\n\npvenode wakeonlan <node>\n\nTry to wake a node via wake on LAN network packet.\n\n<node>: <string>\n\ntarget node for wake on LAN packet\n\n22.7. pvesh - Shell interface for the Proxmox VE API\n\npvesh <COMMAND> [ARGS] [OPTIONS]\n\npvesh create <api_path> [OPTIONS] [FORMAT_OPTIONS]\n\nCall API POST on <api_path>.\n\n<api_path>: <string>\n\nAPI path.\n\n--noproxy <boolean>\n\nDisable automatic proxying.\n\npvesh delete <api_path> [OPTIONS] [FORMAT_OPTIONS]\n\nCall API DELETE on <api_path>.\n\n<api_path>: <string>\n\nAPI path.\n\n--noproxy <boolean>\n\nDisable automatic proxying.\n\npvesh get <api_path> [OPTIONS] [FORMAT_OPTIONS]\n\nCall API GET on <api_path>.\n\n<api_path>: <string>\n\nAPI path.\n\n--noproxy <boolean>\n\nDisable automatic proxying.\n\npvesh help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvesh ls <api_path> [OPTIONS] [FORMAT_OPTIONS]\n\nList child objects on <api_path>.\n\n<api_path>: <string>\n\nAPI path.\n\n--noproxy <boolean>\n\nDisable automatic proxying.\n\npvesh set <api_path> [OPTIONS] [FORMAT_OPTIONS]\n\nCall API PUT on <api_path>.\n\n<api_path>: <string>\n\nAPI path.\n\n--noproxy <boolean>\n\nDisable automatic proxying.\n\npvesh usage <api_path> [OPTIONS]\n\nprint API usage information for <api_path>.\n\n<api_path>: <string>\n\nAPI path.\n\n--command <create | delete | get | set>\n\nAPI command.\n\n--returns <boolean>\n\nIncluding schema for returned data.\n\n--verbose <boolean>\n\nVerbose output format.\n\n22.8. qm - QEMU/KVM Virtual Machine Manager\n\nqm <COMMAND> [ARGS] [OPTIONS]\n\nqm agent\n\nAn alias for qm guest cmd.\n\nqm cleanup <vmid> <clean-shutdown> <guest-requested>\n\nCleans up resources like tap devices, vgpus, etc. Called after a vm shuts down, crashes, etc.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<clean-shutdown>: <boolean>\n\nIndicates if qemu shutdown cleanly.\n\n<guest-requested>: <boolean>\n\nIndicates if the shutdown was requested by the guest or via qmp.\n\nqm clone <vmid> <newid> [OPTIONS]\n\nCreate a copy of virtual machine/template.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<newid>: <integer> (100 - 999999999)\n\nVMID for the clone.\n\n--bwlimit <integer> (0 - N) (default = clone limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--description <string>\n\nDescription for the new VM.\n\n--format <qcow2 | raw | vmdk>\n\nTarget format for file storage. Only valid for full clone.\n\n--full <boolean>\n\nCreate a full copy of all disks. This is always done when you clone a normal VM. For VM templates, we try to create a linked clone by default.\n\n--name <string>\n\nSet a name for the new VM.\n\n--pool <string>\n\nAdd the new VM to the specified pool.\n\n--snapname <string>\n\nThe name of the snapshot.\n\n--storage <storage ID>\n\nTarget storage for full clone.\n\n--target <string>\n\nTarget node. Only allowed if the original VM is on shared storage.\n\nqm cloudinit dump <vmid> <type>\n\nGet automatically generated cloudinit config.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<type>: <meta | network | user>\n\nConfig type.\n\nqm cloudinit pending <vmid>\n\nGet the cloudinit configuration with both current and pending values.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm cloudinit update <vmid>\n\nRegenerate and change cloudinit config drive.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm config <vmid> [OPTIONS]\n\nGet the virtual machine configuration with pending configuration changes applied. Set the current parameter to get the current configuration instead.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--current <boolean> (default = 0)\n\nGet current values (instead of pending values).\n\n--snapshot <string>\n\nFetch config values from given snapshot.\n\nqm create <vmid> [OPTIONS]\n\nCreate or restore a virtual machine.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--acpi <boolean> (default = 1)\n\nEnable/disable ACPI.\n\n--affinity <string>\n\nList of host cores used to execute guest processes, for example: 0,5,8-11\n\n--agent [enabled=]<1|0> [,freeze-fs-on-backup=<1|0>] [,fstrim_cloned_disks=<1|0>] [,type=<virtio|isa>]\n\nEnable/disable communication with the QEMU Guest Agent and its properties.\n\n--arch <aarch64 | x86_64>\n\nVirtual processor architecture. Defaults to the host.\n\n--archive <string>\n\nThe backup archive. Either the file system path to a .tar or .vma file (use - to pipe data from stdin) or a proxmox storage backup volume identifier.\n\n--args <string>\n\nArbitrary arguments passed to kvm.\n\n--audio0 device=<ich9-intel-hda|intel-hda|AC97> [,driver=<spice|none>]\n\nConfigure a audio device, useful in combination with QXL/Spice.\n\n--autostart <boolean> (default = 0)\n\nAutomatic restart after crash (currently ignored).\n\n--balloon <integer> (0 - N)\n\nAmount of target RAM for the VM in MiB. Using zero disables the ballon driver.\n\n--bios <ovmf | seabios> (default = seabios)\n\nSelect BIOS implementation.\n\n--boot [[legacy=]<[acdn]{1,4}>] [,order=<device[;device...]>]\n\nSpecify guest boot order. Use the order= sub-property as usage with no key or legacy= is deprecated.\n\n--bootdisk (ide|sata|scsi|virtio)\\d+\n\nEnable booting from specified disk. Deprecated: Use boot: order=foo;bar instead.\n\n--bwlimit <integer> (0 - N) (default = restore limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--cdrom <volume>\n\nThis is an alias for option -ide2\n\n--cicustom [meta=<volume>] [,network=<volume>] [,user=<volume>] [,vendor=<volume>]\n\ncloud-init: Specify custom files to replace the automatically generated ones at start.\n\n--cipassword <password>\n\ncloud-init: Password to assign the user. Using this is generally not recommended. Use ssh keys instead. Also note that older cloud-init versions do not support hashed passwords.\n\n--citype <configdrive2 | nocloud | opennebula>\n\nSpecifies the cloud-init configuration format. The default depends on the configured operating system type (ostype. We use the nocloud format for Linux, and configdrive2 for windows.\n\n--ciupgrade <boolean> (default = 1)\n\ncloud-init: do an automatic package upgrade after the first boot.\n\n--ciuser <string>\n\ncloud-init: User name to change ssh keys and password for instead of the image’s configured default user.\n\n--cores <integer> (1 - N) (default = 1)\n\nThe number of cores per socket.\n\n--cpu [[cputype=]<string>] [,flags=<+FLAG[;-FLAG...]>] [,hidden=<1|0>] [,hv-vendor-id=<vendor-id>] [,phys-bits=<8-64|host>] [,reported-model=<enum>]\n\nEmulated CPU type.\n\n--cpulimit <number> (0 - 128) (default = 0)\n\nLimit of CPU usage.\n\n--cpuunits <integer> (1 - 262144) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a VM, will be clamped to [1, 10000] in cgroup v2.\n\n--description <string>\n\nDescription for the VM. Shown in the web-interface VM’s summary. This is saved as comment inside the configuration file.\n\n--efidisk0 [file=]<volume> [,efitype=<2m|4m>] [,format=<enum>] [,import-from=<source volume>] [,pre-enrolled-keys=<1|0>] [,size=<DiskSize>]\n\nConfigure a disk for storing EFI vars. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Note that SIZE_IN_GiB is ignored here and that the default EFI vars are copied to the volume instead. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--force <boolean>\n\nAllow to overwrite existing VM.\n\n\tRequires option(s): archive\n--freeze <boolean>\n\nFreeze CPU at startup (use c monitor command to start execution).\n\n--hookscript <string>\n\nScript that will be executed during various steps in the vms lifetime.\n\n--hostpci[n] [[host=]<HOSTPCIID[;HOSTPCIID2...]>] [,device-id=<hex id>] [,legacy-igd=<1|0>] [,mapping=<mapping-id>] [,mdev=<string>] [,pcie=<1|0>] [,rombar=<1|0>] [,romfile=<string>] [,sub-device-id=<hex id>] [,sub-vendor-id=<hex id>] [,vendor-id=<hex id>] [,x-vga=<1|0>]\n\nMap host PCI devices into guest.\n\n--hotplug <string> (default = network,disk,usb)\n\nSelectively enable hotplug features. This is a comma separated list of hotplug features: network, disk, cpu, memory, usb and cloudinit. Use 0 to disable hotplug completely. Using 1 as value is an alias for the default network,disk,usb. USB hotplugging is possible for guests with machine version >= 7.1 and ostype l26 or windows > 7.\n\n--hugepages <1024 | 2 | any>\n\nEnable/disable hugepages memory.\n\n--ide[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,model=<model>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as IDE hard disk or CD-ROM (n is 0 to 3). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--ipconfig[n] [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]\n\ncloud-init: Specify IP addresses and gateways for the corresponding interface.\n\nIP addresses use CIDR notation, gateways are optional but need an IP of the same type specified.\n\nThe special string dhcp can be used for IP addresses to use DHCP, in which case no explicit gateway should be provided. For IPv6 the special string auto can be used to use stateless autoconfiguration. This requires cloud-init 19.4 or newer.\n\nIf cloud-init is enabled and neither an IPv4 nor an IPv6 address is specified, it defaults to using dhcp on IPv4.\n\n--ivshmem size=<integer> [,name=<string>]\n\nInter-VM shared memory. Useful for direct communication between VMs, or to the host.\n\n--keephugepages <boolean> (default = 0)\n\nUse together with hugepages. If enabled, hugepages will not not be deleted after VM shutdown and can be used for subsequent starts.\n\n--keyboard <da | de | de-ch | en-gb | en-us | es | fi | fr | fr-be | fr-ca | fr-ch | hu | is | it | ja | lt | mk | nl | no | pl | pt | pt-br | sl | sv | tr>\n\nKeyboard layout for VNC server. This option is generally not required and is often better handled from within the guest OS.\n\n--kvm <boolean> (default = 1)\n\nEnable/disable KVM hardware virtualization.\n\n--live-restore <boolean>\n\nStart the VM immediately while importing or restoring in the background.\n\n--localtime <boolean>\n\nSet the real time clock (RTC) to local time. This is enabled by default if the ostype indicates a Microsoft Windows OS.\n\n--lock <backup | clone | create | migrate | rollback | snapshot | snapshot-delete | suspended | suspending>\n\nLock/unlock the VM.\n\n--machine [[type=]<machine type>] [,viommu=<intel|virtio>]\n\nSpecify the QEMU machine.\n\n--memory [current=]<integer>\n\nMemory properties.\n\n--migrate_downtime <number> (0 - N) (default = 0.1)\n\nSet maximum tolerated downtime (in seconds) for migrations.\n\n--migrate_speed <integer> (0 - N) (default = 0)\n\nSet maximum speed (in MB/s) for migrations. Value 0 is no limit.\n\n--name <string>\n\nSet a name for the VM. Only used on the configuration web interface.\n\n--nameserver <string>\n\ncloud-init: Sets DNS server IP address for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\n--net[n] [model=]<enum> [,bridge=<bridge>] [,firewall=<1|0>] [,link_down=<1|0>] [,macaddr=<XX:XX:XX:XX:XX:XX>] [,mtu=<integer>] [,queues=<integer>] [,rate=<number>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,<model>=<macaddr>]\n\nSpecify network devices.\n\n--numa <boolean> (default = 0)\n\nEnable/disable NUMA.\n\n--numa[n] cpus=<id[-id];...> [,hostnodes=<id[-id];...>] [,memory=<number>] [,policy=<preferred|bind|interleave>]\n\nNUMA topology.\n\n--onboot <boolean> (default = 0)\n\nSpecifies whether a VM will be started during system bootup.\n\n--ostype <l24 | l26 | other | solaris | w2k | w2k3 | w2k8 | win10 | win11 | win7 | win8 | wvista | wxp>\n\nSpecify guest operating system.\n\n--parallel[n] /dev/parport\\d+|/dev/usb/lp\\d+\n\nMap host parallel devices (n is 0 to 2).\n\n--pool <string>\n\nAdd the VM to the specified pool.\n\n--protection <boolean> (default = 0)\n\nSets the protection flag of the VM. This will disable the remove VM and remove disk operations.\n\n--reboot <boolean> (default = 1)\n\nAllow reboot. If set to 0 the VM exit on reboot.\n\n--rng0 [source=]</dev/urandom|/dev/random|/dev/hwrng> [,max_bytes=<integer>] [,period=<integer>]\n\nConfigure a VirtIO-based Random Number Generator.\n\n--sata[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SATA hard disk or CD-ROM (n is 0 to 5). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--scsi[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,product=<product>] [,queues=<integer>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,scsiblock=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,vendor=<vendor>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SCSI hard disk or CD-ROM (n is 0 to 30). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--scsihw <lsi | lsi53c810 | megasas | pvscsi | virtio-scsi-pci | virtio-scsi-single> (default = lsi)\n\nSCSI controller model\n\n--searchdomain <string>\n\ncloud-init: Sets DNS search domains for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\n--serial[n] (/dev/.+|socket)\n\nCreate a serial device inside the VM (n is 0 to 3)\n\n--shares <integer> (0 - 50000) (default = 1000)\n\nAmount of memory shares for auto-ballooning. The larger the number is, the more memory this VM gets. Number is relative to weights of all other running VMs. Using zero disables auto-ballooning. Auto-ballooning is done by pvestatd.\n\n--smbios1 [base64=<1|0>] [,family=<Base64 encoded string>] [,manufacturer=<Base64 encoded string>] [,product=<Base64 encoded string>] [,serial=<Base64 encoded string>] [,sku=<Base64 encoded string>] [,uuid=<UUID>] [,version=<Base64 encoded string>]\n\nSpecify SMBIOS type 1 fields.\n\n--smp <integer> (1 - N) (default = 1)\n\nThe number of CPUs. Please use option -sockets instead.\n\n--sockets <integer> (1 - N) (default = 1)\n\nThe number of CPU sockets.\n\n--spice_enhancements [foldersharing=<1|0>] [,videostreaming=<off|all|filter>]\n\nConfigure additional enhancements for SPICE.\n\n--sshkeys <filepath>\n\ncloud-init: Setup public SSH keys (one key per line, OpenSSH format).\n\n--start <boolean> (default = 0)\n\nStart VM after it was created successfully.\n\n--startdate (now | YYYY-MM-DD | YYYY-MM-DDTHH:MM:SS) (default = now)\n\nSet the initial date of the real time clock. Valid format for date are:'now' or 2006-06-17T16:01:21 or 2006-06-17.\n\n--startup `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\n--storage <storage ID>\n\nDefault storage.\n\n--tablet <boolean> (default = 1)\n\nEnable/disable the USB tablet device.\n\n--tags <string>\n\nTags of the VM. This is only meta information.\n\n--tdf <boolean> (default = 0)\n\nEnable/disable time drift fix.\n\n--template <boolean> (default = 0)\n\nEnable/disable Template.\n\n--tpmstate0 [file=]<volume> [,import-from=<source volume>] [,size=<DiskSize>] [,version=<v1.2|v2.0>]\n\nConfigure a Disk for storing TPM state. The format is fixed to raw. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Note that SIZE_IN_GiB is ignored here and 4 MiB will be used instead. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--unique <boolean>\n\nAssign a unique random ethernet address.\n\n\tRequires option(s): archive\n--unused[n] [file=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\n--usb[n] [[host=]<HOSTUSBDEVICE|spice>] [,mapping=<mapping-id>] [,usb3=<1|0>]\n\nConfigure an USB device (n is 0 to 4, for machine version >= 7.1 and ostype l26 or windows > 7, n can be up to 14).\n\n--vcpus <integer> (1 - N) (default = 0)\n\nNumber of hotplugged vcpus.\n\n--vga [[type=]<enum>] [,clipboard=<vnc>] [,memory=<integer>]\n\nConfigure the VGA hardware.\n\n--virtio[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>]\n\nUse volume as VIRTIO hard disk (n is 0 to 15). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--vmgenid <UUID> (default = 1 (autogenerated))\n\nSet VM Generation ID. Use 1 to autogenerate on create or update, pass 0 to disable explicitly.\n\n--vmstatestorage <storage ID>\n\nDefault storage for VM state volumes/files.\n\n--watchdog [[model=]<i6300esb|ib700>] [,action=<enum>]\n\nCreate a virtual hardware watchdog device.\n\nqm delsnapshot <vmid> <snapname> [OPTIONS]\n\nDelete a VM snapshot.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<snapname>: <string>\n\nThe name of the snapshot.\n\n--force <boolean>\n\nFor removal from config file, even if removing disk snapshots fails.\n\nqm destroy <vmid> [OPTIONS]\n\nDestroy the VM and all used/owned volumes. Removes any VM specific permissions and firewall rules\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--destroy-unreferenced-disks <boolean> (default = 0)\n\nIf set, destroy additionally all disks not referenced in the config but with a matching VMID from all enabled storages.\n\n--purge <boolean>\n\nRemove VMID from configurations, like backup & replication jobs and HA.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\nqm disk import <vmid> <source> <storage> [OPTIONS]\n\nImport an external disk image as an unused disk in a VM. The image format has to be supported by qemu-img(1).\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<source>: <string>\n\nPath to the disk image to import\n\n<storage>: <storage ID>\n\nTarget storage ID\n\n--format <qcow2 | raw | vmdk>\n\nTarget format\n\nqm disk move <vmid> <disk> [<storage>] [OPTIONS]\n\nMove volume to different storage or to a different VM.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<disk>: <efidisk0 | ide0 | ide1 | ide2 | ide3 | sata0 | sata1 | sata2 | sata3 | sata4 | sata5 | scsi0 | scsi1 | scsi10 | scsi11 | scsi12 | scsi13 | scsi14 | scsi15 | scsi16 | scsi17 | scsi18 | scsi19 | scsi2 | scsi20 | scsi21 | scsi22 | scsi23 | scsi24 | scsi25 | scsi26 | scsi27 | scsi28 | scsi29 | scsi3 | scsi30 | scsi4 | scsi5 | scsi6 | scsi7 | scsi8 | scsi9 | tpmstate0 | unused0 | unused1 | unused10 | unused100 | unused101 | unused102 | unused103 | unused104 | unused105 | unused106 | unused107 | unused108 | unused109 | unused11 | unused110 | unused111 | unused112 | unused113 | unused114 | unused115 | unused116 | unused117 | unused118 | unused119 | unused12 | unused120 | unused121 | unused122 | unused123 | unused124 | unused125 | unused126 | unused127 | unused128 | unused129 | unused13 | unused130 | unused131 | unused132 | unused133 | unused134 | unused135 | unused136 | unused137 | unused138 | unused139 | unused14 | unused140 | unused141 | unused142 | unused143 | unused144 | unused145 | unused146 | unused147 | unused148 | unused149 | unused15 | unused150 | unused151 | unused152 | unused153 | unused154 | unused155 | unused156 | unused157 | unused158 | unused159 | unused16 | unused160 | unused161 | unused162 | unused163 | unused164 | unused165 | unused166 | unused167 | unused168 | unused169 | unused17 | unused170 | unused171 | unused172 | unused173 | unused174 | unused175 | unused176 | unused177 | unused178 | unused179 | unused18 | unused180 | unused181 | unused182 | unused183 | unused184 | unused185 | unused186 | unused187 | unused188 | unused189 | unused19 | unused190 | unused191 | unused192 | unused193 | unused194 | unused195 | unused196 | unused197 | unused198 | unused199 | unused2 | unused20 | unused200 | unused201 | unused202 | unused203 | unused204 | unused205 | unused206 | unused207 | unused208 | unused209 | unused21 | unused210 | unused211 | unused212 | unused213 | unused214 | unused215 | unused216 | unused217 | unused218 | unused219 | unused22 | unused220 | unused221 | unused222 | unused223 | unused224 | unused225 | unused226 | unused227 | unused228 | unused229 | unused23 | unused230 | unused231 | unused232 | unused233 | unused234 | unused235 | unused236 | unused237 | unused238 | unused239 | unused24 | unused240 | unused241 | unused242 | unused243 | unused244 | unused245 | unused246 | unused247 | unused248 | unused249 | unused25 | unused250 | unused251 | unused252 | unused253 | unused254 | unused255 | unused26 | unused27 | unused28 | unused29 | unused3 | unused30 | unused31 | unused32 | unused33 | unused34 | unused35 | unused36 | unused37 | unused38 | unused39 | unused4 | unused40 | unused41 | unused42 | unused43 | unused44 | unused45 | unused46 | unused47 | unused48 | unused49 | unused5 | unused50 | unused51 | unused52 | unused53 | unused54 | unused55 | unused56 | unused57 | unused58 | unused59 | unused6 | unused60 | unused61 | unused62 | unused63 | unused64 | unused65 | unused66 | unused67 | unused68 | unused69 | unused7 | unused70 | unused71 | unused72 | unused73 | unused74 | unused75 | unused76 | unused77 | unused78 | unused79 | unused8 | unused80 | unused81 | unused82 | unused83 | unused84 | unused85 | unused86 | unused87 | unused88 | unused89 | unused9 | unused90 | unused91 | unused92 | unused93 | unused94 | unused95 | unused96 | unused97 | unused98 | unused99 | virtio0 | virtio1 | virtio10 | virtio11 | virtio12 | virtio13 | virtio14 | virtio15 | virtio2 | virtio3 | virtio4 | virtio5 | virtio6 | virtio7 | virtio8 | virtio9>\n\nThe disk you want to move.\n\n<storage>: <storage ID>\n\nTarget storage.\n\n--bwlimit <integer> (0 - N) (default = move limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--delete <boolean> (default = 0)\n\nDelete the original disk after successful copy. By default the original disk is kept as unused disk.\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1\" .\" digest. This can be used to prevent concurrent modifications.\n\n--format <qcow2 | raw | vmdk>\n\nTarget Format.\n\n--target-digest <string>\n\nPrevent changes if the current config file of the target VM has a\" .\" different SHA1 digest. This can be used to detect concurrent modifications.\n\n--target-disk <efidisk0 | ide0 | ide1 | ide2 | ide3 | sata0 | sata1 | sata2 | sata3 | sata4 | sata5 | scsi0 | scsi1 | scsi10 | scsi11 | scsi12 | scsi13 | scsi14 | scsi15 | scsi16 | scsi17 | scsi18 | scsi19 | scsi2 | scsi20 | scsi21 | scsi22 | scsi23 | scsi24 | scsi25 | scsi26 | scsi27 | scsi28 | scsi29 | scsi3 | scsi30 | scsi4 | scsi5 | scsi6 | scsi7 | scsi8 | scsi9 | tpmstate0 | unused0 | unused1 | unused10 | unused100 | unused101 | unused102 | unused103 | unused104 | unused105 | unused106 | unused107 | unused108 | unused109 | unused11 | unused110 | unused111 | unused112 | unused113 | unused114 | unused115 | unused116 | unused117 | unused118 | unused119 | unused12 | unused120 | unused121 | unused122 | unused123 | unused124 | unused125 | unused126 | unused127 | unused128 | unused129 | unused13 | unused130 | unused131 | unused132 | unused133 | unused134 | unused135 | unused136 | unused137 | unused138 | unused139 | unused14 | unused140 | unused141 | unused142 | unused143 | unused144 | unused145 | unused146 | unused147 | unused148 | unused149 | unused15 | unused150 | unused151 | unused152 | unused153 | unused154 | unused155 | unused156 | unused157 | unused158 | unused159 | unused16 | unused160 | unused161 | unused162 | unused163 | unused164 | unused165 | unused166 | unused167 | unused168 | unused169 | unused17 | unused170 | unused171 | unused172 | unused173 | unused174 | unused175 | unused176 | unused177 | unused178 | unused179 | unused18 | unused180 | unused181 | unused182 | unused183 | unused184 | unused185 | unused186 | unused187 | unused188 | unused189 | unused19 | unused190 | unused191 | unused192 | unused193 | unused194 | unused195 | unused196 | unused197 | unused198 | unused199 | unused2 | unused20 | unused200 | unused201 | unused202 | unused203 | unused204 | unused205 | unused206 | unused207 | unused208 | unused209 | unused21 | unused210 | unused211 | unused212 | unused213 | unused214 | unused215 | unused216 | unused217 | unused218 | unused219 | unused22 | unused220 | unused221 | unused222 | unused223 | unused224 | unused225 | unused226 | unused227 | unused228 | unused229 | unused23 | unused230 | unused231 | unused232 | unused233 | unused234 | unused235 | unused236 | unused237 | unused238 | unused239 | unused24 | unused240 | unused241 | unused242 | unused243 | unused244 | unused245 | unused246 | unused247 | unused248 | unused249 | unused25 | unused250 | unused251 | unused252 | unused253 | unused254 | unused255 | unused26 | unused27 | unused28 | unused29 | unused3 | unused30 | unused31 | unused32 | unused33 | unused34 | unused35 | unused36 | unused37 | unused38 | unused39 | unused4 | unused40 | unused41 | unused42 | unused43 | unused44 | unused45 | unused46 | unused47 | unused48 | unused49 | unused5 | unused50 | unused51 | unused52 | unused53 | unused54 | unused55 | unused56 | unused57 | unused58 | unused59 | unused6 | unused60 | unused61 | unused62 | unused63 | unused64 | unused65 | unused66 | unused67 | unused68 | unused69 | unused7 | unused70 | unused71 | unused72 | unused73 | unused74 | unused75 | unused76 | unused77 | unused78 | unused79 | unused8 | unused80 | unused81 | unused82 | unused83 | unused84 | unused85 | unused86 | unused87 | unused88 | unused89 | unused9 | unused90 | unused91 | unused92 | unused93 | unused94 | unused95 | unused96 | unused97 | unused98 | unused99 | virtio0 | virtio1 | virtio10 | virtio11 | virtio12 | virtio13 | virtio14 | virtio15 | virtio2 | virtio3 | virtio4 | virtio5 | virtio6 | virtio7 | virtio8 | virtio9>\n\nThe config key the disk will be moved to on the target VM (for example, ide0 or scsi1). Default is the source disk key.\n\n--target-vmid <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm disk rescan [OPTIONS]\n\nRescan all storages and update disk sizes and unused disk images.\n\n--dryrun <boolean> (default = 0)\n\nDo not actually write changes out to VM config(s).\n\n--vmid <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm disk resize <vmid> <disk> <size> [OPTIONS]\n\nExtend volume size.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<disk>: <efidisk0 | ide0 | ide1 | ide2 | ide3 | sata0 | sata1 | sata2 | sata3 | sata4 | sata5 | scsi0 | scsi1 | scsi10 | scsi11 | scsi12 | scsi13 | scsi14 | scsi15 | scsi16 | scsi17 | scsi18 | scsi19 | scsi2 | scsi20 | scsi21 | scsi22 | scsi23 | scsi24 | scsi25 | scsi26 | scsi27 | scsi28 | scsi29 | scsi3 | scsi30 | scsi4 | scsi5 | scsi6 | scsi7 | scsi8 | scsi9 | tpmstate0 | virtio0 | virtio1 | virtio10 | virtio11 | virtio12 | virtio13 | virtio14 | virtio15 | virtio2 | virtio3 | virtio4 | virtio5 | virtio6 | virtio7 | virtio8 | virtio9>\n\nThe disk you want to resize.\n\n<size>: \\+?\\d+(\\.\\d+)?[KMGT]?\n\nThe new size. With the + sign the value is added to the actual size of the volume and without it, the value is taken as an absolute one. Shrinking disk size is not supported.\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1 digest. This can be used to prevent concurrent modifications.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\nqm disk unlink <vmid> --idlist <string> [OPTIONS]\n\nUnlink/delete disk images.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--force <boolean>\n\nForce physical removal. Without this, we simple remove the disk from the config file and create an additional configuration entry called unused[n], which contains the volume ID. Unlink of unused[n] always cause physical removal.\n\n--idlist <string>\n\nA list of disk IDs you want to delete.\n\nqm guest cmd <vmid> <command>\n\nExecute QEMU Guest Agent commands.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<command>: <fsfreeze-freeze | fsfreeze-status | fsfreeze-thaw | fstrim | get-fsinfo | get-host-name | get-memory-block-info | get-memory-blocks | get-osinfo | get-time | get-timezone | get-users | get-vcpus | info | network-get-interfaces | ping | shutdown | suspend-disk | suspend-hybrid | suspend-ram>\n\nThe QGA command.\n\nqm guest exec <vmid> [<extra-args>] [OPTIONS]\n\nExecutes the given command via the guest agent\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<extra-args>: <array>\n\nExtra arguments as array\n\n--pass-stdin <boolean> (default = 0)\n\nWhen set, read STDIN until EOF and forward to guest agent via input-data (usually treated as STDIN to process launched by guest agent). Allows maximal 1 MiB.\n\n--synchronous <boolean> (default = 1)\n\nIf set to off, returns the pid immediately instead of waiting for the commmand to finish or the timeout.\n\n--timeout <integer> (0 - N) (default = 30)\n\nThe maximum time to wait synchronously for the command to finish. If reached, the pid gets returned. Set to 0 to deactivate\n\nqm guest exec-status <vmid> <pid>\n\nGets the status of the given pid started by the guest-agent\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<pid>: <integer>\n\nThe PID to query\n\nqm guest passwd <vmid> <username> [OPTIONS]\n\nSets the password for the given user to the given password\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<username>: <string>\n\nThe user to set the password for.\n\n--crypted <boolean> (default = 0)\n\nset to 1 if the password has already been passed through crypt()\n\nqm help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\nqm import <vmid> <source> --storage <string> [OPTIONS]\n\nImport a foreign virtual guest from a supported import source, such as an ESXi storage.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<source>: <string>\n\nThe import source volume id.\n\n--acpi <boolean> (default = 1)\n\nEnable/disable ACPI.\n\n--affinity <string>\n\nList of host cores used to execute guest processes, for example: 0,5,8-11\n\n--agent [enabled=]<1|0> [,freeze-fs-on-backup=<1|0>] [,fstrim_cloned_disks=<1|0>] [,type=<virtio|isa>]\n\nEnable/disable communication with the QEMU Guest Agent and its properties.\n\n--arch <aarch64 | x86_64>\n\nVirtual processor architecture. Defaults to the host.\n\n--args <string>\n\nArbitrary arguments passed to kvm.\n\n--audio0 device=<ich9-intel-hda|intel-hda|AC97> [,driver=<spice|none>]\n\nConfigure a audio device, useful in combination with QXL/Spice.\n\n--autostart <boolean> (default = 0)\n\nAutomatic restart after crash (currently ignored).\n\n--balloon <integer> (0 - N)\n\nAmount of target RAM for the VM in MiB. Using zero disables the ballon driver.\n\n--bios <ovmf | seabios> (default = seabios)\n\nSelect BIOS implementation.\n\n--boot [[legacy=]<[acdn]{1,4}>] [,order=<device[;device...]>]\n\nSpecify guest boot order. Use the order= sub-property as usage with no key or legacy= is deprecated.\n\n--bootdisk (ide|sata|scsi|virtio)\\d+\n\nEnable booting from specified disk. Deprecated: Use boot: order=foo;bar instead.\n\n--cdrom <volume>\n\nThis is an alias for option -ide2\n\n--cicustom [meta=<volume>] [,network=<volume>] [,user=<volume>] [,vendor=<volume>]\n\ncloud-init: Specify custom files to replace the automatically generated ones at start.\n\n--cipassword <string>\n\ncloud-init: Password to assign the user. Using this is generally not recommended. Use ssh keys instead. Also note that older cloud-init versions do not support hashed passwords.\n\n--citype <configdrive2 | nocloud | opennebula>\n\nSpecifies the cloud-init configuration format. The default depends on the configured operating system type (ostype. We use the nocloud format for Linux, and configdrive2 for windows.\n\n--ciupgrade <boolean> (default = 1)\n\ncloud-init: do an automatic package upgrade after the first boot.\n\n--ciuser <string>\n\ncloud-init: User name to change ssh keys and password for instead of the image’s configured default user.\n\n--cores <integer> (1 - N) (default = 1)\n\nThe number of cores per socket.\n\n--cpu [[cputype=]<string>] [,flags=<+FLAG[;-FLAG...]>] [,hidden=<1|0>] [,hv-vendor-id=<vendor-id>] [,phys-bits=<8-64|host>] [,reported-model=<enum>]\n\nEmulated CPU type.\n\n--cpulimit <number> (0 - 128) (default = 0)\n\nLimit of CPU usage.\n\n--cpuunits <integer> (1 - 262144) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a VM, will be clamped to [1, 10000] in cgroup v2.\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--description <string>\n\nDescription for the VM. Shown in the web-interface VM’s summary. This is saved as comment inside the configuration file.\n\n--dryrun <boolean> (default = 0)\n\nShow the create command and exit without doing anything.\n\n--efidisk0 [file=]<volume> [,efitype=<2m|4m>] [,format=<enum>] [,pre-enrolled-keys=<1|0>] [,size=<DiskSize>]\n\nConfigure a disk for storing EFI vars.\n\n--format <qcow2 | raw | vmdk>\n\nTarget format\n\n--freeze <boolean>\n\nFreeze CPU at startup (use c monitor command to start execution).\n\n--hookscript <string>\n\nScript that will be executed during various steps in the vms lifetime.\n\n--hostpci[n] [[host=]<HOSTPCIID[;HOSTPCIID2...]>] [,device-id=<hex id>] [,legacy-igd=<1|0>] [,mapping=<mapping-id>] [,mdev=<string>] [,pcie=<1|0>] [,rombar=<1|0>] [,romfile=<string>] [,sub-device-id=<hex id>] [,sub-vendor-id=<hex id>] [,vendor-id=<hex id>] [,x-vga=<1|0>]\n\nMap host PCI devices into guest.\n\n--hotplug <string> (default = network,disk,usb)\n\nSelectively enable hotplug features. This is a comma separated list of hotplug features: network, disk, cpu, memory, usb and cloudinit. Use 0 to disable hotplug completely. Using 1 as value is an alias for the default network,disk,usb. USB hotplugging is possible for guests with machine version >= 7.1 and ostype l26 or windows > 7.\n\n--hugepages <1024 | 2 | any>\n\nEnable/disable hugepages memory.\n\n--ide[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,model=<model>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as IDE hard disk or CD-ROM (n is 0 to 3).\n\n--ipconfig[n] [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]\n\ncloud-init: Specify IP addresses and gateways for the corresponding interface.\n\nIP addresses use CIDR notation, gateways are optional but need an IP of the same type specified.\n\nThe special string dhcp can be used for IP addresses to use DHCP, in which case no explicit gateway should be provided. For IPv6 the special string auto can be used to use stateless autoconfiguration. This requires cloud-init 19.4 or newer.\n\nIf cloud-init is enabled and neither an IPv4 nor an IPv6 address is specified, it defaults to using dhcp on IPv4.\n\n--ivshmem size=<integer> [,name=<string>]\n\nInter-VM shared memory. Useful for direct communication between VMs, or to the host.\n\n--keephugepages <boolean> (default = 0)\n\nUse together with hugepages. If enabled, hugepages will not not be deleted after VM shutdown and can be used for subsequent starts.\n\n--keyboard <da | de | de-ch | en-gb | en-us | es | fi | fr | fr-be | fr-ca | fr-ch | hu | is | it | ja | lt | mk | nl | no | pl | pt | pt-br | sl | sv | tr>\n\nKeyboard layout for VNC server. This option is generally not required and is often better handled from within the guest OS.\n\n--kvm <boolean> (default = 1)\n\nEnable/disable KVM hardware virtualization.\n\n--live-import <boolean> (default = 0)\n\nImmediately start the VM and copy the data in the background.\n\n--localtime <boolean>\n\nSet the real time clock (RTC) to local time. This is enabled by default if the ostype indicates a Microsoft Windows OS.\n\n--lock <backup | clone | create | migrate | rollback | snapshot | snapshot-delete | suspended | suspending>\n\nLock/unlock the VM.\n\n--machine [[type=]<machine type>] [,viommu=<intel|virtio>]\n\nSpecify the QEMU machine.\n\n--memory [current=]<integer>\n\nMemory properties.\n\n--migrate_downtime <number> (0 - N) (default = 0.1)\n\nSet maximum tolerated downtime (in seconds) for migrations.\n\n--migrate_speed <integer> (0 - N) (default = 0)\n\nSet maximum speed (in MB/s) for migrations. Value 0 is no limit.\n\n--name <string>\n\nSet a name for the VM. Only used on the configuration web interface.\n\n--nameserver <string>\n\ncloud-init: Sets DNS server IP address for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\n--net[n] [model=]<enum> [,bridge=<bridge>] [,firewall=<1|0>] [,link_down=<1|0>] [,macaddr=<XX:XX:XX:XX:XX:XX>] [,mtu=<integer>] [,queues=<integer>] [,rate=<number>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,<model>=<macaddr>]\n\nSpecify network devices.\n\n--numa <boolean> (default = 0)\n\nEnable/disable NUMA.\n\n--numa[n] cpus=<id[-id];...> [,hostnodes=<id[-id];...>] [,memory=<number>] [,policy=<preferred|bind|interleave>]\n\nNUMA topology.\n\n--onboot <boolean> (default = 0)\n\nSpecifies whether a VM will be started during system bootup.\n\n--ostype <l24 | l26 | other | solaris | w2k | w2k3 | w2k8 | win10 | win11 | win7 | win8 | wvista | wxp>\n\nSpecify guest operating system.\n\n--parallel[n] /dev/parport\\d+|/dev/usb/lp\\d+\n\nMap host parallel devices (n is 0 to 2).\n\n--protection <boolean> (default = 0)\n\nSets the protection flag of the VM. This will disable the remove VM and remove disk operations.\n\n--reboot <boolean> (default = 1)\n\nAllow reboot. If set to 0 the VM exit on reboot.\n\n--rng0 [source=]</dev/urandom|/dev/random|/dev/hwrng> [,max_bytes=<integer>] [,period=<integer>]\n\nConfigure a VirtIO-based Random Number Generator.\n\n--sata[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SATA hard disk or CD-ROM (n is 0 to 5).\n\n--scsi[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,product=<product>] [,queues=<integer>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,scsiblock=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,vendor=<vendor>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SCSI hard disk or CD-ROM (n is 0 to 30).\n\n--scsihw <lsi | lsi53c810 | megasas | pvscsi | virtio-scsi-pci | virtio-scsi-single> (default = lsi)\n\nSCSI controller model\n\n--searchdomain <string>\n\ncloud-init: Sets DNS search domains for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\n--serial[n] (/dev/.+|socket)\n\nCreate a serial device inside the VM (n is 0 to 3)\n\n--shares <integer> (0 - 50000) (default = 1000)\n\nAmount of memory shares for auto-ballooning. The larger the number is, the more memory this VM gets. Number is relative to weights of all other running VMs. Using zero disables auto-ballooning. Auto-ballooning is done by pvestatd.\n\n--smbios1 [base64=<1|0>] [,family=<Base64 encoded string>] [,manufacturer=<Base64 encoded string>] [,product=<Base64 encoded string>] [,serial=<Base64 encoded string>] [,sku=<Base64 encoded string>] [,uuid=<UUID>] [,version=<Base64 encoded string>]\n\nSpecify SMBIOS type 1 fields.\n\n--smp <integer> (1 - N) (default = 1)\n\nThe number of CPUs. Please use option -sockets instead.\n\n--sockets <integer> (1 - N) (default = 1)\n\nThe number of CPU sockets.\n\n--spice_enhancements [foldersharing=<1|0>] [,videostreaming=<off|all|filter>]\n\nConfigure additional enhancements for SPICE.\n\n--sshkeys <string>\n\ncloud-init: Setup public SSH keys (one key per line, OpenSSH format).\n\n--startdate (now | YYYY-MM-DD | YYYY-MM-DDTHH:MM:SS) (default = now)\n\nSet the initial date of the real time clock. Valid format for date are:'now' or 2006-06-17T16:01:21 or 2006-06-17.\n\n--startup `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\n--storage <storage ID>\n\nDefault storage.\n\n--tablet <boolean> (default = 1)\n\nEnable/disable the USB tablet device.\n\n--tags <string>\n\nTags of the VM. This is only meta information.\n\n--tdf <boolean> (default = 0)\n\nEnable/disable time drift fix.\n\n--template <boolean> (default = 0)\n\nEnable/disable Template.\n\n--tpmstate0 [file=]<volume> [,size=<DiskSize>] [,version=<v1.2|v2.0>]\n\nConfigure a Disk for storing TPM state. The format is fixed to raw.\n\n--unused[n] [file=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\n--usb[n] [[host=]<HOSTUSBDEVICE|spice>] [,mapping=<mapping-id>] [,usb3=<1|0>]\n\nConfigure an USB device (n is 0 to 4, for machine version >= 7.1 and ostype l26 or windows > 7, n can be up to 14).\n\n--vcpus <integer> (1 - N) (default = 0)\n\nNumber of hotplugged vcpus.\n\n--vga [[type=]<enum>] [,clipboard=<vnc>] [,memory=<integer>]\n\nConfigure the VGA hardware.\n\n--virtio[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>]\n\nUse volume as VIRTIO hard disk (n is 0 to 15).\n\n--vmgenid <UUID> (default = 1 (autogenerated))\n\nSet VM Generation ID. Use 1 to autogenerate on create or update, pass 0 to disable explicitly.\n\n--vmstatestorage <storage ID>\n\nDefault storage for VM state volumes/files.\n\n--watchdog [[model=]<i6300esb|ib700>] [,action=<enum>]\n\nCreate a virtual hardware watchdog device.\n\nqm importdisk\n\nAn alias for qm disk import.\n\nqm importovf <vmid> <manifest> <storage> [OPTIONS]\n\nCreate a new VM using parameters read from an OVF manifest\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<manifest>: <string>\n\npath to the ovf file\n\n<storage>: <storage ID>\n\nTarget storage ID\n\n--dryrun <boolean>\n\nPrint a parsed representation of the extracted OVF parameters, but do not create a VM\n\n--format <qcow2 | raw | vmdk>\n\nTarget format\n\nqm list [OPTIONS]\n\nVirtual machine index (per node).\n\n--full <boolean>\n\nDetermine the full status of active VMs.\n\nqm listsnapshot <vmid>\n\nList all snapshots.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm migrate <vmid> <target> [OPTIONS]\n\nMigrate virtual machine. Creates a new migration task.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target>: <string>\n\nTarget node.\n\n--bwlimit <integer> (0 - N) (default = migrate limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--force <boolean>\n\nAllow to migrate VMs which use local devices. Only root may use this option.\n\n--migration_network <string>\n\nCIDR of the (sub) network that is used for migration.\n\n--migration_type <insecure | secure>\n\nMigration traffic is encrypted using an SSH tunnel by default. On secure, completely private networks this can be disabled to increase performance.\n\n--online <boolean>\n\nUse online/live migration if VM is running. Ignored if VM is stopped.\n\n--targetstorage <string>\n\nMapping from source to target storages. Providing only a single storage ID maps all source storages to that storage. Providing the special value 1 will map each source storage to itself.\n\n--with-local-disks <boolean>\n\nEnable live storage migration for local disk\n\nqm monitor <vmid>\n\nEnter QEMU Monitor interface.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm move-disk\n\nAn alias for qm disk move.\n\nqm move_disk\n\nAn alias for qm disk move.\n\nqm mtunnel\n\nUsed by qmigrate - do not use manually.\n\nqm nbdstop <vmid>\n\nStop embedded nbd server.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm pending <vmid>\n\nGet the virtual machine configuration with both current and pending values.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm reboot <vmid> [OPTIONS]\n\nReboot the VM by shutting it down, and starting it again. Applies pending changes.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--timeout <integer> (0 - N)\n\nWait maximal timeout seconds for the shutdown.\n\nqm remote-migrate <vmid> [<target-vmid>] <target-endpoint> --target-bridge <string> --target-storage <string> [OPTIONS]\n\nMigrate virtual machine to a remote cluster. Creates a new migration task. EXPERIMENTAL feature!\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target-vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target-endpoint>: apitoken=<PVEAPIToken=user@realm!token=SECRET> ,host=<ADDRESS> [,fingerprint=<FINGERPRINT>] [,port=<PORT>]\n\nRemote target endpoint\n\n--bwlimit <integer> (0 - N) (default = migrate limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--delete <boolean> (default = 0)\n\nDelete the original VM and related data after successful migration. By default the original VM is kept on the source cluster in a stopped state.\n\n--online <boolean>\n\nUse online/live migration if VM is running. Ignored if VM is stopped.\n\n--target-bridge <string>\n\nMapping from source to target bridges. Providing only a single bridge ID maps all source bridges to that bridge. Providing the special value 1 will map each source bridge to itself.\n\n--target-storage <string>\n\nMapping from source to target storages. Providing only a single storage ID maps all source storages to that storage. Providing the special value 1 will map each source storage to itself.\n\nqm rescan\n\nAn alias for qm disk rescan.\n\nqm reset <vmid> [OPTIONS]\n\nReset virtual machine.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\nqm resize\n\nAn alias for qm disk resize.\n\nqm resume <vmid> [OPTIONS]\n\nResume virtual machine.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--nocheck <boolean>\n\nno description available\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\nqm rollback <vmid> <snapname> [OPTIONS]\n\nRollback VM state to specified snapshot.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<snapname>: <string>\n\nThe name of the snapshot.\n\n--start <boolean> (default = 0)\n\nWhether the VM should get started after rolling back successfully. (Note: VMs will be automatically started if the snapshot includes RAM.)\n\nqm sendkey <vmid> <key> [OPTIONS]\n\nSend key event to virtual machine.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<key>: <string>\n\nThe key (qemu monitor encoding).\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\nqm set <vmid> [OPTIONS]\n\nSet virtual machine options (synchrounous API) - You should consider using the POST method instead for any actions involving hotplug or storage allocation.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--acpi <boolean> (default = 1)\n\nEnable/disable ACPI.\n\n--affinity <string>\n\nList of host cores used to execute guest processes, for example: 0,5,8-11\n\n--agent [enabled=]<1|0> [,freeze-fs-on-backup=<1|0>] [,fstrim_cloned_disks=<1|0>] [,type=<virtio|isa>]\n\nEnable/disable communication with the QEMU Guest Agent and its properties.\n\n--arch <aarch64 | x86_64>\n\nVirtual processor architecture. Defaults to the host.\n\n--args <string>\n\nArbitrary arguments passed to kvm.\n\n--audio0 device=<ich9-intel-hda|intel-hda|AC97> [,driver=<spice|none>]\n\nConfigure a audio device, useful in combination with QXL/Spice.\n\n--autostart <boolean> (default = 0)\n\nAutomatic restart after crash (currently ignored).\n\n--balloon <integer> (0 - N)\n\nAmount of target RAM for the VM in MiB. Using zero disables the ballon driver.\n\n--bios <ovmf | seabios> (default = seabios)\n\nSelect BIOS implementation.\n\n--boot [[legacy=]<[acdn]{1,4}>] [,order=<device[;device...]>]\n\nSpecify guest boot order. Use the order= sub-property as usage with no key or legacy= is deprecated.\n\n--bootdisk (ide|sata|scsi|virtio)\\d+\n\nEnable booting from specified disk. Deprecated: Use boot: order=foo;bar instead.\n\n--cdrom <volume>\n\nThis is an alias for option -ide2\n\n--cicustom [meta=<volume>] [,network=<volume>] [,user=<volume>] [,vendor=<volume>]\n\ncloud-init: Specify custom files to replace the automatically generated ones at start.\n\n--cipassword <password>\n\ncloud-init: Password to assign the user. Using this is generally not recommended. Use ssh keys instead. Also note that older cloud-init versions do not support hashed passwords.\n\n--citype <configdrive2 | nocloud | opennebula>\n\nSpecifies the cloud-init configuration format. The default depends on the configured operating system type (ostype. We use the nocloud format for Linux, and configdrive2 for windows.\n\n--ciupgrade <boolean> (default = 1)\n\ncloud-init: do an automatic package upgrade after the first boot.\n\n--ciuser <string>\n\ncloud-init: User name to change ssh keys and password for instead of the image’s configured default user.\n\n--cores <integer> (1 - N) (default = 1)\n\nThe number of cores per socket.\n\n--cpu [[cputype=]<string>] [,flags=<+FLAG[;-FLAG...]>] [,hidden=<1|0>] [,hv-vendor-id=<vendor-id>] [,phys-bits=<8-64|host>] [,reported-model=<enum>]\n\nEmulated CPU type.\n\n--cpulimit <number> (0 - 128) (default = 0)\n\nLimit of CPU usage.\n\n--cpuunits <integer> (1 - 262144) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a VM, will be clamped to [1, 10000] in cgroup v2.\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--description <string>\n\nDescription for the VM. Shown in the web-interface VM’s summary. This is saved as comment inside the configuration file.\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1 digest. This can be used to prevent concurrent modifications.\n\n--efidisk0 [file=]<volume> [,efitype=<2m|4m>] [,format=<enum>] [,import-from=<source volume>] [,pre-enrolled-keys=<1|0>] [,size=<DiskSize>]\n\nConfigure a disk for storing EFI vars. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Note that SIZE_IN_GiB is ignored here and that the default EFI vars are copied to the volume instead. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--force <boolean>\n\nForce physical removal. Without this, we simple remove the disk from the config file and create an additional configuration entry called unused[n], which contains the volume ID. Unlink of unused[n] always cause physical removal.\n\n\tRequires option(s): delete\n--freeze <boolean>\n\nFreeze CPU at startup (use c monitor command to start execution).\n\n--hookscript <string>\n\nScript that will be executed during various steps in the vms lifetime.\n\n--hostpci[n] [[host=]<HOSTPCIID[;HOSTPCIID2...]>] [,device-id=<hex id>] [,legacy-igd=<1|0>] [,mapping=<mapping-id>] [,mdev=<string>] [,pcie=<1|0>] [,rombar=<1|0>] [,romfile=<string>] [,sub-device-id=<hex id>] [,sub-vendor-id=<hex id>] [,vendor-id=<hex id>] [,x-vga=<1|0>]\n\nMap host PCI devices into guest.\n\n--hotplug <string> (default = network,disk,usb)\n\nSelectively enable hotplug features. This is a comma separated list of hotplug features: network, disk, cpu, memory, usb and cloudinit. Use 0 to disable hotplug completely. Using 1 as value is an alias for the default network,disk,usb. USB hotplugging is possible for guests with machine version >= 7.1 and ostype l26 or windows > 7.\n\n--hugepages <1024 | 2 | any>\n\nEnable/disable hugepages memory.\n\n--ide[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,model=<model>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as IDE hard disk or CD-ROM (n is 0 to 3). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--ipconfig[n] [gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,ip=<IPv4Format/CIDR>] [,ip6=<IPv6Format/CIDR>]\n\ncloud-init: Specify IP addresses and gateways for the corresponding interface.\n\nIP addresses use CIDR notation, gateways are optional but need an IP of the same type specified.\n\nThe special string dhcp can be used for IP addresses to use DHCP, in which case no explicit gateway should be provided. For IPv6 the special string auto can be used to use stateless autoconfiguration. This requires cloud-init 19.4 or newer.\n\nIf cloud-init is enabled and neither an IPv4 nor an IPv6 address is specified, it defaults to using dhcp on IPv4.\n\n--ivshmem size=<integer> [,name=<string>]\n\nInter-VM shared memory. Useful for direct communication between VMs, or to the host.\n\n--keephugepages <boolean> (default = 0)\n\nUse together with hugepages. If enabled, hugepages will not not be deleted after VM shutdown and can be used for subsequent starts.\n\n--keyboard <da | de | de-ch | en-gb | en-us | es | fi | fr | fr-be | fr-ca | fr-ch | hu | is | it | ja | lt | mk | nl | no | pl | pt | pt-br | sl | sv | tr>\n\nKeyboard layout for VNC server. This option is generally not required and is often better handled from within the guest OS.\n\n--kvm <boolean> (default = 1)\n\nEnable/disable KVM hardware virtualization.\n\n--localtime <boolean>\n\nSet the real time clock (RTC) to local time. This is enabled by default if the ostype indicates a Microsoft Windows OS.\n\n--lock <backup | clone | create | migrate | rollback | snapshot | snapshot-delete | suspended | suspending>\n\nLock/unlock the VM.\n\n--machine [[type=]<machine type>] [,viommu=<intel|virtio>]\n\nSpecify the QEMU machine.\n\n--memory [current=]<integer>\n\nMemory properties.\n\n--migrate_downtime <number> (0 - N) (default = 0.1)\n\nSet maximum tolerated downtime (in seconds) for migrations.\n\n--migrate_speed <integer> (0 - N) (default = 0)\n\nSet maximum speed (in MB/s) for migrations. Value 0 is no limit.\n\n--name <string>\n\nSet a name for the VM. Only used on the configuration web interface.\n\n--nameserver <string>\n\ncloud-init: Sets DNS server IP address for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\n--net[n] [model=]<enum> [,bridge=<bridge>] [,firewall=<1|0>] [,link_down=<1|0>] [,macaddr=<XX:XX:XX:XX:XX:XX>] [,mtu=<integer>] [,queues=<integer>] [,rate=<number>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,<model>=<macaddr>]\n\nSpecify network devices.\n\n--numa <boolean> (default = 0)\n\nEnable/disable NUMA.\n\n--numa[n] cpus=<id[-id];...> [,hostnodes=<id[-id];...>] [,memory=<number>] [,policy=<preferred|bind|interleave>]\n\nNUMA topology.\n\n--onboot <boolean> (default = 0)\n\nSpecifies whether a VM will be started during system bootup.\n\n--ostype <l24 | l26 | other | solaris | w2k | w2k3 | w2k8 | win10 | win11 | win7 | win8 | wvista | wxp>\n\nSpecify guest operating system.\n\n--parallel[n] /dev/parport\\d+|/dev/usb/lp\\d+\n\nMap host parallel devices (n is 0 to 2).\n\n--protection <boolean> (default = 0)\n\nSets the protection flag of the VM. This will disable the remove VM and remove disk operations.\n\n--reboot <boolean> (default = 1)\n\nAllow reboot. If set to 0 the VM exit on reboot.\n\n--revert <string>\n\nRevert a pending change.\n\n--rng0 [source=]</dev/urandom|/dev/random|/dev/hwrng> [,max_bytes=<integer>] [,period=<integer>]\n\nConfigure a VirtIO-based Random Number Generator.\n\n--sata[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SATA hard disk or CD-ROM (n is 0 to 5). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--scsi[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,product=<product>] [,queues=<integer>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,scsiblock=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,ssd=<1|0>] [,trans=<none|lba|auto>] [,vendor=<vendor>] [,werror=<enum>] [,wwn=<wwn>]\n\nUse volume as SCSI hard disk or CD-ROM (n is 0 to 30). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--scsihw <lsi | lsi53c810 | megasas | pvscsi | virtio-scsi-pci | virtio-scsi-single> (default = lsi)\n\nSCSI controller model\n\n--searchdomain <string>\n\ncloud-init: Sets DNS search domains for a container. Create will automatically use the setting from the host if neither searchdomain nor nameserver are set.\n\n--serial[n] (/dev/.+|socket)\n\nCreate a serial device inside the VM (n is 0 to 3)\n\n--shares <integer> (0 - 50000) (default = 1000)\n\nAmount of memory shares for auto-ballooning. The larger the number is, the more memory this VM gets. Number is relative to weights of all other running VMs. Using zero disables auto-ballooning. Auto-ballooning is done by pvestatd.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\n--smbios1 [base64=<1|0>] [,family=<Base64 encoded string>] [,manufacturer=<Base64 encoded string>] [,product=<Base64 encoded string>] [,serial=<Base64 encoded string>] [,sku=<Base64 encoded string>] [,uuid=<UUID>] [,version=<Base64 encoded string>]\n\nSpecify SMBIOS type 1 fields.\n\n--smp <integer> (1 - N) (default = 1)\n\nThe number of CPUs. Please use option -sockets instead.\n\n--sockets <integer> (1 - N) (default = 1)\n\nThe number of CPU sockets.\n\n--spice_enhancements [foldersharing=<1|0>] [,videostreaming=<off|all|filter>]\n\nConfigure additional enhancements for SPICE.\n\n--sshkeys <filepath>\n\ncloud-init: Setup public SSH keys (one key per line, OpenSSH format).\n\n--startdate (now | YYYY-MM-DD | YYYY-MM-DDTHH:MM:SS) (default = now)\n\nSet the initial date of the real time clock. Valid format for date are:'now' or 2006-06-17T16:01:21 or 2006-06-17.\n\n--startup `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\n--tablet <boolean> (default = 1)\n\nEnable/disable the USB tablet device.\n\n--tags <string>\n\nTags of the VM. This is only meta information.\n\n--tdf <boolean> (default = 0)\n\nEnable/disable time drift fix.\n\n--template <boolean> (default = 0)\n\nEnable/disable Template.\n\n--tpmstate0 [file=]<volume> [,import-from=<source volume>] [,size=<DiskSize>] [,version=<v1.2|v2.0>]\n\nConfigure a Disk for storing TPM state. The format is fixed to raw. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Note that SIZE_IN_GiB is ignored here and 4 MiB will be used instead. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--unused[n] [file=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\n--usb[n] [[host=]<HOSTUSBDEVICE|spice>] [,mapping=<mapping-id>] [,usb3=<1|0>]\n\nConfigure an USB device (n is 0 to 4, for machine version >= 7.1 and ostype l26 or windows > 7, n can be up to 14).\n\n--vcpus <integer> (1 - N) (default = 0)\n\nNumber of hotplugged vcpus.\n\n--vga [[type=]<enum>] [,clipboard=<vnc>] [,memory=<integer>]\n\nConfigure the VGA hardware.\n\n--virtio[n] [file=]<volume> [,aio=<native|threads|io_uring>] [,backup=<1|0>] [,bps=<bps>] [,bps_max_length=<seconds>] [,bps_rd=<bps>] [,bps_rd_max_length=<seconds>] [,bps_wr=<bps>] [,bps_wr_max_length=<seconds>] [,cache=<enum>] [,cyls=<integer>] [,detect_zeroes=<1|0>] [,discard=<ignore|on>] [,format=<enum>] [,heads=<integer>] [,import-from=<source volume>] [,iops=<iops>] [,iops_max=<iops>] [,iops_max_length=<seconds>] [,iops_rd=<iops>] [,iops_rd_max=<iops>] [,iops_rd_max_length=<seconds>] [,iops_wr=<iops>] [,iops_wr_max=<iops>] [,iops_wr_max_length=<seconds>] [,iothread=<1|0>] [,mbps=<mbps>] [,mbps_max=<mbps>] [,mbps_rd=<mbps>] [,mbps_rd_max=<mbps>] [,mbps_wr=<mbps>] [,mbps_wr_max=<mbps>] [,media=<cdrom|disk>] [,replicate=<1|0>] [,rerror=<ignore|report|stop>] [,ro=<1|0>] [,secs=<integer>] [,serial=<serial>] [,shared=<1|0>] [,size=<DiskSize>] [,snapshot=<1|0>] [,trans=<none|lba|auto>] [,werror=<enum>]\n\nUse volume as VIRTIO hard disk (n is 0 to 15). Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume. Use STORAGE_ID:0 and the import-from parameter to import from an existing volume.\n\n--vmgenid <UUID> (default = 1 (autogenerated))\n\nSet VM Generation ID. Use 1 to autogenerate on create or update, pass 0 to disable explicitly.\n\n--vmstatestorage <storage ID>\n\nDefault storage for VM state volumes/files.\n\n--watchdog [[model=]<i6300esb|ib700>] [,action=<enum>]\n\nCreate a virtual hardware watchdog device.\n\nqm showcmd <vmid> [OPTIONS]\n\nShow command line which is used to start the VM (debug info).\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--pretty <boolean> (default = 0)\n\nPuts each option on a new line to enhance human readability\n\n--snapshot <string>\n\nFetch config values from given snapshot.\n\nqm shutdown <vmid> [OPTIONS]\n\nShutdown virtual machine. This is similar to pressing the power button on a physical machine. This will send an ACPI event for the guest OS, which should then proceed to a clean shutdown.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--forceStop <boolean> (default = 0)\n\nMake sure the VM stops.\n\n--keepActive <boolean> (default = 0)\n\nDo not deactivate storage volumes.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\n--timeout <integer> (0 - N)\n\nWait maximal timeout seconds.\n\nqm snapshot <vmid> <snapname> [OPTIONS]\n\nSnapshot a VM.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<snapname>: <string>\n\nThe name of the snapshot.\n\n--description <string>\n\nA textual description or comment.\n\n--vmstate <boolean>\n\nSave the vmstate\n\nqm start <vmid> [OPTIONS]\n\nStart virtual machine.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--force-cpu <string>\n\nOverride QEMU’s -cpu argument with the given string.\n\n--machine [[type=]<machine type>] [,viommu=<intel|virtio>]\n\nSpecify the QEMU machine.\n\n--migratedfrom <string>\n\nThe cluster node name.\n\n--migration_network <string>\n\nCIDR of the (sub) network that is used for migration.\n\n--migration_type <insecure | secure>\n\nMigration traffic is encrypted using an SSH tunnel by default. On secure, completely private networks this can be disabled to increase performance.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\n--stateuri <string>\n\nSome command save/restore state from this location.\n\n--targetstorage <string>\n\nMapping from source to target storages. Providing only a single storage ID maps all source storages to that storage. Providing the special value 1 will map each source storage to itself.\n\n--timeout <integer> (0 - N) (default = max(30, vm memory in GiB))\n\nWait maximal timeout seconds.\n\nqm status <vmid> [OPTIONS]\n\nShow VM status.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--verbose <boolean>\n\nVerbose output format\n\nqm stop <vmid> [OPTIONS]\n\nStop virtual machine. The qemu process will exit immediately. This is akin to pulling the power plug of a running computer and may damage the VM data.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--keepActive <boolean> (default = 0)\n\nDo not deactivate storage volumes.\n\n--migratedfrom <string>\n\nThe cluster node name.\n\n--overrule-shutdown <boolean> (default = 0)\n\nTry to abort active qmshutdown tasks before stopping.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\n--timeout <integer> (0 - N)\n\nWait maximal timeout seconds.\n\nqm suspend <vmid> [OPTIONS]\n\nSuspend virtual machine.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\n--statestorage <storage ID>\n\nThe storage for the VM state\n\n\tRequires option(s): todisk\n--todisk <boolean> (default = 0)\n\nIf set, suspends the VM to disk. Will be resumed on next VM start.\n\nqm template <vmid> [OPTIONS]\n\nCreate a Template.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--disk <efidisk0 | ide0 | ide1 | ide2 | ide3 | sata0 | sata1 | sata2 | sata3 | sata4 | sata5 | scsi0 | scsi1 | scsi10 | scsi11 | scsi12 | scsi13 | scsi14 | scsi15 | scsi16 | scsi17 | scsi18 | scsi19 | scsi2 | scsi20 | scsi21 | scsi22 | scsi23 | scsi24 | scsi25 | scsi26 | scsi27 | scsi28 | scsi29 | scsi3 | scsi30 | scsi4 | scsi5 | scsi6 | scsi7 | scsi8 | scsi9 | tpmstate0 | virtio0 | virtio1 | virtio10 | virtio11 | virtio12 | virtio13 | virtio14 | virtio15 | virtio2 | virtio3 | virtio4 | virtio5 | virtio6 | virtio7 | virtio8 | virtio9>\n\nIf you want to convert only 1 disk to base image.\n\nqm terminal <vmid> [OPTIONS]\n\nOpen a terminal using a serial device (The VM need to have a serial device configured, for example serial0: socket)\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--escape <string> (default = ^O)\n\nEscape character.\n\n--iface <serial0 | serial1 | serial2 | serial3>\n\nSelect the serial device. By default we simply use the first suitable device.\n\nqm unlink\n\nAn alias for qm disk unlink.\n\nqm unlock <vmid>\n\nUnlock the VM.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm vncproxy <vmid>\n\nProxy VM VNC traffic to stdin/stdout\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\nqm wait <vmid> [OPTIONS]\n\nWait until the VM is stopped.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--timeout <integer> (1 - N)\n\nTimeout in seconds. Default is to wait forever.\n\n22.9. qmrestore - Restore QemuServer vzdump Backups\n\nqmrestore help\n\nqmrestore <archive> <vmid> [OPTIONS]\n\nRestore QemuServer vzdump backups.\n\n<archive>: <string>\n\nThe backup file. You can pass - to read from standard input.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--bwlimit <number> (0 - N)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--force <boolean>\n\nAllow to overwrite existing VM.\n\n--live-restore <boolean>\n\nStart the VM immediately from the backup and restore in background. PBS only.\n\n--pool <string>\n\nAdd the VM to the specified pool.\n\n--storage <storage ID>\n\nDefault storage.\n\n--unique <boolean>\n\nAssign a unique random ethernet address.\n\n22.10. pct - Proxmox Container Toolkit\n\npct <COMMAND> [ARGS] [OPTIONS]\n\npct clone <vmid> <newid> [OPTIONS]\n\nCreate a container clone/copy\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<newid>: <integer> (100 - 999999999)\n\nVMID for the clone.\n\n--bwlimit <number> (0 - N) (default = clone limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--description <string>\n\nDescription for the new CT.\n\n--full <boolean>\n\nCreate a full copy of all disks. This is always done when you clone a normal CT. For CT templates, we try to create a linked clone by default.\n\n--hostname <string>\n\nSet a hostname for the new CT.\n\n--pool <string>\n\nAdd the new CT to the specified pool.\n\n--snapname <string>\n\nThe name of the snapshot.\n\n--storage <storage ID>\n\nTarget storage for full clone.\n\n--target <string>\n\nTarget node. Only allowed if the original VM is on shared storage.\n\npct config <vmid> [OPTIONS]\n\nGet container configuration.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--current <boolean> (default = 0)\n\nGet current values (instead of pending values).\n\n--snapshot <string>\n\nFetch config values from given snapshot.\n\npct console <vmid> [OPTIONS]\n\nLaunch a console for the specified container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--escape \\^?[a-z] (default = ^a)\n\nEscape sequence prefix. For example to use <Ctrl+b q> as the escape sequence pass ^b.\n\npct cpusets\n\nPrint the list of assigned CPU sets.\n\npct create <vmid> <ostemplate> [OPTIONS]\n\nCreate or restore a container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<ostemplate>: <string>\n\nThe OS template or backup file.\n\n--arch <amd64 | arm64 | armhf | i386 | riscv32 | riscv64> (default = amd64)\n\nOS architecture type.\n\n--bwlimit <number> (0 - N) (default = restore limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--cmode <console | shell | tty> (default = tty)\n\nConsole mode. By default, the console command tries to open a connection to one of the available tty devices. By setting cmode to console it tries to attach to /dev/console instead. If you set cmode to shell, it simply invokes a shell inside the container (no login).\n\n--console <boolean> (default = 1)\n\nAttach a console device (/dev/console) to the container.\n\n--cores <integer> (1 - 8192)\n\nThe number of cores assigned to the container. A container can use all available cores by default.\n\n--cpulimit <number> (0 - 8192) (default = 0)\n\nLimit of CPU usage.\n\n\tIf the computer has 2 CPUs, it has a total of 2 CPU time. Value 0 indicates no CPU limit.\n--cpuunits <integer> (0 - 500000) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a container, will be clamped to [1, 10000] in cgroup v2.\n\n--debug <boolean> (default = 0)\n\nTry to be more verbose. For now this only enables debug log-level on start.\n\n--description <string>\n\nDescription for the Container. Shown in the web-interface CT’s summary. This is saved as comment inside the configuration file.\n\n--dev[n] [[path=]<Path>] [,gid=<integer>] [,mode=<Octal access mode>] [,uid=<integer>]\n\nDevice to pass through to the container\n\n--features [force_rw_sys=<1|0>] [,fuse=<1|0>] [,keyctl=<1|0>] [,mknod=<1|0>] [,mount=<fstype;fstype;...>] [,nesting=<1|0>]\n\nAllow containers access to advanced features.\n\n--force <boolean>\n\nAllow to overwrite existing container.\n\n--hookscript <string>\n\nScript that will be exectued during various steps in the containers lifetime.\n\n--hostname <string>\n\nSet a host name for the container.\n\n--ignore-unpack-errors <boolean>\n\nIgnore errors when extracting the template.\n\n--lock <backup | create | destroyed | disk | fstrim | migrate | mounted | rollback | snapshot | snapshot-delete>\n\nLock/unlock the container.\n\n--memory <integer> (16 - N) (default = 512)\n\nAmount of RAM for the container in MB.\n\n--mp[n] [volume=]<volume> ,mp=<Path> [,acl=<1|0>] [,backup=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container mount point. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume.\n\n--nameserver <string>\n\nSets DNS server IP address for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\n--net[n] name=<string> [,bridge=<bridge>] [,firewall=<1|0>] [,gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,hwaddr=<XX:XX:XX:XX:XX:XX>] [,ip=<(IPv4/CIDR|dhcp|manual)>] [,ip6=<(IPv6/CIDR|auto|dhcp|manual)>] [,link_down=<1|0>] [,mtu=<integer>] [,rate=<mbps>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,type=<veth>]\n\nSpecifies network interfaces for the container.\n\n--onboot <boolean> (default = 0)\n\nSpecifies whether a container will be started during system bootup.\n\n--ostype <alpine | archlinux | centos | debian | devuan | fedora | gentoo | nixos | opensuse | ubuntu | unmanaged>\n\nOS type. This is used to setup configuration inside the container, and corresponds to lxc setup scripts in /usr/share/lxc/config/<ostype>.common.conf. Value unmanaged can be used to skip and OS specific setup.\n\n--password <password>\n\nSets root password inside container.\n\n--pool <string>\n\nAdd the VM to the specified pool.\n\n--protection <boolean> (default = 0)\n\nSets the protection flag of the container. This will prevent the CT or CT’s disk remove/update operation.\n\n--restore <boolean>\n\nMark this as restore task.\n\n--rootfs [volume=]<volume> [,acl=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container root.\n\n--searchdomain <string>\n\nSets DNS search domains for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\n--ssh-public-keys <filepath>\n\nSetup public SSH keys (one key per line, OpenSSH format).\n\n--start <boolean> (default = 0)\n\nStart the CT after its creation finished successfully.\n\n--startup `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\n--storage <storage ID> (default = local)\n\nDefault Storage.\n\n--swap <integer> (0 - N) (default = 512)\n\nAmount of SWAP for the container in MB.\n\n--tags <string>\n\nTags of the Container. This is only meta information.\n\n--template <boolean> (default = 0)\n\nEnable/disable Template.\n\n--timezone <string>\n\nTime zone to use in the container. If option isn’t set, then nothing will be done. Can be set to host to match the host time zone, or an arbitrary time zone option from /usr/share/zoneinfo/zone.tab\n\n--tty <integer> (0 - 6) (default = 2)\n\nSpecify the number of tty available to the container\n\n--unique <boolean>\n\nAssign a unique random ethernet address.\n\n\tRequires option(s): restore\n--unprivileged <boolean> (default = 0)\n\nMakes the container run as unprivileged user. (Should not be modified manually.)\n\n--unused[n] [volume=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\npct delsnapshot <vmid> <snapname> [OPTIONS]\n\nDelete a LXC snapshot.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<snapname>: <string>\n\nThe name of the snapshot.\n\n--force <boolean>\n\nFor removal from config file, even if removing disk snapshots fails.\n\npct destroy <vmid> [OPTIONS]\n\nDestroy the container (also delete all uses files).\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--destroy-unreferenced-disks <boolean>\n\nIf set, destroy additionally all disks with the VMID from all enabled storages which are not referenced in the config.\n\n--force <boolean> (default = 0)\n\nForce destroy, even if running.\n\n--purge <boolean> (default = 0)\n\nRemove container from all related configurations. For example, backup jobs, replication jobs or HA. Related ACLs and Firewall entries will always be removed.\n\npct df <vmid>\n\nGet the container’s current disk usage.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct enter <vmid> [OPTIONS]\n\nLaunch a shell for the specified container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--keep-env <boolean> (default = 1)\n\nKeep the current environment. This option will disabled by default with PVE 9. If you rely on a preserved environment, please use this option to be future-proof.\n\npct exec <vmid> [<extra-args>] [OPTIONS]\n\nLaunch a command inside the specified container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<extra-args>: <array>\n\nExtra arguments as array\n\n--keep-env <boolean> (default = 1)\n\nKeep the current environment. This option will disabled by default with PVE 9. If you rely on a preserved environment, please use this option to be future-proof.\n\npct fsck <vmid> [OPTIONS]\n\nRun a filesystem check (fsck) on a container volume.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--device <mp0 | mp1 | mp10 | mp100 | mp101 | mp102 | mp103 | mp104 | mp105 | mp106 | mp107 | mp108 | mp109 | mp11 | mp110 | mp111 | mp112 | mp113 | mp114 | mp115 | mp116 | mp117 | mp118 | mp119 | mp12 | mp120 | mp121 | mp122 | mp123 | mp124 | mp125 | mp126 | mp127 | mp128 | mp129 | mp13 | mp130 | mp131 | mp132 | mp133 | mp134 | mp135 | mp136 | mp137 | mp138 | mp139 | mp14 | mp140 | mp141 | mp142 | mp143 | mp144 | mp145 | mp146 | mp147 | mp148 | mp149 | mp15 | mp150 | mp151 | mp152 | mp153 | mp154 | mp155 | mp156 | mp157 | mp158 | mp159 | mp16 | mp160 | mp161 | mp162 | mp163 | mp164 | mp165 | mp166 | mp167 | mp168 | mp169 | mp17 | mp170 | mp171 | mp172 | mp173 | mp174 | mp175 | mp176 | mp177 | mp178 | mp179 | mp18 | mp180 | mp181 | mp182 | mp183 | mp184 | mp185 | mp186 | mp187 | mp188 | mp189 | mp19 | mp190 | mp191 | mp192 | mp193 | mp194 | mp195 | mp196 | mp197 | mp198 | mp199 | mp2 | mp20 | mp200 | mp201 | mp202 | mp203 | mp204 | mp205 | mp206 | mp207 | mp208 | mp209 | mp21 | mp210 | mp211 | mp212 | mp213 | mp214 | mp215 | mp216 | mp217 | mp218 | mp219 | mp22 | mp220 | mp221 | mp222 | mp223 | mp224 | mp225 | mp226 | mp227 | mp228 | mp229 | mp23 | mp230 | mp231 | mp232 | mp233 | mp234 | mp235 | mp236 | mp237 | mp238 | mp239 | mp24 | mp240 | mp241 | mp242 | mp243 | mp244 | mp245 | mp246 | mp247 | mp248 | mp249 | mp25 | mp250 | mp251 | mp252 | mp253 | mp254 | mp255 | mp26 | mp27 | mp28 | mp29 | mp3 | mp30 | mp31 | mp32 | mp33 | mp34 | mp35 | mp36 | mp37 | mp38 | mp39 | mp4 | mp40 | mp41 | mp42 | mp43 | mp44 | mp45 | mp46 | mp47 | mp48 | mp49 | mp5 | mp50 | mp51 | mp52 | mp53 | mp54 | mp55 | mp56 | mp57 | mp58 | mp59 | mp6 | mp60 | mp61 | mp62 | mp63 | mp64 | mp65 | mp66 | mp67 | mp68 | mp69 | mp7 | mp70 | mp71 | mp72 | mp73 | mp74 | mp75 | mp76 | mp77 | mp78 | mp79 | mp8 | mp80 | mp81 | mp82 | mp83 | mp84 | mp85 | mp86 | mp87 | mp88 | mp89 | mp9 | mp90 | mp91 | mp92 | mp93 | mp94 | mp95 | mp96 | mp97 | mp98 | mp99 | rootfs>\n\nA volume on which to run the filesystem check\n\n--force <boolean> (default = 0)\n\nForce checking, even if the filesystem seems clean\n\npct fstrim <vmid> [OPTIONS]\n\nRun fstrim on a chosen CT and its mountpoints, except bind or read-only mountpoints.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--ignore-mountpoints <boolean>\n\nSkip all mountpoints, only do fstrim on the container root.\n\npct help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npct list\n\nLXC container index (per node).\n\npct listsnapshot <vmid>\n\nList all snapshots.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct migrate <vmid> <target> [OPTIONS]\n\nMigrate the container to another node. Creates a new migration task.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target>: <string>\n\nTarget node.\n\n--bwlimit <number> (0 - N) (default = migrate limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--online <boolean>\n\nUse online/live migration.\n\n--restart <boolean>\n\nUse restart migration\n\n--target-storage <string>\n\nMapping from source to target storages. Providing only a single storage ID maps all source storages to that storage. Providing the special value 1 will map each source storage to itself.\n\n--timeout <integer> (default = 180)\n\nTimeout in seconds for shutdown for restart migration\n\npct mount <vmid>\n\nMount the container’s filesystem on the host. This will hold a lock on the container and is meant for emergency maintenance only as it will prevent further operations on the container other than start and stop.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct move-volume <vmid> <volume> [<storage>] [<target-vmid>] [<target-volume>] [OPTIONS]\n\nMove a rootfs-/mp-volume to a different storage or to a different container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<volume>: <mp0 | mp1 | mp10 | mp100 | mp101 | mp102 | mp103 | mp104 | mp105 | mp106 | mp107 | mp108 | mp109 | mp11 | mp110 | mp111 | mp112 | mp113 | mp114 | mp115 | mp116 | mp117 | mp118 | mp119 | mp12 | mp120 | mp121 | mp122 | mp123 | mp124 | mp125 | mp126 | mp127 | mp128 | mp129 | mp13 | mp130 | mp131 | mp132 | mp133 | mp134 | mp135 | mp136 | mp137 | mp138 | mp139 | mp14 | mp140 | mp141 | mp142 | mp143 | mp144 | mp145 | mp146 | mp147 | mp148 | mp149 | mp15 | mp150 | mp151 | mp152 | mp153 | mp154 | mp155 | mp156 | mp157 | mp158 | mp159 | mp16 | mp160 | mp161 | mp162 | mp163 | mp164 | mp165 | mp166 | mp167 | mp168 | mp169 | mp17 | mp170 | mp171 | mp172 | mp173 | mp174 | mp175 | mp176 | mp177 | mp178 | mp179 | mp18 | mp180 | mp181 | mp182 | mp183 | mp184 | mp185 | mp186 | mp187 | mp188 | mp189 | mp19 | mp190 | mp191 | mp192 | mp193 | mp194 | mp195 | mp196 | mp197 | mp198 | mp199 | mp2 | mp20 | mp200 | mp201 | mp202 | mp203 | mp204 | mp205 | mp206 | mp207 | mp208 | mp209 | mp21 | mp210 | mp211 | mp212 | mp213 | mp214 | mp215 | mp216 | mp217 | mp218 | mp219 | mp22 | mp220 | mp221 | mp222 | mp223 | mp224 | mp225 | mp226 | mp227 | mp228 | mp229 | mp23 | mp230 | mp231 | mp232 | mp233 | mp234 | mp235 | mp236 | mp237 | mp238 | mp239 | mp24 | mp240 | mp241 | mp242 | mp243 | mp244 | mp245 | mp246 | mp247 | mp248 | mp249 | mp25 | mp250 | mp251 | mp252 | mp253 | mp254 | mp255 | mp26 | mp27 | mp28 | mp29 | mp3 | mp30 | mp31 | mp32 | mp33 | mp34 | mp35 | mp36 | mp37 | mp38 | mp39 | mp4 | mp40 | mp41 | mp42 | mp43 | mp44 | mp45 | mp46 | mp47 | mp48 | mp49 | mp5 | mp50 | mp51 | mp52 | mp53 | mp54 | mp55 | mp56 | mp57 | mp58 | mp59 | mp6 | mp60 | mp61 | mp62 | mp63 | mp64 | mp65 | mp66 | mp67 | mp68 | mp69 | mp7 | mp70 | mp71 | mp72 | mp73 | mp74 | mp75 | mp76 | mp77 | mp78 | mp79 | mp8 | mp80 | mp81 | mp82 | mp83 | mp84 | mp85 | mp86 | mp87 | mp88 | mp89 | mp9 | mp90 | mp91 | mp92 | mp93 | mp94 | mp95 | mp96 | mp97 | mp98 | mp99 | rootfs | unused0 | unused1 | unused10 | unused100 | unused101 | unused102 | unused103 | unused104 | unused105 | unused106 | unused107 | unused108 | unused109 | unused11 | unused110 | unused111 | unused112 | unused113 | unused114 | unused115 | unused116 | unused117 | unused118 | unused119 | unused12 | unused120 | unused121 | unused122 | unused123 | unused124 | unused125 | unused126 | unused127 | unused128 | unused129 | unused13 | unused130 | unused131 | unused132 | unused133 | unused134 | unused135 | unused136 | unused137 | unused138 | unused139 | unused14 | unused140 | unused141 | unused142 | unused143 | unused144 | unused145 | unused146 | unused147 | unused148 | unused149 | unused15 | unused150 | unused151 | unused152 | unused153 | unused154 | unused155 | unused156 | unused157 | unused158 | unused159 | unused16 | unused160 | unused161 | unused162 | unused163 | unused164 | unused165 | unused166 | unused167 | unused168 | unused169 | unused17 | unused170 | unused171 | unused172 | unused173 | unused174 | unused175 | unused176 | unused177 | unused178 | unused179 | unused18 | unused180 | unused181 | unused182 | unused183 | unused184 | unused185 | unused186 | unused187 | unused188 | unused189 | unused19 | unused190 | unused191 | unused192 | unused193 | unused194 | unused195 | unused196 | unused197 | unused198 | unused199 | unused2 | unused20 | unused200 | unused201 | unused202 | unused203 | unused204 | unused205 | unused206 | unused207 | unused208 | unused209 | unused21 | unused210 | unused211 | unused212 | unused213 | unused214 | unused215 | unused216 | unused217 | unused218 | unused219 | unused22 | unused220 | unused221 | unused222 | unused223 | unused224 | unused225 | unused226 | unused227 | unused228 | unused229 | unused23 | unused230 | unused231 | unused232 | unused233 | unused234 | unused235 | unused236 | unused237 | unused238 | unused239 | unused24 | unused240 | unused241 | unused242 | unused243 | unused244 | unused245 | unused246 | unused247 | unused248 | unused249 | unused25 | unused250 | unused251 | unused252 | unused253 | unused254 | unused255 | unused26 | unused27 | unused28 | unused29 | unused3 | unused30 | unused31 | unused32 | unused33 | unused34 | unused35 | unused36 | unused37 | unused38 | unused39 | unused4 | unused40 | unused41 | unused42 | unused43 | unused44 | unused45 | unused46 | unused47 | unused48 | unused49 | unused5 | unused50 | unused51 | unused52 | unused53 | unused54 | unused55 | unused56 | unused57 | unused58 | unused59 | unused6 | unused60 | unused61 | unused62 | unused63 | unused64 | unused65 | unused66 | unused67 | unused68 | unused69 | unused7 | unused70 | unused71 | unused72 | unused73 | unused74 | unused75 | unused76 | unused77 | unused78 | unused79 | unused8 | unused80 | unused81 | unused82 | unused83 | unused84 | unused85 | unused86 | unused87 | unused88 | unused89 | unused9 | unused90 | unused91 | unused92 | unused93 | unused94 | unused95 | unused96 | unused97 | unused98 | unused99>\n\nVolume which will be moved.\n\n<storage>: <storage ID>\n\nTarget Storage.\n\n<target-vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target-volume>: <mp0 | mp1 | mp10 | mp100 | mp101 | mp102 | mp103 | mp104 | mp105 | mp106 | mp107 | mp108 | mp109 | mp11 | mp110 | mp111 | mp112 | mp113 | mp114 | mp115 | mp116 | mp117 | mp118 | mp119 | mp12 | mp120 | mp121 | mp122 | mp123 | mp124 | mp125 | mp126 | mp127 | mp128 | mp129 | mp13 | mp130 | mp131 | mp132 | mp133 | mp134 | mp135 | mp136 | mp137 | mp138 | mp139 | mp14 | mp140 | mp141 | mp142 | mp143 | mp144 | mp145 | mp146 | mp147 | mp148 | mp149 | mp15 | mp150 | mp151 | mp152 | mp153 | mp154 | mp155 | mp156 | mp157 | mp158 | mp159 | mp16 | mp160 | mp161 | mp162 | mp163 | mp164 | mp165 | mp166 | mp167 | mp168 | mp169 | mp17 | mp170 | mp171 | mp172 | mp173 | mp174 | mp175 | mp176 | mp177 | mp178 | mp179 | mp18 | mp180 | mp181 | mp182 | mp183 | mp184 | mp185 | mp186 | mp187 | mp188 | mp189 | mp19 | mp190 | mp191 | mp192 | mp193 | mp194 | mp195 | mp196 | mp197 | mp198 | mp199 | mp2 | mp20 | mp200 | mp201 | mp202 | mp203 | mp204 | mp205 | mp206 | mp207 | mp208 | mp209 | mp21 | mp210 | mp211 | mp212 | mp213 | mp214 | mp215 | mp216 | mp217 | mp218 | mp219 | mp22 | mp220 | mp221 | mp222 | mp223 | mp224 | mp225 | mp226 | mp227 | mp228 | mp229 | mp23 | mp230 | mp231 | mp232 | mp233 | mp234 | mp235 | mp236 | mp237 | mp238 | mp239 | mp24 | mp240 | mp241 | mp242 | mp243 | mp244 | mp245 | mp246 | mp247 | mp248 | mp249 | mp25 | mp250 | mp251 | mp252 | mp253 | mp254 | mp255 | mp26 | mp27 | mp28 | mp29 | mp3 | mp30 | mp31 | mp32 | mp33 | mp34 | mp35 | mp36 | mp37 | mp38 | mp39 | mp4 | mp40 | mp41 | mp42 | mp43 | mp44 | mp45 | mp46 | mp47 | mp48 | mp49 | mp5 | mp50 | mp51 | mp52 | mp53 | mp54 | mp55 | mp56 | mp57 | mp58 | mp59 | mp6 | mp60 | mp61 | mp62 | mp63 | mp64 | mp65 | mp66 | mp67 | mp68 | mp69 | mp7 | mp70 | mp71 | mp72 | mp73 | mp74 | mp75 | mp76 | mp77 | mp78 | mp79 | mp8 | mp80 | mp81 | mp82 | mp83 | mp84 | mp85 | mp86 | mp87 | mp88 | mp89 | mp9 | mp90 | mp91 | mp92 | mp93 | mp94 | mp95 | mp96 | mp97 | mp98 | mp99 | rootfs | unused0 | unused1 | unused10 | unused100 | unused101 | unused102 | unused103 | unused104 | unused105 | unused106 | unused107 | unused108 | unused109 | unused11 | unused110 | unused111 | unused112 | unused113 | unused114 | unused115 | unused116 | unused117 | unused118 | unused119 | unused12 | unused120 | unused121 | unused122 | unused123 | unused124 | unused125 | unused126 | unused127 | unused128 | unused129 | unused13 | unused130 | unused131 | unused132 | unused133 | unused134 | unused135 | unused136 | unused137 | unused138 | unused139 | unused14 | unused140 | unused141 | unused142 | unused143 | unused144 | unused145 | unused146 | unused147 | unused148 | unused149 | unused15 | unused150 | unused151 | unused152 | unused153 | unused154 | unused155 | unused156 | unused157 | unused158 | unused159 | unused16 | unused160 | unused161 | unused162 | unused163 | unused164 | unused165 | unused166 | unused167 | unused168 | unused169 | unused17 | unused170 | unused171 | unused172 | unused173 | unused174 | unused175 | unused176 | unused177 | unused178 | unused179 | unused18 | unused180 | unused181 | unused182 | unused183 | unused184 | unused185 | unused186 | unused187 | unused188 | unused189 | unused19 | unused190 | unused191 | unused192 | unused193 | unused194 | unused195 | unused196 | unused197 | unused198 | unused199 | unused2 | unused20 | unused200 | unused201 | unused202 | unused203 | unused204 | unused205 | unused206 | unused207 | unused208 | unused209 | unused21 | unused210 | unused211 | unused212 | unused213 | unused214 | unused215 | unused216 | unused217 | unused218 | unused219 | unused22 | unused220 | unused221 | unused222 | unused223 | unused224 | unused225 | unused226 | unused227 | unused228 | unused229 | unused23 | unused230 | unused231 | unused232 | unused233 | unused234 | unused235 | unused236 | unused237 | unused238 | unused239 | unused24 | unused240 | unused241 | unused242 | unused243 | unused244 | unused245 | unused246 | unused247 | unused248 | unused249 | unused25 | unused250 | unused251 | unused252 | unused253 | unused254 | unused255 | unused26 | unused27 | unused28 | unused29 | unused3 | unused30 | unused31 | unused32 | unused33 | unused34 | unused35 | unused36 | unused37 | unused38 | unused39 | unused4 | unused40 | unused41 | unused42 | unused43 | unused44 | unused45 | unused46 | unused47 | unused48 | unused49 | unused5 | unused50 | unused51 | unused52 | unused53 | unused54 | unused55 | unused56 | unused57 | unused58 | unused59 | unused6 | unused60 | unused61 | unused62 | unused63 | unused64 | unused65 | unused66 | unused67 | unused68 | unused69 | unused7 | unused70 | unused71 | unused72 | unused73 | unused74 | unused75 | unused76 | unused77 | unused78 | unused79 | unused8 | unused80 | unused81 | unused82 | unused83 | unused84 | unused85 | unused86 | unused87 | unused88 | unused89 | unused9 | unused90 | unused91 | unused92 | unused93 | unused94 | unused95 | unused96 | unused97 | unused98 | unused99>\n\nThe config key the volume will be moved to. Default is the source volume key.\n\n--bwlimit <number> (0 - N) (default = clone limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--delete <boolean> (default = 0)\n\nDelete the original volume after successful copy. By default the original is kept as an unused volume entry.\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1 \" . \"digest. This can be used to prevent concurrent modifications.\n\n--target-digest <string>\n\nPrevent changes if current configuration file of the target \" . \"container has a different SHA1 digest. This can be used to prevent \" . \"concurrent modifications.\n\npct move_volume\n\nAn alias for pct move-volume.\n\npct pending <vmid>\n\nGet container configuration, including pending changes.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct pull <vmid> <path> <destination> [OPTIONS]\n\nCopy a file from the container to the local system.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<path>: <string>\n\nPath to a file inside the container to pull.\n\n<destination>: <string>\n\nDestination\n\n--group <string>\n\nOwner group name or id.\n\n--perms <string>\n\nFile permissions to use (octal by default, prefix with 0x for hexadecimal).\n\n--user <string>\n\nOwner user name or id.\n\npct push <vmid> <file> <destination> [OPTIONS]\n\nCopy a local file to the container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<file>: <string>\n\nPath to a local file.\n\n<destination>: <string>\n\nDestination inside the container to write to.\n\n--group <string>\n\nOwner group name or id. When using a name it must exist inside the container.\n\n--perms <string>\n\nFile permissions to use (octal by default, prefix with 0x for hexadecimal).\n\n--user <string>\n\nOwner user name or id. When using a name it must exist inside the container.\n\npct reboot <vmid> [OPTIONS]\n\nReboot the container by shutting it down, and starting it again. Applies pending changes.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--timeout <integer> (0 - N)\n\nWait maximal timeout seconds for the shutdown.\n\npct remote-migrate <vmid> [<target-vmid>] <target-endpoint> --target-bridge <string> --target-storage <string> [OPTIONS]\n\nMigrate container to a remote cluster. Creates a new migration task. EXPERIMENTAL feature!\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target-vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<target-endpoint>: apitoken=<PVEAPIToken=user@realm!token=SECRET> ,host=<ADDRESS> [,fingerprint=<FINGERPRINT>] [,port=<PORT>]\n\nRemote target endpoint\n\n--bwlimit <integer> (0 - N) (default = migrate limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--delete <boolean> (default = 0)\n\nDelete the original CT and related data after successful migration. By default the original CT is kept on the source cluster in a stopped state.\n\n--online <boolean>\n\nUse online/live migration.\n\n--restart <boolean>\n\nUse restart migration\n\n--target-bridge <string>\n\nMapping from source to target bridges. Providing only a single bridge ID maps all source bridges to that bridge. Providing the special value 1 will map each source bridge to itself.\n\n--target-storage <string>\n\nMapping from source to target storages. Providing only a single storage ID maps all source storages to that storage. Providing the special value 1 will map each source storage to itself.\n\n--timeout <integer> (default = 180)\n\nTimeout in seconds for shutdown for restart migration\n\npct rescan [OPTIONS]\n\nRescan all storages and update disk sizes and unused disk images.\n\n--dryrun <boolean> (default = 0)\n\nDo not actually write changes out to conifg.\n\n--vmid <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct resize <vmid> <disk> <size> [OPTIONS]\n\nResize a container mount point.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<disk>: <mp0 | mp1 | mp10 | mp100 | mp101 | mp102 | mp103 | mp104 | mp105 | mp106 | mp107 | mp108 | mp109 | mp11 | mp110 | mp111 | mp112 | mp113 | mp114 | mp115 | mp116 | mp117 | mp118 | mp119 | mp12 | mp120 | mp121 | mp122 | mp123 | mp124 | mp125 | mp126 | mp127 | mp128 | mp129 | mp13 | mp130 | mp131 | mp132 | mp133 | mp134 | mp135 | mp136 | mp137 | mp138 | mp139 | mp14 | mp140 | mp141 | mp142 | mp143 | mp144 | mp145 | mp146 | mp147 | mp148 | mp149 | mp15 | mp150 | mp151 | mp152 | mp153 | mp154 | mp155 | mp156 | mp157 | mp158 | mp159 | mp16 | mp160 | mp161 | mp162 | mp163 | mp164 | mp165 | mp166 | mp167 | mp168 | mp169 | mp17 | mp170 | mp171 | mp172 | mp173 | mp174 | mp175 | mp176 | mp177 | mp178 | mp179 | mp18 | mp180 | mp181 | mp182 | mp183 | mp184 | mp185 | mp186 | mp187 | mp188 | mp189 | mp19 | mp190 | mp191 | mp192 | mp193 | mp194 | mp195 | mp196 | mp197 | mp198 | mp199 | mp2 | mp20 | mp200 | mp201 | mp202 | mp203 | mp204 | mp205 | mp206 | mp207 | mp208 | mp209 | mp21 | mp210 | mp211 | mp212 | mp213 | mp214 | mp215 | mp216 | mp217 | mp218 | mp219 | mp22 | mp220 | mp221 | mp222 | mp223 | mp224 | mp225 | mp226 | mp227 | mp228 | mp229 | mp23 | mp230 | mp231 | mp232 | mp233 | mp234 | mp235 | mp236 | mp237 | mp238 | mp239 | mp24 | mp240 | mp241 | mp242 | mp243 | mp244 | mp245 | mp246 | mp247 | mp248 | mp249 | mp25 | mp250 | mp251 | mp252 | mp253 | mp254 | mp255 | mp26 | mp27 | mp28 | mp29 | mp3 | mp30 | mp31 | mp32 | mp33 | mp34 | mp35 | mp36 | mp37 | mp38 | mp39 | mp4 | mp40 | mp41 | mp42 | mp43 | mp44 | mp45 | mp46 | mp47 | mp48 | mp49 | mp5 | mp50 | mp51 | mp52 | mp53 | mp54 | mp55 | mp56 | mp57 | mp58 | mp59 | mp6 | mp60 | mp61 | mp62 | mp63 | mp64 | mp65 | mp66 | mp67 | mp68 | mp69 | mp7 | mp70 | mp71 | mp72 | mp73 | mp74 | mp75 | mp76 | mp77 | mp78 | mp79 | mp8 | mp80 | mp81 | mp82 | mp83 | mp84 | mp85 | mp86 | mp87 | mp88 | mp89 | mp9 | mp90 | mp91 | mp92 | mp93 | mp94 | mp95 | mp96 | mp97 | mp98 | mp99 | rootfs>\n\nThe disk you want to resize.\n\n<size>: \\+?\\d+(\\.\\d+)?[KMGT]?\n\nThe new size. With the + sign the value is added to the actual size of the volume and without it, the value is taken as an absolute one. Shrinking disk size is not supported.\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1 digest. This can be used to prevent concurrent modifications.\n\npct restore <vmid> <ostemplate> [OPTIONS]\n\nCreate or restore a container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<ostemplate>: <string>\n\nThe OS template or backup file.\n\n--arch <amd64 | arm64 | armhf | i386 | riscv32 | riscv64> (default = amd64)\n\nOS architecture type.\n\n--bwlimit <number> (0 - N) (default = restore limit from datacenter or storage config)\n\nOverride I/O bandwidth limit (in KiB/s).\n\n--cmode <console | shell | tty> (default = tty)\n\nConsole mode. By default, the console command tries to open a connection to one of the available tty devices. By setting cmode to console it tries to attach to /dev/console instead. If you set cmode to shell, it simply invokes a shell inside the container (no login).\n\n--console <boolean> (default = 1)\n\nAttach a console device (/dev/console) to the container.\n\n--cores <integer> (1 - 8192)\n\nThe number of cores assigned to the container. A container can use all available cores by default.\n\n--cpulimit <number> (0 - 8192) (default = 0)\n\nLimit of CPU usage.\n\n\tIf the computer has 2 CPUs, it has a total of 2 CPU time. Value 0 indicates no CPU limit.\n--cpuunits <integer> (0 - 500000) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a container, will be clamped to [1, 10000] in cgroup v2.\n\n--debug <boolean> (default = 0)\n\nTry to be more verbose. For now this only enables debug log-level on start.\n\n--description <string>\n\nDescription for the Container. Shown in the web-interface CT’s summary. This is saved as comment inside the configuration file.\n\n--dev[n] [[path=]<Path>] [,gid=<integer>] [,mode=<Octal access mode>] [,uid=<integer>]\n\nDevice to pass through to the container\n\n--features [force_rw_sys=<1|0>] [,fuse=<1|0>] [,keyctl=<1|0>] [,mknod=<1|0>] [,mount=<fstype;fstype;...>] [,nesting=<1|0>]\n\nAllow containers access to advanced features.\n\n--force <boolean>\n\nAllow to overwrite existing container.\n\n--hookscript <string>\n\nScript that will be exectued during various steps in the containers lifetime.\n\n--hostname <string>\n\nSet a host name for the container.\n\n--ignore-unpack-errors <boolean>\n\nIgnore errors when extracting the template.\n\n--lock <backup | create | destroyed | disk | fstrim | migrate | mounted | rollback | snapshot | snapshot-delete>\n\nLock/unlock the container.\n\n--memory <integer> (16 - N) (default = 512)\n\nAmount of RAM for the container in MB.\n\n--mp[n] [volume=]<volume> ,mp=<Path> [,acl=<1|0>] [,backup=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container mount point. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume.\n\n--nameserver <string>\n\nSets DNS server IP address for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\n--net[n] name=<string> [,bridge=<bridge>] [,firewall=<1|0>] [,gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,hwaddr=<XX:XX:XX:XX:XX:XX>] [,ip=<(IPv4/CIDR|dhcp|manual)>] [,ip6=<(IPv6/CIDR|auto|dhcp|manual)>] [,link_down=<1|0>] [,mtu=<integer>] [,rate=<mbps>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,type=<veth>]\n\nSpecifies network interfaces for the container.\n\n--onboot <boolean> (default = 0)\n\nSpecifies whether a container will be started during system bootup.\n\n--ostype <alpine | archlinux | centos | debian | devuan | fedora | gentoo | nixos | opensuse | ubuntu | unmanaged>\n\nOS type. This is used to setup configuration inside the container, and corresponds to lxc setup scripts in /usr/share/lxc/config/<ostype>.common.conf. Value unmanaged can be used to skip and OS specific setup.\n\n--password <password>\n\nSets root password inside container.\n\n--pool <string>\n\nAdd the VM to the specified pool.\n\n--protection <boolean> (default = 0)\n\nSets the protection flag of the container. This will prevent the CT or CT’s disk remove/update operation.\n\n--rootfs [volume=]<volume> [,acl=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container root.\n\n--searchdomain <string>\n\nSets DNS search domains for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\n--ssh-public-keys <filepath>\n\nSetup public SSH keys (one key per line, OpenSSH format).\n\n--start <boolean> (default = 0)\n\nStart the CT after its creation finished successfully.\n\n--startup `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\n--storage <storage ID> (default = local)\n\nDefault Storage.\n\n--swap <integer> (0 - N) (default = 512)\n\nAmount of SWAP for the container in MB.\n\n--tags <string>\n\nTags of the Container. This is only meta information.\n\n--template <boolean> (default = 0)\n\nEnable/disable Template.\n\n--timezone <string>\n\nTime zone to use in the container. If option isn’t set, then nothing will be done. Can be set to host to match the host time zone, or an arbitrary time zone option from /usr/share/zoneinfo/zone.tab\n\n--tty <integer> (0 - 6) (default = 2)\n\nSpecify the number of tty available to the container\n\n--unique <boolean>\n\nAssign a unique random ethernet address.\n\n\tRequires option(s): restore\n--unprivileged <boolean> (default = 0)\n\nMakes the container run as unprivileged user. (Should not be modified manually.)\n\n--unused[n] [volume=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\npct resume <vmid>\n\nResume the container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct rollback <vmid> <snapname> [OPTIONS]\n\nRollback LXC state to specified snapshot.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<snapname>: <string>\n\nThe name of the snapshot.\n\n--start <boolean> (default = 0)\n\nWhether the container should get started after rolling back successfully\n\npct set <vmid> [OPTIONS]\n\nSet container options.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--arch <amd64 | arm64 | armhf | i386 | riscv32 | riscv64> (default = amd64)\n\nOS architecture type.\n\n--cmode <console | shell | tty> (default = tty)\n\nConsole mode. By default, the console command tries to open a connection to one of the available tty devices. By setting cmode to console it tries to attach to /dev/console instead. If you set cmode to shell, it simply invokes a shell inside the container (no login).\n\n--console <boolean> (default = 1)\n\nAttach a console device (/dev/console) to the container.\n\n--cores <integer> (1 - 8192)\n\nThe number of cores assigned to the container. A container can use all available cores by default.\n\n--cpulimit <number> (0 - 8192) (default = 0)\n\nLimit of CPU usage.\n\n\tIf the computer has 2 CPUs, it has a total of 2 CPU time. Value 0 indicates no CPU limit.\n--cpuunits <integer> (0 - 500000) (default = cgroup v1: 1024, cgroup v2: 100)\n\nCPU weight for a container, will be clamped to [1, 10000] in cgroup v2.\n\n--debug <boolean> (default = 0)\n\nTry to be more verbose. For now this only enables debug log-level on start.\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--description <string>\n\nDescription for the Container. Shown in the web-interface CT’s summary. This is saved as comment inside the configuration file.\n\n--dev[n] [[path=]<Path>] [,gid=<integer>] [,mode=<Octal access mode>] [,uid=<integer>]\n\nDevice to pass through to the container\n\n--digest <string>\n\nPrevent changes if current configuration file has different SHA1 digest. This can be used to prevent concurrent modifications.\n\n--features [force_rw_sys=<1|0>] [,fuse=<1|0>] [,keyctl=<1|0>] [,mknod=<1|0>] [,mount=<fstype;fstype;...>] [,nesting=<1|0>]\n\nAllow containers access to advanced features.\n\n--hookscript <string>\n\nScript that will be exectued during various steps in the containers lifetime.\n\n--hostname <string>\n\nSet a host name for the container.\n\n--lock <backup | create | destroyed | disk | fstrim | migrate | mounted | rollback | snapshot | snapshot-delete>\n\nLock/unlock the container.\n\n--memory <integer> (16 - N) (default = 512)\n\nAmount of RAM for the container in MB.\n\n--mp[n] [volume=]<volume> ,mp=<Path> [,acl=<1|0>] [,backup=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container mount point. Use the special syntax STORAGE_ID:SIZE_IN_GiB to allocate a new volume.\n\n--nameserver <string>\n\nSets DNS server IP address for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\n--net[n] name=<string> [,bridge=<bridge>] [,firewall=<1|0>] [,gw=<GatewayIPv4>] [,gw6=<GatewayIPv6>] [,hwaddr=<XX:XX:XX:XX:XX:XX>] [,ip=<(IPv4/CIDR|dhcp|manual)>] [,ip6=<(IPv6/CIDR|auto|dhcp|manual)>] [,link_down=<1|0>] [,mtu=<integer>] [,rate=<mbps>] [,tag=<integer>] [,trunks=<vlanid[;vlanid...]>] [,type=<veth>]\n\nSpecifies network interfaces for the container.\n\n--onboot <boolean> (default = 0)\n\nSpecifies whether a container will be started during system bootup.\n\n--ostype <alpine | archlinux | centos | debian | devuan | fedora | gentoo | nixos | opensuse | ubuntu | unmanaged>\n\nOS type. This is used to setup configuration inside the container, and corresponds to lxc setup scripts in /usr/share/lxc/config/<ostype>.common.conf. Value unmanaged can be used to skip and OS specific setup.\n\n--protection <boolean> (default = 0)\n\nSets the protection flag of the container. This will prevent the CT or CT’s disk remove/update operation.\n\n--revert <string>\n\nRevert a pending change.\n\n--rootfs [volume=]<volume> [,acl=<1|0>] [,mountoptions=<opt[;opt...]>] [,quota=<1|0>] [,replicate=<1|0>] [,ro=<1|0>] [,shared=<1|0>] [,size=<DiskSize>]\n\nUse volume as container root.\n\n--searchdomain <string>\n\nSets DNS search domains for a container. Create will automatically use the setting from the host if you neither set searchdomain nor nameserver.\n\n--startup `[[order=]\\d+] [,up=\\d+] [,down=\\d+] `\n\nStartup and shutdown behavior. Order is a non-negative number defining the general startup order. Shutdown in done with reverse ordering. Additionally you can set the up or down delay in seconds, which specifies a delay to wait before the next VM is started or stopped.\n\n--swap <integer> (0 - N) (default = 512)\n\nAmount of SWAP for the container in MB.\n\n--tags <string>\n\nTags of the Container. This is only meta information.\n\n--template <boolean> (default = 0)\n\nEnable/disable Template.\n\n--timezone <string>\n\nTime zone to use in the container. If option isn’t set, then nothing will be done. Can be set to host to match the host time zone, or an arbitrary time zone option from /usr/share/zoneinfo/zone.tab\n\n--tty <integer> (0 - 6) (default = 2)\n\nSpecify the number of tty available to the container\n\n--unprivileged <boolean> (default = 0)\n\nMakes the container run as unprivileged user. (Should not be modified manually.)\n\n--unused[n] [volume=]<volume>\n\nReference to unused volumes. This is used internally, and should not be modified manually.\n\npct shutdown <vmid> [OPTIONS]\n\nShutdown the container. This will trigger a clean shutdown of the container, see lxc-stop(1) for details.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--forceStop <boolean> (default = 0)\n\nMake sure the Container stops.\n\n--timeout <integer> (0 - N) (default = 60)\n\nWait maximal timeout seconds.\n\npct snapshot <vmid> <snapname> [OPTIONS]\n\nSnapshot a container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<snapname>: <string>\n\nThe name of the snapshot.\n\n--description <string>\n\nA textual description or comment.\n\npct start <vmid> [OPTIONS]\n\nStart the container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--debug <boolean> (default = 0)\n\nIf set, enables very verbose debug log-level on start.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\npct status <vmid> [OPTIONS]\n\nShow CT status.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--verbose <boolean>\n\nVerbose output format\n\npct stop <vmid> [OPTIONS]\n\nStop the container. This will abruptly stop all processes running in the container.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n--overrule-shutdown <boolean> (default = 0)\n\nTry to abort active vzshutdown tasks before stopping.\n\n--skiplock <boolean>\n\nIgnore locks - only root is allowed to use this option.\n\npct suspend <vmid>\n\nSuspend the container. This is experimental.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct template <vmid>\n\nCreate a Template.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct unlock <vmid>\n\nUnlock the VM.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\npct unmount <vmid>\n\nUnmount the container’s filesystem.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n22.11. pveam - Proxmox VE Appliance Manager\n\npveam <COMMAND> [ARGS] [OPTIONS]\n\npveam available [OPTIONS]\n\nList available templates.\n\n--section <mail | system | turnkeylinux>\n\nRestrict list to specified section.\n\npveam download <storage> <template>\n\nDownload appliance templates.\n\n<storage>: <storage ID>\n\nThe storage where the template will be stored\n\n<template>: <string>\n\nThe template which will downloaded\n\npveam help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npveam list <storage>\n\nGet list of all templates on storage\n\n<storage>: <storage ID>\n\nOnly list templates on specified storage\n\npveam remove <template_path>\n\nRemove a template.\n\n<template_path>: <string>\n\nThe template to remove.\n\npveam update\n\nUpdate Container Template Database.\n\n22.12. pvecm - Proxmox VE Cluster Manager\n\npvecm <COMMAND> [ARGS] [OPTIONS]\n\npvecm add <hostname> [OPTIONS]\n\nAdds the current node to an existing cluster.\n\n<hostname>: <string>\n\nHostname (or IP) of an existing cluster member.\n\n--fingerprint ([A-Fa-f0-9]{2}:){31}[A-Fa-f0-9]{2}\n\nCertificate SHA 256 fingerprint.\n\n--force <boolean>\n\nDo not throw error if node already exists.\n\n--link[n] [address=]<IP> [,priority=<integer>]\n\nAddress and priority information of a single corosync link. (up to 8 links supported; link0..link7)\n\n--nodeid <integer> (1 - N)\n\nNode id for this node.\n\n--use_ssh <boolean>\n\nAlways use SSH to join, even if peer may do it over API.\n\n--votes <integer> (0 - N)\n\nNumber of votes for this node\n\npvecm addnode <node> [OPTIONS]\n\nAdds a node to the cluster configuration. This call is for internal use.\n\n<node>: <string>\n\nThe cluster node name.\n\n--apiversion <integer>\n\nThe JOIN_API_VERSION of the new node.\n\n--force <boolean>\n\nDo not throw error if node already exists.\n\n--link[n] [address=]<IP> [,priority=<integer>]\n\nAddress and priority information of a single corosync link. (up to 8 links supported; link0..link7)\n\n--new_node_ip <string>\n\nIP Address of node to add. Used as fallback if no links are given.\n\n--nodeid <integer> (1 - N)\n\nNode id for this node.\n\n--votes <integer> (0 - N)\n\nNumber of votes for this node\n\npvecm apiver\n\nReturn the version of the cluster join API available on this node.\n\npvecm create <clustername> [OPTIONS]\n\nGenerate new cluster configuration. If no links given, default to local IP address as link0.\n\n<clustername>: <string>\n\nThe name of the cluster.\n\n--link[n] [address=]<IP> [,priority=<integer>]\n\nAddress and priority information of a single corosync link. (up to 8 links supported; link0..link7)\n\n--nodeid <integer> (1 - N)\n\nNode id for this node.\n\n--votes <integer> (1 - N)\n\nNumber of votes for this node.\n\npvecm delnode <node>\n\nRemoves a node from the cluster configuration.\n\n<node>: <string>\n\nThe cluster node name.\n\npvecm expected <expected>\n\nTells corosync a new value of expected votes.\n\n<expected>: <integer> (1 - N)\n\nExpected votes\n\npvecm help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvecm keygen <filename>\n\nGenerate new cryptographic key for corosync.\n\n<filename>: <string>\n\nOutput file name\n\npvecm mtunnel [<extra-args>] [OPTIONS]\n\nUsed by VM/CT migration - do not use manually.\n\n<extra-args>: <array>\n\nExtra arguments as array\n\n--get_migration_ip <boolean> (default = 0)\n\nreturn the migration IP, if configured\n\n--migration_network <string>\n\nthe migration network used to detect the local migration IP\n\n--run-command <boolean>\n\nRun a command with a tcp socket as standard input. The IP address and port are printed via this command’s stdandard output first, each on a separate line.\n\npvecm nodes\n\nDisplays the local view of the cluster nodes.\n\npvecm qdevice remove\n\nRemove a configured QDevice\n\npvecm qdevice setup <address> [OPTIONS]\n\nSetup the use of a QDevice\n\n<address>: <string>\n\nSpecifies the network address of an external corosync QDevice\n\n--force <boolean>\n\nDo not throw error on possible dangerous operations.\n\n--network <string>\n\nThe network which should be used to connect to the external qdevice\n\npvecm status\n\nDisplays the local view of the cluster status.\n\npvecm updatecerts [OPTIONS]\n\nUpdate node certificates (and generate all needed files/directories).\n\n--force <boolean>\n\nForce generation of new SSL certificate.\n\n--silent <boolean>\n\nIgnore errors (i.e. when cluster has no quorum).\n\n--unmerge-known-hosts <boolean> (default = 0)\n\nUnmerge legacy SSH known hosts.\n\n22.13. pvesr - Proxmox VE Storage Replication\n\npvesr <COMMAND> [ARGS] [OPTIONS]\n\npvesr create-local-job <id> <target> [OPTIONS]\n\nCreate a new replication job\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\n<target>: <string>\n\nTarget node.\n\n--comment <string>\n\nDescription.\n\n--disable <boolean>\n\nFlag to disable/deactivate the entry.\n\n--rate <number> (1 - N)\n\nRate limit in mbps (megabytes per second) as floating point number.\n\n--remove_job <full | local>\n\nMark the replication job for removal. The job will remove all local replication snapshots. When set to full, it also tries to remove replicated volumes on the target. The job then removes itself from the configuration file.\n\n--schedule <string> (default = */15)\n\nStorage replication schedule. The format is a subset of systemd calendar events.\n\n--source <string>\n\nFor internal use, to detect if the guest was stolen.\n\npvesr delete <id> [OPTIONS]\n\nMark replication job for removal.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\n--force <boolean> (default = 0)\n\nWill remove the jobconfig entry, but will not cleanup.\n\n--keep <boolean> (default = 0)\n\nKeep replicated data at target (do not remove).\n\npvesr disable <id>\n\nDisable a replication job.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\npvesr enable <id>\n\nEnable a replication job.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\npvesr finalize-local-job <id> [<extra-args>] [OPTIONS]\n\nFinalize a replication job. This removes all replications snapshots with timestamps different than <last_sync>.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\n<extra-args>: <array>\n\nThe list of volume IDs to consider.\n\n--last_sync <integer> (0 - N)\n\nTime (UNIX epoch) of last successful sync. If not specified, all replication snapshots gets removed.\n\npvesr help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvesr list\n\nList replication jobs.\n\npvesr prepare-local-job <id> [<extra-args>] [OPTIONS]\n\nPrepare for starting a replication job. This is called on the target node before replication starts. This call is for internal use, and return a JSON object on stdout. The method first test if VM <vmid> reside on the local node. If so, stop immediately. After that the method scans all volume IDs for snapshots, and removes all replications snapshots with timestamps different than <last_sync>. It also removes any unused volumes. Returns a hash with boolean markers for all volumes with existing replication snapshots.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\n<extra-args>: <array>\n\nThe list of volume IDs to consider.\n\n--force <boolean> (default = 0)\n\nAllow to remove all existion volumes (empty volume list).\n\n--last_sync <integer> (0 - N)\n\nTime (UNIX epoch) of last successful sync. If not specified, all replication snapshots get removed.\n\n--parent_snapname <string>\n\nThe name of the snapshot.\n\n--scan <string>\n\nList of storage IDs to scan for stale volumes.\n\npvesr read <id>\n\nRead replication job configuration.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\npvesr run [OPTIONS]\n\nThis method is called by the systemd-timer and executes all (or a specific) sync jobs.\n\n--id [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\n--mail <boolean> (default = 0)\n\nSend an email notification in case of a failure.\n\n--verbose <boolean> (default = 0)\n\nPrint more verbose logs to stdout.\n\npvesr schedule-now <id>\n\nSchedule replication job to start as soon as possible.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\npvesr set-state <vmid> <state>\n\nSet the job replication state on migration. This call is for internal use. It will accept the job state as ja JSON obj.\n\n<vmid>: <integer> (100 - 999999999)\n\nThe (unique) ID of the VM.\n\n<state>: <string>\n\nJob state as JSON decoded string.\n\npvesr status [OPTIONS]\n\nList status of all replication jobs on this node.\n\n--guest <integer> (100 - 999999999)\n\nOnly list replication jobs for this guest.\n\npvesr update <id> [OPTIONS]\n\nUpdate replication job configuration.\n\n<id>: [1-9][0-9]{2,8}-\\d{1,9}\n\nReplication Job ID. The ID is composed of a Guest ID and a job number, separated by a hyphen, i.e. <GUEST>-<JOBNUM>.\n\n--comment <string>\n\nDescription.\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--digest <string>\n\nPrevent changes if current configuration file has a different digest. This can be used to prevent concurrent modifications.\n\n--disable <boolean>\n\nFlag to disable/deactivate the entry.\n\n--rate <number> (1 - N)\n\nRate limit in mbps (megabytes per second) as floating point number.\n\n--remove_job <full | local>\n\nMark the replication job for removal. The job will remove all local replication snapshots. When set to full, it also tries to remove replicated volumes on the target. The job then removes itself from the configuration file.\n\n--schedule <string> (default = */15)\n\nStorage replication schedule. The format is a subset of systemd calendar events.\n\n--source <string>\n\nFor internal use, to detect if the guest was stolen.\n\n22.14. pveum - Proxmox VE User Manager\n\npveum <COMMAND> [ARGS] [OPTIONS]\n\npveum acl delete <path> --roles <string> [OPTIONS]\n\nUpdate Access Control List (add or remove permissions).\n\n<path>: <string>\n\nAccess control path\n\n--groups <string>\n\nList of groups.\n\n--propagate <boolean> (default = 1)\n\nAllow to propagate (inherit) permissions.\n\n--roles <string>\n\nList of roles.\n\n--tokens <string>\n\nList of API tokens.\n\n--users <string>\n\nList of users.\n\npveum acl list [FORMAT_OPTIONS]\n\nGet Access Control List (ACLs).\n\npveum acl modify <path> --roles <string> [OPTIONS]\n\nUpdate Access Control List (add or remove permissions).\n\n<path>: <string>\n\nAccess control path\n\n--groups <string>\n\nList of groups.\n\n--propagate <boolean> (default = 1)\n\nAllow to propagate (inherit) permissions.\n\n--roles <string>\n\nList of roles.\n\n--tokens <string>\n\nList of API tokens.\n\n--users <string>\n\nList of users.\n\npveum acldel\n\nAn alias for pveum acl delete.\n\npveum aclmod\n\nAn alias for pveum acl modify.\n\npveum group add <groupid> [OPTIONS]\n\nCreate new group.\n\n<groupid>: <string>\n\nno description available\n\n--comment <string>\n\nno description available\n\npveum group delete <groupid>\n\nDelete group.\n\n<groupid>: <string>\n\nno description available\n\npveum group list [FORMAT_OPTIONS]\n\nGroup index.\n\npveum group modify <groupid> [OPTIONS]\n\nUpdate group data.\n\n<groupid>: <string>\n\nno description available\n\n--comment <string>\n\nno description available\n\npveum groupadd\n\nAn alias for pveum group add.\n\npveum groupdel\n\nAn alias for pveum group delete.\n\npveum groupmod\n\nAn alias for pveum group modify.\n\npveum help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npveum passwd <userid> [OPTIONS]\n\nChange user password.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n--confirmation-password <string>\n\nThe current password of the user performing the change.\n\npveum pool add <poolid> [OPTIONS]\n\nCreate new pool.\n\n<poolid>: <string>\n\nno description available\n\n--comment <string>\n\nno description available\n\npveum pool delete <poolid>\n\nDelete pool.\n\n<poolid>: <string>\n\nno description available\n\npveum pool list [OPTIONS] [FORMAT_OPTIONS]\n\nList pools or get pool configuration.\n\n--poolid <string>\n\nno description available\n\n--type <lxc | qemu | storage>\n\nno description available\n\n\tRequires option(s): poolid\n\npveum pool modify <poolid> [OPTIONS]\n\nUpdate pool.\n\n<poolid>: <string>\n\nno description available\n\n--allow-move <boolean> (default = 0)\n\nAllow adding a guest even if already in another pool. The guest will be removed from its current pool and added to this one.\n\n--comment <string>\n\nno description available\n\n--delete <boolean> (default = 0)\n\nRemove the passed VMIDs and/or storage IDs instead of adding them.\n\n--storage <string>\n\nList of storage IDs to add or remove from this pool.\n\n--vms <string>\n\nList of guest VMIDs to add or remove from this pool.\n\npveum realm add <realm> --type <string> [OPTIONS]\n\nAdd an authentication server.\n\n<realm>: <string>\n\nAuthentication domain ID\n\n--acr-values ^[^\\x00-\\x1F\\x7F <>#\"]*$\n\nSpecifies the Authentication Context Class Reference values that theAuthorization Server is being requested to use for the Auth Request.\n\n--autocreate <boolean> (default = 0)\n\nAutomatically create users if they do not exist.\n\n--base_dn <string>\n\nLDAP base domain name\n\n--bind_dn <string>\n\nLDAP bind domain name\n\n--capath <string> (default = /etc/ssl/certs)\n\nPath to the CA certificate store\n\n--case-sensitive <boolean> (default = 1)\n\nusername is case-sensitive\n\n--cert <string>\n\nPath to the client certificate\n\n--certkey <string>\n\nPath to the client certificate key\n\n--check-connection <boolean> (default = 0)\n\nCheck bind connection to the server.\n\n--client-id <string>\n\nOpenID Client ID\n\n--client-key <string>\n\nOpenID Client Key\n\n--comment <string>\n\nDescription.\n\n--default <boolean>\n\nUse this as default realm\n\n--domain \\S+\n\nAD domain name\n\n--filter <string>\n\nLDAP filter for user sync.\n\n--group_classes <string> (default = groupOfNames, group, univentionGroup, ipausergroup)\n\nThe objectclasses for groups.\n\n--group_dn <string>\n\nLDAP base domain name for group sync. If not set, the base_dn will be used.\n\n--group_filter <string>\n\nLDAP filter for group sync.\n\n--group_name_attr <string>\n\nLDAP attribute representing a groups name. If not set or found, the first value of the DN will be used as name.\n\n--issuer-url <string>\n\nOpenID Issuer Url\n\n--mode <ldap | ldap+starttls | ldaps> (default = ldap)\n\nLDAP protocol mode.\n\n--password <string>\n\nLDAP bind password. Will be stored in /etc/pve/priv/realm/<REALM>.pw.\n\n--port <integer> (1 - 65535)\n\nServer port.\n\n--prompt (?:none|login|consent|select_account|\\S+)\n\nSpecifies whether the Authorization Server prompts the End-User for reauthentication and consent.\n\n--scopes <string> (default = email profile)\n\nSpecifies the scopes (user details) that should be authorized and returned, for example email or profile.\n\n--secure <boolean>\n\nUse secure LDAPS protocol. DEPRECATED: use mode instead.\n\n--server1 <string>\n\nServer IP address (or DNS name)\n\n--server2 <string>\n\nFallback Server IP address (or DNS name)\n\n--sslversion <tlsv1 | tlsv1_1 | tlsv1_2 | tlsv1_3>\n\nLDAPS TLS/SSL version. It’s not recommended to use version older than 1.2!\n\n--sync-defaults-options [enable-new=<1|0>] [,full=<1|0>] [,purge=<1|0>] [,remove-vanished=([acl];[properties];[entry])|none] [,scope=<users|groups|both>]\n\nThe default options for behavior of synchronizations.\n\n--sync_attributes \\w+=[^,]+(,\\s*\\w+=[^,]+)*\n\nComma separated list of key=value pairs for specifying which LDAP attributes map to which PVE user field. For example, to map the LDAP attribute mail to PVEs email, write email=mail. By default, each PVE user field is represented by an LDAP attribute of the same name.\n\n--tfa type=<TFATYPE> [,digits=<COUNT>] [,id=<ID>] [,key=<KEY>] [,step=<SECONDS>] [,url=<URL>]\n\nUse Two-factor authentication.\n\n--type <ad | ldap | openid | pam | pve>\n\nRealm type.\n\n--user_attr \\S{2,}\n\nLDAP user attribute name\n\n--user_classes <string> (default = inetorgperson, posixaccount, person, user)\n\nThe objectclasses for users.\n\n--username-claim <string>\n\nOpenID claim used to generate the unique username.\n\n--verify <boolean> (default = 0)\n\nVerify the server’s SSL certificate\n\npveum realm delete <realm>\n\nDelete an authentication server.\n\n<realm>: <string>\n\nAuthentication domain ID\n\npveum realm list [FORMAT_OPTIONS]\n\nAuthentication domain index.\n\npveum realm modify <realm> [OPTIONS]\n\nUpdate authentication server settings.\n\n<realm>: <string>\n\nAuthentication domain ID\n\n--acr-values ^[^\\x00-\\x1F\\x7F <>#\"]*$\n\nSpecifies the Authentication Context Class Reference values that theAuthorization Server is being requested to use for the Auth Request.\n\n--autocreate <boolean> (default = 0)\n\nAutomatically create users if they do not exist.\n\n--base_dn <string>\n\nLDAP base domain name\n\n--bind_dn <string>\n\nLDAP bind domain name\n\n--capath <string> (default = /etc/ssl/certs)\n\nPath to the CA certificate store\n\n--case-sensitive <boolean> (default = 1)\n\nusername is case-sensitive\n\n--cert <string>\n\nPath to the client certificate\n\n--certkey <string>\n\nPath to the client certificate key\n\n--check-connection <boolean> (default = 0)\n\nCheck bind connection to the server.\n\n--client-id <string>\n\nOpenID Client ID\n\n--client-key <string>\n\nOpenID Client Key\n\n--comment <string>\n\nDescription.\n\n--default <boolean>\n\nUse this as default realm\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--digest <string>\n\nPrevent changes if current configuration file has a different digest. This can be used to prevent concurrent modifications.\n\n--domain \\S+\n\nAD domain name\n\n--filter <string>\n\nLDAP filter for user sync.\n\n--group_classes <string> (default = groupOfNames, group, univentionGroup, ipausergroup)\n\nThe objectclasses for groups.\n\n--group_dn <string>\n\nLDAP base domain name for group sync. If not set, the base_dn will be used.\n\n--group_filter <string>\n\nLDAP filter for group sync.\n\n--group_name_attr <string>\n\nLDAP attribute representing a groups name. If not set or found, the first value of the DN will be used as name.\n\n--issuer-url <string>\n\nOpenID Issuer Url\n\n--mode <ldap | ldap+starttls | ldaps> (default = ldap)\n\nLDAP protocol mode.\n\n--password <string>\n\nLDAP bind password. Will be stored in /etc/pve/priv/realm/<REALM>.pw.\n\n--port <integer> (1 - 65535)\n\nServer port.\n\n--prompt (?:none|login|consent|select_account|\\S+)\n\nSpecifies whether the Authorization Server prompts the End-User for reauthentication and consent.\n\n--scopes <string> (default = email profile)\n\nSpecifies the scopes (user details) that should be authorized and returned, for example email or profile.\n\n--secure <boolean>\n\nUse secure LDAPS protocol. DEPRECATED: use mode instead.\n\n--server1 <string>\n\nServer IP address (or DNS name)\n\n--server2 <string>\n\nFallback Server IP address (or DNS name)\n\n--sslversion <tlsv1 | tlsv1_1 | tlsv1_2 | tlsv1_3>\n\nLDAPS TLS/SSL version. It’s not recommended to use version older than 1.2!\n\n--sync-defaults-options [enable-new=<1|0>] [,full=<1|0>] [,purge=<1|0>] [,remove-vanished=([acl];[properties];[entry])|none] [,scope=<users|groups|both>]\n\nThe default options for behavior of synchronizations.\n\n--sync_attributes \\w+=[^,]+(,\\s*\\w+=[^,]+)*\n\nComma separated list of key=value pairs for specifying which LDAP attributes map to which PVE user field. For example, to map the LDAP attribute mail to PVEs email, write email=mail. By default, each PVE user field is represented by an LDAP attribute of the same name.\n\n--tfa type=<TFATYPE> [,digits=<COUNT>] [,id=<ID>] [,key=<KEY>] [,step=<SECONDS>] [,url=<URL>]\n\nUse Two-factor authentication.\n\n--user_attr \\S{2,}\n\nLDAP user attribute name\n\n--user_classes <string> (default = inetorgperson, posixaccount, person, user)\n\nThe objectclasses for users.\n\n--verify <boolean> (default = 0)\n\nVerify the server’s SSL certificate\n\npveum realm sync <realm> [OPTIONS]\n\nSyncs users and/or groups from the configured LDAP to user.cfg. NOTE: Synced groups will have the name name-$realm, so make sure those groups do not exist to prevent overwriting.\n\n<realm>: <string>\n\nAuthentication domain ID\n\n--dry-run <boolean> (default = 0)\n\nIf set, does not write anything.\n\n--enable-new <boolean> (default = 1)\n\nEnable newly synced users immediately.\n\n--full <boolean>\n\nDEPRECATED: use remove-vanished instead. If set, uses the LDAP Directory as source of truth, deleting users or groups not returned from the sync and removing all locally modified properties of synced users. If not set, only syncs information which is present in the synced data, and does not delete or modify anything else.\n\n--purge <boolean>\n\nDEPRECATED: use remove-vanished instead. Remove ACLs for users or groups which were removed from the config during a sync.\n\n--remove-vanished ([acl];[properties];[entry])|none (default = none)\n\nA semicolon-seperated list of things to remove when they or the user vanishes during a sync. The following values are possible: entry removes the user/group when not returned from the sync. properties removes the set properties on existing user/group that do not appear in the source (even custom ones). acl removes acls when the user/group is not returned from the sync. Instead of a list it also can be none (the default).\n\n--scope <both | groups | users>\n\nSelect what to sync.\n\npveum role add <roleid> [OPTIONS]\n\nCreate new role.\n\n<roleid>: <string>\n\nno description available\n\n--privs <string>\n\nno description available\n\npveum role delete <roleid>\n\nDelete role.\n\n<roleid>: <string>\n\nno description available\n\npveum role list [FORMAT_OPTIONS]\n\nRole index.\n\npveum role modify <roleid> [OPTIONS]\n\nUpdate an existing role.\n\n<roleid>: <string>\n\nno description available\n\n--append <boolean>\n\nno description available\n\n\tRequires option(s): privs\n--privs <string>\n\nno description available\n\npveum roleadd\n\nAn alias for pveum role add.\n\npveum roledel\n\nAn alias for pveum role delete.\n\npveum rolemod\n\nAn alias for pveum role modify.\n\npveum ticket <username> [OPTIONS]\n\nCreate or verify authentication ticket.\n\n<username>: <string>\n\nUser name\n\n--new-format <boolean> (default = 1)\n\nThis parameter is now ignored and assumed to be 1.\n\n--otp <string>\n\nOne-time password for Two-factor authentication.\n\n--path <string>\n\nVerify ticket, and check if user have access privs on path\n\n\tRequires option(s): privs\n--privs <string>\n\nVerify ticket, and check if user have access privs on path\n\n\tRequires option(s): path\n--realm <string>\n\nYou can optionally pass the realm using this parameter. Normally the realm is simply added to the username <username>@<relam>.\n\n--tfa-challenge <string>\n\nThe signed TFA challenge string the user wants to respond to.\n\npveum user add <userid> [OPTIONS]\n\nCreate new user.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n--comment <string>\n\nno description available\n\n--email <string>\n\nno description available\n\n--enable <boolean> (default = 1)\n\nEnable the account (default). You can set this to 0 to disable the account\n\n--expire <integer> (0 - N)\n\nAccount expiration date (seconds since epoch). 0 means no expiration date.\n\n--firstname <string>\n\nno description available\n\n--groups <string>\n\nno description available\n\n--keys [0-9a-zA-Z!=]{0,4096}\n\nKeys for two factor auth (yubico).\n\n--lastname <string>\n\nno description available\n\n--password <string>\n\nInitial password.\n\npveum user delete <userid>\n\nDelete user.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\npveum user list [OPTIONS] [FORMAT_OPTIONS]\n\nUser index.\n\n--enabled <boolean>\n\nOptional filter for enable property.\n\n--full <boolean> (default = 0)\n\nInclude group and token information.\n\npveum user modify <userid> [OPTIONS]\n\nUpdate user configuration.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n--append <boolean>\n\nno description available\n\n\tRequires option(s): groups\n--comment <string>\n\nno description available\n\n--email <string>\n\nno description available\n\n--enable <boolean> (default = 1)\n\nEnable the account (default). You can set this to 0 to disable the account\n\n--expire <integer> (0 - N)\n\nAccount expiration date (seconds since epoch). 0 means no expiration date.\n\n--firstname <string>\n\nno description available\n\n--groups <string>\n\nno description available\n\n--keys [0-9a-zA-Z!=]{0,4096}\n\nKeys for two factor auth (yubico).\n\n--lastname <string>\n\nno description available\n\npveum user permissions [<userid>] [OPTIONS] [FORMAT_OPTIONS]\n\nRetrieve effective permissions of given user/token.\n\n<userid>: (?^:^(?^:[^\\s:/]+)\\@(?^:[A-Za-z][A-Za-z0-9\\.\\-_]+)(?:!(?^:[A-Za-z][A-Za-z0-9\\.\\-_]+))?$)\n\nUser ID or full API token ID\n\n--path <string>\n\nOnly dump this specific path, not the whole tree.\n\npveum user tfa delete <userid> [OPTIONS]\n\nDelete TFA entries from a user.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n--id <string>\n\nThe TFA ID, if none provided, all TFA entries will be deleted.\n\npveum user tfa list [<userid>]\n\nList TFA entries.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\npveum user tfa unlock <userid>\n\nUnlock a user’s TFA authentication.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\npveum user token add <userid> <tokenid> [OPTIONS] [FORMAT_OPTIONS]\n\nGenerate a new API token for a specific user. NOTE: returns API token value, which needs to be stored as it cannot be retrieved afterwards!\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n<tokenid>: (?^:[A-Za-z][A-Za-z0-9\\.\\-_]+)\n\nUser-specific token identifier.\n\n--comment <string>\n\nno description available\n\n--expire <integer> (0 - N) (default = same as user)\n\nAPI token expiration date (seconds since epoch). 0 means no expiration date.\n\n--privsep <boolean> (default = 1)\n\nRestrict API token privileges with separate ACLs (default), or give full privileges of corresponding user.\n\npveum user token list <userid> [FORMAT_OPTIONS]\n\nGet user API tokens.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\npveum user token modify <userid> <tokenid> [OPTIONS] [FORMAT_OPTIONS]\n\nUpdate API token for a specific user.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n<tokenid>: (?^:[A-Za-z][A-Za-z0-9\\.\\-_]+)\n\nUser-specific token identifier.\n\n--comment <string>\n\nno description available\n\n--expire <integer> (0 - N) (default = same as user)\n\nAPI token expiration date (seconds since epoch). 0 means no expiration date.\n\n--privsep <boolean> (default = 1)\n\nRestrict API token privileges with separate ACLs (default), or give full privileges of corresponding user.\n\npveum user token permissions <userid> <tokenid> [OPTIONS] [FORMAT_OPTIONS]\n\nRetrieve effective permissions of given token.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n<tokenid>: (?^:[A-Za-z][A-Za-z0-9\\.\\-_]+)\n\nUser-specific token identifier.\n\n--path <string>\n\nOnly dump this specific path, not the whole tree.\n\npveum user token remove <userid> <tokenid> [FORMAT_OPTIONS]\n\nRemove API token for a specific user.\n\n<userid>: <string>\n\nFull User ID, in the name@realm format.\n\n<tokenid>: (?^:[A-Za-z][A-Za-z0-9\\.\\-_]+)\n\nUser-specific token identifier.\n\npveum useradd\n\nAn alias for pveum user add.\n\npveum userdel\n\nAn alias for pveum user delete.\n\npveum usermod\n\nAn alias for pveum user modify.\n\n22.15. vzdump - Backup Utility for VMs and Containers\n\nvzdump help\n\nvzdump {<vmid>} [OPTIONS]\n\nCreate backup.\n\n<vmid>: <string>\n\nThe ID of the guest system you want to backup.\n\n--all <boolean> (default = 0)\n\nBackup all known guest systems on this host.\n\n--bwlimit <integer> (0 - N) (default = 0)\n\nLimit I/O bandwidth (in KiB/s).\n\n--compress <0 | 1 | gzip | lzo | zstd> (default = 0)\n\nCompress dump file.\n\n--dumpdir <string>\n\nStore resulting files to specified directory.\n\n--exclude <string>\n\nExclude specified guest systems (assumes --all)\n\n--exclude-path <array>\n\nExclude certain files/directories (shell globs). Paths starting with / are anchored to the container’s root, other paths match relative to each subdirectory.\n\n--fleecing [[enabled=]<1|0>] [,storage=<storage ID>]\n\nOptions for backup fleecing (VM only).\n\n--ionice <integer> (0 - 8) (default = 7)\n\nSet IO priority when using the BFQ scheduler. For snapshot and suspend mode backups of VMs, this only affects the compressor. A value of 8 means the idle priority is used, otherwise the best-effort priority is used with the specified value.\n\n--lockwait <integer> (0 - N) (default = 180)\n\nMaximal time to wait for the global lock (minutes).\n\n--mailnotification <always | failure> (default = always)\n\nDeprecated: use notification targets/matchers instead. Specify when to send a notification mail\n\n--mailto <string>\n\nDeprecated: Use notification targets/matchers instead. Comma-separated list of email addresses or users that should receive email notifications.\n\n--maxfiles <integer> (1 - N)\n\nDeprecated: use prune-backups instead. Maximal number of backup files per guest system.\n\n--mode <snapshot | stop | suspend> (default = snapshot)\n\nBackup mode.\n\n--node <string>\n\nOnly run if executed on this node.\n\n--notes-template <string>\n\nTemplate string for generating notes for the backup(s). It can contain variables which will be replaced by their values. Currently supported are {\\{\\cluster}}, {\\{\\guestname}}, {\\{\\node}}, and {\\{\\vmid}}, but more might be added in the future. Needs to be a single line, newline and backslash need to be escaped as \\n and \\\\ respectively.\n\n\tRequires option(s): storage\n--notification-mode <auto | legacy-sendmail | notification-system> (default = auto)\n\nDetermine which notification system to use. If set to legacy-sendmail, vzdump will consider the mailto/mailnotification parameters and send emails to the specified address(es) via the sendmail command. If set to notification-system, a notification will be sent via PVE’s notification system, and the mailto and mailnotification will be ignored. If set to auto (default setting), an email will be sent if mailto is set, and the notification system will be used if not.\n\n--notification-policy <always | failure | never> (default = always)\n\nDeprecated: Do not use\n\n--notification-target <string>\n\nDeprecated: Do not use\n\n--performance [max-workers=<integer>] [,pbs-entries-max=<integer>]\n\nOther performance-related settings.\n\n--pigz <integer> (default = 0)\n\nUse pigz instead of gzip when N>0. N=1 uses half of cores, N>1 uses N as thread count.\n\n--pool <string>\n\nBackup all known guest systems included in the specified pool.\n\n--protected <boolean>\n\nIf true, mark backup(s) as protected.\n\n\tRequires option(s): storage\n--prune-backups [keep-all=<1|0>] [,keep-daily=<N>] [,keep-hourly=<N>] [,keep-last=<N>] [,keep-monthly=<N>] [,keep-weekly=<N>] [,keep-yearly=<N>] (default = keep-all=1)\n\nUse these retention options instead of those from the storage configuration.\n\n--quiet <boolean> (default = 0)\n\nBe quiet.\n\n--remove <boolean> (default = 1)\n\nPrune older backups according to prune-backups.\n\n--script <string>\n\nUse specified hook script.\n\n--stdexcludes <boolean> (default = 1)\n\nExclude temporary files and logs.\n\n--stdout <boolean>\n\nWrite tar to stdout, not to a file.\n\n--stop <boolean> (default = 0)\n\nStop running backup jobs on this host.\n\n--stopwait <integer> (0 - N) (default = 10)\n\nMaximal time to wait until a guest system is stopped (minutes).\n\n--storage <storage ID>\n\nStore resulting file to this storage.\n\n--tmpdir <string>\n\nStore temporary files to specified directory.\n\n--zstd <integer> (default = 1)\n\nZstd threads. N=0 uses half of the available cores, if N is set to a value bigger than 0, N is used as thread count.\n\n22.16. ha-manager - Proxmox VE HA Manager\n\nha-manager <COMMAND> [ARGS] [OPTIONS]\n\nha-manager add <sid> [OPTIONS]\n\nCreate a new HA resource.\n\n<sid>: <type>:<name>\n\nHA resource ID. This consists of a resource type followed by a resource specific name, separated with colon (example: vm:100 / ct:100). For virtual machines and containers, you can simply use the VM or CT id as a shortcut (example: 100).\n\n--comment <string>\n\nDescription.\n\n--group <string>\n\nThe HA group identifier.\n\n--max_relocate <integer> (0 - N) (default = 1)\n\nMaximal number of service relocate tries when a service failes to start.\n\n--max_restart <integer> (0 - N) (default = 1)\n\nMaximal number of tries to restart the service on a node after its start failed.\n\n--state <disabled | enabled | ignored | started | stopped> (default = started)\n\nRequested resource state.\n\n--type <ct | vm>\n\nResource type.\n\nha-manager config [OPTIONS]\n\nList HA resources.\n\n--type <ct | vm>\n\nOnly list resources of specific type\n\nha-manager crm-command migrate <sid> <node>\n\nRequest resource migration (online) to another node.\n\n<sid>: <type>:<name>\n\nHA resource ID. This consists of a resource type followed by a resource specific name, separated with colon (example: vm:100 / ct:100). For virtual machines and containers, you can simply use the VM or CT id as a shortcut (example: 100).\n\n<node>: <string>\n\nTarget node.\n\nha-manager crm-command node-maintenance disable <node>\n\nChange the node-maintenance request state.\n\n<node>: <string>\n\nThe cluster node name.\n\nha-manager crm-command node-maintenance enable <node>\n\nChange the node-maintenance request state.\n\n<node>: <string>\n\nThe cluster node name.\n\nha-manager crm-command relocate <sid> <node>\n\nRequest resource relocatzion to another node. This stops the service on the old node, and restarts it on the target node.\n\n<sid>: <type>:<name>\n\nHA resource ID. This consists of a resource type followed by a resource specific name, separated with colon (example: vm:100 / ct:100). For virtual machines and containers, you can simply use the VM or CT id as a shortcut (example: 100).\n\n<node>: <string>\n\nTarget node.\n\nha-manager crm-command stop <sid> <timeout>\n\nRequest the service to be stopped.\n\n<sid>: <type>:<name>\n\nHA resource ID. This consists of a resource type followed by a resource specific name, separated with colon (example: vm:100 / ct:100). For virtual machines and containers, you can simply use the VM or CT id as a shortcut (example: 100).\n\n<timeout>: <integer> (0 - N)\n\nTimeout in seconds. If set to 0 a hard stop will be performed.\n\nha-manager groupadd <group> --nodes <string> [OPTIONS]\n\nCreate a new HA group.\n\n<group>: <string>\n\nThe HA group identifier.\n\n--comment <string>\n\nDescription.\n\n--nodes <node>[:<pri>]{,<node>[:<pri>]}*\n\nList of cluster node names with optional priority.\n\n--nofailback <boolean> (default = 0)\n\nThe CRM tries to run services on the node with the highest priority. If a node with higher priority comes online, the CRM migrates the service to that node. Enabling nofailback prevents that behavior.\n\n--restricted <boolean> (default = 0)\n\nResources bound to restricted groups may only run on nodes defined by the group.\n\n--type <group>\n\nGroup type.\n\nha-manager groupconfig\n\nGet HA groups.\n\nha-manager groupremove <group>\n\nDelete ha group configuration.\n\n<group>: <string>\n\nThe HA group identifier.\n\nha-manager groupset <group> [OPTIONS]\n\nUpdate ha group configuration.\n\n<group>: <string>\n\nThe HA group identifier.\n\n--comment <string>\n\nDescription.\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--digest <string>\n\nPrevent changes if current configuration file has a different digest. This can be used to prevent concurrent modifications.\n\n--nodes <node>[:<pri>]{,<node>[:<pri>]}*\n\nList of cluster node names with optional priority.\n\n--nofailback <boolean> (default = 0)\n\nThe CRM tries to run services on the node with the highest priority. If a node with higher priority comes online, the CRM migrates the service to that node. Enabling nofailback prevents that behavior.\n\n--restricted <boolean> (default = 0)\n\nResources bound to restricted groups may only run on nodes defined by the group.\n\nha-manager help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\nha-manager migrate\n\nAn alias for ha-manager crm-command migrate.\n\nha-manager relocate\n\nAn alias for ha-manager crm-command relocate.\n\nha-manager remove <sid>\n\nDelete resource configuration.\n\n<sid>: <type>:<name>\n\nHA resource ID. This consists of a resource type followed by a resource specific name, separated with colon (example: vm:100 / ct:100). For virtual machines and containers, you can simply use the VM or CT id as a shortcut (example: 100).\n\nha-manager set <sid> [OPTIONS]\n\nUpdate resource configuration.\n\n<sid>: <type>:<name>\n\nHA resource ID. This consists of a resource type followed by a resource specific name, separated with colon (example: vm:100 / ct:100). For virtual machines and containers, you can simply use the VM or CT id as a shortcut (example: 100).\n\n--comment <string>\n\nDescription.\n\n--delete <string>\n\nA list of settings you want to delete.\n\n--digest <string>\n\nPrevent changes if current configuration file has a different digest. This can be used to prevent concurrent modifications.\n\n--group <string>\n\nThe HA group identifier.\n\n--max_relocate <integer> (0 - N) (default = 1)\n\nMaximal number of service relocate tries when a service failes to start.\n\n--max_restart <integer> (0 - N) (default = 1)\n\nMaximal number of tries to restart the service on a node after its start failed.\n\n--state <disabled | enabled | ignored | started | stopped> (default = started)\n\nRequested resource state.\n\nha-manager status [OPTIONS]\n\nDisplay HA manger status.\n\n--verbose <boolean> (default = 0)\n\nVerbose output. Include complete CRM and LRM status (JSON).\n\n23. Appendix B: Service Daemons\n23.1. pve-firewall - Proxmox VE Firewall Daemon\n\npve-firewall <COMMAND> [ARGS] [OPTIONS]\n\npve-firewall compile\n\nCompile and print firewall rules. This is useful for testing.\n\npve-firewall help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npve-firewall localnet\n\nPrint information about local network.\n\npve-firewall restart\n\nRestart the Proxmox VE firewall service.\n\npve-firewall simulate [OPTIONS]\n\nSimulate firewall rules. This does not simulates the kernel routing table, but simply assumes that routing from source zone to destination zone is possible.\n\n--dest <string>\n\nDestination IP address.\n\n--dport <integer>\n\nDestination port.\n\n--from (host|outside|vm\\d+|ct\\d+|([a-zA-Z][a-zA-Z0-9]{0,9})/(\\S+)) (default = outside)\n\nSource zone.\n\n--protocol (tcp|udp) (default = tcp)\n\nProtocol.\n\n--source <string>\n\nSource IP address.\n\n--sport <integer>\n\nSource port.\n\n--to (host|outside|vm\\d+|ct\\d+|([a-zA-Z][a-zA-Z0-9]{0,9})/(\\S+)) (default = host)\n\nDestination zone.\n\n--verbose <boolean> (default = 0)\n\nVerbose output.\n\npve-firewall start [OPTIONS]\n\nStart the Proxmox VE firewall service.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npve-firewall status\n\nGet firewall status.\n\npve-firewall stop\n\nStop the Proxmox VE firewall service. Note, stopping actively removes all Proxmox VE related iptable rules rendering the host potentially unprotected.\n\n23.2. pvedaemon - Proxmox VE API Daemon\n\npvedaemon <COMMAND> [ARGS] [OPTIONS]\n\npvedaemon help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvedaemon restart\n\nRestart the daemon (or start if not running).\n\npvedaemon start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npvedaemon status\n\nGet daemon status.\n\npvedaemon stop\n\nStop the daemon.\n\n23.3. pveproxy - Proxmox VE API Proxy Daemon\n\npveproxy <COMMAND> [ARGS] [OPTIONS]\n\npveproxy help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npveproxy restart\n\nRestart the daemon (or start if not running).\n\npveproxy start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npveproxy status\n\nGet daemon status.\n\npveproxy stop\n\nStop the daemon.\n\n23.4. pvestatd - Proxmox VE Status Daemon\n\npvestatd <COMMAND> [ARGS] [OPTIONS]\n\npvestatd help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvestatd restart\n\nRestart the daemon (or start if not running).\n\npvestatd start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npvestatd status\n\nGet daemon status.\n\npvestatd stop\n\nStop the daemon.\n\n23.5. spiceproxy - SPICE Proxy Service\n\nspiceproxy <COMMAND> [ARGS] [OPTIONS]\n\nspiceproxy help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\nspiceproxy restart\n\nRestart the daemon (or start if not running).\n\nspiceproxy start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\nspiceproxy status\n\nGet daemon status.\n\nspiceproxy stop\n\nStop the daemon.\n\n23.6. pmxcfs - Proxmox Cluster File System\n\npmxcfs [OPTIONS]\n\nHelp Options:\n\n-h, --help\n\nShow help options\n\nApplication Options:\n\n-d, --debug\n\nTurn on debug messages\n\n-f, --foreground\n\nDo not daemonize server\n\n-l, --local\n\nForce local mode (ignore corosync.conf, force quorum)\n\nThis service is usually started and managed using systemd toolset. The service is called pve-cluster.\n\nsystemctl start pve-cluster\nsystemctl stop pve-cluster\nsystemctl status pve-cluster\n23.7. pve-ha-crm - Cluster Resource Manager Daemon\n\npve-ha-crm <COMMAND> [ARGS] [OPTIONS]\n\npve-ha-crm help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npve-ha-crm start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npve-ha-crm status\n\nGet daemon status.\n\npve-ha-crm stop\n\nStop the daemon.\n\n23.8. pve-ha-lrm - Local Resource Manager Daemon\n\npve-ha-lrm <COMMAND> [ARGS] [OPTIONS]\n\npve-ha-lrm help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npve-ha-lrm start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npve-ha-lrm status\n\nGet daemon status.\n\npve-ha-lrm stop\n\nStop the daemon.\n\n23.9. pvescheduler - Proxmox VE Scheduler Daemon\n\npvescheduler <COMMAND> [ARGS] [OPTIONS]\n\npvescheduler help [OPTIONS]\n\nGet help about specified command.\n\n--extra-args <array>\n\nShows help for a specific command\n\n--verbose <boolean>\n\nVerbose output format.\n\npvescheduler restart\n\nRestart the daemon (or start if not running).\n\npvescheduler start [OPTIONS]\n\nStart the daemon.\n\n--debug <boolean> (default = 0)\n\nDebug mode - stay in foreground\n\npvescheduler status\n\nGet daemon status.\n\npvescheduler stop\n\nStop the daemon.\n\n24. Appendix C: Configuration Files\n24.1. Datacenter Configuration\n\nThe file /etc/pve/datacenter.cfg is a configuration file for Proxmox VE. It contains cluster wide default values used by all nodes.\n\n24.1.1. File Format\n\nThe file uses a simple colon separated key/value format. Each line has the following format:\n\nOPTION: value\n\nBlank lines in the file are ignored, and lines starting with a # character are treated as comments and are also ignored.\n\n24.1.2. Options\nbwlimit: [clone=<LIMIT>] [,default=<LIMIT>] [,migration=<LIMIT>] [,move=<LIMIT>] [,restore=<LIMIT>]\n\nSet I/O bandwidth limit for various operations (in KiB/s).\n\nclone=<LIMIT>\n\nbandwidth limit in KiB/s for cloning disks\n\ndefault=<LIMIT>\n\ndefault bandwidth limit in KiB/s\n\nmigration=<LIMIT>\n\nbandwidth limit in KiB/s for migrating guests (including moving local disks)\n\nmove=<LIMIT>\n\nbandwidth limit in KiB/s for moving disks\n\nrestore=<LIMIT>\n\nbandwidth limit in KiB/s for restoring guests from backups\n\nconsole: <applet | html5 | vv | xtermjs>\n\nSelect the default Console viewer. You can either use the builtin java applet (VNC; deprecated and maps to html5), an external virt-viewer comtatible application (SPICE), an HTML5 based vnc viewer (noVNC), or an HTML5 based console client (xtermjs). If the selected viewer is not available (e.g. SPICE not activated for the VM), the fallback is noVNC.\n\ncrs: [ha=<basic|static>] [,ha-rebalance-on-start=<1|0>]\n\nCluster resource scheduling settings.\n\nha=<basic | static> (default = basic)\n\nConfigures how the HA manager should select nodes to start or recover services. With basic, only the number of services is used, with static, static CPU and memory configuration of services is considered.\n\nha-rebalance-on-start=<boolean> (default = 0)\n\nSet to use CRS for selecting a suited node when a HA services request-state changes from stop to start.\n\ndescription: <string>\n\nDatacenter description. Shown in the web-interface datacenter notes panel. This is saved as comment inside the configuration file.\n\nemail_from: <string>\n\nSpecify email address to send notification from (default is root@$hostname)\n\nfencing: <both | hardware | watchdog> (default = watchdog)\n\nSet the fencing mode of the HA cluster. Hardware mode needs a valid configuration of fence devices in /etc/pve/ha/fence.cfg. With both all two modes are used.\n\n\thardware and both are EXPERIMENTAL & WIP\nha: shutdown_policy=<enum>\n\nCluster wide HA settings.\n\nshutdown_policy=<conditional | failover | freeze | migrate> (default = conditional)\n\nDescribes the policy for handling HA services on poweroff or reboot of a node. Freeze will always freeze services which are still located on the node on shutdown, those services won’t be recovered by the HA manager. Failover will not mark the services as frozen and thus the services will get recovered to other nodes, if the shutdown node does not come up again quickly (< 1min). conditional chooses automatically depending on the type of shutdown, i.e., on a reboot the service will be frozen but on a poweroff the service will stay as is, and thus get recovered after about 2 minutes. Migrate will try to move all running services to another node when a reboot or shutdown was triggered. The poweroff process will only continue once no running services are located on the node anymore. If the node comes up again, the service will be moved back to the previously powered-off node, at least if no other migration, reloaction or recovery took place.\n\nhttp_proxy: http://.*\n\nSpecify external http proxy which is used for downloads (example: http://username:password@host:port/)\n\nkeyboard: <da | de | de-ch | en-gb | en-us | es | fi | fr | fr-be | fr-ca | fr-ch | hu | is | it | ja | lt | mk | nl | no | pl | pt | pt-br | sl | sv | tr>\n\nDefault keybord layout for vnc server.\n\nlanguage: <ar | ca | da | de | en | es | eu | fa | fr | he | hr | it | ja | ka | kr | nb | nl | nn | pl | pt_BR | ru | sl | sv | tr | ukr | zh_CN | zh_TW>\n\nDefault GUI language.\n\nmac_prefix: <string> (default = BC:24:11)\n\nPrefix for the auto-generated MAC addresses of virtual guests. The default BC:24:11 is the Organizationally Unique Identifier (OUI) assigned by the IEEE to Proxmox Server Solutions GmbH for a MAC Address Block Large (MA-L). You’re allowed to use this in local networks, i.e., those not directly reachable by the public (e.g., in a LAN or NAT/Masquerading).\n\nNote that when you run multiple cluster that (partially) share the networks of their virtual guests, it’s highly recommended that you extend the default MAC prefix, or generate a custom (valid) one, to reduce the chance of MAC collisions. For example, add a separate extra hexadecimal to the Proxmox OUI for each cluster, like BC:24:11:0 for the first, BC:24:11:1 for the second, and so on. Alternatively, you can also separate the networks of the guests logically, e.g., by using VLANs.\n\n+ For publicly accessible guests it’s recommended that you get your own OUI from the IEEE registered or coordinate with your, or your hosting providers, network admins.\n\nmax_workers: <integer> (1 - N)\n\nDefines how many workers (per node) are maximal started on actions like stopall VMs or task from the ha-manager.\n\nmigration: [type=]<secure|insecure> [,network=<CIDR>]\n\nFor cluster wide migration settings.\n\nnetwork=<CIDR>\n\nCIDR of the (sub) network that is used for migration.\n\ntype=<insecure | secure> (default = secure)\n\nMigration traffic is encrypted using an SSH tunnel by default. On secure, completely private networks this can be disabled to increase performance.\n\nmigration_unsecure: <boolean>\n\nMigration is secure using SSH tunnel by default. For secure private networks you can disable it to speed up migration. Deprecated, use the migration property instead!\n\nnext-id: [lower=<integer>] [,upper=<integer>]\n\nControl the range for the free VMID auto-selection pool.\n\nlower=<integer> (default = 100)\n\nLower, inclusive boundary for free next-id API range.\n\nupper=<integer> (default = 1000000)\n\nUpper, exclusive boundary for free next-id API range.\n\nnotify: [fencing=<always|never>] [,package-updates=<auto|always|never>] [,replication=<always|never>] [,target-fencing=<TARGET>] [,target-package-updates=<TARGET>] [,target-replication=<TARGET>]\n\nCluster-wide notification settings.\n\nfencing=<always | never>\n\nUNUSED - Use datacenter notification settings instead.\n\npackage-updates=<always | auto | never> (default = auto)\n\nDEPRECATED: Use datacenter notification settings instead. Control how often the daily update job should send out notifications:\n\nauto daily for systems with a valid subscription, as those are assumed to be production-ready and thus should know about pending updates.\n\nalways every update, if there are new pending updates.\n\nnever never send a notification for new pending updates.\n\nreplication=<always | never>\n\nUNUSED - Use datacenter notification settings instead.\n\ntarget-fencing=<TARGET>\n\nUNUSED - Use datacenter notification settings instead.\n\ntarget-package-updates=<TARGET>\n\nUNUSED - Use datacenter notification settings instead.\n\ntarget-replication=<TARGET>\n\nUNUSED - Use datacenter notification settings instead.\n\nregistered-tags: <tag>[;<tag>...]\n\nA list of tags that require a Sys.Modify on / to set and delete. Tags set here that are also in user-tag-access also require Sys.Modify.\n\ntag-style: [case-sensitive=<1|0>] [,color-map=<tag>:<hex-color>[:<hex-color-for-text>][;<tag>=...]] [,ordering=<config|alphabetical>] [,shape=<enum>]\n\nTag style options.\n\ncase-sensitive=<boolean> (default = 0)\n\nControls if filtering for unique tags on update should check case-sensitive.\n\ncolor-map=<tag>:<hex-color>[:<hex-color-for-text>][;<tag>=...]\n\nManual color mapping for tags (semicolon separated).\n\nordering=<alphabetical | config> (default = alphabetical)\n\nControls the sorting of the tags in the web-interface and the API update.\n\nshape=<circle | dense | full | none> (default = circle)\n\nTag shape for the web ui tree. full draws the full tag. circle draws only a circle with the background color. dense only draws a small rectancle (useful when many tags are assigned to each guest).none disables showing the tags.\n\nu2f: [appid=<APPID>] [,origin=<URL>]\n\nu2f\n\nappid=<APPID>\n\nU2F AppId URL override. Defaults to the origin.\n\norigin=<URL>\n\nU2F Origin override. Mostly useful for single nodes with a single URL.\n\nuser-tag-access: [user-allow=<enum>] [,user-allow-list=<tag>[;<tag>...]]\n\nPrivilege options for user-settable tags\n\nuser-allow=<existing | free | list | none> (default = free)\n\nControls which tags can be set or deleted on resources a user controls (such as guests). Users with the Sys.Modify privilege on / are alwaysunrestricted.\n\nnone no tags are usable.\n\nlist tags from user-allow-list are usable.\n\nexisting like list, but already existing tags of resources are also usable.\n\nfree no tag restrictions.\n\nuser-allow-list=<tag>[;<tag>...]\n\nList of tags users are allowed to set and delete (semicolon separated) for user-allow values list and existing.\n\nwebauthn: [allow-subdomains=<1|0>] [,id=<DOMAINNAME>] [,origin=<URL>] [,rp=<RELYING_PARTY>]\n\nwebauthn configuration\n\nallow-subdomains=<boolean> (default = 1)\n\nWhether to allow the origin to be a subdomain, rather than the exact URL.\n\nid=<DOMAINNAME>\n\nRelying party ID. Must be the domain name without protocol, port or location. Changing this will break existing credentials.\n\norigin=<URL>\n\nSite origin. Must be a https:// URL (or http://localhost). Should contain the address users type in their browsers to access the web interface. Changing this may break existing credentials.\n\nrp=<RELYING_PARTY>\n\nRelying party name. Any text identifier. Changing this may break existing credentials.\n\n25. Appendix D: Calendar Events\n25.1. Schedule Format\n\nProxmox VE has a very flexible scheduling configuration. It is based on the systemd time calendar event format.[58] Calendar events may be used to refer to one or more points in time in a single expression.\n\nSuch a calendar event uses the following format:\n\n[WEEKDAY] [[YEARS-]MONTHS-DAYS] [HOURS:MINUTES[:SECONDS]]\n\nThis format allows you to configure a set of days on which the job should run. You can also set one or more start times. It tells the replication scheduler the moments in time when a job should start. With this information we, can create a job which runs every workday at 10 PM: 'mon,tue,wed,thu,fri 22' which could be abbreviated to: 'mon..fri 22', most reasonable schedules can be written quite intuitive this way.\n\n\tHours are formatted in 24-hour format.\n\nTo allow a convenient and shorter configuration, one or more repeat times per guest can be set. They indicate that replications are done on the start-time(s) itself and the start-time(s) plus all multiples of the repetition value. If you want to start replication at 8 AM and repeat it every 15 minutes until 9 AM you would use: '8:00/15'\n\nHere you see that if no hour separation (:), is used the value gets interpreted as minute. If such a separation is used, the value on the left denotes the hour(s), and the value on the right denotes the minute(s). Further, you can use * to match all possible values.\n\nTo get additional ideas look at more Examples below.\n\n25.2. Detailed Specification\nweekdays\n\nDays are specified with an abbreviated English version: sun, mon, tue, wed, thu, fri and sat. You may use multiple days as a comma-separated list. A range of days can also be set by specifying the start and end day separated by “..”, for example mon..fri. These formats can be mixed. If omitted '*' is assumed.\n\ntime-format\n\nA time format consists of hours and minutes interval lists. Hours and minutes are separated by ':'. Both hour and minute can be list and ranges of values, using the same format as days. First are hours, then minutes. Hours can be omitted if not needed. In this case '*' is assumed for the value of hours. The valid range for values is 0-23 for hours and 0-59 for minutes.\n\n25.2.1. Examples:\n\nThere are some special values that have a specific meaning:\n\nTable 19. Special Values\nValue\tSyntax\n\n\nminutely\n\n\t\n\n*-*-* *:*:00\n\n\n\n\nhourly\n\n\t\n\n*-*-* *:00:00\n\n\n\n\ndaily\n\n\t\n\n*-*-* 00:00:00\n\n\n\n\nweekly\n\n\t\n\nmon *-*-* 00:00:00\n\n\n\n\nmonthly\n\n\t\n\n*-*-01 00:00:00\n\n\n\n\nyearly or annually\n\n\t\n\n*-01-01 00:00:00\n\n\n\n\nquarterly\n\n\t\n\n*-01,04,07,10-01 00:00:00\n\n\n\n\nsemiannually or semi-annually\n\n\t\n\n*-01,07-01 00:00:00\n\nTable 20. Schedule Examples\nSchedule String\tAlternative\tMeaning\n\n\nmon,tue,wed,thu,fri\n\n\t\n\nmon..fri\n\n\t\n\nEvery working day at 0:00\n\n\n\n\nsat,sun\n\n\t\n\nsat..sun\n\n\t\n\nOnly on weekends at 0:00\n\n\n\n\nmon,wed,fri\n\n\t\n\n— \n\n\t\n\nOnly on Monday, Wednesday and Friday at 0:00\n\n\n\n\n12:05\n\n\t\n\n12:05\n\n\t\n\nEvery day at 12:05 PM\n\n\n\n\n*/5\n\n\t\n\n0/5\n\n\t\n\nEvery five minutes\n\n\n\n\nmon..wed 30/10\n\n\t\n\nmon,tue,wed 30/10\n\n\t\n\nMonday, Tuesday, Wednesday 30, 40 and 50 minutes after every full hour\n\n\n\n\nmon..fri 8..17,22:0/15\n\n\t\n\n— \n\n\t\n\nEvery working day every 15 minutes between 8 AM and 6 PM and between 10 PM and 11 PM\n\n\n\n\nfri 12..13:5/20\n\n\t\n\nfri 12,13:5/20\n\n\t\n\nFriday at 12:05, 12:25, 12:45, 13:05, 13:25 and 13:45\n\n\n\n\n12,14,16,18,20,22:5\n\n\t\n\n12/2:5\n\n\t\n\nEvery day starting at 12:05 until 22:05, every 2 hours\n\n\n\n\n*\n\n\t\n\n*/1\n\n\t\n\nEvery minute (minimum interval)\n\n\n\n\n*-05\n\n\t\n\n— \n\n\t\n\nOn the 5th day of every Month\n\n\n\n\nSat *-1..7 15:00\n\n\t\n\n— \n\n\t\n\nFirst Saturday each Month at 15:00\n\n\n\n\n2015-10-21\n\n\t\n\n— \n\n\t\n\n21st October 2015 at 00:00\n\n26. Appendix E: QEMU vCPU List\n26.1. Introduction\n\nThis is a list of AMD and Intel x86-64/amd64 CPU types as defined in QEMU, going back to 2007.\n\n26.2. Intel CPU Types\n\nIntel processors\n\nNahelem : 1st generation of the Intel Core processor\n\nNahelem-IBRS (v2) : add Spectre v1 protection (+spec-ctrl)\n\nWestmere : 1st generation of the Intel Core processor (Xeon E7-)\n\nWestmere-IBRS (v2) : add Spectre v1 protection (+spec-ctrl)\n\nSandyBridge : 2nd generation of the Intel Core processor\n\nSandyBridge-IBRS (v2) : add Spectre v1 protection (+spec-ctrl)\n\nIvyBridge : 3rd generation of the Intel Core processor\n\nIvyBridge-IBRS (v2): add Spectre v1 protection (+spec-ctrl)\n\nHaswell : 4th generation of the Intel Core processor\n\nHaswell-noTSX (v2) : disable TSX (-hle, -rtm)\n\nHaswell-IBRS (v3) : re-add TSX, add Spectre v1 protection (+hle, +rtm, +spec-ctrl)\n\nHaswell-noTSX-IBRS (v4) : disable TSX (-hle, -rtm)\n\nBroadwell: 5th generation of the Intel Core processor\n\nSkylake: 1st generation Xeon Scalable server processors\n\nSkylake-IBRS (v2) : add Spectre v1 protection, disable CLFLUSHOPT (+spec-ctrl, -clflushopt)\n\nSkylake-noTSX-IBRS (v3) : disable TSX (-hle, -rtm)\n\nSkylake-v4: add EPT switching (+vmx-eptp-switching)\n\nCascadelake: 2nd generation Xeon Scalable processor\n\nCascadelake-v2 : add arch_capabilities msr (+arch-capabilities, +rdctl-no, +ibrs-all, +skip-l1dfl-vmentry, +mds-no)\n\nCascadelake-v3 : disable TSX (-hle, -rtm)\n\nCascadelake-v4 : add EPT switching (+vmx-eptp-switching)\n\nCascadelake-v5 : add XSAVES (+xsaves, +vmx-xsaves)\n\nCooperlake : 3rd generation Xeon Scalable processors for 4 & 8 sockets servers\n\nCooperlake-v2 : add XSAVES (+xsaves, +vmx-xsaves)\n\nIcelake: 3rd generation Xeon Scalable server processors\n\nIcelake-v2 : disable TSX (-hle, -rtm)\n\nIcelake-v3 : add arch_capabilities msr (+arch-capabilities, +rdctl-no, +ibrs-all, +skip-l1dfl-vmentry, +mds-no, +pschange-mc-no, +taa-no)\n\nIcelake-v4 : add missing flags (+sha-ni, +avx512ifma, +rdpid, +fsrm, +vmx-rdseed-exit, +vmx-pml, +vmx-eptp-switching)\n\nIcelake-v5 : add XSAVES (+xsaves, +vmx-xsaves)\n\nIcelake-v6 : add \"5-level EPT\" (+vmx-page-walk-5)\n\nSapphireRapids : 4th generation Xeon Scalable server processors\n\n26.3. AMD CPU Types\n\nAMD processors\n\nOpteron_G3 : K10\n\nOpteron_G4 : Bulldozer\n\nOpteron_G5 : Piledriver\n\nEPYC : 1st generation of Zen processors\n\nEPYC-IBPB (v2) : add Spectre v1 protection (+ibpb)\n\nEPYC-v3 : add missing flags (+perfctr-core, +clzero, +xsaveerptr, +xsaves)\n\nEPYC-Rome : 2nd generation of Zen processors\n\nEPYC-Rome-v2 : add Spectre v2, v4 protection (+ibrs, +amd-ssbd)\n\nEPYC-Milan : 3rd generation of Zen processors\n\nEPYC-Milan-v2 : add missing flags (+vaes, +vpclmulqdq, +stibp-always-on, +amd-psfd, +no-nested-data-bp, +lfence-always-serializing, +null-sel-clr-base)\n\n27. Appendix F: Firewall Macro Definitions\nAmanda\n\t\n\nAmanda Backup\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n10080\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n10080\n\n\t\n\nAuth\n\t\n\nAuth (identd) traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n113\n\n\t\n\nBGP\n\t\n\nBorder Gateway Protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n179\n\n\t\n\nBitTorrent\n\t\n\nBitTorrent traffic for BitTorrent 3.1 and earlier\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6881:6889\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n6881\n\n\t\n\nBitTorrent32\n\t\n\nBitTorrent traffic for BitTorrent 3.2 and later\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6881:6999\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n6881\n\n\t\n\nCVS\n\t\n\nConcurrent Versions System pserver traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n2401\n\n\t\n\nCeph\n\t\n\nCeph Storage Cluster traffic (Ceph Monitors, OSD & MDS Daemons)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6789\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3300\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6800:7300\n\n\t\n\nCitrix\n\t\n\nCitrix/ICA traffic (ICA, ICA Browser, CGP)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n1494\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n1604\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n2598\n\n\t\n\nDAAP\n\t\n\nDigital Audio Access Protocol traffic (iTunes, Rythmbox daemons)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3689\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n3689\n\n\t\n\nDCC\n\t\n\nDistributed Checksum Clearinghouse spam filtering mechanism\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6277\n\n\t\n\nDHCPfwd\n\t\n\nForwarded DHCP traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n67:68\n\n\t\n\n67:68\n\nDHCPv6\n\t\n\nDHCPv6 traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n546:547\n\n\t\n\n546:547\n\nDNS\n\t\n\nDomain Name System traffic (upd and tcp)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n53\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n53\n\n\t\n\nDistcc\n\t\n\nDistributed Compiler service\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3632\n\n\t\n\nFTP\n\t\n\nFile Transfer Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n21\n\n\t\n\nFinger\n\t\n\nFinger protocol (RFC 742)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n79\n\n\t\n\nGNUnet\n\t\n\nGNUnet secure peer-to-peer networking traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n2086\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n2086\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n1080\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n1080\n\n\t\n\nGRE\n\t\n\nGeneric Routing Encapsulation tunneling protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\n47\n\n\t\n\n\t\n\nGit\n\t\n\nGit distributed revision control traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n9418\n\n\t\n\nHKP\n\t\n\nOpenPGP HTTP key server protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n11371\n\n\t\n\nHTTP\n\t\n\nHypertext Transfer Protocol (WWW)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n80\n\n\t\n\nHTTPS\n\t\n\nHypertext Transfer Protocol (WWW) over SSL\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n443\n\n\t\n\nICPV2\n\t\n\nInternet Cache Protocol V2 (Squid) traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n3130\n\n\t\n\nICQ\n\t\n\nAOL Instant Messenger traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n5190\n\n\t\n\nIMAP\n\t\n\nInternet Message Access Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n143\n\n\t\n\nIMAPS\n\t\n\nInternet Message Access Protocol over SSL\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n993\n\n\t\n\nIPIP\n\t\n\nIPIP capsulation traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\n94\n\n\t\n\n\t\n\nIPsec\n\t\n\nIPsec traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n500\n\n\t\n\n500\n\n\n\n\nPARAM\n\n\t\n\n50\n\n\t\n\n\t\n\nIPsecah\n\t\n\nIPsec authentication (AH) traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n500\n\n\t\n\n500\n\n\n\n\nPARAM\n\n\t\n\n51\n\n\t\n\n\t\n\nIPsecnat\n\t\n\nIPsec traffic and Nat-Traversal\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n500\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n4500\n\n\t\n\n\n\n\nPARAM\n\n\t\n\n50\n\n\t\n\n\t\n\nIRC\n\t\n\nInternet Relay Chat traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6667\n\n\t\n\nJetdirect\n\t\n\nHP Jetdirect printing\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n9100\n\n\t\n\nL2TP\n\t\n\nLayer 2 Tunneling Protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n1701\n\n\t\n\nLDAP\n\t\n\nLightweight Directory Access Protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n389\n\n\t\n\nLDAPS\n\t\n\nSecure Lightweight Directory Access Protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n636\n\n\t\n\nMDNS\n\t\n\nMulticast DNS\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n5353\n\n\t\n\nMSNP\n\t\n\nMicrosoft Notification Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n1863\n\n\t\n\nMSSQL\n\t\n\nMicrosoft SQL Server\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n1433\n\n\t\n\nMail\n\t\n\nMail traffic (SMTP, SMTPS, Submission)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n25\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n465\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n587\n\n\t\n\nMunin\n\t\n\nMunin networked resource monitoring traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n4949\n\n\t\n\nMySQL\n\t\n\nMySQL server\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3306\n\n\t\n\nNNTP\n\t\n\nNNTP traffic (Usenet).\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n119\n\n\t\n\nNNTPS\n\t\n\nEncrypted NNTP traffic (Usenet)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n563\n\n\t\n\nNTP\n\t\n\nNetwork Time Protocol (ntpd)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n123\n\n\t\n\nNeighborDiscovery\n\t\n\nIPv6 neighbor solicitation, neighbor and router advertisement\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nicmpv6\n\n\t\n\nrouter-solicitation\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nicmpv6\n\n\t\n\nrouter-advertisement\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nicmpv6\n\n\t\n\nneighbor-solicitation\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nicmpv6\n\n\t\n\nneighbor-advertisement\n\n\t\n\nOSPF\n\t\n\nOSPF multicast traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\n89\n\n\t\n\n\t\n\nOpenVPN\n\t\n\nOpenVPN traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n1194\n\n\t\n\nPCA\n\t\n\nSymantec PCAnywere (tm)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n5632\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n5631\n\n\t\n\nPMG\n\t\n\nProxmox Mail Gateway web interface\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n8006\n\n\t\n\nPOP3\n\t\n\nPOP3 traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n110\n\n\t\n\nPOP3S\n\t\n\nEncrypted POP3 traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n995\n\n\t\n\nPPtP\n\t\n\nPoint-to-Point Tunneling Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\n47\n\n\t\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n1723\n\n\t\n\nPing\n\t\n\nICMP echo request\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nicmp\n\n\t\n\necho-request\n\n\t\n\nPostgreSQL\n\t\n\nPostgreSQL server\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n5432\n\n\t\n\nPrinter\n\t\n\nLine Printer protocol printing\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n515\n\n\t\n\nRDP\n\t\n\nMicrosoft Remote Desktop Protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3389\n\n\t\n\nRIP\n\t\n\nRouting Information Protocol (bidirectional)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n520\n\n\t\n\nRNDC\n\t\n\nBIND remote management protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n953\n\n\t\n\nRazor\n\t\n\nRazor Antispam System\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n2703\n\n\t\n\nRdate\n\t\n\nRemote time retrieval (rdate)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n37\n\n\t\n\nRsync\n\t\n\nRsync server\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n873\n\n\t\n\nSANE\n\t\n\nSANE network scanning\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n6566\n\n\t\n\nSMB\n\t\n\nMicrosoft SMB traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n135,445\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n137:139\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n1024:65535\n\n\t\n\n137\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n135,139,445\n\n\t\n\nSMBswat\n\t\n\nSamba Web Administration Tool\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n901\n\n\t\n\nSMTP\n\t\n\nSimple Mail Transfer Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n25\n\n\t\n\nSMTPS\n\t\n\nEncrypted Simple Mail Transfer Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n465\n\n\t\n\nSNMP\n\t\n\nSimple Network Management Protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n161:162\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n161\n\n\t\n\nSPAMD\n\t\n\nSpam Assassin SPAMD traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n783\n\n\t\n\nSPICEproxy\n\t\n\nProxmox VE SPICE display proxy traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3128\n\n\t\n\nSSH\n\t\n\nSecure shell traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n22\n\n\t\n\nSVN\n\t\n\nSubversion server (svnserve)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3690\n\n\t\n\nSixXS\n\t\n\nSixXS IPv6 Deployment and Tunnel Broker\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3874\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n3740\n\n\t\n\n\n\n\nPARAM\n\n\t\n\n41\n\n\t\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n5072,8374\n\n\t\n\nSquid\n\t\n\nSquid web proxy traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n3128\n\n\t\n\nSubmission\n\t\n\nMail message submission traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n587\n\n\t\n\nSyslog\n\t\n\nSyslog protocol (RFC 5424) traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n514\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n514\n\n\t\n\nTFTP\n\t\n\nTrivial File Transfer Protocol traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n69\n\n\t\n\nTelnet\n\t\n\nTelnet traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n23\n\n\t\n\nTelnets\n\t\n\nTelnet over SSL\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n992\n\n\t\n\nTime\n\t\n\nRFC 868 Time protocol\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n37\n\n\t\n\nTrcrt\n\t\n\nTraceroute (for up to 30 hops) traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\nudp\n\n\t\n\n33434:33524\n\n\t\n\n\n\n\nPARAM\n\n\t\n\nicmp\n\n\t\n\necho-request\n\n\t\n\nVNC\n\t\n\nVNC traffic for VNC display’s 0 - 99\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n5900:5999\n\n\t\n\nVNCL\n\t\n\nVNC traffic from Vncservers to Vncviewers in listen mode\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n5500\n\n\t\n\nWeb\n\t\n\nWWW traffic (HTTP and HTTPS)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n80\n\n\t\n\n\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n443\n\n\t\n\nWebcache\n\t\n\nWeb Cache/Proxy traffic (port 8080)\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n8080\n\n\t\n\nWebmin\n\t\n\nWebmin traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n10000\n\n\t\n\nWhois\n\t\n\nWhois (nicname, RFC 3912) traffic\n\nAction\tproto\tdport\tsport\n\n\nPARAM\n\n\t\n\ntcp\n\n\t\n\n43\n\n\t\n\n28. Appendix G: Markdown Primer\n\nMarkdown is a text-to-HTML conversion tool for web writers. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).\n\nhttps://daringfireball.net/projects/markdown/\n— John Gruber\n\nThe Proxmox VE web interface has support for using Markdown to rendering rich text formatting in node and virtual guest notes.\n\nProxmox VE supports CommonMark with most extensions of GFM (GitHub Flavoured Markdown), like tables or task-lists.\n\n28.1. Markdown Basics\n\nNote that we only describe the basics here, please search the web for more extensive resources, for example on https://www.markdownguide.org/\n\n28.1.1. Headings\n# This is a Heading h1\n## This is a Heading h2\n##### This is a Heading h5\n28.1.2. Emphasis\n\nUse *text* or _text_ for emphasis.\n\nUse **text** or __text__ for bold, heavy-weight text.\n\nCombinations are also possible, for example:\n\n_You **can** combine them_\n28.1.3. Links\n\nYou can use automatic detection of links, for example, https://forum.proxmox.com/ would transform it into a clickable link.\n\nYou can also control the link text, for example:\n\nNow, [the part in brackets will be the link text](https://forum.proxmox.com/).\n28.1.4. Lists\nUnordered Lists\n\nUse * or - for unordered lists, for example:\n\n* Item 1\n* Item 2\n* Item 2a\n* Item 2b\n\nAdding an indentation can be used to created nested lists.\n\nOrdered Lists\n1. Item 1\n1. Item 2\n1. Item 3\n  1. Item 3a\n  1. Item 3b\n\tThe integer of ordered lists does not need to be correct, they will be numbered automatically.\nTask Lists\n\nTask list use a empty box [ ] for unfinished tasks and a box with an X for finished tasks.\n\nFor example:\n\n- [X] First task already done!\n- [X] Second one too\n- [ ] This one is still to-do\n- [ ] So is this one\n28.1.5. Tables\n\nTables use the pipe symbol | to separate columns, and - to separate the table header from the table body, in that separation one can also set the text alignment, making one column left-, center-, or right-aligned.\n\n| Left columns  | Right columns |  Some  | More | Cols.| Centering Works Too\n| ------------- |--------------:|--------|------|------|:------------------:|\n| left foo      | right foo     | First  | Row  | Here | >center<           |\n| left bar      | right bar     | Second | Row  | Here | 12345              |\n| left baz      | right baz     | Third  | Row  | Here | Test               |\n| left zab      | right zab     | Fourth | Row  | Here | ☁️☁️☁️              |\n| left rab      | right rab     | And    | Last | Here | The End            |\n\nNote that you do not need to align the columns nicely with white space, but that makes editing tables easier.\n\n28.1.6. Block Quotes\n\nYou can enter block quotes by prefixing a line with >, similar as in plain-text emails.\n\n> Markdown is a lightweight markup language with plain-text-formatting syntax,\n> created in 2004 by John Gruber with Aaron Swartz.\n>\n>> Markdown is often used to format readme files, for writing messages in online discussion forums,\n>> and to create rich text using a plain text editor.\n28.1.7. Code and Snippets\n\nYou can use backticks to avoid processing for a few word or paragraphs. That is useful for avoiding that a code or configuration hunk gets mistakenly interpreted as markdown.\n\nInline code\n\nSurrounding part of a line with single backticks allows to write code inline, for examples:\n\nThis hosts IP address is `10.0.0.1`.\nWhole blocks of code\n\nFor code blocks spanning several lines you can use triple-backticks to start and end such a block, for example:\n\n```\n# This is the network config I want to remember here\nauto vmbr2\niface vmbr2 inet static\n        address 10.0.0.1/24\n        bridge-ports ens20\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n\n```\n29. Appendix H: GNU Free Documentation License\n\nVersion 1.3, 3 November 2008\n\nCopyright (C) 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc.\n    <http://fsf.org/>\nEveryone is permitted to copy and distribute verbatim copies\nof this license document, but changing it is not allowed.\n0. PREAMBLE\n\nThe purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.\n\nThis License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software.\n\nWe have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.\n\n1. APPLICABILITY AND DEFINITIONS\n\nThis License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.\n\nA \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.\n\nA \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document’s overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.\n\nThe \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none.\n\nThe \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.\n\nA \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\".\n\nExamples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.\n\nThe \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work’s title, preceding the beginning of the body of the text.\n\nThe \"publisher\" means any person or entity that distributes copies of the Document to the public.\n\nA section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition.\n\nThe Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.\n\n2. VERBATIM COPYING\n\nYou may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3.\n\nYou may also lend copies, under the same conditions stated above, and you may publicly display copies.\n\n3. COPYING IN QUANTITY\n\nIf you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document’s license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.\n\nIf the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.\n\nIf you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.\n\nIt is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.\n\n4. MODIFICATIONS\n\nYou may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version:\n\nUse in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.\n\nList on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.\n\nState on the Title page the name of the publisher of the Modified Version, as the publisher.\n\nPreserve all the copyright notices of the Document.\n\nAdd an appropriate copyright notice for your modifications adjacent to the other copyright notices.\n\nInclude, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.\n\nPreserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document’s license notice.\n\nInclude an unaltered copy of this License.\n\nPreserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.\n\nPreserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.\n\nFor any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.\n\nPreserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.\n\nDelete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version.\n\nDo not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section.\n\nPreserve any Warranty Disclaimers.\n\nIf the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version’s license notice. These titles must be distinct from any other section titles.\n\nYou may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties—for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.\n\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.\n\nThe author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.\n\n5. COMBINING DOCUMENTS\n\nYou may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.\n\nThe combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.\n\nIn the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".\n\n6. COLLECTIONS OF DOCUMENTS\n\nYou may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.\n\nYou may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.\n\n7. AGGREGATION WITH INDEPENDENT WORKS\n\nA compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation’s users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.\n\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.\n\n8. TRANSLATION\n\nTranslation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.\n\nIf a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.\n\n9. TERMINATION\n\nYou may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License.\n\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\n\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\n\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.\n\n10. FUTURE REVISIONS OF THIS LICENSE\n\nThe Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See http://www.gnu.org/copyleft/.\n\nEach version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Document.\n\n11. RELICENSING\n\n\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site.\n\n\"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization.\n\n\"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document.\n\nAn MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008.\n\nThe operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.\n\n1. smartmontools homepage https://www.smartmontools.org\n2. OpenZFS dRAID https://openzfs.github.io/openzfs-docs/Basic%20Concepts/dRAID%20Howto.html\n3. Systems installed with Proxmox VE 6.4 or later, EFI systems installed with Proxmox VE 5.4 or later\n4. https://bugzilla.proxmox.com/show_bug.cgi?id=2350\n5. https://github.com/openzfs/zfs/issues/11688\n6. acme.sh https://github.com/acmesh-official/acme.sh\n7. These are all installs with root on ext4 or xfs and installs with root on ZFS on non-EFI systems\n8. Booting ZFS on root with GRUB https://github.com/zfsonlinux/zfs/wiki/Debian-Stretch-Root-on-ZFS\n9. GRUB Manual https://www.gnu.org/software/grub/manual/grub/grub.html\n10. Systems using proxmox-boot-tool will call proxmox-boot-tool refresh upon update-grub.\n11. votequorum_qdevice_master_wins manual page https://manpages.debian.org/bookworm/libvotequorum-dev/votequorum_qdevice_master_wins.3.en.html\n12. Ceph User Management\n13. RBD configuration reference https://docs.ceph.com/en/quincy/rbd/rbd-config-ref/\n14. Ceph intro https://docs.ceph.com/en/quincy/start/intro/\n15. Ceph architecture https://docs.ceph.com/en/quincy/architecture/\n16. Ceph glossary https://docs.ceph.com/en/quincy/glossary\n17. Full Mesh Network for Ceph https://pve.proxmox.com/wiki/Full_Mesh_Network_for_Ceph_Server\n18. Ceph Monitor https://docs.ceph.com/en/quincy/start/intro/\n19. Ceph Manager https://docs.ceph.com/en/quincy/mgr/\n20. Ceph Bluestore https://ceph.com/community/new-luminous-bluestore/\n21. PG calculator https://web.archive.org/web/20210301111112/http://ceph.com/pgcalc/\n22. Placement Groups https://docs.ceph.com/en/quincy/rados/operations/placement-groups/\n23. Automated Scaling https://docs.ceph.com/en/quincy/rados/operations/placement-groups/#automated-scaling\n24. Ceph pool operation https://docs.ceph.com/en/quincy/rados/operations/pools/\n25. Ceph Erasure Coded Pool Recovery https://docs.ceph.com/en/quincy/rados/operations/erasure-code/#erasure-coded-pool-recovery\n26. Ceph Erasure Code Profile https://docs.ceph.com/en/quincy/rados/operations/erasure-code/#erasure-code-profiles\n27. CRUSH https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf\n28. CRUSH map https://docs.ceph.com/en/quincy/rados/operations/crush-map/\n29. Configuring multiple active MDS daemons https://docs.ceph.com/en/quincy/cephfs/multimds/\n30. Ceph scrubbing https://docs.ceph.com/en/quincy/rados/configuration/osd-config-ref/#scrubbing\n31. Ceph log and debugging https://docs.ceph.com/en/quincy/rados/troubleshooting/log-and-debug/\n32. Ceph troubleshooting https://docs.ceph.com/en/quincy/rados/troubleshooting/\n33. See this benchmark on the KVM wiki https://www.linux-kvm.org/page/Using_VirtIO_NIC\n34. See this benchmark for details https://events.static.linuxfound.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf\n35. TRIM, UNMAP, and discard https://en.wikipedia.org/wiki/Trim_%28computing%29\n36. Meltdown Attack https://meltdownattack.com/\n37. spectre-meltdown-checker https://meltdown.ovh/\n38. PCID is now a critical performance/security feature on x86 https://groups.google.com/forum/m/#!topic/mechanical-sympathy/L9mHTbeQLNU\n39. https://en.wikipedia.org/wiki/Non-uniform_memory_access\n40. if the command numactl --hardware | grep available returns more than one node, then your host system has a NUMA architecture\n41. A good explanation of the inner workings of the balloon driver can be found here https://rwmj.wordpress.com/2010/07/17/virtio-balloon/\n42. https://www.kraxel.org/blog/2014/10/qemu-using-cirrus-considered-harmful/ qemu: using cirrus considered harmful\n43. See the OVMF Project https://github.com/tianocore/tianocore.github.io/wiki/OVMF\n44. Alex Williamson has a good blog entry about this https://vfio.blogspot.co.at/2014/08/primary-graphics-assignment-without-vga.html\n45. Looking Glass: https://looking-glass.io/\n46. Official vmgenid Specification https://docs.microsoft.com/en-us/windows/desktop/hyperv_v2/virtual-machine-generation-identifier\n47. Online GUID generator http://guid.one/\n48. https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/get-started/virtual-dc/virtualized-domain-controller-architecture\n49. this includes all newest major versions of container templates shipped by Proxmox VE\n50. for example Alpine Linux\n51. /etc/os-release replaces the multitude of per-distribution release files https://manpages.debian.org/stable/systemd/os-release.5.en.html\n52. AppId https://developers.yubico.com/U2F/App_ID.html\n53. Multi-facet apps: https://developers.yubico.com/U2F/App_ID.html\n54. Lempel–Ziv–Oberhumer a lossless data compression algorithm https://en.wikipedia.org/wiki/Lempel-Ziv-Oberhumer\n55. gzip - based on the DEFLATE algorithm https://en.wikipedia.org/wiki/Gzip\n56. Zstandard a lossless data compression algorithm https://en.wikipedia.org/wiki/Zstandard\n57. pigz - parallel implementation of gzip https://zlib.net/pigz/\n58. see man 7 systemd.time for more information\nVersion 8.2.2\nLast updated Thu Apr 25 09:24:16 CEST 2024"
  }
]