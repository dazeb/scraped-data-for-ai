[
  {
    "title": "PremAI | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/PremAI",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nAnyscale\nCohere\nOpenAI\nPremAI\nTogether\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsRemote Language Model ClientsPremAI\nPremAI\nPremAI​\n\nPremAI is an all-in-one platform that simplifies the process of creating robust, production-ready applications powered by Generative AI. By streamlining the development process, PremAI allows you to concentrate on enhancing user experience and driving overall growth for your application.\n\nPrerequisites​\n\nRefer to the quick start guide to getting started with the PremAI platform, create your first project and grab your API key.\n\nSetting up the PremAI Client​\n\nThe constructor initializes the base class LM to support prompting requests to supported PremAI hosted models. This requires the following parameters:\n\nmodel (str): Models supported by PremAI. Example: mistral-tiny. We recommend using the model selected in project launchpad.\nproject_id (int): The project id which contains the model of choice.\napi_key (Optional[str], optional): API provider from PremAI. Defaults to None.\n**kwargs: Additional language model arguments will be passed to the API provider.\n\nExample of PremAI constructor:\n\nclass PremAI(LM):\n    def __init__(\n        self,\n        model: str,\n        project_id: int,\n        api_key: str,\n        base_url: Optional[str] = None,\n        session_id: Optional[int] = None,\n        **kwargs,\n    ) -> None:\n\nUnder the Hood​\n__call__(self, prompt: str, **kwargs) -> str​\n\nParameters:\n\nprompt (str): Prompt to send to PremAI.\n**kwargs: Additional keyword arguments for completion request. You can find all the additional kwargs here.\n\nReturns:\n\nstr: Completions string from the chosen LLM provider\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nUsing the PremAI client​\npremai_client = dspy.PremAI(project_id=1111)\n\n\nPlease note that, this is a dummy project_id. You need to change this to the project_id you are interested to use with dspy.\n\ndspy.configure(lm=premai_client)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") \nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = premai_client(prompt='What is the capital of Paris?')\nprint(response)\n\nNative RAG Support​\n\nPremAI Repositories allow users to upload documents (.txt, .pdf, etc.) and connect those repositories to the LLMs to serve as vector databases and support native RAG. You can learn more about PremAI repositories here.\n\nRepositories are also supported through the dspy-premai integration. Here is how you can use this workflow:\n\nquery = \"what is the diameter of individual Galaxy\"\nrepository_ids = [1991, ]\nrepositories = dict(\n    ids=repository_ids,\n    similarity_threshold=0.3,\n    limit=3\n)\n\n\nFirst, we start by defining our repository with some valid repository ids. You can learn more about how to get the repository id here.\n\nNote: This is similar to LM integrations where now you are overriding the repositories connected in the launchpad when you invoke the argument' repositories'.\n\nNow, we connect the repository with our chat object to invoke RAG-based generations.\n\nresponse = premai_client(prompt=query, max_tokens=100, repositories=repositories)\n\nprint(response)\nprint(\"---\")\nprint(json.dumps(premai_client.history, indent=4))\n\nPrevious\nOpenAI\nNext\nTogether\nPremAI\nPrerequisites\nSetting up the PremAI Client\nUnder the Hood\nUsing the PremAI client\nNative RAG Support\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "OpenAI | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/OpenAI",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nAnyscale\nCohere\nOpenAI\nPremAI\nTogether\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsRemote Language Model ClientsOpenAI\nOpenAI\nOpenAI​\nPrerequisites​\nOpenAI api_key (for non-cached examples)\nSetting up the OpenAI Client​\n\nThe constructor initializes the base class LM to support prompting requests to OpenAI models. This requires the following parameters:\n\napi_key (Optional[str], optional): OpenAI API provider authentication token. Defaults to None.\napi_provider (Literal[\"openai\", \"azure\"], optional): OpenAI API provider to use. Defaults to \"openai\".\napi_base (Optional[str], optional): Base URL for the OpenAI API endpoint. Defaults to None.\nmodel_type (Literal[\"chat\", \"text\"]): Specified model type to use. Defaults to \"gpt-3.5-turbo-instruct\".\n**kwargs: Additional language model arguments to pass to OpenAI request. This is initialized with default values for relevant text generation parameters needed for communicating with the GPT API, such as temperature, max_tokens, top_p, frequency_penalty, presence_penalty, and n.\n\nExample of the OpenAI constructor:\n\nclass GPT3(LM): #This is a wrapper for the OpenAI class - dspy.OpenAI = dsp.GPT3\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo-instruct\",\n        api_key: Optional[str] = None,\n        api_provider: Literal[\"openai\", \"azure\"] = \"openai\",\n        api_base: Optional[str] = None,\n        model_type: Literal[\"chat\", \"text\"] = None,\n        **kwargs,\n    ):\n\nUnder the Hood​\n__call__(self, prompt: str, only_completed: bool = True, return_sorted: bool = False, **kwargs) -> List[Dict[str, Any]]​\n\nParameters:\n\nprompt (str): Prompt to send to OpenAI.\nonly_completed (bool, optional): Flag to return only completed responses and ignore completion due to length. Defaults to True.\nreturn_sorted (bool, optional): Flag to sort the completion choices using the returned averaged log-probabilities. Defaults to False.\n**kwargs: Additional keyword arguments for completion request.\n\nReturns:\n\nList[Dict[str, Any]]: List of completion choices.\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nAfter generation, the completions are post-processed based on the model_type parameter. If the parameter is set to 'chat', the generated content look like choice[\"message\"][\"content\"]. Otherwise, the generated text will be choice[\"text\"].\n\nUsing the OpenAI client​\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\n\nSending Requests via OpenAI Client​\nRecommended Configure default LM using dspy.configure.\n\nThis allows you to define programs in DSPy and simply call modules on your input fields, having DSPy internally call the prompt on the configured LM.\n\ndspy.configure(lm=turbo)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to turbo\nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = turbo(prompt='What is the capital of Paris?')\nprint(response)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nCohere\nNext\nPremAI\nOpenAI\nPrerequisites\nSetting up the OpenAI Client\nUnder the Hood\nUsing the OpenAI client\nSending Requests via OpenAI Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Cohere | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/Cohere",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nAnyscale\nCohere\nOpenAI\nPremAI\nTogether\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsRemote Language Model ClientsCohere\nCohere\nCohere​\nPrerequisites​\npip install cohere\n\nCohere api_key (for non-cached examples)\nSetting up the Cohere Client​\n\nThe constructor initializes the base class LM to support prompting requests to Cohere models. This requires the following parameters:\n\nParameters:\n\nmodel (str): Cohere pretrained models. Defaults to command-xlarge-nightly.\napi_key (Optional[str], optional): API provider provider authentication token. Defaults to None. This then internally initializes the cohere.Client.\nstop_sequences (List[str], optional): List of stopping tokens to end generation.\nmax_num_generations internally set: Maximum number of completions generations by Cohere client. Defaults to 5.\n\nExample of the Cohere constructor:\n\nclass Cohere(LM):\n    def __init__(\n        self,\n        model: str = \"command-xlarge-nightly\",\n        api_key: Optional[str] = None,\n        stop_sequences: List[str] = [],\n    ):\n\nUnder the Hood​\n__call__(self, prompt: str, only_completed: bool = True, return_sorted: bool = False, **kwargs) -> List[Dict[str, Any]]​\n\nParameters:\n\nprompt (str): Prompt to send to Cohere.\nonly_completed (bool, optional): Flag to return only completed responses and ignore completion due to length. Defaults to True.\nreturn_sorted (bool, optional): Flag to sort the completion choices using the returned averaged log-probabilities. Defaults to False.\n**kwargs: Additional keyword arguments for completion request.\n\nReturns:\n\nList[str]: List of generated completions.\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nThe method calculates the number of iterations required to generate the specified number of completions n based on the self.max_num_generations that the Cohere model can produce in a single request. As it completes the iterations, it updates the official num_generations argument passed to the request payload and calls request with the updated arguments.\n\nThis process iteratively constructs a choices list from which the generated completions are retrieved.\n\nIf return_sorted is set and more than one generation is requested, the completions are sorted by their likelihood scores in descending order and returned as a list with the most likely completion appearing first.\n\nUsing the Cohere client​\ncohere = dsp.Cohere(model='command-xlarge-nightly')\n\nSending Requests via Cohere Client​\nRecommended Configure default LM using dspy.configure.\n\nThis allows you to define programs in DSPy and simply call modules on your input fields, having DSPy internally call the prompt on the configured LM.\n\ndspy.configure(lm=cohere)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to cohere\nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = cohere(prompt='What is the capital of Paris?')\nprint(response)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nAnyscale\nNext\nOpenAI\nCohere\nPrerequisites\nSetting up the Cohere Client\nUnder the Hood\nUsing the Cohere client\nSending Requests via Cohere Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Guide: DSPy Modules | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/modules/guide",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModulesGuide: DSPy Modules\nGuide: DSPy Modules\n\nQuick Recap​\n\nThis guide assumes you followed the intro tutorial to build your first few DSPy programs.\n\nRemember that DSPy program is just Python code that calls one or more DSPy modules, like dspy.Predict or dspy.ChainOfThought, to use LMs.\n\n1) What is a DSPy Module?​\n\nA DSPy module is a building block for programs that use LMs.\n\nEach built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature.\n\nA DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n\nMultiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n\n2) What DSPy Modules are currently built-in?​\n\ndspy.Predict:\n\ndspy.ChainOfThought:\n\ndspy.ProgramOfThought:\n\ndspy.ReAct:\n\ndspy.MultiChainComparison:\n\nWe also have some function-style modules:\n\ndspy.majority:\n3) How do I use a built-in module, like dspy.Predict or dspy.ChainOfThought?​\n\nLet's start with the most fundamental one, dspy.Predict. Internally, all of the others are just built using it!\n\nWe'll assume you are already at least a little familiar with DSPy signatures, which are declarative specs for defining the behavior of any module we use in DSPy. To use a module, we first declare it by giving it a signature. Then we call the module with the input arguments, and extract the output fields!\n\nsentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n\n# 1) Declare with a signature.\nclassify = dspy.Predict('sentence -> sentiment')\n\n# 2) Call with input argument(s). \nresponse = classify(sentence=sentence)\n\n# 3) Access the output.\nprint(response.sentiment)\n\nPositive\n\n\nWhen we declare a module, we can pass configuration keys to it.\n\nBelow, we'll pass n=5 to request five completions. We can also pass temperature or max_len, etc.\n\nLet's use dspy.ChainOfThought. In many cases, simply swapping dspy.ChainOfThought in place of dspy.Predict improves quality.\n\nquestion = \"What's something great about the ColBERT retrieval model?\"\n\n# 1) Declare with a signature, and pass some config.\nclassify = dspy.ChainOfThought('question -> answer', n=5)\n\n# 2) Call with input argument.\nresponse = classify(question=question)\n\n# 3) Access the outputs.\nresponse.completions.answer\n\n['One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.',\n 'Its ability to efficiently retrieve relevant information from large document collections.',\n 'One great thing about the ColBERT retrieval model is its superior performance compared to other models and its efficient use of pre-trained language models.',\n 'One great thing about the ColBERT retrieval model is its superior efficiency and accuracy compared to other models.',\n 'One great thing about the ColBERT retrieval model is its ability to incorporate user feedback and support complex queries.']\n\n\nLet's discuss the output object here.\n\nThe dspy.ChainOfThought module will generally inject a rationale before the output field(s) of your signature.\n\nLet's inspect the (first) rationale and answer!\n\nprint(f\"Rationale: {response.rationale}\")\nprint(f\"Answer: {response.answer}\")\n\n\nRationale: produce the answer. We can consider the fact that ColBERT has shown to outperform other state-of-the-art retrieval models in terms of efficiency and effectiveness. It uses contextualized embeddings and performs document retrieval in a way that is both accurate and scalable. Answer: One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.\n\nThis is accessible whether we request one or many completions.\n\nWe can also access the different completions as a list of Predictions or as several lists, one for each field.\n\nresponse.completions[3].rationale == response.completions.rationale[3]\n\nTrue\n\n4) How do I use more complex built-in modules?​\n\nThe others are very similar, dspy.ReAct and dspy.ProgramOfThought etc. They mainly change the internal behavior with which your signature is implemented!\n\nCheck out further examples in each module's respective guide.\n\n5) How do I compose multiple modules into a bigger program?​\n\nDSPy is just Python code that uses modules in any control flow you like. (There's some magic internally at compile time to trace your LM calls.)\n\nWhat this means is that, you can just call the modules freely. No weird abstractions for chaining calls.\n\nThis is basically PyTorch's design approach for define-by-run / dynamic computation graphs. Refer to the intro tutorials for examples.\n\nWritten By: Arnav Singhvi\n\nPrevious\nReAct\nNext\nRetrieve\nQuick Recap\n1) What is a DSPy Module?\n2) What DSPy Modules are currently built-in?\n3) How do I use a built-in module, like dspy.Predict or dspy.ChainOfThought?\n4) How do I use more complex built-in modules?\n5) How do I compose multiple modules into a bigger program?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Anyscale | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/Anyscale",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nAnyscale\nCohere\nOpenAI\nPremAI\nTogether\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsRemote Language Model ClientsAnyscale\nAnyscale\nAnyscale​\nPrerequisites​\nAnyscale api_key and api_base (for non-cached examples). Set these within your developer environment .env as follows:\nANYSCALE_API_BASE = ...\nANYSCALE_API_KEY = ...\n\n\nwhich will be retrieved within the Anyscale Client as:\n\nself.api_base = os.getenv(\"ANYSCALE_API_BASE\")\nself.token = os.getenv(\"ANYSCALE_API_KEY\")\n\nSetting up the Anyscale Client​\n\nThe constructor initializes the HFModel base class to support the handling of prompting models. This requires the following parameters:\n\nParameters:\n\nmodel (str): ID of model hosted on Anyscale endpoint.\n**kwargs: Additional keyword arguments to configure the Anyscale client.\n\nExample of the Anyscale constructor:\n\nclass Anyscale(HFModel):\n    def __init__(self, model, **kwargs):\n\nUnder the Hood​\n_generate(self, prompt, use_chat_api=False, **kwargs):​\n\nParameters:\n\nprompt (str): Prompt to send to Anyscale.\nuse_chat_api (bool): Flag to use the Anyscale Chat models endpoint. Defaults to False.\n**kwargs: Additional keyword arguments for completion request.\n\nReturns:\n\ndict: dictionary with prompt and list of response choices.\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nThe Anyscale token is set within the request headers to ensure authorization to send requests to the endpoint.\n\nIf use_chat_api is set, the method sets up Anyscale url chat endpoint and prompt template for chat models. It then retrieves the generated JSON response and sets up the completions list by retrieving the response's message : content.\n\nIf use_chat_api is not set, the method uses the default Anyscale url endpoint. It similarly retrieves the generated JSON response and but sets up the completions list by retrieving the response's text as the completion.\n\nFinally, after processing the requests and responses, the method constructs the response dictionary with two keys: the original request prompt and choices, a list of dictionaries representing generated completions with the key text holding the response's generated text.\n\nUsing the Anyscale client​\nanyscale = dspy.Anyscale(model='meta-llama/Llama-2-70b-chat-hf')\n\nSending Requests via Anyscale Client​\nRecommended Configure default LM using dspy.configure.\n\nThis allows you to define programs in DSPy and simply call modules on your input fields, having DSPy internally call the prompt on the configured LM.\n\ndspy.configure(lm=anyscale)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to anyscale\nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = anyscale(prompt='What is the capital of Paris?')\nprint(response)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nRemote Language Model Clients\nNext\nCohere\nAnyscale\nPrerequisites\nSetting up the Anyscale Client\nUnder the Hood\nUsing the Anyscale client\nSending Requests via Anyscale Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "ReAct | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/modules/react",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModulesReAct\nReAct\nBackground​\n\nDSPy supports ReAct, an LLM agent designed to tackle complex tasks in an interactive fashion. ReAct is composed of an iterative loop of interpretation, decision and action-based activities (\"Thought, Action, and Observation\") based on an evolving set of input and output fields. Through this real-time iterative approach, the ReAct agent can both analyze and adapt to its responses over time as new information becomes available.\n\nInstantiating ReAct​\n\nTo instantiate the ReAct module, define and pass in a DSPy Signature.\n\n# Define a simple signature for basic question answering\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n# Pass signature to ReAct module\nreact_module = dspy.ReAct(BasicQA)\n\nUnder the Hood​\nReAct Cycle​\n\nReAct operates under a dynamic signature integration process, accounting for the signature inputs and performing the Thoughts, Action, Observation cycle to respond with the signature outputs. Thoughts (or reasoning) lead to Actions (such as queries or activities). These Actions then result in Observations (like results or responses), which subsequently feedback into the next Thought.\n\nThis cycle is maintained for a predefined number of iterations, specified by max_iters. The default value for the Thought-Action-Observation cycle is 5 iterations. Once the maximum iterations are reached, React will return the final output if the Action has finished (Finish[answer]) or an empty string to indicate the agent could not determine a final output.\n\nCAUTION\n\nCurrently, ReAct supports only one output field in its signature. We plan to expand this in future developments.\n\nReAct Tools​\n\nTools in ReAct can shape the agent's interaction and response mechanisms, and DSPy ensures this customizability by allowing users to pass in their toolsets tailored for their task scenarios. The default tool is the dspy.Retrieve module (serving to retrieve information from Retrieval Models during the Action step) with default num_results=3, and these can be passed as arguments to the initialization of the ReAct module.\n\nTying It All Together​\n\nUsing ReAct mirrors the simplicity of the base Predict and ChainOfThought modules. Here is an example call:\n\n# Call the ReAct module on a particular input\nquestion = 'Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?'\nresult = react_module(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Final Predicted Answer (after ReAct process): {result.answer}\")\n\nQuestion: Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?\nFinal Predicted Answer (after ReAct process): The Apple Remote and the Siri Remote can control the Front Row media program.\n\n\nLet's take a peek at how ReAct functioned internally by inspecting its history, up to maximum iterations. (This assumes the initial DSPy setup and configurations of LMs and RMs).\n\nlm.inspect_history(n=3)\n\n-------------------------Step 1---------------------------------\nYou will be given `question` and you will respond with `answer`.\n\nTo do this, you will interleave Thought, Action, and Observation steps.\n\nThought can reason about the current situation, and Action can be the following types:\n\n(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n(2) Finish[answer], which returns the final `answer` and finishes the task\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nThought 1: next steps to take based on last observation\nAction 1: always either Search[query] or, when done, Finish[answer]\n\n---\n\nQuestion: Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?\nThought 1: I need to find out what program the Apple Remote was originally designed to interact with.\nAction 1: Search[\"program Apple Remote was originally designed to interact with\"]\n\n-------------------------Step 2---------------------------------\nYou will be given `question` and you will respond with `answer`.\n\nTo do this, you will interleave Thought, Action, and Observation steps.\n\nThought can reason about the current situation, and Action can be the following types:\n\n(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n(2) Finish[answer], which returns the final `answer` and finishes the task\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\n\nThought 1: next steps to take based on last observation\n\nAction 1: always either Search[query] or, when done, Finish[answer]\n\nObservation 1: observations based on action\n\nThought 2: next steps to take based on last observation\n\nAction 2: always either Search[query] or, when done, Finish[answer]\n\n---\n\nQuestion: Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?\n\n[Thought 1, Action 1]\n\nObservation 1:\n[1] «Apple Remote | The Apple Remote is a remote control device released [...].»\n[2] «ITunes Remote | iTunes Remote (also known simply as Remote) is a software [...].»\n[3] «Siri Remote | The Siri Remote is a remote control device released in tandem [...]»\n\nThought 2: I have found that the Apple Remote was originally designed to interact with the Front Row media program on the iSight iMac G5. I should search for other devices that can control this program.\n\nAction 2: Search[\"devices that can control Front Row media program\"]\n\nObservation 2:\n[1] «Apple Remote» - The Apple Remote is the primary control mechanism for the Front Row media program [...].\n[2] «Siri Remote» - The Siri Remote is the successor of the Apple Remote and is designed [...].\n\n-------------------------Step 3---------------------------------\n\nYou will be given `question` and you will respond with `answer`.\n\nTo do this, you will interleave Thought, Action, and Observation steps.\n\nThought can reason about the current situation, and Action can be the following types:\n\n(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n(2) Finish[answer], which returns the final `answer` and finishes the task\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\n\nThought 1: next steps to take based on last observation\n\nAction 1: always either Search[query] or, when done, Finish[answer]\n\nObservation 1: observations based on action\n\nThought 2: next steps to take based on last observation\n\nAction 2: always either Search[query] or, when done, Finish[answer]\n\nObservation 2: observations based on action\n\nThought 3: next steps to take based on last observation\n\nAction 3: always either Search[query] or, when done, Finish[answer]\n\n---\n\nQuestion: Aside from the Apple Remote, what other devices can control the program Apple Remote was originally designed to interact with?\n\n[Thought 1, Action 1, Observation 1, Thought 2, Action 2]\n\nObservation 2:\n[1] «Front Row (software) | Front Row is a discontinued media center [...].»\n[2] «Apple Remote | The Apple Remote is a remote control device [...].»\n[3] «Media player (software) | A media player is a computer program for [...].»\n\nThought 3: I have found that the Apple Remote and the Siri Remote can control the Front Row media program. I should provide this information as the answer.\n\nAction 3: Finish[The Apple Remote and the Siri Remote can control the Front Row media program.]\n\nAnswer: The Apple Remote and the Siri Remote can control the Front Row media program.\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nProgram of Thought\nNext\nGuide: DSPy Modules\nBackground\nInstantiating ReAct\nUnder the Hood\nReAct Cycle\nReAct Tools\nTying It All Together\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Program of Thought | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/modules/program-of-thought",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModulesProgram of Thought\nProgram of Thought\nBackground​\n\nDSPy supports the Program of Thought (PoT) prompting technique, integrating an advanced module capable of problem-solving with program execution capabilities. PoT builds upon Chain of Thought by translating reasoning steps into executable programming language statements through iterative refinement. This enhances the accuracy of the output and self-corrects errors within the generated code. Upon completing these iterations, PoT delegates execution to a program interpreter. Currently, this class supports the generation and execution of Python code.\n\nInstantiating Program of Thought​\n\nProgram of Thought is instantiated based on a user-defined DSPy Signature, which can take a simple form such as input_fields -> output_fields. Users can also specify a max_iters to set the maximum number of iterations for the self-refinement process of the generated code. The default value is 3 iterations. Once the maximum iterations are reached, PoT will produce the final output.\n\nimport dsp\nimport dspy\nfrom ..primitives.program import Module\nfrom ..primitives.python_interpreter import CodePrompt, PythonInterpreter\nimport re\n\nclass ProgramOfThought(Module):\n    def __init__(self, signature, max_iters=3):\n        ...\n\n#Example Usage\n\n#Define a simple signature for basic question answering\ngenerate_answer_signature = dspy.Signature(\"question -> answer\")\ngenerate_answer_signature.attach(question=(\"Question:\", \"\")).attach(answer=(\"Answer:\", \"often between 1 and 5 words\"))\n\n# Pass signature to ProgramOfThought Module\npot = dspy.ProgramOfThought(generate_answer_signature)\n\nUnder the Hood​\n\nProgram of Thought operates in 3 key modes: generate, regenerate, and answer. Each mode is a Chain of Thought module encapsulating signatures and instructions for each mode's individual purpose. These are dynamically created as the PoT iterations complete, accounting for the user's input and output fields internally.\n\nGenerate mode:\n\nInitiates the generation of Python code with the signature (question -> generated_code) for the initial code generation.\n\nRegenerate mode:\n\nUsed for refining the generation of Python code, considering the previously generated code and existing errors, with the signature (question, previous_code, error -> generated_code). If errors persist past the maximum iterations, the user is alerted and the output is returned as None.\n\nAnswer mode:\n\nExecutes the last stored generated code and outputs the final answer, with the signature (question, final_generated_code, code_output -> answer).\n\nKey internals:\n\nCode Parsing: Program of Thought internally processes each code generation as a string and filters out extraneous bits to ensure the code block conforms to executable Python syntax. If the code is empty or does not match these guidelines, the parser returns an error string, signaling the PoT process for regeneration.\nCode Execution: Program of Thought relies on a Python interpreter adapted by CAMEL-AI to execute code generated by LLMs. The final code generation is formatted as a CodePrompt instance and executed by the PythonInterpreter. This adaptation is present in DSPy primitives.\nTying It All Together​\n\nUsing ProgramOfThought mirrors the simplicity of the base Predict and ChainOfThought modules. Here is an example call:\n\n#Call the ProgramOfThought module on a particular input\nquestion = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'\nresult = pot(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Final Predicted Answer (after ProgramOfThought process): {result.answer}\")\n\nQuestion: Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?\nFinal Predicted Answer (after ProgramOfThought process): 12\n\n\nLet's take a peek at how ProgramOfThought functioned internally by inspecting its history, up to maximum iterations (+ 1 to view the final generation). (This assumes the initial DSPy setup and configurations of LMs and RMs).\n\nlm.inspect_history(n=4)\n\nYou will be given `question` and you will respond with `generated_code`.\nGenerating executable Python code that programmatically computes the correct `generated_code`.\nAfter you're done with the computation, make sure the last line in your code evaluates to the correct value for `generated_code`.\n\n---\n\nFollow the following format.\n\nQuestion: \nReasoning: Let's think step by step in order to ${produce the generated_code}. We ...\nCode: python code that answers the question\n\n---\n\nQuestion: Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?\nReasoning: Let's think step by step in order to produce the generated_code. We start with the initial number of apples that Sarah has, which is 5. Then, we add the number of apples she buys from the store, which is 7. Finally, we compute the total number of apples Sarah has by adding these two quantities together.\nCode: \n\napples_initial = 5\napples_bought = 7\n\ntotal_apples = apples_initial + apples_bought\ntotal_apples\n\nGiven the final code `question`, `final_generated_code`, `code_output`, provide the final `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: \n\nCode: python code that answers the question\n\nCode Output: output of previously-generated python code\n\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\n\nAnswer: often between 1 and 5 words\n\n---\n\nQuestion: Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?\n\nCode:\napples_initial = 5\napples_bought = 7\n\ntotal_apples = apples_initial + apples_bought\ntotal_apples\n\nCode Output: 12\n\nReasoning: Let's think step by step in order to produce the answer. We start with the initial number of apples Sarah has, which is 5. Then, we add the number of apples she bought, which is 7. The total number of apples Sarah has now is 12.\n\nAnswer: 12\n\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nChainOfThoughtWithHint\nNext\nReAct\nBackground\nInstantiating Program of Thought\nUnder the Hood\nTying It All Together\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "ChainOfThoughtWithHint | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/modules/chain-of-thought-with-hint",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModulesChainOfThoughtWithHint\nChainOfThoughtWithHint\n\nThis class builds upon the ChainOfThought class by introducing an additional input field to provide hints for reasoning. The inclusion of a hint allows for a more directed problem-solving process, which can be especially useful in complex scenarios.\n\nChainOfThoughtWithHint is instantiated with a user-defined DSPy Signature, and the inclusion of a hint argument that takes in a string-form phrase to provide a hint within the prompt template.\n\nLet's take a look at an example:\n\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n#Pass signature to ChainOfThought module\ngenerate_answer = dspy.ChainOfThoughtWithHint(BasicQA)\n\n# Call the predictor on a particular input alongside a hint.\nquestion='What is the color of the sky?'\nhint = \"It's what you often see during a sunny day.\"\npred = generate_answer(question=question, hint=hint)\n\nprint(f\"Question: {question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nDSPy Assertions\nNext\nProgram of Thought\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "DSPy Assertions | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/modules/assertions",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModulesDSPy Assertions\nDSPy Assertions\nIntroduction​\n\nLanguage models (LMs) have transformed how we interact with machine learning, offering vast capabilities in natural language understanding and generation. However, ensuring these models adhere to domain-specific constraints remains a challenge. Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints.\n\nTo address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.\n\ndspy.Assert and dspy.Suggest API​\n\nWe introduce two primary constructs within DSPy Assertions:\n\ndspy.Assert:\n\nParameters:\nconstraint (bool): Outcome of Python-defined boolean validation check.\nmsg (Optional[str]): User-defined error message providing feedback or correction guidance.\nbacktrack (Optional[module]): Specifies target module for retry attempts upon constraint failure. The default backtracking module is the last module before the assertion.\nBehavior: Initiates retry upon failure, dynamically adjusting the pipeline's execution. If failures persist, it halts execution and raises a dspy.AssertionError.\n\ndspy.Suggest:\n\nParameters: Similar to dspy.Assert.\nBehavior: Encourages self-refinement through retries without enforcing hard stops. Logs failures after maximum backtracking attempts and continues execution.\n\ndspy.Assert vs. Python Assertions: Unlike conventional Python assert statements that terminate the program upon failure, dspy.Assert conducts a sophisticated retry mechanism, allowing the pipeline to adjust.\n\nSpecifically, when a constraint is not met:\n\nBacktracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through\nDynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:\nPast Output: your model's past output that did not pass the validation_fn\nInstruction: your user-defined feedback message on what went wrong and what possibly to fix\n\nIf the error continues past the max_backtracking_attempts, then dspy.Assert will halt the pipeline execution, altering you with an dspy.AssertionError. This ensures your program doesn't continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment.\n\ndspy.Suggest vs. dspy.Assert: dspy.Suggest on the other hand offers a softer approach. It maintains the same retry backtracking as dspy.Assert but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the max_backtracking_attempts, dspy.Suggest will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a \"best-effort\" manner without halting execution.\n\ndspy.Suggest are best utilized as \"helpers\" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.\n\ndspy.Assert are recommended during the development stage as \"checkers\" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.\n\nUse Case: Including Assertions in DSPy Programs​\n\nWe start with using an example of a multi-hop QA SimplifiedBaleen pipeline as defined in the intro walkthrough.\n\nclass SimplifiedBaleen(dspy.Module):\n    def __init__(self, passages_per_hop=2, max_hops=2):\n        super().__init__()\n\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n        self.max_hops = max_hops\n\n    def forward(self, question):\n        context = []\n        prev_queries = [question]\n\n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n            prev_queries.append(query)\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n        \n        pred = self.generate_answer(context=context, question=question)\n        pred = dspy.Prediction(context=context, answer=pred.answer)\n        return pred\n\nbaleen = SimplifiedBaleen()\n\nbaleen(question = \"Which award did Gary Zukav's first book receive?\")\n\n\nTo include DSPy Assertions, we simply define our validation functions and declare our assertions following the respective model generation.\n\nFor this use case, suppose we want to impose the following constraints:\n\nLength - each query should be less than 100 characters\nUniqueness - each generated query should differ from previously-generated queries.\n\nWe can define these validation checks as boolean functions:\n\n#simplistic boolean check for query length\nlen(query) <= 100\n\n#Python function for validating distinct queries\ndef validate_query_distinction_local(previous_queries, query):\n    \"\"\"check if query is distinct from previous queries\"\"\"\n    if previous_queries == []:\n        return True\n    if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.8):\n        return False\n    return True\n\n\nWe can declare these validation checks through dspy.Suggest statements (as we want to test the program in a best-effort demonstration). We want to keep these after the query generation query = self.generate_query[hop](context=context, question=question).query.\n\ndspy.Suggest(\n    len(query) <= 100,\n    \"Query should be short and less than 100 characters\",\n)\n\ndspy.Suggest(\n    validate_query_distinction_local(prev_queries, query),\n    \"Query should be distinct from: \"\n    + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\n)\n\n\nIt is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!\n\nLet's take a look at how the SimplifiedBaleen program will look with Assertions included:\n\nclass SimplifiedBaleenAssertions(dspy.Module):\n    def __init__(self, passages_per_hop=2, max_hops=2):\n        super().__init__()\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n        self.max_hops = max_hops\n\n    def forward(self, question):\n        context = []\n        prev_queries = [question]\n\n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n\n            dspy.Suggest(\n                len(query) <= 100,\n                \"Query should be short and less than 100 characters\",\n            )\n\n            dspy.Suggest(\n                validate_query_distinction_local(prev_queries, query),\n                \"Query should be distinct from: \"\n                + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\n            )\n\n            prev_queries.append(query)\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n        \n        if all_queries_distinct(prev_queries):\n            self.passed_suggestions += 1\n\n        pred = self.generate_answer(context=context, question=question)\n        pred = dspy.Prediction(context=context, answer=pred.answer)\n        return pred\n\n\nNow calling programs with DSPy Assertions requires one last step, and that is transforming the program to wrap it with internal assertions backtracking and Retry logic.\n\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\n\nbaleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)\n\n# backtrack_handler is parameterized over a few settings for the backtracking mechanism\n# To change the number of max retry attempts, you can do\nbaleen_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenAssertions(), \n    functools.partial(backtrack_handler, max_backtracks=1))\n\n\nAlternatively, you can also directly call activate_assertions on the program with dspy.Assert/Suggest statements using the default backtracking mechanism (max_backtracks=2):\n\nbaleen_with_assertions = SimplifiedBaleenAssertions().activate_assertions()\n\n\nNow let's take a look at the internal LM backtracking by inspecting the history of the LM query generations. Here we see that when a query fails to pass the validation check of being less than 100 characters, its internal GenerateSearchQuery signature is dynamically modified during the backtracking+Retry process to include the past query and the corresponding user-defined instruction: \"Query should be short and less than 100 characters\".\n\nWrite a simple search query that will help answer a complex question.\n\n---\n\nFollow the following format.\n\nContext: may contain relevant facts\n\nQuestion: ${question}\n\nReasoning: Let's think step by step in order to ${produce the query}. We ...\n\nQuery: ${query}\n\n---\n\nContext:\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is [...]»\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was [...]»\n\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\n\nReasoning: Let's think step by step in order to find the answer to this question. First, we need to identify the actress who played Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" Then, we need to find out if this actress also acted in the short film \"The Shore.\"\n\nQuery: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\n\n\n\nWrite a simple search query that will help answer a complex question.\n\n---\n\nFollow the following format.\n\nContext: may contain relevant facts\n\nQuestion: ${question}\n\nPast Query: past output with errors\n\nInstructions: Some instructions you must satisfy\n\nQuery: ${query}\n\n---\n\nContext:\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is an Irish television and film actress, best known for her role as Octavia of the Julii in the HBO/BBC series \"Rome,\" as Stacey Ehrmantraut in AMC's \"Better Call Saul\" and as the voice of F.R.I.D.A.Y. in various films in the Marvel Cinematic Universe. She is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\"»\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was an Italian born American actress who had a brief Broadway stage career before leaving to become a wife and mother. Born in Naples she came to acting in 1894 playing a Mexican girl in a play at the Empire Theatre. Wilson Barrett engaged her for a role in his play \"The Sign of the Cross\" which he took on tour of the United States. Riccardo played the role of Ancaria and later played Berenice in the same play. Robert B. Mantell in 1898 who struck by her beauty also cast her in two Shakespeare plays, \"Romeo and Juliet\" and \"Othello\". Author Lewis Strang writing in 1899 said Riccardo was the most promising actress in America at the time. Towards the end of 1898 Mantell chose her for another Shakespeare part, Ophelia im Hamlet. Afterwards she was due to join Augustin Daly's Theatre Company but Daly died in 1899. In 1899 she gained her biggest fame by playing Iras in the first stage production of Ben-Hur.»\n\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\n\nPast Query: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\n\nInstructions: Query should be short and less than 100 characters\n\nQuery: \"actress Ophelia RSC Hamlet\" + \"actress The Shore\"\n\n\nAssertion-Driven Optimizations​\n\nDSPy Assertions work with optimizations that DSPy offers, particularly with BootstrapFewShotWithRandomSearch, including the following settings:\n\nCompilation with Assertions This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.\nCompilation + Inference with Assertions -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time.\nteleprompter = BootstrapFewShotWithRandomSearch(\n    metric=validate_context_and_answer_and_hops,\n    max_bootstrapped_demos=max_bootstrapped_demos,\n    num_candidate_programs=6,\n)\n\n#Compilation with Assertions\ncompiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)\n\n#Compilation + Inference with Assertions\ncompiled_baleen_with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)\n\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nModules\nNext\nChainOfThoughtWithHint\nIntroduction\ndspy.Assert and dspy.Suggest API\nUse Case: Including Assertions in DSPy Programs\nAssertion-Driven Optimizations\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Retrieve | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/modules/retrieve",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModulesRetrieve\nRetrieve\nBackground​\n\nDSPy supports retrieval through the Retrieve module that serves to process user queries and output relevant passages from retrieval corpuses. This module ties in with the DSPy-supported Retrieval Model Clients which are retrieval servers that users can utilize to retrieve information for information retrieval tasks.\n\nInstantiating Retrieve​\n\nRetrieve is simply instantiate by a user-defined k number of retrieval passages to return for a query.\n\nclass Retrieve(Parameter):\n    def __init__(self, k=3):\n        self.stage = random.randbytes(8).hex()\n        self.k = k\n\n\nAdditionally, configuring a Retrieval model client or server through dspy.configure allows for user retrieval in DSPy programs through internal calls from Retrieve.\n\n#Example Usage\n\n#Define a retrieval model server to send retrieval requests to\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\n#Configure retrieval server internally\ndspy.settings.configure(rm=colbertv2_wiki17_abstracts)\n\n#Define Retrieve Module\nretriever = dspy.Retrieve(k=3)\n\nUnder the Hood​\n\nRetrieve makes use of the internally configured retriever to send a single query or list of multiple queries to determine the top-k relevant passages. The module queries the retriever for each provided query, accumulating scores (or probabilities if the by_prob arg is set) for each passage and returns the passages sorted by their cumulative scores or probabilities.\n\nThe Retrieve module can also integrate a reranker if this is configured, in which case, the reranker re-scores the retrieved passages based on their relevance to the quer, improving accuracy of the results.\n\nTying it All Together​\n\nWe can now call the Retrieve module on a sample query or list of queries and observe the top-K relevant passages.\n\nquery='When was the first FIFA World Cup held?'\n\n# Call the retriever on a particular query.\ntopK_passages = retriever(query).passages\n\nprint(f\"Top {retriever.k} passages for question: {query} \\n\", '-' * 30, '\\n')\n\nfor idx, passage in enumerate(topK_passages):\n    print(f'{idx+1}]', passage, '\\n')\n\nTop 3 passages for question: When was the first FIFA World Cup held? \n ------------------------------ \n\n1] History of the FIFA World Cup | The FIFA World Cup was first held in 1930, when FIFA president Jules Rimet [...]. \n\n2] 1950 FIFA World Cup | The 1950 FIFA World Cup, held in Brazil from 24 June to 16 July 1950, [...]. \n\n3] 1970 FIFA World Cup | The 1970 FIFA World Cup was the ninth FIFA World Cup, the quadrennial [...].\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nGuide: DSPy Modules\nNext\nTyped Predictors\nBackground\nInstantiating Retrieve\nUnder the Hood\nTying it All Together\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Understanding Typed Predictors | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/typed_predictors/understanding_predictors",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nFunctional Typed Predictors\nUnderstanding Typed Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveTyped PredictorsUnderstanding Typed Predictors\nUnderstanding Typed Predictors\nWhy use a Typed Predictor?​\nHow to use a Typed Predictor?​\nPrompt of Typed Predictors​\nHow Typed Predictors work?​\nPrevious\nFunctional Typed Predictors\nNext\nLanguage Model Clients\nWhy use a Typed Predictor?\nHow to use a Typed Predictor?\nPrompt of Typed Predictors\nHow Typed Predictors work?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Functional Typed Predictors | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/typed_predictors/functional_typed_predictors",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nFunctional Typed Predictors\nUnderstanding Typed Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveTyped PredictorsFunctional Typed Predictors\nFunctional Typed Predictors\nTyped Predictors as Decorators​\nFunctional Typed Predictors in dspy.Module​\nHow Functional Typed Predictors work?​\nPrevious\nTyped Predictors\nNext\nUnderstanding Typed Predictors\nTyped Predictors as Decorators\nFunctional Typed Predictors in dspy.Module\nHow Functional Typed Predictors work?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Creating a Custom Dataset | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/data-handling/loading-custom-data",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nExamples in DSPy\nUtilizing Built-in Datasets\nCreating a Custom Dataset\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveData HandlingCreating a Custom Dataset\nCreating a Custom Dataset\n\nWe've seen how to work with with Example objects and use the HotPotQA class to load the HuggingFace HotPotQA dataset as a list of Example objects. But in production, such structured datasets are rare. Instead, you'll find yourself working on a custom dataset and might question: how do I create my own dataset or what format should it be?\n\nIn DSPy, your dataset is a list of Examples, which we can accomplish in two ways:\n\nRecommended: The Pythonic Way: Using native python utility and logic.\nAdvanced: Using DSPy's Dataset class\nRecommended: The Pythonic Way​\n\nTo create a list of Example objects, we can simply load data from the source and formulate it into a Python list. Let's load an example CSV sample.csv that contains 3 fields: (context, question and summary) via Pandas. From there, we can construct our data list.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"sample.csv\")\nprint(df.shape)\n\n\nOutput:\n\n(1000, 3)\n\ndataset = []\n\nfor context, question, answer in df.values:\n    dataset.append(dspy.Example(context=context, question=question, answer=answer).with_inputs(\"context\", \"question\"))\n\nprint(dataset[:3])\n\n\nOutput:\n\n[Example({'context': nan, 'question': 'Which is a species of fish? Tope or Rope', 'answer': 'Tope'}) (input_keys={'question', 'context'}),\n Example({'context': nan, 'question': 'Why can camels survive for long without water?', 'answer': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.'}) (input_keys={'question', 'context'}),\n Example({'context': nan, 'question': \"Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\", 'answer': 'The name of the third daughter is Alice'}) (input_keys={'question', 'context'})]\n\n\nWhile this is fairly simple, let's take a look at how loading datasets would look in DSPy - via the DSPythonic way!\n\nAdvanced: Using DSPy's Dataset class (Optional)​\n\nLet's take advantage of the Dataset class we defined in the previous article to accomplish the preprocessing:\n\nLoad data from CSV to a dataframe.\nSplit the data to train, dev and test splits.\nPopulate _train, _dev and _test class attributes. Note that these attributes should be a list of dictionary, or an iterator over mapping like HuggingFace Dataset, to make it work.\n\nThis is all done through the __init__ method, which is the only method we have to implement.\n\nimport pandas as pd\nfrom dspy.datasets.dataset import Dataset\n\nclass CSVDataset(Dataset):\n    def __init__(self, file_path, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        \n        df = pd.read_csv(file_path)\n        self._train = df.iloc[0:700].to_dict(orient='records')\n\n        self._dev = df.iloc[700:].to_dict(orient='records')\n\ndataset = CSVDataset(\"sample.csv\")\nprint(dataset.train[:3])\n\n\nOutput:\n\n[Example({'context': nan, 'question': 'Which is a species of fish? Tope or Rope', 'answer': 'Tope'}) (input_keys={'question', 'context'}),\n Example({'context': nan, 'question': 'Why can camels survive for long without water?', 'answer': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.'}) (input_keys={'question', 'context'}),\n Example({'context': nan, 'question': \"Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\", 'answer': 'The name of the third daughter is Alice'}) (input_keys={'question', 'context'})]\n\n\nLet's understand the code step by step:\n\nIt inherits the base Dataset class from DSPy. This inherits all the useful data loading/processing functionality.\nWe load the data in CSV into a DataFrame.\nWe get the train split i.e first 700 rows in the DataFrame and convert it to lists of dicts using to_dict(orient='records') method and is then assigned to self._train.\nWe get the dev split i.e first 300 rows in the DataFrame and convert it to lists of dicts using to_dict(orient='records') method and is then assigned to self._dev.\n\nUsing the Dataset base class now makes loading custom datasets incredibly easy and avoids having to write all that boilerplate code ourselves for every new dataset.\n\nCAUTION\n\nWe did not populate _test attribute in the above code, which is fine and won't cause any unneccesary error as such. However it'll give you an error if you try to access the test split.\n\ndataset.test[:5]\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-59-5202f6de3c7b> in <cell line: 1>()\n----> 1 dataset.test[:5]\n\n/usr/local/lib/python3.10/dist-packages/dspy/datasets/dataset.py in test(self)\n     51     def test(self):\n     52         if not hasattr(self, '_test_'):\n---> 53             self._test_ = self._shuffle_and_sample('test', self._test, self.test_size, self.test_seed)\n     54 \n     55         return self._test_\n\nAttributeError: 'CSVDataset' object has no attribute '_test'\n\n\nTo prevent that you'll just need to make sure _test is not None and populated with the appropriate data.\n\nYou can overide the methods in Dataset class to customize your class even more.\n\nIn summary, the Dataset base class provides a simplistic way to load and preprocess custom datasets with minimal code!\n\nWritten By: Herumb Shandilya\n\nPrevious\nUtilizing Built-in Datasets\nNext\nSignatures\nRecommended: The Pythonic Way\nAdvanced: Using DSPy's Dataset class (Optional)\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Utilizing Built-in Datasets | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/data-handling/built-in-datasets",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nExamples in DSPy\nUtilizing Built-in Datasets\nCreating a Custom Dataset\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveData HandlingUtilizing Built-in Datasets\nUtilizing Built-in Datasets\n\nIt's easy to use your own data in DSPy: a dataset is just a list of Example objects. Using DSPy well involves being able to find and re-purpose existing datasets for your own pipelines in new ways; DSPy makes this a particularly powerful strategy.\n\nFor convenience, DSPy currently also provides support for the following dataset out of the box:\n\nHotPotQA (multi-hop question answering)\nGSM8k (math questions)\nColor (basic dataset of colors)\nLoading HotPotQA​\n\nHotPotQA is which is a collection of question-answer pairs.\n\nfrom dspy.datasets import HotPotQA\n\ndataset = HotPotQA(train_seed=1, train_size=5, eval_seed=2023, dev_size=50, test_size=0)\n\nprint(dataset.train)\n\n\nOutput:\n\n[Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys=None),\n Example({'question': 'which  American actor was Candace Kita  guest starred with ', 'answer': 'Bill Murray'}) (input_keys=None),\n Example({'question': 'Which of these publications was most recently published, Who Put the Bomp or Self?', 'answer': 'Self'}) (input_keys=None),\n Example({'question': 'The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?', 'answer': '1950'}) (input_keys=None),\n Example({'question': 'Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?', 'answer': 'Tae Kwon Do Times'}) (input_keys=None)]\n\n\nWe just loaded trainset (5 examples) and devset (50 examples). Each example in our training set contains just a question and its (human-annotated) answer. As you can see, it is loaded as a list of Example objects. However, one thing to note is that it doesn't set the input keys implicitly, so that is something that we'll need to do!!\n\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n\nprint(trainset)\n\n\nOutput:\n\n[Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys={'question'}),\n Example({'question': 'which  American actor was Candace Kita  guest starred with ', 'answer': 'Bill Murray'}) (input_keys={'question'}),\n Example({'question': 'Which of these publications was most recently published, Who Put the Bomp or Self?', 'answer': 'Self'}) (input_keys={'question'}),\n Example({'question': 'The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?', 'answer': '1950'}) (input_keys={'question'}),\n Example({'question': 'Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?', 'answer': 'Tae Kwon Do Times'}) (input_keys={'question'})]\n\n\nDSPy typically requires very minimal labeling. Whereas your pipeline may involve six or seven complex steps, you only need labels for the initial question and the final answer. DSPy will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!\n\nAdvanced: Inside DSPy's Dataset class (Optional)​\n\nWe've seen how you can use HotPotQA dataset class and load the HotPotQA dataset, but how does it actually work? The HotPotQA class inherits from the Dataset class, which takes care of the conversion of the data loaded from a source into train-test-dev split, all of which are list of examples. In the HotPotQA class, you only implement the __init__ method, where you populate the splits from HuggingFace into the variables _train, _test and _dev. The rest of the process is handled by methods in the Dataset class.\n\nBut how do the methods of the Dataset class convert the data from HuggingFace? Let's take a deep breath and think step by step...pun intended. In example above, we can see the splits accessed by .train, .dev and .test methods, so let's take a look at the implementation of the train() method:\n\n@property\ndef train(self):\n    if not hasattr(self, '_train_'):\n        self._train_ = self._shuffle_and_sample('train', self._train, self.train_size, self.train_seed)\n\n    return self._train_\n\n\nAs you can see, the train() method serves as a property, not a regular method. Within this property, it first checks if the _train_ attribute exists. If not, it calls the _shuffle_and_sample() method to process the self._train where the HuggingFace dataset is loaded. Let's see the _shuffle_and_sample() method:\n\ndef _shuffle_and_sample(self, split, data, size, seed=0):\n    data = list(data)\n    base_rng = random.Random(seed)\n\n    if self.do_shuffle:\n        base_rng.shuffle(data)\n\n    data = data[:size]\n    output = []\n\n    for example in data:\n        output.append(Example(**example, dspy_uuid=str(uuid.uuid4()), dspy_split=split))\n    \n        return output\n\n\nThe _shuffle_and_sample() method does two things:\n\nIt shuffles the data if self.do_shuffle is True.\nIt then takes a sample of size size from the shuffled data.\nIt then loops through the sampled data and converts each element in data into an Example object. The Example along with example data also contains a unique ID, and the split name.\n\nConverting the raw examples into Example objects allows the Dataset class to process them in a standardized way later. For example, the collate method, which is used by the PyTorch DataLoader, expects each item to be an Example.\n\nTo summarize, the Dataset class handles all the necessary data processing and provides a simple API to access the different splits. This differentiates from the dataset classes like HotpotQA which require only definitions on how to load the raw data.\n\nWritten By: Herumb Shandilya\n\nPrevious\nExamples in DSPy\nNext\nCreating a Custom Dataset\nLoading HotPotQA\nAdvanced: Inside DSPy's Dataset class (Optional)\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Examples in DSPy | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/data-handling/examples",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nExamples in DSPy\nUtilizing Built-in Datasets\nCreating a Custom Dataset\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveData HandlingExamples in DSPy\nExamples in DSPy\n\nWorking in DSPy involves training sets, development sets, and test sets. This is like traditional ML, but you usually need far fewer labels (or zero labels) to use DSPy effectively.\n\nThe core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set.\n\nDSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example.\n\nCreating an Example​\n\nWhen you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example:\n\nqa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\n\nprint(qa_pair)\nprint(qa_pair.question)\nprint(qa_pair.answer)\n\n\nOutput:\n\nExample({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)\nThis is a question?\nThis is an answer.\n\n\nExamples can have any field keys and any value types, though usually values are strings.\n\nobject = Example(field1=value1, field2=value2, field3=value3, ...)\n\nSpecifying Input Keys​\n\nIn traditional ML, there are separated \"inputs\" and \"labels\".\n\nIn DSPy, the Example objects have a with_inputs() method, which can mark specific fields as inputs. (The rest are just metadata or labels.)\n\n# Single Input.\nprint(qa_pair.with_inputs(\"question\"))\n\n# Multiple Inputs; be careful about marking your labels as inputs unless you mean it.\nprint(qa_pair.with_inputs(\"question\", \"answer\"))\n\n\nThis flexibility allows for customized tailoring of the Example object for different DSPy scenarios.\n\nWhen you call with_inputs(), you get a new copy of the example. The original object is kept unchanged.\n\nElement Access and Updation​\n\nValues can be accessed using the .(dot) operator. You can access the value of key name in defined object Example(name=\"John Doe\", job=\"sleep\") through object.name.\n\nTo access or exclude certain keys, use inputs() and labels() methods to return new Example objects containing only input or non-input keys, respectively.\n\narticle_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\n\ninput_key_only = article_summary.inputs()\nnon_input_key_only = article_summary.labels()\n\nprint(\"Example object with Input fields only:\", input_key_only)\nprint(\"Example object with Non-Input fields only:\", non_input_key_only)\n\n\nOutput\n\nExample object with Input fields only: Example({'article': 'This is an article.'}) (input_keys=None)\nExample object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\n\n\nTo exclude keys, use without():\n\narticle_summary = dspy.Example(context=\"This is an article.\", question=\"This is a question?\", answer=\"This is an answer.\", rationale= \"This is a rationale.\").with_inputs(\"context\", \"question\")\n\nprint(\"Example object without answer & rationale keys:\", article_summary.without(\"answer\", \"rationale\"))\n\n\nOutput\n\nExample object without answer & rationale keys: Example({'context': 'This is an article.', 'question': 'This is a question?'}) (input_keys=None)\n\n\nUpdating values is simply assigning a new value using the . operator.\n\narticle_summary.context = \"new context\"\n\nIterating over Example​\n\nIteration in the Example class also functions like a dictionary, supporting methods like keys(), values(), etc:\n\nfor k, v in article_summary.items():\n    print(f\"{k} = {v}\")\n\n\nOutput\n\ncontext = This is an article.\nquestion = This is a question?\nanswer = This is an answer.\nrationale = This is a rationale.\n\n\nWritten By: Herumb Shandilya\n\nPrevious\nData Handling\nNext\nUtilizing Built-in Datasets\nCreating an Example\nSpecifying Input Keys\nElement Access and Updation\nIterating over Example\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "AzureAISearch | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/Azure",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nChromadbRM\nColBERTv2\nMilvusRM\nAzureAISearch\nCreating Custom RM Client\nTeleprompters\nDeep DiveRetrieval Model ClientsAzureAISearch\nAzureAISearch\n\nA retrieval module that utilizes Azure AI Search to retrieve top passages for a given query.\n\nPrerequisites​\npip install azure-search-documents\n\nSetting up the AzureAISearchRM Client​\n\nThe constructor initializes an instance of the AzureAISearchRM class and sets up parameters for sending queries and retrieving results with the Azure AI Search server.\n\nsearch_service_name (str): The name of the Azure AI Search service.\nsearch_api_key (str): The API key for accessing the Azure AI Search service.\nsearch_index_name (str): The name of the search index in the Azure AI Search service.\nfield_text (str): The name of the field containing text content in the search index. This field will be mapped to the \"content\" field in the dsp framework.\nfield_vector (Optional[str]): The name of the field containing vector content in the search index.\nk (int, optional): The default number of top passages to retrieve. Defaults to 3.\nazure_openai_client (Optional[openai.AzureOpenAI]): An instance of the AzureOpenAI client. Either openai_client or embedding_func must be provided. Defaults to None.\nopenai_embed_model (Optional[str]): The name of the OpenAI embedding model. Defaults to \"text-embedding-ada-002\".\nembedding_func (Optional[Callable]): A function for generating embeddings. Either openai_client or embedding_func must be provided. Defaults to None.\nsemantic_ranker (bool, optional): Whether to use semantic ranking. Defaults to False.\nfilter (str, optional): Additional filter query. Defaults to None.\nquery_language (str, optional): The language of the query. Defaults to \"en-Us\".\nquery_speller (str, optional): The speller mode. Defaults to \"lexicon\".\nuse_semantic_captions (bool, optional): Whether to use semantic captions. Defaults to False.\nquery_type (Optional[QueryType], optional): The type of query. Defaults to QueryType.FULL.\nsemantic_configuration_name (str, optional): The name of the semantic configuration. Defaults to None.\nis_vector_search (Optional[bool]): Whether to enable vector search. Defaults to False.\nis_hybrid_search (Optional[bool]): Whether to enable hybrid search. Defaults to False.\nis_fulltext_search (Optional[bool]): Whether to enable fulltext search. Defaults to True.\nvector_filter_mode (Optional[VectorFilterMode]): The vector filter mode. Defaults to None.\n\nAvailable Query Types:\n\nSIMPLE \"\"\"Uses the simple query syntax for searches. Search text is interpreted using a simple query #: language that allows for symbols such as +, * and \"\". Queries are evaluated across all #: searchable fields by default, unless the searchFields parameter is specified.\"\"\"\n\nFULL \"\"\"Uses the full Lucene query syntax for searches. Search text is interpreted using the Lucene #: query language which allows field-specific and weighted searches, as well as other advanced #: features.\"\"\"\n\nSEMANTIC \"\"\"Best suited for queries expressed in natural language as opposed to keywords. Improves #: precision of search results by re-ranking the top search results using a ranking model trained #: on the Web corpus.\"\"\n\nMore Details: https://learn.microsoft.com/en-us/azure/search/search-query-overview\n\nAvailable Vector Filter Mode:\n\nPOST_FILTER = \"postFilter\" \"\"\"The filter will be applied after the candidate set of vector results is returned. Depending on #: the filter selectivity, this can result in fewer results than requested by the parameter 'k'.\"\"\"\n\nPRE_FILTER = \"preFilter\" \"\"\"The filter will be applied before the search query.\"\"\"\n\nMore Details: https://learn.microsoft.com/en-us/azure/search/vector-search-filters\n\nNote\n\nThe AzureAISearchRM client allows you to perform Vector search, Hybrid search, or Full text search.\nBy default, the AzureAISearchRM client uses the Azure OpenAI Client for generating embeddings. If you want to use something else, you can provide your custom embedding_func, but either the openai_client or embedding_func must be provided.\nIf you need to enable semantic search, either with vector, hybrid, or full text search, then set the semantic_ranker flag to True.\nIf semantic_ranker is True, always set the query_type to QueryType.SEMANTIC and always provide the semantic_configuration_name.\n\nExample of the AzureAISearchRM constructor:\n\nAzureAISearchRM(\n    search_service_name: str,\n    search_api_key: str,\n    search_index_name: str,\n    field_text: str,\n    field_vector: Optional[str] = None,\n    k: int = 3,\n    azure_openai_client: Optional[openai.AzureOpenAI] = None,\n    openai_embed_model: Optional[str] = \"text-embedding-ada-002\",\n    embedding_func: Optional[Callable] = None,\n    semantic_ranker: bool = False,\n    filter: str = None,\n    query_language: str = \"en-Us\",\n    query_speller: str = \"lexicon\",\n    use_semantic_captions: bool = False,\n    query_type: Optional[QueryType] = QueryType.FULL,\n    semantic_configuration_name: str = None,\n    is_vector_search: Optional[bool] = False,\n    is_hybrid_search: Optional[bool] = False,\n    is_fulltext_search: Optional[bool] = True,\n    vector_filter_mode: Optional[VectorFilterMode.PRE_FILTER] = None\n)\n\nUnder the Hood​\nforward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction​\n\nParameters:\n\nquery_or_queries (Union[str, List[str]]): The query or queries to search for.\nk (Optional[int], optional): The number of results to retrieve. If not specified, defaults to the value set during initialization.\n\nReturns:\n\ndspy.Prediction: Contains the retrieved passages, each represented as a dotdict with a long_text attribute.\n\nInternally, the method handles the specifics of preparing the request query to the Azure AI Search service and corresponding payload to obtain the response.\n\nThe function handles the retrieval of the top-k passages based on the provided query.\n\nSending Retrieval Requests via AzureAISearchRM Client​\nRecommended Configure default RM using dspy.configure.\n\nThis allows you to define programs in DSPy and have DSPy internally conduct retrieval using dsp.retrieve on the query on the configured RM.\n\nimport dspy\nfrom dspy.retrieve.azureaisearch_rm import AzureAISearchRM\n\nazure_search = AzureAISearchRM(\n    \"search_service_name\",\n    \"search_api_key\",\n    \"search_index_name\",\n    \"field_text\",\n    \"k\"=3\n)\n\ndspy.settings.configure(rm=azure_search)\nretrieve = dspy.Retrieve(k=3)\nretrieval_response = retrieve(\"What is Thermodynamics\").passages\n\nfor result in retrieval_response:\n    print(\"Text:\", result, \"\\n\")\n\nGenerate responses using the client directly.\nfrom dspy.retrieve.azureaisearch_rm import AzureAISearchRM\n\nazure_search = AzureAISearchRM(\n    \"search_service_name\",\n    \"search_api_key\",\n    \"search_index_name\",\n    \"field_text\",\n    \"k\"=3\n)\n\nretrieval_response = azure_search(\"What is Thermodynamics\", k=3)\nfor result in retrieval_response:\n    print(\"Text:\", result.long_text, \"\\n\")\n\nExample of Semantic Hybrid Search.\nfrom dspy.retrieve.azureaisearch_rm import AzureAISearchRM\n\nazure_search = AzureAISearchRM(\n    search_service_name=\"search_service_name\",\n    search_api_key=\"search_api_key\",\n    search_index_name=\"search_index_name\",\n    field_text=\"field_text\",\n    field_vector=\"field_vector\",\n    k=3,\n    azure_openai_client=\"azure_openai_client\",\n    openai_embed_model=\"text-embedding-ada-002\"\n    semantic_ranker=True,\n    query_type=QueryType.SEMANTIC,\n    semantic_configuration_name=\"semantic_configuration_name\",\n    is_hybrid_search=True,\n)\n\nretrieval_response = azure_search(\"What is Thermodynamics\", k=3)\nfor result in retrieval_response:\n    print(\"Text:\", result.long_text, \"\\n\")\n\n\nWritten By: Prajapati Harishkumar Kishorkumar\n\nPrevious\nMilvusRM\nNext\nCreating Custom RM Client\nPrerequisites\nSetting up the AzureAISearchRM Client\nUnder the Hood\nforward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction\nSending Retrieval Requests via AzureAISearchRM Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "MilvusRM | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/MilvusRM",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nChromadbRM\nColBERTv2\nMilvusRM\nAzureAISearch\nCreating Custom RM Client\nTeleprompters\nDeep DiveRetrieval Model ClientsMilvusRM\nMilvusRM\n\nMilvusRM uses OpenAI's text-embedding-3-small embedding by default or any customized embedding function. To support passage retrieval, it assumes that a Milvus collection has been created and populated with the following field:\n\ntext: The text of the passage\nSet up the MilvusRM Client​\n\nThe constructor initializes an instance of the MilvusRM class, with the option to use OpenAI's text-embedding-3-small embeddings or any customized embedding function .\n\ncollection_name (str): The name of the Milvus collection to query against.\nuri (str, optional): The Milvus connection uri. Defaults to \"http://localhost:19530\".\ntoken (str, optional): The Milvus connection token. Defaults to None.\ndb_name (str, optional): The Milvus database name. Defaults to \"default\".\nembedding_function (callable, optional): The function to convert a list of text to embeddings. The embedding function should take a list of text strings as input and output a list of embeddings. Defaults to None. By default, it will get OpenAI client by the environment variable OPENAI_API_KEY and use OpenAI's embedding model \"text-embedding-3-small\" with the default dimension.\nk (int, optional): The number of top passages to retrieve. Defaults to 3.\n\nExample of the MilvusRM constructor:\n\nMilvusRM(\n    collection_name: str,\n    uri: Optional[str] = \"http://localhost:19530\",\n    token: Optional[str] = None,\n    db_name: Optional[str] = \"default\",\n    embedding_function: Optional[Callable] = None,\n    k: int = 3,\n)\n\nUnder the Hood​\nforward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction​\n\nParameters:\n\nquery_or_queries (Union[str, List[str]]): The query or list of queries to search for.\nk (Optional[int], optional): The number of results to retrieve. If not specified, defaults to the value set during initialization.\n\nReturns:\n\ndspy.Prediction: Contains the retrieved passages, each represented as a dotdict with a long_text attribute.\n\nSearch the Milvus collection for the top k passages matching the given query or queries, using embeddings generated via the default OpenAI embedding or the specified embedding_function.\n\nSending Retrieval Requests via MilvusRM Client​\nfrom dspy.retrieve.milvus_rm import MilvusRM\nimport os\n\nos.envrion[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n\nretriever_model = MilvusRM(\n    collection_name=\"<YOUR_COLLECTION_NAME>\",\n    uri=\"<YOUR_MILVUS_URI>\",\n    token=\"<YOUR_MILVUS_TOKEN>\"  # ignore this if no token is required for Milvus connection\n    )\n\nresults = retriever_model(\"Explore the significance of quantum computing\", k=5)\n\nfor result in results:\n    print(\"Document:\", result.long_text, \"\\n\")\n\nPrevious\nColBERTv2\nNext\nAzureAISearch\nSet up the MilvusRM Client\nUnder the Hood\nforward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction\nSending Retrieval Requests via MilvusRM Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "ColBERTv2 | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/ColBERTv2",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nChromadbRM\nColBERTv2\nMilvusRM\nAzureAISearch\nCreating Custom RM Client\nTeleprompters\nDeep DiveRetrieval Model ClientsColBERTv2\nColBERTv2\nSetting up the ColBERTv2 Client​\n\nThe constructor initializes the ColBERTv2 class instance and sets up the request parameters for interacting with the ColBERTv2 retrieval server. This server is hosted remotely at 'http://20.102.90.50:2017/wiki17_abstracts.\n\nurl (str): URL for ColBERTv2 server. Defaults to \"http://0.0.0.0\"\nport (Union[str, int], Optional): Port endpoint for ColBERTv2 server. Defaults to None.\npost_requests (bool, Optional): Flag for using HTTP POST requests. Defaults to False.\n\nExample of the ColBERTv2 constructor:\n\nclass ColBERTv2:\n    def __init__(\n        self,\n        url: str = \"http://0.0.0.0\",\n        port: Optional[Union[str, int]] = None,\n        post_requests: bool = False,\n    ):\n\nUnder the Hood​\n__call__(self, query: str, k: int = 10, simplify: bool = False) -> Union[list[str], list[dotdict]]​\n\nParameters:\n\nquery (str): Search query string used for retrieval sent to ColBERTv2 server.\nk (int, optional): Number of passages to retrieve. Defaults to 10.\nsimplify (bool, optional): Flag for simplifying output to a list of strings. Defaults to False.\n\nReturns:\n\nUnion[list[str], list[dotdict]]: Depending on simplify flag, either a list of strings representing the passage content (True) or a list of dotdict instances containing passage details (False).\n\nInternally, the method handles the specifics of preparing the request query to the ColBERTv2 server and corresponding payload to obtain the response.\n\nThe function handles the retrieval of the top-k passages based on the provided query.\n\nIf post_requests is set, the method sends a query to the server via a POST request else via a GET request.\n\nIt then processes and returns the top-k passages from the response with the list of retrieved passages dependent on the simplify flag return condition above.\n\nSending Retrieval Requests via ColBERTv2 Client​\nRecommended Configure default RM using dspy.configure.\n\nThis allows you to define programs in DSPy and have DSPy internally conduct retrieval using dsp.retrieve on the query on the configured RM.\n\nimport dspy\nimport dsp\n\ndspy.settings.configure(rm=colbertv2_wiki17_abstracts)\nretrieval_response = dsp.retrieve(\"When was the first FIFA World Cup held?\", k=5)\n\nfor result in retrieval_response:\n    print(\"Text:\", result, \"\\n\")\n\nGenerate responses using the client directly.\nimport dspy\n\nretrieval_response = colbertv2_wiki17_abstracts('When was the first FIFA World Cup held?', k=5)\n\nfor result in retrieval_response:\n    print(\"Text:\", result['text'], \"\\n\")\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nChromadbRM\nNext\nMilvusRM\nSetting up the ColBERTv2 Client\nUnder the Hood\n__call__(self, query: str, k: int = 10, simplify: bool = False) -> Union[list[str], list[dotdict]]\nSending Retrieval Requests via ColBERTv2 Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "ChromadbRM | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/ChromadbRM",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nChromadbRM\nColBERTv2\nMilvusRM\nAzureAISearch\nCreating Custom RM Client\nTeleprompters\nDeep DiveRetrieval Model ClientsChromadbRM\nChromadbRM\nAdapted from documentation provided by https://github.com/animtel​\nChromadbRM\n\nChromadbRM have the flexibility from a variety of embedding functions as outlined in the chromadb embeddings documentation. While different options are available, this example demonstrates how to utilize OpenAI embeddings specifically.\n\nSetting up the ChromadbRM Client​\n\nThe constructor initializes an instance of the ChromadbRM class, with the option to use OpenAI's embeddings or any alternative supported by chromadb, as detailed in the official chromadb embeddings documentation.\n\ncollection_name (str): The name of the chromadb collection.\npersist_directory (str): Path to the directory where chromadb data is persisted.\nembedding_function (Optional[EmbeddingFunction[Embeddable]], optional): The function used for embedding documents and queries. Defaults to DefaultEmbeddingFunction() if not specified.\nk (int, optional): The number of top passages to retrieve. Defaults to 7.\n\nExample of the ChromadbRM constructor:\n\nChromadbRM(\n    collection_name: str,\n    persist_directory: str,\n    embedding_function: Optional[EmbeddingFunction[Embeddable]] = OpenAIEmbeddingFunction(),\n    k: int = 7,\n)\n\nUnder the Hood​\nforward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction​\n\nParameters:\n\nquery_or_queries (Union[str, List[str]]): The query or list of queries to search for.\nk (Optional[int], optional): The number of results to retrieve. If not specified, defaults to the value set during initialization.\n\nReturns:\n\ndspy.Prediction: Contains the retrieved passages, each represented as a dotdict with a long_text attribute.\n\nSearch the chromadb collection for the top k passages matching the given query or queries, using embeddings generated via the specified embedding_function.\n\nSending Retrieval Requests via ChromadbRM Client​\nfrom dspy.retrieve.chromadb_rm import ChromadbRM\nimport os\nimport openai\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nembedding_function = OpenAIEmbeddingFunction(\n    api_key=os.environ.get('OPENAI_API_KEY'),\n    model_name=\"text-embedding-ada-002\"\n)\n\nretriever_model = ChromadbRM(\n    'your_collection_name',\n    '/path/to/your/db',\n    embedding_function=embedding_function,\n    k=5\n)\n\nresults = retriever_model(\"Explore the significance of quantum computing\", k=5)\n\nfor result in results:\n    print(\"Document:\", result.long_text, \"\\n\")\n\nPrevious\nRetrieval Model Clients\nNext\nColBERTv2\nSetting up the ChromadbRM Client\nUnder the Hood\nforward(self, query_or_queries: Union[str, List[str]], k: Optional[int] = None) -> dspy.Prediction\nSending Retrieval Requests via ChromadbRM Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Together | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/Together",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nAnyscale\nCohere\nOpenAI\nPremAI\nTogether\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsRemote Language Model ClientsTogether\nTogether\nTogetherß​\nAdapted from documentation provided by https://github.com/insop​\nPrerequisites​\nTogether api_key and api_base (for non-cached examples). Set these within your developer environment .env as follows:\nTOGETHER_API_BASE = ...\nTOGETHER_API_KEY = ...\n\n\nwhich will be retrieved within the Together Client as:\n\nself.api_base = os.getenv(\"TOGETHER_API_BASE\")\nself.token = os.getenv(\"TOGETHER_API_KEY\")\n\nSetting up the Together Client​\n\nThe constructor initializes the HFModel base class to support the handling of prompting models. This requires the following parameters:\n\nParameters:\n\nmodel (str): ID of model hosted on Together endpoint.\n**kwargs: Additional keyword arguments to configure the Together client.\n\nExample of the Together constructor:\n\nclass Together(HFModel):\n    def __init__(self, model, **kwargs):\n\nUnder the Hood​\n_generate(self, prompt, use_chat_api=False, **kwargs):​\n\nParameters:\n\nprompt (str): Prompt to send to Together.\nuse_chat_api (bool): Flag to use the Together Chat models endpoint. Defaults to False.\n**kwargs: Additional keyword arguments for completion request.\n\nReturns:\n\ndict: dictionary with prompt and list of response choices.\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nThe Together token is set within the request headers to ensure authorization to send requests to the endpoint.\n\nIf use_chat_api is set, the method sets up Together url chat endpoint and prompt template for chat models. It then retrieves the generated JSON response and sets up the completions list by retrieving the response's message : content.\n\nIf use_chat_api is not set, the method uses the default Together url endpoint. It similarly retrieves the generated JSON response and but sets up the completions list by retrieving the response's text as the completion.\n\nFinally, after processing the requests and responses, the method constructs the response dictionary with two keys: the original request prompt and choices, a list of dictionaries representing generated completions with the key text holding the response's generated text.\n\nUsing the Together client​\ntogether = dspy.Together(model=\"mistralai/Mistral-7B-v0.1\")\n\nSending Requests via Together Client​\nRecommended Configure default LM using dspy.configure.\n\nThis allows you to define programs in DSPy and simply call modules on your input fields, having DSPy internally call the prompt on the configured LM.\n\ndspy.configure(lm=together)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to together\nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = together(prompt='What is the capital of Paris?')\nprint(response)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nPremAI\nNext\nCreating a Custom Local Model (LM) Client\nTogetherß\nPrerequisites\nSetting up the Together Client\nUnder the Hood\nUsing the Together client\nSending Requests via Together Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Remote Language Model Clients | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/remote-language-model-clients",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nAnyscale\nCohere\nOpenAI\nPremAI\nTogether\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsRemote Language Model Clients\nRemote Language Model Clients\n\nRemote Language Model Clients in DSPy\n\n📄️ Anyscale\n\nAnyscale\n\n📄️ Cohere\n\nCohere\n\n📄️ OpenAI\n\nOpenAI\n\n📄️ PremAI\n\nPremAI\n\n📄️ Together\n\nTogetherß\n\nPrevious\nLanguage Model Clients\nNext\nAnyscale\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Executing Signatures | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/signature/executing-signatures",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nUnderstanding Signatures\nExecuting Signatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveSignaturesExecuting Signatures\nExecuting Signatures\n\nSo far we've understood what signatures are and how we can use them to craft our prompt, but now let's take a look at how to execute them.\n\nConfiguring LM​\n\nTo execute signatures, we require DSPy modules which are themselves dependent on a client connection to a language model (LM) client. DSPy supports LM APIs and local model hosting. Within this example, we will make use of the OpenAI client and configure the GPT-3.5 (gpt-3.5-turbo) model.\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\ndspy.settings.configure(lm=turbo)\n\nExecuting Signatures​\n\nLet's make use of the simplest module in DSPy - the Predict module that takes this signature as input to construct the prompt sent to the LM and generates a response for it.\n\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n# Define the predictor.\npredictor = dspy.Predict(BasicQA)\n\n# Call the predictor on a particular input.\npred = predictor(question=devset[0].question)\n\n# Print the input and the prediction.\nprint(f\"Question: {devset[0].question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\nprint(f\"Actual Answer: {devset[0].answer}\")\n\n\nOutput:\n\nQuestion: Are both Cangzhou and Qionghai in the Hebei province of China?\nPredicted Answer: No.\nActual Answer: no\n\n\nThe Predict module generates a response via the LM we configured above and executes the prompt crafted by the signature. This returns the output i.e. answer which is present in the object returned by the predictor and can be accessed via . operator.\n\nInspecting Output​\n\nLet's dive deeper into DSPy uses our signature to build up the prompt, which we can do through the inspect_history method on the configured LM following the program's execution. This method returns the last n prompts executed by LM.\n\nturbo.inspect_history(n=1)\n\n\nOutput:\n\nAnswer questions with short factoid answers.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: often between 1 and 5 words\n\n---\n\nQuestion: Are both Cangzhou and Qionghai in the Hebei province of China?\nAnswer: No.\n\n\nAdditionally, if you want to store or use this prompt, you can access the history attribute of the LM object, which stores a list of dictionaries containing respective prompt:response entries for each LM generation.\n\nturbo.history[0]\n\n\nOutput:\n\n{'prompt': \"Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\nQuestion's Answer: often between 1 and 5 words\\n\\n---\\n\\nQuestion: Are both Cangzhou and Qionghai in the Hebei province of China?\\nQuestion's Answer:\",\n 'response': <OpenAIObject chat.completion id=chatcmpl-8kCPsxikpVpmSaxdGLUIqubFZS05p at 0x7c3ba41fa840> JSON: {\n   \"id\": \"chatcmpl-8kCPsxikpVpmSaxdGLUIqubFZS05p\",\n   \"object\": \"chat.completion\",\n   \"created\": 1706021508,\n   \"model\": \"gpt-3.5-turbo-0613\",\n   \"choices\": [\n     {\n       \"index\": 0,\n       \"message\": {\n         \"role\": \"assistant\",\n         \"content\": \"No.\"\n       },\n       \"logprobs\": null,\n       \"finish_reason\": \"stop\"\n     }\n   ],\n   \"usage\": {\n     \"prompt_tokens\": 64,\n     \"completion_tokens\": 2,\n     \"total_tokens\": 66\n   },\n   \"system_fingerprint\": null\n },\n 'kwargs': {'stringify_request': '{\"temperature\": 0.0, \"max_tokens\": 150, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"model\": \"gpt-3.5-turbo\", \"messages\": [{\"role\": \"user\", \"content\": \"Answer questions with short factoid answers.\\\\n\\\\n---\\\\n\\\\nFollow the following format.\\\\n\\\\nQuestion: ${question}\\\\nQuestion\\'s Answer: often between 1 and 5 words\\\\n\\\\n---\\\\n\\\\nQuestion: Are both Cangzhou and Qionghai in the Hebei province of China?\\\\nQuestion\\'s Answer:\"}]}'},\n 'raw_kwargs': {}}\n\nHow Predict works?​\n\nThe output of predictor is a Prediction class object which mirrors the Example class with additional functionalities for LM completion interactivity.\n\nHow does Predict module actually 'predict' though? Here is a step-by-step breakdown:\n\nA call to the predictor will get executed in __call__ method of Predict Module which executes the forward method of the class.\n\nIn forward method, DSPy initializes the signature, LM call parameters and few-shot examples, if any.\n\nThe _generate method formats the few shots example to mirror the signature and uses the LM object we configured to generate the output as a Prediction object.\n\nIn case you are wondering how the prompt is constructed, the DSPy Signature framework internally handles the prompt structure, utilizing the DSP Template primitive to craft the prompt.\n\nPredict gives you a predefined pipeline to execute signature which is nice but you can build much more complicated pipelines with this by creating custom Modules.\n\nWritten By: Herumb Shandilya\n\nPrevious\nUnderstanding Signatures\nNext\nModules\nConfiguring LM\nExecuting Signatures\nInspecting Output\nHow Predict works?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "HFClientVLLM | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/local_models/HFClientVLLM",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nCreating a Custom Local Model (LM) Client\nlocal_models\nHFClientTGI\nHFClientVLLM\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model Clientslocal_modelsHFClientVLLM\nHFClientVLLM\nHFClient vLLM​\nPrerequisites - Launching vLLM Server locally​\n\nRefer to the vLLM Server API for setting up the vLLM server locally.\n\n#Example vLLM Server Launch\n\n python -m vllm.entrypoints.api_server --model meta-llama/Llama-2-7b-hf --port 8080\n\n\nThis command will start the server and make it accessible at http://localhost:8080.\n\nSetting up the vLLM Client​\n\nThe constructor initializes the HFModel base class to support the handling of prompting models, configuring the client for communicating with the hosted vLLM server to generate requests. This requires the following parameters:\n\nmodel (str): ID of model connected to the vLLM server.\nport (int): Port for communicating to the vLLM server.\nurl (str): Base URL of hosted vLLM server. This will often be \"http://localhost\".\n**kwargs: Additional keyword arguments to configure the vLLM client.\n\nExample of the vLLM constructor:\n\nclass HFClientVLLM(HFModel):\n    def __init__(self, model, port, url=\"http://localhost\", **kwargs):\n\nUnder the Hood​\n_generate(self, prompt, **kwargs) -> dict​\n\nParameters:\n\nprompt (str): Prompt to send to model hosted on vLLM server.\n**kwargs: Additional keyword arguments for completion request.\n\nReturns:\n\ndict: dictionary with prompt and list of response choices.\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nAfter generation, the method parses the JSON response received from the server and retrieves the output through json_response[\"choices\"] and stored as the completions list.\n\nLastly, the method constructs the response dictionary with two keys: the original request prompt and choices, a list of dictionaries representing generated completions with the key text holding the response's generated text.\n\nUsing the vLLM Client​\nvllm_llama2 = dspy.HFClientVLLM(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\n\nSending Requests via vLLM Client​\nRecommended Configure default LM using dspy.configure.\n\nThis allows you to define programs in DSPy and simply call modules on your input fields, having DSPy internally call the prompt on the configured LM.\n\ndspy.configure(lm=vllm_llama2)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to vllm_llama2\nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = vllm_llama2._generate(prompt='What is the capital of Paris?')\nprint(response)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nHFClientTGI\nNext\nRetrieval Model Clients\nHFClient vLLM\nPrerequisites - Launching vLLM Server locally\nSetting up the vLLM Client\nUnder the Hood\nUsing the vLLM Client\nSending Requests via vLLM Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Signature Optimizer | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/signature-optimizer",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nBootstrapFewShot\nSignature Optimizer\nDeep DiveTelepromptersSignature Optimizer\nSignature Optimizer\n\nCOPRO which aims to improve the output prefixes and instruction of the signatures in a module in a zero/few shot setting. This teleprompter is especially beneficial for fine-tuning the prompt for language models and ensure they perform tasks more effectively, all from a vague and unrefined prompt.\n\nSetting up a Sample Pipeline​\n\nWe'll be creating our CoT pipeline from scratch including the metric itself! So let's start by configuring the LM which will be OpenAI LM client with gpt-3.5-turbo as the LLM in use.\n\nimport dspy\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\ndspy.settings.configure(lm=turbo)\n\n\nNow that we have the LM client setup it's time to import the train-dev split in HotPotQA class that DSPy provides us:\n\nfrom dspy.datasets import HotPotQA\n\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n\ntrainset, devset = dataset.train, dataset.dev\n\n\nWe'll now define a class based signature for QA task similar to question->answer and pass it to ChainOfThought module, that will give us the result via Chain Of Thought from the LM client for this signature.\n\nclass CoTSignature(dspy.Signature):\n    \"\"\"Answer the question and give the reasoning for the same.\"\"\"\n\n    question = dspy.InputField(desc=\"question about something\")\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\nclass CoTPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.signature = CoTSignature\n        self.predictor = dspy.ChainOfThought(self.signature)\n\n    def forward(self, question):\n        result = self.predictor(question=question)\n        return dspy.Prediction(\n            answer=result.answer,\n            reasoning=result.rationale,\n        )\n\n\nNow we need to evaluate this pipeline too!! So we'll use the Evaluate class that DSPy provides us, as for the metric we'll use the validate_context_and_answer that we'll define. validate_context_and_answer uses dspy.evaluate.answer_exact_match metric in DSPy which in essence sees if pred and example are same or not.\n\nfrom dspy.evaluate import Evaluate\n\ndef validate_context_and_answer(example, pred, trace=None):\n    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n    return answer_EM\n\nNUM_THREADS = 5\nevaluate = Evaluate(devset=devset, metric=validate_context_and_answer, num_threads=NUM_THREADS, display_progress=True, display_table=False)\n\n\nTo evaluate the CoTPipeline we'll need to create an object of it and pass it as an arg to the evaluator call.\n\ncot_baseline = CoTPipeline()\n\ndevset_with_input = [dspy.Example({\"question\": r[\"question\"], \"answer\": r[\"answer\"]}).with_inputs(\"question\") for r in devset]\nevaluate(cot_baseline, devset=devset_with_input)\n\n\nNow we have the baseline pipeline ready to use, so let's try using the COPRO teleprompter and optimizing our pipeline to make it even better!\n\nUsing COPRO​\n\nLet's start by importing and initializing our teleprompter, for the metric we'll be using the same validate_context_and_answer imported and used above:\n\nfrom dspy.teleprompt import COPRO\n\nteleprompter = COPRO(\n    metric=validate_context_and_answer,\n    verbose=True,\n)\n\n\nIn this teleprompter there is a breadth and depth argument that defines the number of instruction/prefix candidate and number of iterations in the optimization step. We'll understand this in depth in the next section. This teleprompter comes up with better instruction candidates for the signature and better prefix candidates for the output fields of the signature. Let's start optimizing our CoT module by calling the compile method in the teleprompter:\n\nkwargs = dict(num_threads=64, display_progress=True, display_table=0) # Used in Evaluate class in the optimization process\n\ncompiled_prompt_opt = teleprompter.compile(cot, trainset=devset, eval_kwargs=kwargs)\n\n\nOnce the training is done you'll have better instructions and prefixes that you'll need to edit in signature manually. So let's say the output during optimization is like:\n\ni: \"Please answer the question and provide your reasoning for the answer. Your response should be clear and detailed, explaining the rationale behind your decision. Please ensure that your answer is well-reasoned and supported by relevant explanations and examples.\"\np: \"[Rationale]\"\nAverage Metric (78.9) ...\n\n\nThen you'll copy this and edit the original instruction class to:\n\nclass CoTSignature(dspy.Signature):\n    \"\"\"Please answer the question and provide your reasoning for the answer. Your response should be clear and detailed, explaining the rationale behind your decision. Please ensure that your answer is well-reasoned and supported by relevant explanations and examples.\"\"\"\n\n    question = dspy.InputField(desc=\"question about something\")\n    reasoning = dspy.OutputField(desc=\"reasoning for the answer\", prefix=\"[Rationale]\")\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\nINFO\n\nThe prefix would be proposed only for the output field that is defined first i.e. reasoning in CoTSignature.\n\nReinitialize the Pipeline object and reevaluate the pipeline! And now you have a more powerful predictor with more optimized Signature!\n\nHow COPRO works?​\n\nIt is interesting that to get optimal prefixes and instruction, COPRO uses Signatures. Basically COPRO uses Signature to optimize Signature!! Let's look at the codebase a bit more closely:\n\nclass BasicGenerateInstruction(Signature):\n    \"\"\"You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don't be afraid to be creative.\"\"\"\n\n    basic_instruction = dspy.InputField(desc=\"The initial instructions before optimization\")\n    proposed_instruction = dspy.OutputField(desc=\"The improved instructions for the language model\")\n    proposed_prefix_for_output_field = dspy.OutputField(desc=\"The string at the end of the prompt, which will help the model start solving the task\")\n\nclass GenerateInstructionGivenAttempts(dspy.Signature):\n    \"\"\"You are an instruction optimizer for large language models. I will give some task instructions I've tried, along with their corresponding validation scores. The instructions are arranged in increasing order based on their scores, where higher scores indicate better quality.\n\nYour task is to propose a new instruction that will lead a good language model to perform the task even better. Don't be afraid to be creative.\"\"\"\n\n    attempted_instructions = dspy.InputField(format=dsp.passages2text)\n    proposed_instruction = dspy.OutputField(desc=\"The improved instructions for the language model\")\n    proposed_prefix_for_output_field = dspy.OutputField(desc=\"The string at the end of the prompt, which will help the model start solving the task\")\n\n\nThese two signatures are what give use the optimal instruction and prefixes. Now, the BasicGenerateInstruction will generate n instruction and prefixes based on the breadth parameter, basically n=breadth. This happens only one time in the start to seed the instruction attempts.\n\nIt uses these instructions and pass them to GenerateInstructionGivenAttempts which outputs hopefully a more optimal instruction. This then happens for m iterations which is the depth parameter in DSPy.\n\nLet's break down the process stepwise:\n\nStarting Point: Use BasicGenerateInstruction to create initial optimized instructions and prefixes. This is based on a basic instruction input.\nIterative Improvement: Pass these initial instructions to GenerateInstructionGivenAttempts.\nRepeat Optimization: In each iteration (up to m times):\nEvaluate the current instructions and their effectiveness.\nPropose new, more optimized instructions and prefixes based on the evaluation.\nOutcome: After m iterations, the system ideally converges to a set of highly optimized instructions and corresponding prefixes that lead to better performance of the language model on the given task.\n\nThis iterative approach allows for continuous refinement of instructions and prefixes, leveraging the strengths of the teleprompter and improving task performance over time.\n\nWritten By: Herumb Shandilya\n\nPrevious\nBootstrapFewShot\nSetting up a Sample Pipeline\nUsing COPRO\nHow COPRO works?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "HFClientTGI | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/local_models/HFClientTGI",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nCreating a Custom Local Model (LM) Client\nlocal_models\nHFClientTGI\nHFClientVLLM\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model Clientslocal_modelsHFClientTGI\nHFClientTGI\nHFClient TGI​\nPrerequisites - Launching TGI Server locally​\n\nRefer to the Text Generation-Inference Server API for setting up the TGI server locally.\n\n#Example TGI Server Launch\n\nmodel=meta-llama/Llama-2-7b-hf # set to the specific Hugging Face model ID you wish to use.\nnum_shard=1 # set to the number of shards you wish to use.\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data -e HUGGING_FACE_HUB_TOKEN={your_token} ghcr.io/huggingface/text-generation-inference:latest --model-id $model --num-shard $num_shard\n\n\nThis command will start the server and make it accessible at http://localhost:8080.\n\nSetting up the TGI Client​\n\nThe constructor initializes the HFModel base class to support the handling of prompting HuggingFace models. It configures the client for communicating with the hosted TGI server to generate requests. This requires the following parameters:\n\nmodel (str): ID of Hugging Face model connected to the TGI server.\nport (int or list): Port for communicating to the TGI server. This can be a single port number (8080) or a list of TGI ports ([8080, 8081, 8082]) to route the requests to.\nurl (str): Base URL of hosted TGI server. This will often be \"http://localhost\".\nhttp_request_kwargs (dict): Dictionary of additional keyword agruments to pass to the HTTP request function to the TGI server. This is None by default.\n**kwargs: Additional keyword arguments to configure the TGI client.\n\nExample of the TGI constructor:\n\nclass HFClientTGI(HFModel):\n    def __init__(self, model, port, url=\"http://future-hgx-1\", http_request_kwargs=None, **kwargs):\n\nUnder the Hood​\n_generate(self, prompt, **kwargs) -> dict​\n\nParameters:\n\nprompt (str): Prompt to send to model hosted on TGI server.\n**kwargs: Additional keyword arguments for completion request.\n\nReturns:\n\ndict: dictionary with prompt and list of response choices.\n\nInternally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.\n\nAfter generation, the method parses the JSON response received from the server and retrieves the output through json_response[\"generated_text\"]. This is then stored in the completions list.\n\nIf the JSON response includes the additional details argument and correspondingly, the best_of_sequences within details, this indicates multiple sequences were generated. This is also usually the case when best_of > 1 in the initialized kwargs. Each of these sequences is accessed through x[\"generated_text\"] and added to the completions list.\n\nLastly, the method constructs the response dictionary with two keys: the original request prompt and choices, a list of dictionaries representing generated completions with the key text holding the response's generated text.\n\nUsing the TGI Client​\ntgi_llama2 = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\n\nSending Requests via TGI Client​\nRecommended Configure default LM using dspy.configure.\n\nThis allows you to define programs in DSPy and simply call modules on your input fields, having DSPy internally call the prompt on the configured LM.\n\ndspy.configure(lm=tgi_llama2)\n\n#Example DSPy CoT QA program\nqa = dspy.ChainOfThought('question -> answer')\n\nresponse = qa(question=\"What is the capital of Paris?\") #Prompted to tgi_llama2\nprint(response.answer)\n\nGenerate responses using the client directly.\nresponse = tgi_llama2._generate(prompt='What is the capital of Paris?')\nprint(response)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nCreating a Custom Local Model (LM) Client\nNext\nHFClientVLLM\nHFClient TGI\nPrerequisites - Launching TGI Server locally\nSetting up the TGI Client\nUnder the Hood\nUsing the TGI Client\nSending Requests via TGI Client\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/local-language-model-clients",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nPage Not Found\n\nWe could not find what you were looking for.\n\nPlease contact the owner of the site that linked you to the original URL and let them know their link is broken.\n\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Retrieval Model Clients | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/retrieval-model-clients",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nChromadbRM\nColBERTv2\nMilvusRM\nAzureAISearch\nCreating Custom RM Client\nTeleprompters\nDeep DiveRetrieval Model Clients\nRetrieval Model Clients\n\nRetrieval Models in DSPy\n\n📄️ ChromadbRM\n\nAdapted from documentation provided by https://github.com/animtel\n\n📄️ ColBERTv2\n\nSetting up the ColBERTv2 Client\n\n📄️ MilvusRM\n\nMilvusRM uses OpenAI's text-embedding-3-small embedding by default or any customized embedding function.\n\n📄️ AzureAISearch\n\nA retrieval module that utilizes Azure AI Search to retrieve top passages for a given query.\n\n📄️ Creating Custom RM Client\n\nDSPy provides support for various retrieval modules out of the box like ColBERTv2, AzureCognitiveSearch, Pinecone, Weaviate, etc. Unlike Language Model (LM) modules, creating a custom RM module is much more simple and flexible.\n\nPrevious\nHFClientVLLM\nNext\nChromadbRM\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/teleprompters",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nBootstrapFewShot\nSignature Optimizer\nDeep DiveTeleprompters\nTeleprompters\n\nTeleprompters are powerful optimizers (included in DSPy) that can learn to bootstrap and select effective prompts for the modules of any program. (The \"tele-\" in the name means \"at a distance\", i.e., automatic prompting at a distance.)\n\n📄️ BootstrapFewShot\n\nWhen compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric—and returns a new optimized program. Different teleprompters apply different strategies for optimization. This family of teleprompters is focused on optimizing the few shot examples. Let's take an example of a Sample pipeline and see how we can use teleprompter to optimizes it.\n\n📄️ Signature Optimizer\n\nCOPRO which aims to improve the output prefixes and instruction of the signatures in a module in a zero/few shot setting. This teleprompter is especially beneficial for fine-tuning the prompt for language models and ensure they perform tasks more effectively, all from a vague and unrefined prompt.\n\nPrevious\nCreating Custom RM Client\nNext\nBootstrapFewShot\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Language Model Clients | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/language-model-clients",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model Clients\nLanguage Model Clients\n\nLanguage Model Clients in DSPy\n\n🗃️ Remote Language Model Clients\n\n5 items\n\n📄️ Creating a Custom Local Model (LM) Client\n\nDSPy provides you with multiple LM clients that you can use to execute any of your pipelines. However, if you have an API or LM that is unable to be executed by any of the existing clients hosted in DSPy, you can create one yourself!! It's not too difficult so let's see how!!\n\n🗃️ local_models\n\n2 items\n\nPrevious\nUnderstanding Typed Predictors\nNext\nRemote Language Model Clients\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Signatures | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/signatures",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nUnderstanding Signatures\nExecuting Signatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveSignatures\nSignatures\n\nSignatures in DSPy\n\n📄️ Understanding Signatures\n\nA DSPy Signature is the most basic form of task description which simply requires inputs and outputs and optionally, a small description about them and the task too.\n\n📄️ Executing Signatures\n\nSo far we've understood what signatures are and how we can use them to craft our prompt, but now let's take a look at how to execute them.\n\nPrevious\nCreating a Custom Dataset\nNext\nUnderstanding Signatures\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Typed Predictors | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/typed-predictors",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nFunctional Typed Predictors\nUnderstanding Typed Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveTyped Predictors\nTyped Predictors\n\nTyped Predictors in DSPy\n\n📄️ Functional Typed Predictors\n\nTyped Predictors as Decorators\n\n📄️ Understanding Typed Predictors\n\nWhy use a Typed Predictor?\n\nPrevious\nRetrieve\nNext\nFunctional Typed Predictors\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Modules | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/modules",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nDSPy Assertions\nChainOfThoughtWithHint\nProgram of Thought\nReAct\nGuide: DSPy Modules\nRetrieve\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveModules\nModules\n\nModules in DSPy\n\n📄️ DSPy Assertions\n\nIntroduction\n\n📄️ ChainOfThoughtWithHint\n\nThis class builds upon the ChainOfThought class by introducing an additional input field to provide hints for reasoning. The inclusion of a hint allows for a more directed problem-solving process, which can be especially useful in complex scenarios.\n\n📄️ Program of Thought\n\nBackground\n\n📄️ ReAct\n\nBackground\n\n📄️ Guide: DSPy Modules\n\nQuick Recap\n\n📄️ Retrieve\n\nBackground\n\nPrevious\nExecuting Signatures\nNext\nDSPy Assertions\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Data Handling | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/data-handling",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nExamples in DSPy\nUtilizing Built-in Datasets\nCreating a Custom Dataset\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveData Handling\nData Handling\n\nData Handling in DSPy\n\n📄️ Examples in DSPy\n\nWorking in DSPy involves training sets, development sets, and test sets. This is like traditional ML, but you usually need far fewer labels (or zero labels) to use DSPy effectively.\n\n📄️ Utilizing Built-in Datasets\n\nIt's easy to use your own data in DSPy: a dataset is just a list of Example objects. Using DSPy well involves being able to find and re-purpose existing datasets for your own pipelines in new ways; DSPy makes this a particularly powerful strategy.\n\n📄️ Creating a Custom Dataset\n\nWe've seen how to work with with Example objects and use the HotPotQA class to load the HuggingFace HotPotQA dataset as a list of Example objects. But in production, such structured datasets are rare. Instead, you'll find yourself working on a custom dataset and might question: how do I create my own dataset or what format should it be?\n\nPrevious\nDeep Dive\nNext\nExamples in DSPy\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Creating Custom RM Client | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nChromadbRM\nColBERTv2\nMilvusRM\nAzureAISearch\nCreating Custom RM Client\nTeleprompters\nDeep DiveRetrieval Model ClientsCreating Custom RM Client\nCreating Custom RM Client\n\nDSPy provides support for various retrieval modules out of the box like ColBERTv2, AzureCognitiveSearch, Pinecone, Weaviate, etc. Unlike Language Model (LM) modules, creating a custom RM module is much more simple and flexible.\n\nAs of now, DSPy offers 2 ways to create a custom RM: the Pythonic way and the DSPythonic way. We'll take a look at both, understand why both are performing the same behavior, and how you can implement each!\n\nI/O of RM Client​\n\nBefore understanding the implementation, let's understand the idea and I/O within RM modules.\n\nThe input to an RM module is either 1) a single query or 2) a list of queries.\n\nThe output is the top-k passages per query retrieved from a retrieval model, vector store, or search client.\n\nConventionally, we simply call the RM module object through the __call__ method, inputting the query/queries as argument(s) of the call with the corresponding output returned as a list of strings.\n\nWe'll see how this I/O is essentially same in both methods of implementation but differs in their delivery.\n\nThe Pythonic Way​\n\nTo account for our RM I/O, we create a class that conducts the retrieval logic, which we implement in the __init__ and __call__ methods:\n\nfrom typing import List, Union\n\nclass PythonicRMClient:\n    def __init__(self):\n        pass\n\n    def __call__(self, query: Union[str, List[str]], k:int) -> Union[List[str], List[List[str]]]:\n        pass\n\nINFO\n\nDon't worry about the extensive type-hinting above. typing is a package that provides type-definitions for function inputs and outputs.\n\nUnion covers all possible types of the argument/output. So:\n\nUnion[str, List[str]]: Assigned to query to work with a single query string or a list of queries strings.\nUnion[List[str], List[List[str]]]: Assigned to the output of __call__ to work with a single query string as a list or a list of multiple query string lists.\n\nSo let's start by implementing PythonicRMClient for a local retrieval model hosted on a API with endpoint being /. We'll start by implementing the __init__ method, which simply initializes the class attributes, url and port, and attaches the port to the url if present.\n\ndef __init__(self, url: str, port:int = None):\n    self.url = f`{url}:{port}` if port else url\n\n\nNow it's time to write the retrieval logic in __call__ method:\n\ndef __call__(self, query:str, k:int) -> List[str]:\n    params = {\"query\": query, \"k\": k}\n    response = requests.get(self.url, params=params)\n\n    response = response.json()[\"retrieved_passages\"]    # List of top-k passages\n    return response\n\n\nThis serves to represent our API request call to retrieve our list of top-k passages which we return as the response. Let's bring it all together and see how our RM class looks like:\n\nfrom typing import List\n\nclass PythonicRMClient:\n    def __init__(self, url: str, port:int = None):\n        self.url = f`{url}:{port}` if port else url\n\n    def __call__(self, query:str, k:int) -> List[str]:\n        # Only accept single query input, feel free to modify it to support \n\n        params = {\"query\": query, \"k\": k}\n        response = requests.get(self.url, params=params)\n\n        response = response.json()[\"retrieved_passages\"]    # List of top k passages\n        return response\n\n\nThat's all!! This is the most basic way to implement a RM model and mirrors DSP-v1-hosted RM models like ColBERTv2 and AzureCognitiveSearch.\n\nNow let's take a look at how we streamline this process in DSPy!\n\nThe DSPythonic Way​\n\nThe DSPythonic way mirrors the Pythonic way in maintaining the same input but now returning an object of dspy.Prediction class, the standard output format for any DSPy module as we've seen in previous docs. Additionally, this class would now inherit the dspy.Retrieve class to maintain state management within the DSPy library.\n\nSo let's implement __init__ and forward method where our class's __call__ is calling the forward method as is=:\n\nimport dspy\nfrom typing import List, Union, Optional\n\nclass DSPythonicRMClient(dspy.Retrieve):\n    def __init__(self, k:int):\n        pass\n\n    def forward(self, query: Union[str, List[str]], k:Optional[str]) -> dspy.Prediction:\n        pass\n\n\nUnlike PythonicRMClient, we initialize k as part of the initialization call and the forward method will take query/queries as arguments and the k number of retrieved passages as an optional argument. k is used within the inherited dspy.Retrieve initialization when we call super().__init__().\n\nWe'll be implementing DSPythonicRMClient for the same local retrieval model API we used above. We'll start by implementing the __init__ method, which mirrors the PythonicRMClient.\n\ndef __init__(self, url: str, port:int = None, k:int = 3):\n    super().__init__(k=k)\n\n    self.url = f`{url}:{port}` if port else url\n\n\nWe'll now implement the forward method, returning the output as a dspy.Prediction object under the passage attribute which is standard among all the RM modules. The call will default to the defined self.k argument unless overridden in this call.\n\ndef forward(self, query:str, k:Optional[int]) -> dspy.Prediction:\n    params = {\"query\": query, \"k\": k if k else self.k}\n    response = requests.get(self.url, params=params)\n\n    response = response.json()[\"retrieved_passages\"]    # List of top k passages\n    return dspy.Prediction(\n        passages=response\n    )\n\n\nLet's bring it all together and see how our RM class looks like:\n\nimport dspy\nfrom typing import List, Union, Optional\n\nclass DSPythonicRMClient(dspy.Retrieve):\n    def __init__(self, url: str, port:int = None, k:int = 3):\n        super().__init__(k=k)\n\n        self.url = f`{url}:{port}` if port else url\n\n    def forward(self, query_or_queries:str, k:Optional[int]) -> dspy.Prediction:\n        params = {\"query\": query_or_queries, \"k\": k if k else self.k}\n        response = requests.get(self.url, params=params)\n\n        response = response.json()[\"retrieved_passages\"]    # List of top k passages\n        return dspy.Prediction(\n            passages=response\n        )\n\n\nThat's all!! This is the way to implement a custom RM model client within DSPy and how more recently-supported RM models like QdrantRM, WeaviateRM, etc. are implemented in DSPy.\n\nLet's take a look at how we use these retrievers.\n\nUsing Custom RM Models​\n\nDSPy offers two ways of using custom RM clients: Direct Method and using dspy.Retrieve.\n\nDirect Method​\n\nThe most straightforward way to use your custom RM is by directly using its object within the DSPy Pipeline.\n\nLet's take a look at the following pseudocode of a DSPy Pipeline as an example:\n\nclass DSPyPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        url = \"http://0.0.0.0\"\n        port = 3000\n\n        self.pythonic_rm = PythonicRMClient(url=url, port=port)\n        self.dspythonic_rm = DSPythonicRMClient(url=url, port=port, k=3)\n\n        ...\n\n    def forward(self, *args):\n        ...\n\n        passages_from_pythonic_rm = self.pythonic_rm(query)\n        passages_from_dspythonic_rm = self.dspythonic_rm(query).passages\n\n        ...\n\n\nThis ensures you retrieve a list of passages from your RM client and can interact with the results within your forward pass in whichever way needed for your pipeline's purpose!\n\nUsing dspy.Retrieve​\n\nThis way is more experimental in essence, allowing you to maintain the same pipeline and experiment with different RMs. How? By configuring it!\n\nimport dspy\n\nlm = ...\nurl = \"http://0.0.0.0\"\nport = 3000\n\n# pythonic_rm = PythonicRMClient(url=url, port=port)\ndspythonic_rm = DSPythonicRMClient(url=url, port=port, k=3)\n\ndspy.settings.configure(lm=lm, rm=dspythonic_rm)\n\n\nNow, in the pipeline, you just need to use dspy.Retrieve which will use this rm client to get the top-k passage for a given query!\n\nclass DSPyPipeline(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        url = \"http://0.0.0.0\"\n        port = 3000\n\n        self.rm = dspy.Retrieve(k=3)\n        ...\n\n    def forward(self, *args):\n        ...\n\n        passages = self.rm(query)\n\n        ...\n\n\nNow if you'd like to use a different RM, you can just update the rm parameter via dspy.settings.configure.\n\nHOW dspy.Retrieve USES rm\n\nWhen we call dspy.Retrieve the __call__ method will execute the forward method as is. In forward, the top-k passages are received by the dsp.retrieveEnsemble method in search.py.\n\nIf an rm is not initialized in dsp.settings, this would raise an error.\n\nWritten By: Arnav Singhvi\n\nPrevious\nAzureAISearch\nNext\nTeleprompters\nI/O of RM Client\nThe Pythonic Way\nThe DSPythonic Way\nUsing Custom RM Models\nDirect Method\nUsing dspy.Retrieve\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Creating a Custom Local Model (LM) Client | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRemote Language Model Clients\nCreating a Custom Local Model (LM) Client\nlocal_models\nRetrieval Model Clients\nTeleprompters\nDeep DiveLanguage Model ClientsCreating a Custom Local Model (LM) Client\nCreating a Custom Local Model (LM) Client\n\nDSPy provides you with multiple LM clients that you can use to execute any of your pipelines. However, if you have an API or LM that is unable to be executed by any of the existing clients hosted in DSPy, you can create one yourself!! It's not too difficult so let's see how!!\n\nFormat of LM Client​\n\nAn LM client needs to implement 3 methods at minimum: __init__, basic_request and __call__. So your custom LM client should follow the template below:\n\nfrom dsp import LM\n\n\nclass CustomLMClient(LM):\n    def __init__(self):\n        self.provider = \"default\"\n\n        self.history = []\n\n    def basic_request(self, prompt, **kwargs):\n        pass\n\n    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n        pass\n\n\nWhile you can mostly add to and customize the client per your requirements, the client should be configurable with some key components to utilize every feature of DSPy without interruption. One of these key features is viewing the history of calls made to the LM via inspect_history. To elaborate:\n\n__init__: Should contain the self.provider=\"default and self.history=[]. self.history will contain the prompt-completion pair created via LM call since the object was initialized. self.provider is used in inspect_history method and for most part you can leave it as \"default\".\nbasic_request: This function makes the call to the LM and retrieves the completion for the given prompt over the given kwargs which usually have parameters like temperature, max_tokens, etc. After you receive the completion from the LM, you must update the self.history list by appending the dictionary {\"prompt\": prompt, \"response\": response} to it. These fields are mandatory but you can add any other parameters as you see fit.\n__call__: This function should return the list of completions returned by the model. This can be a list of string completions in the basic case. Or it can be a tuple pair with the completion and corresponding likelihood as returned by the Cohere LM client. Additionally, the completion should be received by the request call which unless modified just calls basic_request as is, ensuring the history is updated as well.\n\nBy now you must've realized the reason we have these rules is mainly for making the history inspection and modules work without breaking.\n\nINFO\n\nYou can mitigate issues with updating the history in the __call__ itself. So if you can take care of history updates in __call__ itself, you just need to implement __init__ and __call__.\n\nOr if you are up for it, feel free to rewrite inspect_history method as per your requirements!\n\nImplementing our Custom LM​\n\nBased on whatever we have learned so far, let's implement our custom LM that calls the Claude API. In Claude, we need to initialize 2 components to make a successful call: API_KEY and BASE_URL. The base URL from on the Anthropic docs is https://api.anthropic.com/v1/messages. Let's write our __init__ method:\n\ndef __init__(model, api_key):\n    self.model = model\n    self.api_key = api_key\n    self.provider = \"default\"\n    self.history = []\n\n    self.base_url = \"https://api.anthropic.com/v1/messages\"\n\n\nBased on the implementation above, we want to now pass in our model like claude-2 and the api_key which you'll see in Claude's API Console. Now it's time to define the basic_request method where we'll make the call to self.base_url and get the completion:\n\ndef basic_request(self, prompt: str, **kwargs):\n    headers = {\n        \"x-api-key\": self.api_key,\n        \"anthropic-version\": \"2023-06-01\",\n        \"anthropic-beta\": \"messages-2023-12-15\",\n        \"content-type\": \"application/json\"\n    }\n\n    data = {\n        **kwargs,\n        \"model\": self.model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    }\n\n    response = requests.post(self.base_url, headers=headers, json=data)\n    response = response.json()\n\n    self.history.append({\n        \"prompt\": prompt,\n        \"response\": response,\n        \"kwargs\": kwargs,\n    })\n    return response\n\n\nNow it's time to define __call__ method that'll bring it all together:\n\ndef __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n    response = self.request(prompt, **kwargs)\n\n    completions = [result[\"text\"] for result in response[\"content\"]]\n\n    return completions\n\n\nTo write it all in a single class, we'll get:\n\nfrom dsp import LM\n\nclass Claude(LM):\n    def __init__(model, api_key):\n        self.model = model\n        self.api_key = api_key\n        self.provider = \"default\"\n\n        self.base_url = \"https://api.anthropic.com/v1/messages\"\n\n    def basic_request(self, prompt: str, **kwargs):\n        headers = {\n            \"x-api-key\": self.api_key,\n            \"anthropic-version\": \"2023-06-01\",\n            \"anthropic-beta\": \"messages-2023-12-15\",\n            \"content-type\": \"application/json\"\n        }\n\n        data = {\n            **kwargs,\n            \"model\": self.model,\n            \"messages\": [\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=data)\n        response = response.json()\n\n        self.history.append({\n            \"prompt\": prompt,\n            \"response\": response,\n            \"kwargs\": kwargs,\n        })\n        return response\n\n    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n        response = self.request(prompt, **kwargs)\n\n        completions = [result[\"text\"] for result in response[\"content\"]]\n\n        return completions\n\n\nThat's it! Now we can configure this as an lm in DSPy and use it in the pipeline like any other LM Client:\n\nimport dspy\n\nclaude = Claude(model='claude-2')\n\ndspy.settings.configure(lm=claude)\n\n\nWritten By: Arnav Singhvi\n\nPrevious\nTogether\nNext\nHFClientTGI\nFormat of LM Client\nImplementing our Custom LM\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "BootstrapFewShot | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/bootstrap-fewshot",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nBootstrapFewShot\nSignature Optimizer\nDeep DiveTelepromptersBootstrapFewShot\nBootstrapFewShot\n\nWhen compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric—and returns a new optimized program. Different teleprompters apply different strategies for optimization. This family of teleprompters is focused on optimizing the few shot examples. Let's take an example of a Sample pipeline and see how we can use teleprompter to optimizes it.\n\nSetting up a Sample Pipeline​\n\nWe'll be making a basic answer generation pipeline over GSM8K dataset that we saw in the Minimal Example, we won't be changing anything in it! So let's start by configuring the LM which will be OpenAI LM client with gpt-3.5-turbo as the LLM in use.\n\nimport dspy\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=250)\ndspy.settings.configure(lm=turbo)\n\n\nNow that we have the LM client setup it's time to import the train-dev split in GSM8k class that DSPy provides us:\n\nfrom dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n\ngms8k = GSM8K()\n\ntrainset, devset = gms8k.train, gms8k.dev\n\n\nWe'll now define a basic QA inline signature i.e. question->answer and pass it to ChainOfThought module, that applies necessary addition for CoT style prompting to the Signature.\n\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -> answer\")\n    \n    def forward(self, question):\n        return self.prog(question=question)\n\n\nNow we need to evaluate this pipeline too!! So we'll use the Evaluate class that DSPy provides us, as for the metric we'll use the gsm8k_metric that we imported above.\n\nfrom dspy.evaluate import Evaluate\n\nevaluate = Evaluate(devset=devset[:], metric=gsm8k_metric, num_threads=NUM_THREADS, display_progress=True, display_table=False)\n\n\nTo evaluate the CoT pipeline we'll need to create an object of it and pass it as an arg to the evaluator call.\n\ncot_baseline = CoT()\n\nevaluate(cot_baseline, devset=devset[:])\n\n\nNow we have the baseline pipeline ready to use, so let's try using the BootstrapFewShot teleprompter and optimizing our pipeline to make it even better!\n\nUsing BootstrapFewShot​\n\nLet's start by importing and initializing our teleprompter, for the metric we'll be using the same gsm8k_metric imported and used above:\n\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nteleprompter = BootstrapFewShotWithRandomSearch(\n    metric=gsm8k_metric, \n    max_bootstrapped_demos=8, \n    max_labeled_demos=8,\n)\n\n\nmetric is a an obvious parameter but what are max_bootstrapped_demos and max_labeled_demos parameters? Let's see the difference via a table:\n\nFeature\tmax_labeled_demos\tmax_bootstrapped_demos\nPurpose\tRefers to the maximum number of labeled demonstrations (examples) that will be used for training the student module directly. Labeled demonstrations are typically pre-existing, manually labeled examples that the module can learn from.\tRefers to the maximum number of demonstrations that will be bootstrapped. Bootstrapping in this context likely means generating new training examples based on the predictions of a teacher module or some other process. These bootstrapped demonstrations are then used alongside or instead of the manually labeled examples.\nTraining Usage\tDirectly used in training; typically more reliable due to manual labeling.\tAugment training data; potentially less accurate as they are generated examples.\nData Source\tPre-existing dataset of manually labeled examples.\tGenerated during the training process, often using outputs from a teacher module.\nInfluence on Training\tHigher quality and reliability, assuming labels are accurate.\tProvides more data but may introduce noise or inaccuracies.\n\nThis teleprompter augments any necessary field even if you data doesn't have it, for example we don't have rationale for labelling however you'll see the rationale for each few shot example in the prompt that this teleprompter curated, how? By generating them all via a teacher module which is an optional parameter, since we didn't pass it the teleprompted creates a teacher from the module we are training or the student module.\n\nIn the next section, we'll seen this process step by step but for now let's start optimizing our CoT module by calling the compile method in the teleprompter:\n\ncot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)\n\n\nOnce the training is done you'll have a more optimized module that you can save or load again for use anytime:\n\ncot_compiled.save('turbo_gsm8k.json')\n\n# Loading:\n# cot = CoT()\n# cot.load('turbo_gsm8k.json')\n\nHow BootstrapFewShot works?​\n\nLabeledFewShot is the most vanilla teleprompter that takes a training set as input and it assigns subsets of the trainset in each student's predictor's demos attribute. You can think of it as basically adding few shot examples to the prompt.\n\nBootStrapFewShot starts by doing this only, it starts by:\n\nInitializing a student program which which is the one we are optimizing and a teacher program which unless specified otherwise is a clone of the student.\n\nThen to the teacher it add the demos by using LabeledFewShot teleprompter.\n\nMappings are created between the names of predictors and their corresponding instances in both student and teacher models.\n\nThe maximum number of bootstrap demonstrations (max_bootstraps) is determined. This limits the amount of initial training data generated.\n\nThe process iterates over each example in the training set. For each example, the method checks if the maximum number of bootstraps has been reached. If so, the process stops.\n\nFor each training example, the teacher model attempts to generate a prediction.\n\nIf the teacher model successfully generates a prediction, the trace of this prediction process is captured. This trace includes details about which predictors were called, the inputs they received, and the outputs they produced.\n\nIf the prediction is successful, a demonstration (demo) is created for each step in the trace. This demo includes the inputs to the predictor and the outputs it generated.\n\nThis is how it works, aside from that. BootstrapFewShotWithOptuna, BootstrapFewShotWithRandomSearch etc. which work on the same principle with slight changes in the example discovery process.\n\nWritten By: Herumb Shandilya\n\nPrevious\nTeleprompters\nNext\nSignature Optimizer\nSetting up a Sample Pipeline\nUsing BootstrapFewShot\nHow BootstrapFewShot works?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Understanding Signatures | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/deep-dive/signature/understanding-signatures",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nUnderstanding Signatures\nExecuting Signatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep DiveSignaturesUnderstanding Signatures\nUnderstanding Signatures\n\nA DSPy Signature is the most basic form of task description which simply requires inputs and outputs and optionally, a small description about them and the task too.\n\nThere are 2 ways to define a Signature: Inline and Class-Based. But before diving into creating signatures, let's understand what a signature is and why we need it.\n\nWhat is a Signature?​\n\nIn the typical LLM pipeline, you'll have two key components at work i.e. an LLM and a prompt. In DSPy, we have an LLM configured at the beginning of any DSPy script via the LM(Language Model - which is shown in the next blog) and a prompt defined via Signatures.\n\nA Signature is usually composed of 2 essential components: Input Fields and Output Fields. You can optionally pass an instruction defining more robust requirements of your task. An Input Field is an attribute of Signature that defines an input to the prompt and an Output Field is an attribute of Signature that defines an output of the prompt received from an LLM call. Let's understand this by an example.\n\nLet's think of a basic Question-Answer task where the question serves as an input to the LLM from which you receive an answer response. We directly map this in DSPy as the question serves as the Signature's Input Field and the answer as the Signature's Output Field .\n\nNow that we understand the components of a Signature, let's see how we can declare a signature and what a prompt for that signature looks like.\n\nInline Method​\n\nDSPy offers an intuitive, simple approach for defining tasks: simply state the inputs and outputs to convey the task in its simplest form. For example, if your input is question and output is answer, it should be clear that the task is a Question-Answer task. If your inputs are context and question and outputs are answer and reason, this should imply some form of Chain-Of-Thought prompting, potentially within a RAG pipeline.\n\nInspired by this simplicity, DSPy Signatures mirrors an Einops-like abstract manner:\n\ninput_field_1,input_field_2,input_field_3...->output_field_1,output_field_2,output_field_3...\n\n\nInput Fields of the Signature are declared on the left side of -> with the Output Fields on the right side. So let's go ahead and define DSPy signatures for the QA and RAG tasks:\n\nQA Task: question->answer\nRAG Task: context,question->answer,rationale\n\n\nThis simplistic naming of the fields is essential for the LLM to understand the nature of inputs and outputs, reducing sensitivity and ensuring clarity for expected inputs and generations.\n\nHowever, this barebones signature may not provide a clear picture for how the model should approach the task, and to meet these needs, DSPy modules offer simplistic yet robust instructional templates that integrate the Signatures. Let's take a deeper look at the prompt constructed by DSPy to understand it better when used within a dspy.Predict module as dspy.Predict(question->answer):\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion:\n\n\nAs you can see, DSPy populates the instruction Given the fields ``question``, produce the fields ``answer``. to define the task and provides instructions for the prompt format. And this format is pretty standard for any Signature you create as we can see in this prompting setup for RAG:\n\nNow these instructional templates are well defined for their respective prompting techniques (CoT, ProgramOfThought, ReAct), leaving the user only having to define their task's Signature input and outputs with the rest handled by the DSPy modules library!\n\nHowever, it would be nice to give more instructions beyond the simplistic in-line signature and for this, we turn to class-based signatures.\n\nClass Based Method​\n\nA Signature class comprises of three things:\n\nTask Description/Instruction: We define in the signature class docstring.\nInputs Field: We define these as dspy.InputField().\nOutputs Field: We define these as dspy.OutputField().\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\", prefix=\"Question's Answer:\")\n\n\nThe I/O Fields take 3 inputs: desc, prefix and format. desc is the description to the input, prefix is the placeholder text of the field in the prompt(one that has been ${field_name} until now) and format which is a method that'll define how to handle non-string inputs. If the input to field is a list rather than a string, we can specify this through format.\n\nBoth InputField and OutputField are similar in implementation as well:\n\nclass InputField(Field):\n    def __init__(self, *, prefix=None, desc=None, format=None):\n        super().__init__(prefix=prefix, desc=desc, input=True, format=format)\n\nclass OutputField(Field):\n    def __init__(self, *, prefix=None, desc=None, format=None):\n        super().__init__(prefix=prefix, desc=desc, input=False, format=format)\n\n\nLet's take a look at how a prompt for the class based signature looks like:\n\nAnswer questions with short factoid answers.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nQuestion's Answer: often between 1 and 5 words\n\n---\n\nQuestion:\n\n\nAs you can see, the instruction is more well-defined by our task's instruction in the docstring. The prefix and description for the answer field reflects our definitions. This ensures a more refined prompt structure, giving the user more control on defining its contents per task requirements.\n\nWritten By: Herumb Shandilya\n\nPrevious\nSignatures\nNext\nExecuting Signatures\nWhat is a Signature?\nInline Method\nClass Based Method\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "DSPy Assertions | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/assertions",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksDSPy Assertions\nDSPy Assertions\nIntroduction​\n\nLanguage models (LMs) have transformed how we interact with machine learning, offering vast capabilities in natural language understanding and generation. However, ensuring these models adhere to domain-specific constraints remains a challenge. Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints.\n\nTo address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.\n\ndspy.Assert and dspy.Suggest API​\n\nWe introduce two primary constructs within DSPy Assertions:\n\ndspy.Assert:\n\nParameters:\nconstraint (bool): Outcome of Python-defined boolean validation check.\nmsg (Optional[str]): User-defined error message providing feedback or correction guidance.\nbacktrack (Optional[module]): Specifies target module for retry attempts upon constraint failure. The default backtracking module is the last module before the assertion.\nBehavior: Initiates retry upon failure, dynamically adjusting the pipeline's execution. If failures persist, it halts execution and raises a dspy.AssertionError.\n\ndspy.Suggest:\n\nParameters: Similar to dspy.Assert.\nBehavior: Encourages self-refinement through retries without enforcing hard stops. Logs failures after maximum backtracking attempts and continues execution.\n\ndspy.Assert vs. Python Assertions: Unlike conventional Python assert statements that terminate the program upon failure, dspy.Assert conducts a sophisticated retry mechanism, allowing the pipeline to adjust.\n\nSpecifically, when a constraint is not met:\n\nBacktracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through\nDynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:\nPast Output: your model's past output that did not pass the validation_fn\nInstruction: your user-defined feedback message on what went wrong and what possibly to fix\n\nIf the error continues past the max_backtracking_attempts, then dspy.Assert will halt the pipeline execution, alerting you with an dspy.AssertionError. This ensures your program doesn't continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment.\n\ndspy.Suggest vs. dspy.Assert: dspy.Suggest on the other hand offers a softer approach. It maintains the same retry backtracking as dspy.Assert but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the max_backtracking_attempts, dspy.Suggest will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a \"best-effort\" manner without halting execution.\n\ndspy.Suggest are best utilized as \"helpers\" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.\n\ndspy.Assert are recommended during the development stage as \"checkers\" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.\n\nUse Case: Including Assertions in DSPy Programs​\n\nWe start with using an example of a multi-hop QA SimplifiedBaleen pipeline as defined in the intro walkthrough.\n\nclass SimplifiedBaleen(dspy.Module):\n    def __init__(self, passages_per_hop=2, max_hops=2):\n        super().__init__()\n\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n        self.max_hops = max_hops\n\n    def forward(self, question):\n        context = []\n        prev_queries = [question]\n\n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n            prev_queries.append(query)\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n        \n        pred = self.generate_answer(context=context, question=question)\n        pred = dspy.Prediction(context=context, answer=pred.answer)\n        return pred\n\nbaleen = SimplifiedBaleen()\n\nbaleen(question = \"Which award did Gary Zukav's first book receive?\")\n\n\nTo include DSPy Assertions, we simply define our validation functions and declare our assertions following the respective model generation.\n\nFor this use case, suppose we want to impose the following constraints:\n\nLength - each query should be less than 100 characters\nUniqueness - each generated query should differ from previously-generated queries.\n\nWe can define these validation checks as boolean functions:\n\n#simplistic boolean check for query length\nlen(query) <= 100\n\n#Python function for validating distinct queries\ndef validate_query_distinction_local(previous_queries, query):\n    \"\"\"check if query is distinct from previous queries\"\"\"\n    if previous_queries == []:\n        return True\n    if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.8):\n        return False\n    return True\n\n\nWe can declare these validation checks through dspy.Suggest statements (as we want to test the program in a best-effort demonstration). We want to keep these after the query generation query = self.generate_query[hop](context=context, question=question).query.\n\ndspy.Suggest(\n    len(query) <= 100,\n    \"Query should be short and less than 100 characters\",\n)\n\ndspy.Suggest(\n    validate_query_distinction_local(prev_queries, query),\n    \"Query should be distinct from: \"\n    + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\n)\n\n\nIt is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!\n\nLet's take a look at how the SimplifiedBaleen program will look with Assertions included:\n\nclass SimplifiedBaleenAssertions(dspy.Module):\n    def __init__(self, passages_per_hop=2, max_hops=2):\n        super().__init__()\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n        self.max_hops = max_hops\n\n    def forward(self, question):\n        context = []\n        prev_queries = [question]\n\n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n\n            dspy.Suggest(\n                len(query) <= 100,\n                \"Query should be short and less than 100 characters\",\n            )\n\n            dspy.Suggest(\n                validate_query_distinction_local(prev_queries, query),\n                \"Query should be distinct from: \"\n                + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\n            )\n\n            prev_queries.append(query)\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n        \n        if all_queries_distinct(prev_queries):\n            self.passed_suggestions += 1\n\n        pred = self.generate_answer(context=context, question=question)\n        pred = dspy.Prediction(context=context, answer=pred.answer)\n        return pred\n\n\nNow calling programs with DSPy Assertions requires one last step, and that is transforming the program to wrap it with internal assertions backtracking and Retry logic.\n\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\n\nbaleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)\n\n# backtrack_handler is parameterized over a few settings for the backtracking mechanism\n# To change the number of max retry attempts, you can do\nbaleen_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenAssertions(), \n    functools.partial(backtrack_handler, max_backtracks=1))\n\n\nAlternatively, you can also directly call activate_assertions on the program with dspy.Assert/Suggest statements using the default backtracking mechanism (max_backtracks=2):\n\nbaleen_with_assertions = SimplifiedBaleenAssertions().activate_assertions()\n\n\nNow let's take a look at the internal LM backtracking by inspecting the history of the LM query generations. Here we see that when a query fails to pass the validation check of being less than 100 characters, its internal GenerateSearchQuery signature is dynamically modified during the backtracking+Retry process to include the past query and the corresponding user-defined instruction: \"Query should be short and less than 100 characters\".\n\nWrite a simple search query that will help answer a complex question.\n\n---\n\nFollow the following format.\n\nContext: may contain relevant facts\n\nQuestion: ${question}\n\nReasoning: Let's think step by step in order to ${produce the query}. We ...\n\nQuery: ${query}\n\n---\n\nContext:\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is [...]»\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was [...]»\n\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\n\nReasoning: Let's think step by step in order to find the answer to this question. First, we need to identify the actress who played Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" Then, we need to find out if this actress also acted in the short film \"The Shore.\"\n\nQuery: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\n\n\n\nWrite a simple search query that will help answer a complex question.\n\n---\n\nFollow the following format.\n\nContext: may contain relevant facts\n\nQuestion: ${question}\n\nPast Query: past output with errors\n\nInstructions: Some instructions you must satisfy\n\nQuery: ${query}\n\n---\n\nContext:\n[1] «Kerry Condon | Kerry Condon (born 4 January 1983) is an Irish television and film actress, best known for her role as Octavia of the Julii in the HBO/BBC series \"Rome,\" as Stacey Ehrmantraut in AMC's \"Better Call Saul\" and as the voice of F.R.I.D.A.Y. in various films in the Marvel Cinematic Universe. She is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\"»\n[2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was an Italian born American actress who had a brief Broadway stage career before leaving to become a wife and mother. Born in Naples she came to acting in 1894 playing a Mexican girl in a play at the Empire Theatre. Wilson Barrett engaged her for a role in his play \"The Sign of the Cross\" which he took on tour of the United States. Riccardo played the role of Ancaria and later played Berenice in the same play. Robert B. Mantell in 1898 who struck by her beauty also cast her in two Shakespeare plays, \"Romeo and Juliet\" and \"Othello\". Author Lewis Strang writing in 1899 said Riccardo was the most promising actress in America at the time. Towards the end of 1898 Mantell chose her for another Shakespeare part, Ophelia im Hamlet. Afterwards she was due to join Augustin Daly's Theatre Company but Daly died in 1899. In 1899 she gained her biggest fame by playing Iras in the first stage production of Ben-Hur.»\n\nQuestion: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of \"Hamlet.\" ?\n\nPast Query: \"actress who played Ophelia in Royal Shakespeare Company production of Hamlet\" + \"actress in short film The Shore\"\n\nInstructions: Query should be short and less than 100 characters\n\nQuery: \"actress Ophelia RSC Hamlet\" + \"actress The Shore\"\n\n\nAssertion-Driven Optimizations​\n\nDSPy Assertions work with optimizations that DSPy offers, particularly with BootstrapFewShotWithRandomSearch, including the following settings:\n\nCompilation with Assertions This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.\nCompilation + Inference with Assertions -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time.\nteleprompter = BootstrapFewShotWithRandomSearch(\n    metric=validate_context_and_answer_and_hops,\n    max_bootstrapped_demos=max_bootstrapped_demos,\n    num_candidate_programs=6,\n)\n\n#Compilation with Assertions\ncompiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)\n\n#Compilation + Inference with Assertions\ncompiled_baleen_with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)\n\n\nPrevious\nOptimizers (formerly Teleprompters)\nNext\nTyped Predictors\nIntroduction\ndspy.Assert and dspy.Suggest API\nUse Case: Including Assertions in DSPy Programs\nAssertion-Driven Optimizations\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Optimizers (formerly Teleprompters) | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/optimizers",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksOptimizers (formerly Teleprompters)\nOptimizers (formerly Teleprompters)\n\nA DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.\n\nThere are many built-in optimizers in DSPy, which apply vastly different strategies. A typical DSPy optimizer takes three things:\n\nYour DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program.\n\nYour metric. This is a function that evaluates the output of your program, and assigns it a score (higher is better).\n\nA few training inputs. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).\n\nIf you happen to have a lot of data, DSPy can leverage that. But you can start small and get strong results.\n\nNote: Formerly called DSPy Teleprompters. We are making an official name update, which will be reflected throughout the library and documentation.\n\nWhat does a DSPy Optimizer tune? How does it tune them?​\n\nTraditional deep neural networks (DNNs) can be optimized with gradient descent, given a loss function and some training data.\n\nDSPy programs consist of multiple calls to LMs, stacked together as [DSPy modules]. Each DSPy module has internal parameters of three kinds: (1) the LM weights, (2) the instructions, and (3) demonstrations of the input/output behavior.\n\nGiven a metric, DSPy can optimize all of these three with multi-stage optimization algorithms. These can combine gradient descent (for LM weights) and discrete LM-driven optimization, i.e. for crafting/updating instructions and for creating/validating demonstrations. DSPy Demonstrations are like few-shot examples, but they're far more powerful. They can be created from scratch, given your program, and their creation and selection can be optimized in many effective ways.\n\nIn many cases, we found that compiling leads to better prompts than human writing. Not because DSPy optimizers are more creative than humans, but simply because they can try more things, much more systematically, and tune the metrics directly.\n\nWhat DSPy Optimizers are currently available?​\n\nSubclasses of Teleprompter\n\nAll of these can be accessed via from dspy.teleprompt import *.\n\nAutomatic Few-Shot Learning​\n\nThese optimizers extend the signature by automatically generating and including optimized examples within the prompt sent to the model, implementing few-shot learning.\n\nLabeledFewShot: Simply constructs few-shot examples (demos) from provided labeled input and output data points. Requires k (number of examples for the prompt) and trainset to randomly select k examples from.\n\nBootstrapFewShot: Uses a teacher module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in trainset. Parameters include max_labeled_demos (the number of demonstrations randomly selected from the trainset) and max_bootstrapped_demos (the number of additional examples generated by the teacher). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a teacher program that is a different DSPy program that has compatible structure, for harder tasks.\n\nBootstrapFewShotWithRandomSearch: Applies BootstrapFewShot several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of BootstrapFewShot, with the addition of num_candidate_programs, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, LabeledFewShot optimized program, BootstrapFewShot compiled program with unshuffled examples and num_candidate_programs of BootstrapFewShot compiled programs with randomized example sets.\n\nBootstrapFewShotWithOptuna: Applies BootstrapFewShot with Optuna optimization across demonstration sets, running trials to maximize evaluation metrics and selecting the best demonstrations.\n\nKNNFewShot. Selects demonstrations through k-Nearest Neighbors algorithm to pick a diverse set of examples from different clusters. Vectorizes the examples, and then clusters them, using cluster centers with BootstrapFewShot for bootstrapping/selection process. This will be useful when there's a lot of data over random spaces: using KNN helps optimize the trainset for BootstrapFewShot. See this notebook for an example.\n\nAutomatic Instruction Optimization​\n\nThese optimizers produce optimal instructions for the prompt and, in the case of MIPRO also optimize the set of few-shot demonstrations.\n\nCOPRO: Generates and refines new instructions for each step, and optimizes them with coordinate ascent (hill-climbing using the metric function and the trainset). Parameters include depth which is the number of iterations of prompt improvement the optimizer runs over.\n\nMIPRO: Generates instructions and few-shot examples in each step. The instruction generation is data-aware and demonstration-aware. Uses Bayesian Optimization to effectively search over the space of generation instructions/demonstrations across your modules.\n\nAutomatic Finetuning​\n\nThis optimizer is used to fine-tune the underlying LLM(s).\n\nBootstrapFinetune: Distills a prompt-based DSPy program into weight updates (for smaller LMs). The output is a DSPy program that has the same steps, but where each step is conducted by a finetuned model instead of a prompted LM.\nProgram Transformations​\nEnsemble: Ensembles a set of DSPy programs and either uses the full set or randomly samples a subset into a single program.\nWhich optimizer should I use?​\n\nAs a rule of thumb, if you don't know where to start, use BootstrapFewShotWithRandomSearch.\n\nHere's the general guidance on getting started:\n\nIf you have very little data, e.g. 10 examples of your task, use BootstrapFewShot.\n\nIf you have slightly more data, e.g. 50 examples of your task, use BootstrapFewShotWithRandomSearch.\n\nIf you have more data than that, e.g. 300 examples or more, use MIPRO.\n\nIf you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with BootstrapFinetune.\n\nHow do I use an optimizer?​\n\nThey all share this general interface, with some differences in the keyword arguments (hyperparameters).\n\nLet's see this with the most common one, BootstrapFewShotWithRandomSearch.\n\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\n# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 8-shot examples of your program's steps.\n# The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.\nconfig = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=10, num_threads=4)\n\nteleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)\noptimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)\n\nSaving and loading optimizer output​\n\nAfter running a program through an optimizer, it's useful to also save it. At a later point, a program can be loaded from a file and used for inference. For this, the load and save methods can be used.\n\nSaving a program​\noptimized_program.save(YOUR_SAVE_PATH)\n\n\nThe resulting file is in plain-text JSON format. It contains all the parameters and steps in the source program. You can always read it and see what the optimizer generated.\n\nLoading a program​\n\nTo load a program from a file, you can instantiate an object from that class and then call the load method on it.\n\nloaded_program = YOUR_PROGRAM_CLASS()\nloaded_program.load(path=YOUR_SAVE_PATH)\n\nPrevious\nMetrics\nNext\nDSPy Assertions\nWhat does a DSPy Optimizer tune? How does it tune them?\nWhat DSPy Optimizers are currently available?\nWhich optimizer should I use?\nHow do I use an optimizer?\nSaving and loading optimizer output\nSaving a program\nLoading a program\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Metrics | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/metrics",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksMetrics\nMetrics\n\nDSPy is a machine learning framework, so you must think about your automatic metrics for evaluation (to track your progress) and optimization (so DSPy can make your programs more effective).\n\nWhat is a metric and how do I define a metric for my task?​\n\nA metric is just a function that will take examples from your data and take the output of your system, and return a score that quantifies how good the output is. What makes outputs from your system good or bad?\n\nFor simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks.\n\nHowever, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).\n\nGetting this right on the first try is unlikely, but you should start with something simple and iterate.\n\nSimple metrics​\n\nA DSPy metric is just a function in Python that takes example (e.g., from your training or dev set) and the output pred from your DSPy program, and outputs a float (or int or bool) score.\n\nYour metric should also accept an optional third argument called trace. You can ignore this for a moment, but it will enable some powerful tricks if you want to use your metric for optimization.\n\nHere's a simple example of a metric that's comparing example.answer and pred.answer. This particular metric will return a bool.\n\ndef validate_answer(example, pred, trace=None):\n    return example.answer.lower() == pred.answer.lower()\n\n\nSome people find these utilities (built-in) convenient:\n\ndspy.evaluate.metrics.answer_exact_match\ndspy.evaluate.metrics.answer_passage_match\n\nYour metrics could be more complex, e.g. check for multiple properties. The metric below will return a float if trace is None (i.e., if it's used for evaluation or optimization), and will return a bool otherwise (i.e., if it's used to bootstrap demonstrations).\n\ndef validate_context_and_answer(example, pred, trace=None):\n    # check the gold label and the predicted answer are the same\n    answer_match = example.answer.lower() == pred.answer.lower()\n\n    # check the predicted answer comes from one of the retrieved contexts\n    context_match = any((pred.answer.lower() in c) for c in pred.context)\n\n    if trace is None: # if we're doing evaluation or optimization\n        return (answer_match + context_match) / 2.0\n    else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\n        return answer_match and context_match\n\n\nDefining a good metric is an iterative process, so doing some initial evaluations and looking at your data and your outputs are key.\n\nEvaluation​\n\nOnce you have a metric, you can run evaluations in a simple Python loop.\n\nscores = []\nfor x in devset:\n    pred = program(**x.inputs())\n    score = metric(x, pred)\n    scores.append(score)\n\n\nIf you need some utilities, you can also use the built-in Evaluate utility. It can help with things like parallel evaluation (multiple threads) or showing you a sample of inputs/outputs and the metric scores.\n\nfrom dspy.evaluate import Evaluate\n\n# Set up the evaluator, which can be re-used in your code.\nevaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)\n\n# Launch evaluation.\nevaluator(YOUR_PROGRAM, metric=YOUR_METRIC)\n\nIntermediate: Using AI feedback for your metric​\n\nFor most applications, your system will output long-form outputs, so your metric should check multiple dimensions of the output using AI feedback from LMs.\n\nThis simple signature could come in handy.\n\n# Define the signature for automatic assessments.\nclass Assess(dspy.Signature):\n    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\n\n    assessed_text = dspy.InputField()\n    assessment_question = dspy.InputField()\n    assessment_answer = dspy.OutputField(desc=\"Yes or No\")\n\n\nFor example, below is a simple metric that uses GPT-4-turbo to check if a generated tweet (1) answers a given question correctly and (2) whether it's also engaging. We also check that (3) len(tweet) <= 280 characters.\n\ngpt4T = dspy.OpenAI(model='gpt-4-1106-preview', max_tokens=1000, model_type='chat')\n\ndef metric(gold, pred, trace=None):\n    question, answer, tweet = gold.question, gold.answer, pred.output\n\n    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n    \n    with dspy.context(lm=gpt4T):\n        correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n        engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n\n    correct, engaging = [m.assessment_answer.lower() == 'yes' for m in [correct, engaging]]\n    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n\n    if trace is not None: return score >= 2\n    return score / 2.0\n\n\nWhen compiling, trace is not None, and we want to be strict about judging things, so we will only return True if score >= 2. Otherwise, we return a score out of 1.0 (i.e., score / 2.0).\n\nAdvanced: Using a DSPy program as your metric​\n\nIf your metric is itself a DSPy program, one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to define and optimize by collecting a few examples.\n\nAdvanced: Accessing the trace​\n\nWhen your metric is used during evaluation runs, DSPy will not try to track the steps of your program.\n\nBut during compiling (optimization), DSPy will trace your LM calls. The trace will contain inputs/outputs to each DSPy predictor and you can leverage that to validate intermediate steps for optimization.\n\ndef validate_hops(example, pred, trace=None):\n    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\n\n    if max([len(h) for h in hops]) > 100: return False\n    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\n\n    return True\n\nPrevious\nData\nNext\nOptimizers (formerly Teleprompters)\nWhat is a metric and how do I define a metric for my task?\nSimple metrics\nEvaluation\nIntermediate: Using AI feedback for your metric\nAdvanced: Using a DSPy program as your metric\nAdvanced: Accessing the trace\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Data | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/data",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksData\nData\n\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n\nFor each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n\nHow much data do I need and how do I collect data for my task?​\n\nConcretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n\nHow can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.\n\nHowever, chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.\n\nIf there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can also start using/deploying/demoing your system and collect some initial data that way.\n\nDSPy Example objects​\n\nThe core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set.\n\nDSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example.\n\nWhen you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example:\n\nqa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\n\nprint(qa_pair)\nprint(qa_pair.question)\nprint(qa_pair.answer)\n\n\nOutput:\n\nExample({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)\nThis is a question?\nThis is an answer.\n\n\nExamples can have any field keys and any value types, though usually values are strings.\n\nobject = Example(field1=value1, field2=value2, field3=value3, ...)\n\n\nYou can now express your training set for example as:\n\ntrainset = [dspy.Example(report=\"LONG REPORT 1\", summary=\"short summary 1\"), ...]\n\nSpecifying Input Keys​\n\nIn traditional ML, there are separated \"inputs\" and \"labels\".\n\nIn DSPy, the Example objects have a with_inputs() method, which can mark specific fields as inputs. (The rest are just metadata or labels.)\n\n# Single Input.\nprint(qa_pair.with_inputs(\"question\"))\n\n# Multiple Inputs; be careful about marking your labels as inputs unless you mean it.\nprint(qa_pair.with_inputs(\"question\", \"answer\"))\n\n\nValues can be accessed using the .(dot) operator. You can access the value of key name in defined object Example(name=\"John Doe\", job=\"sleep\") through object.name.\n\nTo access or exclude certain keys, use inputs() and labels() methods to return new Example objects containing only input or non-input keys, respectively.\n\narticle_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\n\ninput_key_only = article_summary.inputs()\nnon_input_key_only = article_summary.labels()\n\nprint(\"Example object with Input fields only:\", input_key_only)\nprint(\"Example object with Non-Input fields only:\", non_input_key_only)\n\n\nOutput\n\nExample object with Input fields only: Example({'article': 'This is an article.'}) (input_keys=None)\nExample object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\n\nPrevious\nModules\nNext\nMetrics\nHow much data do I need and how do I collect data for my task?\nDSPy Example objects\nSpecifying Input Keys\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Modules | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/modules",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksModules\nModules\n\nA DSPy module is a building block for programs that use LMs.\n\nEach built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature.\n\nA DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n\nMultiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n\nHow do I use a built-in module, like dspy.Predict or dspy.ChainOfThought?​\n\nLet's start with the most fundamental module, dspy.Predict. Internally, all other DSPy modules are just built using dspy.Predict.\n\nWe'll assume you are already at least a little familiar with DSPy signatures, which are declarative specs for defining the behavior of any module we use in DSPy.\n\nTo use a module, we first declare it by giving it a signature. Then we call the module with the input arguments, and extract the output fields!\n\nsentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n\n# 1) Declare with a signature.\nclassify = dspy.Predict('sentence -> sentiment')\n\n# 2) Call with input argument(s). \nresponse = classify(sentence=sentence)\n\n# 3) Access the output.\nprint(response.sentiment)\n\n\nOutput:\n\nPositive\n\n\nWhen we declare a module, we can pass configuration keys to it.\n\nBelow, we'll pass n=5 to request five completions. We can also pass temperature or max_len, etc.\n\nLet's use dspy.ChainOfThought. In many cases, simply swapping dspy.ChainOfThought in place of dspy.Predict improves quality.\n\nquestion = \"What's something great about the ColBERT retrieval model?\"\n\n# 1) Declare with a signature, and pass some config.\nclassify = dspy.ChainOfThought('question -> answer', n=5)\n\n# 2) Call with input argument.\nresponse = classify(question=question)\n\n# 3) Access the outputs.\nresponse.completions.answer\n\n\nOutput:\n\n['One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.',\n 'Its ability to efficiently retrieve relevant information from large document collections.',\n 'One great thing about the ColBERT retrieval model is its superior performance compared to other models and its efficient use of pre-trained language models.',\n 'One great thing about the ColBERT retrieval model is its superior efficiency and accuracy compared to other models.',\n 'One great thing about the ColBERT retrieval model is its ability to incorporate user feedback and support complex queries.']\n\n\nLet's discuss the output object here.\n\nThe dspy.ChainOfThought module will generally inject a rationale before the output field(s) of your signature.\n\nLet's inspect the (first) rationale and answer!\n\nprint(f\"Rationale: {response.rationale}\")\nprint(f\"Answer: {response.answer}\")\n\n\nOutput:\n\nRationale: produce the answer. We can consider the fact that ColBERT has shown to outperform other state-of-the-art retrieval models in terms of efficiency and effectiveness. It uses contextualized embeddings and performs document retrieval in a way that is both accurate and scalable.\nAnswer: One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.\n\n\nThis is accessible whether we request one or many completions.\n\nWe can also access the different completions as a list of Predictions or as several lists, one for each field.\n\nresponse.completions[3].rationale == response.completions.rationale[3]\n\n\nOutput:\n\nTrue\n\nWhat other DSPy modules are there? How can I use them?​\n\nThe others are very similar. They mainly change the internal behavior with which your signature is implemented!\n\ndspy.Predict: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n\ndspy.ChainOfThought: Teaches the LM to think step-by-step before committing to the signature's response.\n\ndspy.ProgramOfThought: Teaches the LM to output code, whose execution results will dictate the response.\n\ndspy.ReAct: An agent that can use tools to implement the given signature.\n\ndspy.MultiChainComparison: Can compare multiple outputs from ChainOfThought to produce a final prediction.\n\nWe also have some function-style modules:\n\ndspy.majority: Can do basic voting to return the most popular response from a set of predictions.\n\nCheck out further examples in each module's respective guide.\n\nHow do I compose multiple modules into a bigger program?​\n\nDSPy is just Python code that uses modules in any control flow you like. (There's some magic internally at compile time to trace your LM calls.)\n\nWhat this means is that, you can just call the modules freely. No weird abstractions for chaining calls.\n\nThis is basically PyTorch's design approach for define-by-run / dynamic computation graphs. Refer to the intro tutorials for examples.\n\nPrevious\nSignatures\nNext\nData\nHow do I use a built-in module, like dspy.Predict or dspy.ChainOfThought?\nWhat other DSPy modules are there? How can I use them?\nHow do I compose multiple modules into a bigger program?\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Signatures | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/signatures",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksSignatures\nSignatures\n\nWhen we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.\n\nA signature is a declarative specification of input/output behavior of a DSPy module. Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it.\n\nYou're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but the differences are that:\n\nWhile typical function signatures just describe things, DSPy Signatures define and control the behavior of modules.\n\nThe field names matter in DSPy Signatures. You express semantic roles in plain English: a question is different from an answer, a sql_query is different from python_code.\n\nWhy should I use a DSPy Signature?​\n\ntl;dr For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes).\n\nLong Answer: Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning.\n\nWriting signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly.\n\nInline DSPy Signatures​\n\nSignatures can be defined as a short string, with argument names that define semantic roles for inputs/outputs.\n\nQuestion Answering: \"question -> answer\"\n\nSentiment Classification: \"sentence -> sentiment\"\n\nSummarization: \"document -> summary\"\n\nYour signatures can also have multiple input/output fields.\n\nRetrieval-Augmented Question Answering: \"context, question -> answer\"\n\nMultiple-Choice Question Answering with Reasoning: \"question, choices -> reasoning, selection\"\n\nTip: For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don't prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it's probably fine to say \"document -> summary\", \"text -> gist\", or \"long_context -> tldr\".\n\nExample A: Sentiment Classification​\nsentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n\nclassify = dspy.Predict('sentence -> sentiment')\nclassify(sentence=sentence).sentiment\n\n\nOutput:\n\n'Positive'\n\nExample B: Summarization​\n# Example from the XSum dataset.\ndocument = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n\nsummarize = dspy.ChainOfThought('document -> summary')\nresponse = summarize(document=document)\n\nprint(response.summary)\n\n\nOutput:\n\nThe 21-year-old Lee made seven appearances and scored one goal for West Ham last season. He had loan spells in League One with Blackpool and Colchester United, scoring twice for the latter. He has now signed a contract with Barnsley, but the length of the contract has not been revealed.\n\n\nMany DSPy modules (except dspy.Predict) return auxiliary information by expanding your signature under the hood.\n\nFor example, dspy.ChainOfThought also adds a rationale field that includes the LM's reasoning before it generates the output summary.\n\nprint(\"Rationale:\", response.rationale)\n\n\nOutput:\n\nRationale: produce the summary. We need to highlight the key points about Lee's performance for West Ham, his loan spells in League One, and his new contract with Barnsley. We also need to mention that his contract length has not been disclosed.\n\nClass-based DSPy Signatures​\n\nFor some advanced tasks, you need more verbose signatures. This is typically to:\n\nClarify something about the nature of the task (expressed below as a docstring).\n\nSupply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\n\nSupply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\n\nExample C: Classification​\n\nNotice how the docstring contains (minimal) instructions, which in this case are necessary to have a fully-defined task.\n\nSome optimizers in DSPy, like COPRO, can take this simple docstring and then generate more effective variants if needed.\n\nclass Emotion(dspy.Signature):\n    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\n    \n    sentence = dspy.InputField()\n    sentiment = dspy.OutputField()\n\nsentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n\nclassify = dspy.Predict(Emotion)\nclassify(sentence=sentence)\n\n\nOutput:\n\nPrediction(\n    sentiment='Fear'\n)\n\n\nTip: There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of the your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).\n\nExample D: A metric that evaluates faithfulness to citations​\nclass CheckCitationFaithfulness(dspy.Signature):\n    \"\"\"Verify that the text is based on the provided context.\"\"\"\n\n    context = dspy.InputField(desc=\"facts here are assumed to be true\")\n    text = dspy.InputField()\n    faithfulness = dspy.OutputField(desc=\"True/False indicating if text is faithful to context\")\n\ncontext = \"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\n\ntext = \"Lee scored 3 goals for Colchester United.\"\n\nfaithfulness = dspy.ChainOfThought(CheckCitationFaithfulness)\nfaithfulness(context=context, text=text)\n\n\nOutput:\n\nPrediction(\n    rationale=\"produce the faithfulness. We know that Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. However, there is no mention of him scoring three goals for Colchester United.\",\n    faithfulness='False'\n)\n\nUsing signatures to build modules & compiling them​\n\nWhile signatures are convenient for prototyping with structured inputs/outputs, that's not the main reason to use them!\n\nYou should compose multiple signatures into bigger DSPy modules and compile these modules into optimized prompts and finetunes.\n\nPrevious\nLanguage Models\nNext\nModules\nWhy should I use a DSPy Signature?\nInline DSPy Signatures\nExample A: Sentiment Classification\nExample B: Summarization\nClass-based DSPy Signatures\nExample C: Classification\nExample D: A metric that evaluates faithfulness to citations\nUsing signatures to build modules & compiling them\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Language Models | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/language_models",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksLanguage Models\nLanguage Models\n\nThe most powerful features in DSPy revolve around algorithmically optimizing the prompts (or weights) of LMs, especially when you're building programs that use the LMs within a pipeline.\n\nLet's first make sure you can set up your language model. DSPy support clients for many remote and local LMs.\n\nSetting up the LM client.​\n\nYou can just call the constructor that connects to the LM. Then, use dspy.configure to declare this as the default LM.\n\nFor example, to use OpenAI language models, you can do it as follows.\n\ngpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', max_tokens=300)\ndspy.configure(lm=gpt3_turbo)\n\nDirectly calling the LM.​\n\nYou can simply call the LM with a string to give it a raw prompt, i.e. a string.\n\ngpt3_turbo(\"hello! this is a raw prompt to GPT-3.5\")\n\n\nOutput:\n\n['Hello! How can I assist you today?']\n\n\nThis is almost never the recommended way to interact with LMs in DSPy, but it is allowed.\n\nUsing the LM with DSPy signatures.​\n\nYou can also use the LM via DSPy signature (input/output spec) and modules, which we discuss in more depth in the remaining guides.\n\n# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\nqa = dspy.ChainOfThought('question -> answer')\n\n# Run with the default LM configured with `dspy.configure` above.\nresponse = qa(question=\"How many floors are in the castle David Gregory inherited?\")\nprint(response.answer)\n\n\nOutput:\n\nThe castle David Gregory inherited has 7 floors.\n\nUsing multiple LMs at once.​\n\nThe default LM above is GPT-3.5, gpt3_turbo. What if I want to run a piece of code with, say, GPT-4 or LLama-2?\n\nInstead of changing the default LM, you can just change it inside a block of code.\n\nTip: Using dspy.configure and dspy.context is thread-safe!\n\n# Run with the default LM configured above, i.e. GPT-3.5\nresponse = qa(question=\"How many floors are in the castle David Gregory inherited?\")\nprint('GPT-3.5:', response.answer)\n\ngpt4_turbo = dspy.OpenAI(model='gpt-4-1106-preview', max_tokens=300)\n\n# Run with GPT-4 instead\nwith dspy.context(lm=gpt4_turbo):\n    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n    print('GPT-4-turbo:', response.answer)\n\n\nOutput:\n\nGPT-3.5: The castle David Gregory inherited has 7 floors.\nGPT-4-turbo: The number of floors in the castle David Gregory inherited cannot be determined with the information provided.\n\nTips and Tricks.​\n\nIn DSPy, all LM calls are cached. If you repeat the same call, you will get the same outputs. (If you change the inputs or configurations, you will get new outputs.)\n\nTo generate 5 outputs, you can use n=5 in the module constructor, or pass config=dict(n=5) when invoking the module.\n\nqa = dspy.ChainOfThought('question -> answer', n=5)\n\nresponse = qa(question=\"How many floors are in the castle David Gregory inherited?\")\nresponse.completions.answer\n\n\nOutput:\n\n[\"The specific number of floors in David Gregory's inherited castle is not provided here, so further research would be needed to determine the answer.\",\n    'The castle David Gregory inherited has 4 floors.',\n    'The castle David Gregory inherited has 5 floors.',\n    'David Gregory inherited 10 floors in the castle.',\n    'The castle David Gregory inherited has 5 floors.']\n\n\nIf you just call qa(...) in a loop with the same input, it will always return the same value! That's by design.\n\nTo loop and generate one output at a time with the same input, bypass the cache by making sure each request is (slightly) unique, as below.\n\nfor idx in range(5):\n    response = qa(question=\"How many floors are in the castle David Gregory inherited?\", config=dict(temperature=0.7+0.0001*idx))\n    print(f'{idx+1}.', response.answer)\n\n\nOutput:\n\n1. The specific number of floors in David Gregory's inherited castle is not provided here, so further research would be needed to determine the answer.\n2. It is not possible to determine the exact number of floors in the castle David Gregory inherited without specific information about the castle's layout and history.\n3. The castle David Gregory inherited has 5 floors.\n4. We need more information to determine the number of floors in the castle David Gregory inherited.\n5. The castle David Gregory inherited has a total of 6 floors.\n\nRemote LMs.​\n\nThese models are managed services. You just need to sign up and obtain an API key. Calling any of the remote LMs below assumes authentication and mirrors the following format for setting up the LM:\n\nlm = dspy.{provider_listed_below}(model=\"your model\", model_request_kwargs=\"...\")\n\n\ndspy.OpenAI for GPT-3.5 and GPT-4.\n\ndspy.Cohere\n\ndspy.Anyscale for hosted Llama2 models.\n\ndspy.Together for hosted various open source models.\n\ndspy.PremAI for hosted best open source and closed source models.\n\nLocal LMs.​\n\nYou need to host these models on your own GPU(s). Below, we include pointers for how to do that.\n\ndspy.HFClientTGI: for HuggingFace models through the Text Generation Inference (TGI) system. Tutorial: How do I install and launch the TGI server?\ntgi_mistral = dspy.HFClientTGI(model=\"mistralai/Mistral-7B-Instruct-v0.2\", port=8080, url=\"http://localhost\")\n\ndspy.HFClientVLLM: for HuggingFace models through vLLM. Tutorial: How do I install and launch the vLLM server?\nvllm_mistral = dspy.HFClientVLLM(model=\"mistralai/Mistral-7B-Instruct-v0.2\", port=8080, url=\"http://localhost\")\n\ndspy.HFModel (experimental) Tutorial: How do I initialize models using HFModel\nmistral = dspy.HFModel(model = 'mistralai/Mistral-7B-Instruct-v0.2')\n\ndspy.Ollama (experimental) for open source models through Ollama. Tutorial: How do I install and use Ollama on a local computer?\\n\",\nollama_mistral = dspy.OllamaLocal(model='mistral')\n\ndspy.ChatModuleClient (experimental): How do I install and use MLC?\nmodel = 'dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1'\nmodel_path = 'dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so'\n\nllama = dspy.ChatModuleClient(model=model, model_path=model_path)\n\nPrevious\nUsing DSPy in 8 Steps\nNext\nSignatures\nSetting up the LM client.\nDirectly calling the LM.\nUsing the LM with DSPy signatures.\nUsing multiple LMs at once.\nTips and Tricks.\nRemote LMs.\nLocal LMs.\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Using DSPy in 8 Steps | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksUsing DSPy in 8 Steps\nUsing DSPy in 8 Steps\n\nUsing DSPy well for solving a new task is just doing good machine learning with LMs.\n\nWhat this means is that it's an iterative process. You make some initial choices, which will be sub-optimal, and then you refine them incrementally.\n\nAs we discuss below, you will define your task and the metrics you want to maximize, and prepare a few example inputs — typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers (modules) to use, giving each layer a signature (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy optimizer to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.\n\n1) Define your task.​\n\nYou cannot use DSPy well if you haven't defined the problem you're trying to solve.\n\nExpected Input/Output Behavior: Are you trying to build a chatbot over your data? A code assistant? A system for extracting information from papers? Or perhaps a translation system? Or a system for highlighting snippets from search results? Or a system to summarize information on a topic, with citations?\n\nIt's often useful to come up with just 3-4 examples of the inputs and outputs of your program (e.g., questions and their answers, or topics and their summaries).\n\nIf you need help thinking about your task, we recently created a Discord server for the community.\n\nQuality and Cost Specs: You probably don't have infinite budget. Your final system can't be too expensive to run, and it should probably respond to users quickly enough.\n\nTake this as an opportunity to guess what kind of language model you'd like to use. Maybe GPT-3.5? Or a small open model, like Mistral-7B or Llama2-13B-chat? Or Mixtral? Or maybe you really need GPT-4-turbo? Or perhaps your resources are very constrained, and you want your final LM to be T5-base.\n\n2) Define your pipeline.​\n\nWhat should your DSPy program do? Can it just be a simple chain-of-thought step? Or do you need the LM to use retrieval? Or maybe other tools, like a calculator or a calendar API?\n\nIs there a typical workflow for solving your problem in multiple well-defined steps? Or do you want a fully open-ended LM (or open-ended tool use with agents) for your task?\n\nThink about this space but always start simple. Almost every task should probably start with just a single dspy.ChainofThought module, and then add complexity incrementally as you go.\n\nThen write your (initial) DSPy program. Again: start simple, and let the next few steps guide any complexity you will add.\n\n3) Explore a few examples.​\n\nBy this point, you probably have a few examples of the task you're trying to solve.\n\nRun them through your pipeline. Consider using a large and powerful LM at this point, or a couple of different LMs, just to understand what's possible. (DSPy will make swapping these LMs pretty easy - LM Guide.)\n\nAt this point, you're still using your pipeline zero-shot, so it will be far from perfect. DSPy will help you optimize the instructions, few-shot examples, and even weights of your LM calls below, but understanding where things go wrong in zero-shot usage will go a long way.\n\nRecord the interesting (both easy and hard) examples you try: even if you don't have labels, simply tracking the inputs you tried will be useful for DSPy optimizers below.\n\n4) Define your data.​\n\nNow it's time to more formally declare your training and validation data for DSPy evaluation and optimization - Data Guide.\n\nYou can use DSPy optimizers usefully with as few as 10 examples, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n\nHow can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.\n\nHowever, chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.\n\nIf there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can also start using/deploying/demoing your system and collect some initial data that way.\n\n5) Define your metric.​\n\nWhat makes outputs from your system good or bad? Invest in defining metrics and improving them over time incrementally. It's really hard to consistently improve what you aren't able to define.\n\nA metric is just a function that will take examples from your data and take the output of your system, and return a score that quantifies how good the output is - Metric Guide.\n\nFor simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks.\n\nHowever, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).\n\nGetting this right on the first try is unlikely, but you should start with something simple and iterate. (If your metric is itself a DSPy program, notice that one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to define and optimize by collecting a few examples.)\n\n6) Collect preliminary \"zero-shot\" evaluations.​\n\nNow that you have some data and a metric, run evaluation on your pipeline before any optimizer runs.\n\nLook at the outputs and the metric scores. This will probably allow you to spot any major issues, and it will define a baseline for your next step.\n\n7) Compile with a DSPy optimizer.​\n\nGiven some data and a metric, we can now optimize the program you built - Optimizer Guide.\n\nDSPy includes many optimizers that do different things. Remember: DSPy optimizers will create examples of each step, craft instructions, and/or update LM weights. In general, you don't need to have labels for your pipeline steps, but your data examples need to have input values and whatever labels your metric requires (e.g., no labels if your metric is reference-free, but final output labels otherwise in most cases).\n\nHere's the general guidance on getting started:\n\nIf you have very little data, e.g. 10 examples of your task, use BootstrapFewShot\n\nIf you have slightly more data, e.g. 50 examples of your task, use BootstrapFewShotWithRandomSearch.\n\nIf you have more data than that, e.g. 300 examples or more, use MIPRO.\n\nIf you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with BootstrapFinetune.\n\n8) Iterate.​\n\nAt this point, you are either very happy with everything (we've seen quite a few people get it right on first try with DSPy) or, more likely, you've made a lot of progress but you don't like something about the final program or the metric.\n\nAt this point, go back to step 1 and revisit the major questions. Did you define your task well? Do you need to collect (or find online) more data for your problem? Do you want to update your metric? And do you want to use a more sophisticated optimizer? Do you need to consider advanced features like DSPy Assertions? Or, perhaps most importantly, do you want to add some more complexity or steps in your DSPy program itself? Do you want to use multiple optimizers in a sequence?\n\nIterative development is key. DSPy gives you the pieces to do that incrementally: iterating on your data, your program structure, your assertions, your metric, and your optimization steps.\n\nOptimizing complex LM programs is an entirely new paradigm that only exists in DSPy at the time of writing, so naturally the norms around what to do are still emerging. If you need help, we recently created a Discord server for the community.\n\nPrevious\nDSPy Building Blocks\nNext\nLanguage Models\n1) Define your task.\n2) Define your pipeline.\n3) Explore a few examples.\n4) Define your data.\n5) Define your metric.\n6) Collect preliminary \"zero-shot\" evaluations.\n7) Compile with a DSPy optimizer.\n8) Iterate.\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Minimal Working Example | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/quick-start/minimal-example",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nInstallation\nMinimal Working Example\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nQuick StartMinimal Working Example\nMinimal Working Example\n\nIn this post, we walk you through a minimal working example using the DSPy library.\n\nWe make use of the GSM8K dataset and the OpenAI GPT-3.5-turbo model to simulate prompting tasks within DSPy.\n\nSetup​\n\nBefore we jump into the example, let's ensure our environment is properly configured. We'll start by importing the necessary modules and configuring our language model:\n\nimport dspy\nfrom dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n\n# Set up the LM\nturbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=250)\ndspy.settings.configure(lm=turbo)\n\n# Load math questions from the GSM8K dataset\ngsm8k = GSM8K()\ngsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]\n\n\nLet's take a look at what gsm8k_trainset and gsm8k_devset are:\n\nprint(gsm8k_trainset)\n\n\nThe gsm8k_trainset and gsm8k_devset datasets contain a list of Examples with each example having question and answer field.\n\nDefine the Module​\n\nWith our environment set up, let's define a custom program that utilizes the ChainOfThought module to perform step-by-step reasoning to generate answers:\n\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -> answer\")\n    \n    def forward(self, question):\n        return self.prog(question=question)\n\nCompile and Evaluate the Model​\n\nWith our simple program in place, let's move on to compiling it with the BootstrapFewShot teleprompter:\n\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\nconfig = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n\n# Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\nteleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\noptimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset)\n\n\nNote that BootstrapFewShot is not an optimizing teleprompter, i.e. it simple creates and validates examples for steps of the pipeline (in this case, the chain-of-thought reasoning) but does not optimize the metric. Other teleprompters like BootstrapFewShotWithRandomSearch and MIPRO will apply direct optimization.\n\nEvaluate​\n\nNow that we have a compiled (optimized) DSPy program, let's move to evaluating its performance on the dev dataset.\n\nfrom dspy.evaluate import Evaluate\n\n# Set up the evaluator, which can be used multiple times.\nevaluate = Evaluate(devset=gsm8k_devset, metric=gsm8k_metric, num_threads=4, display_progress=True, display_table=0)\n\n# Evaluate our `optimized_cot` program.\nevaluate(optimized_cot)\n\nInspect the Model's History​\n\nFor a deeper understanding of the model's interactions, we can review the most recent generations through inspecting the model's history:\n\nturbo.inspect_history(n=1)\n\n\nAnd there you have it! You've successfully created a working example using the DSPy library.\n\nThis example showcases how to set up your environment, define a custom module, compile a model, and rigorously evaluate its performance using the provided dataset and teleprompter configurations.\n\nFeel free to adapt and expand upon this example to suit your specific use case while exploring the extensive capabilities of DSPy.\n\nIf you want to try what you just built, run optimized_cot(question='Your Question Here').\n\nWritten By: Herumb Shandilya\n\nPrevious\nInstallation\nNext\nDSPy Building Blocks\nSetup\nDefine the Module\nCompile and Evaluate the Model\nEvaluate\nInspect the Model's History\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Installation | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/quick-start/installation",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nInstallation\nMinimal Working Example\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nQuick StartInstallation\nInstallation\n\nTo install DSPy run:\n\npip install dspy-ai\n\n\nOr open our intro notebook in Google Colab: \n\nBy default, DSPy depends on openai==0.28. However, if you install openai>=1.0, the library will use that just fine. Both are supported.\n\nFor the optional Pinecone, Qdrant, ChromaDB, Marqo, or Milvus retrieval integration(s), include the extra(s) below:\n\nINSTALLATION COMMAND\nNo Extras\nPinecone\nQdrant\nChromaDB\nMarqo\nMongoDB\nWeaviate\nMilvus\npip install dspy-ai\n\nPrevious\nQuick Start\nNext\nMinimal Working Example\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Typed Predictors | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/building-blocks/typed_predictors",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building BlocksTyped Predictors\nTyped Predictors\n\nIn DSPy Signatures, we have InputField and OutputField that define the nature of inputs and outputs of the field. However, the inputs and output to these fields are always str-typed, which requires input and output string processing.\n\nPydantic BaseModel is a great way to enforce type constraints on the fields, but it is not directly compatible with the dspy.Signature. Typed Predictors resolves this as a way to enforce the type constraints on the inputs and outputs of the fields in a dspy.Signature.\n\nExecuting Typed Predictors​\n\nUsing Typed Predictors is not too different than any other module with the minor additions of type hints to signature attributes and using a special Predictor module instead of dspy.Predict. Let's take a look at a simple example to understand this.\n\nDefining Input and Output Models​\n\nLet's take a simple task as an example i.e. given the context and query, the LLM should return an answer and confidence_score. Let's define our Input and Output models via pydantic.\n\nfrom pydantic import BaseModel, Field\n\nclass Input(BaseModel):\n    context: str = Field(description=\"The context for the question\")\n    query: str = Field(description=\"The question to be answered\")\n\nclass Output(BaseModel):\n    answer: str = Field(description=\"The answer for the question\")\n    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")\n\n\nAs you can see, we can describe the attributes by defining a simple Signature that takes in the input and returns the output.\n\nCreating Typed Predictor​\n\nA Typed Predictor needs a Typed Signature, which extends a dspy.Signature with the addition of specifying \"field type\".\n\nclass QASignature(dspy.Signature):\n    \"\"\"Answer the question based on the context and query provided, and on the scale of 10 tell how confident you are about the answer.\"\"\"\n\n    input: Input = dspy.InputField()\n    output: Output = dspy.OutputField()\n\n\nNow that we have the QASignature, let's define a Typed Predictor that executes this Signature while conforming to the type constraints.\n\npredictor = dspy.TypedPredictor(QASignature)\n\n\nSimilar to other modules, we pass the QASignature to dspy.TypedPredictor which enforces the typed constraints.\n\nAnd similarly to dspy.Predict, we can also use a \"string signature\", which we type as:\n\npredictor = dspy.TypedPredictor(\"input:Input -> output:Output\")\n\nI/O in Typed Predictors​\n\nNow let's test out the Typed Predictor by providing some sample input to the predictor and verifying the output type. We can create an Input instance and pass it to the predictor to get a dictionary of the output.\n\ndoc_query_pair = Input(\n    context=\"The quick brown fox jumps over the lazy dog\",\n    query=\"What does the fox jumps over?\",\n)\n\nprediction = predictor(input=doc_query_pair)\n\n\nLet's see the output and its type.\n\nanswer = prediction.output.answer\nconfidence_score = prediction.output.confidence\n\nprint(f\"Prediction: {prediction}\\n\\n\")\nprint(f\"Answer: {answer}, Answer Type: {type(answer)}\")\nprint(f\"Confidence Score: {confidence_score}, Confidence Score Type: {type(confidence_score)}\")\n\nTyped Chain of Thoughts with dspy.TypedChainOfThought​\n\nExtending the analogous comparison of TypedPredictor to dspy.Predict, we create TypedChainOfThought, the typed counterpart of dspy.ChainOfThought:\n\ncot_predictor = dspy.TypedChainOfThought(QASignature)\n\ndoc_query_pair = Input(\n    context=\"The quick brown fox jumps over the lazy dog\",\n    query=\"What does the fox jumps over?\",\n)\n\nprediction = cot_predictor(input=doc_query_pair)\n\nTyped Predictors as Decorators​\n\nWhile the dspy.TypedPredictor and dspy.TypedChainOfThought provide a convenient way to use typed predictors, you can also use them as decorators to enforce type constraints on the inputs and outputs of the function. This relies on the internal definitions of the Signature class and its function arguments, outputs, and docstrings.\n\n@dspy.predictor\ndef answer(doc_query_pair: Input) -> Output:\n    \"\"\"Answer the question based on the context and query provided, and on the scale of 0-1 tell how confident you are about the answer.\"\"\"\n    pass\n\n@dspy.cot\ndef answer(doc_query_pair: Input) -> Output:\n    \"\"\"Answer the question based on the context and query provided, and on the scale of 0-1 tell how confident you are about the answer.\"\"\"\n    pass\n\nprediction = answer(doc_query_pair=doc_query_pair)\n\nComposing Functional Typed Predictors in dspy.Module​\n\nIf you're creating DSPy pipelines via dspy.Module, then you can simply use Functional Typed Predictors by creating these class methods and using them as decorators. Here is an example of using functional typed predictors to create a SimplifiedBaleen pipeline:\n\nclass SimplifiedBaleen(FunctionalModule):\n    def __init__(self, passages_per_hop=3, max_hops=1):\n        super().__init__()\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.max_hops = max_hops\n\n    @cot\n    def generate_query(self, context: list[str], question) -> str:\n        \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n        pass\n\n    @cot\n    def generate_answer(self, context: list[str], question) -> str:\n        \"\"\"Answer questions with short factoid answers.\"\"\"\n        pass\n\n    def forward(self, question):\n        context = []\n\n        for _ in range(self.max_hops):\n            query = self.generate_query(context=context, question=question)\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n\n        answer = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=answer)\n\nOptimizing Typed Predictors​\n\nTyped predictors can be optimized on the Signature instructions through the optimize_signature optimizer. Here is an example of this optimization on the QASignature:\n\nimport dspy\nfrom dspy.evaluate import Evaluate\nfrom dspy.evaluate.metrics import answer_exact_match\nfrom dspy.teleprompt.signature_opt_typed import optimize_signature\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=4000)\ngpt4 = dspy.OpenAI(model='gpt-4', max_tokens=4000)\ndspy.settings.configure(lm=turbo)\n\nevaluator = Evaluate(devset=devset, metric=answer_exact_match, num_threads=10, display_progress=True)\n\nresult = optimize_signature(\n    student=dspy.TypedPredictor(QASignature),\n    evaluator=evaluator,\n    initial_prompts=6,\n    n_iterations=100,\n    max_examples=30,\n    verbose=True,\n    prompt_model=gpt4,\n)\n\nPrevious\nDSPy Assertions\nNext\nTutorials\nExecuting Typed Predictors\nDefining Input and Output Models\nCreating Typed Predictor\nI/O in Typed Predictors\nTyped Chain of Thoughts with dspy.TypedChainOfThought\nTyped Predictors as Decorators\nComposing Functional Typed Predictors in dspy.Module\nOptimizing Typed Predictors\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Additional Resources | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/tutorials/other_tutorial",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\n[01] RAG: Retrieval-Augmented Generation\n[02] Multi-Hop Question Answering\nCommunity Examples\nAdditional Resources\nFAQs\nDSPy Cheatsheet\nDeep Dive\nTutorialsAdditional Resources\nAdditional Resources\nTutorials​\nLevel\tTutorial\tRun in Colab\tDescription\nBeginner\tGetting Started\t\tIntroduces the basic building blocks in DSPy. Tackles the task of complex question answering with HotPotQA.\nBeginner\tMinimal Working Example\tN/A\tBuilds and optimizes a very simple chain-of-thought program in DSPy for math question answering. Very short.\nBeginner\tCompiling for Tricky Tasks\tN/A\tTeaches LMs to reason about logical statements and negation. Uses GPT-4 to bootstrap few-shot CoT demonstrations for GPT-3.5. Establishes a state-of-the-art result on ScoNe. Contributed by Chris Potts.\nBeginner\tLocal Models & Custom Datasets\t\tIllustrates two different things together: how to use local models (Llama-2-13B in particular) and how to use your own data examples for training and development.\nIntermediate\tThe DSPy Paper\tN/A\tSections 3, 5, 6, and 7 of the DSPy paper can be consumed as a tutorial. They include explained code snippets, results, and discussions of the abstractions and API.\nIntermediate\tDSPy Assertions\t\tIntroduces example of applying DSPy Assertions while generating long-form responses to questions with citations. Presents comparative evaluation in both zero-shot and compiled settings.\nIntermediate\tFinetuning for Complex Programs\t\tTeaches a local T5 model (770M) to do exceptionally well on HotPotQA. Uses only 200 labeled answers. Uses no hand-written prompts, no calls to OpenAI, and no labels for retrieval or reasoning.\nAdvanced\tInformation Extraction\t\tTackles extracting information from long articles (biomedical research papers). Combines in-context learning and retrieval to set SOTA on BioDEX. Contributed by Karel D’Oosterlinck.\nResources​\nDSPy talk at ScaleByTheBay Nov 2023.\nDSPy webinar with MLOps Learners, a bit longer with Q&A.\nHands-on Overviews of DSPy by the community: DSPy Explained! by Connor Shorten, DSPy explained by code_your_own_ai, DSPy Crash Course by AI Bites\nInterviews: Weaviate Podcast in-person, and you can find 6-7 other remote podcasts on YouTube from a few different perspectives/audiences.\nTracing in DSPy with Arize Phoenix: Tutorial for tracing your prompts and the steps of your DSPy programs\nTracing & Optimization Tracking in DSPy with Parea AI: Tutorial on tracing & evaluating a DSPy RAG program\nPrevious\nCommunity Examples\nNext\nFAQs\nTutorials\nResources\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Community Examples | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/tutorials/examples",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\n[01] RAG: Retrieval-Augmented Generation\n[02] Multi-Hop Question Answering\nCommunity Examples\nAdditional Resources\nFAQs\nDSPy Cheatsheet\nDeep Dive\nTutorialsCommunity Examples\nCommunity Examples\n\nThe DSPy team believes complexity has to be justified. We take this seriously: we never release a complex tutorial (above) or example (below) unless we can demonstrate empirically that this complexity has generally led to improved quality or cost. This kind of rule is rarely enforced by other frameworks or docs, but you can count on it in DSPy examples.\n\nThere's a bunch of examples in the examples/ directory and in the top-level directory. We welcome contributions!\n\nYou can find other examples tweeted by @lateinteraction on Twitter/X.\n\nSome other examples (not exhaustive, feel free to add more via PR):\n\nApplying DSPy Assertions\nLong-form Answer Generation with Citations, by Arnav Singhvi\nGenerating Answer Choices for Quiz Questions, by Arnav Singhvi\nGenerating Tweets for QA, by Arnav Singhvi\nCompiling LCEL runnables from LangChain in DSPy\nAI feedback, or writing LM-based metrics in DSPy\nDSPy Optimizers Benchmark on a bunch of different tasks, by Michael Ryan\nIndian Languages NLI with gains due to compiling by Saiful Haq\nSophisticated Extreme Multi-Class Classification, IReRa, by Karel D’Oosterlinck\nDSPy on BIG-Bench Hard Example, by Chris Levy\nUsing Ollama with DSPy for Mistral (quantized) by @jrknox1977\nUsing DSPy, \"The Unreasonable Effectiveness of Eccentric Automatic Prompts\" (paper) by VMware's Rick Battle & Teja Gollapudi, and interview at TheRegister\n\nThere are also recent cool examples at Weaviate's DSPy cookbook by Connor Shorten. See tutorial on YouTube.\n\nPrevious\n[02] Multi-Hop Question Answering\nNext\nAdditional Resources\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "[02] Multi-Hop Question Answering | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/tutorials/simplified-baleen",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\n[01] RAG: Retrieval-Augmented Generation\n[02] Multi-Hop Question Answering\nCommunity Examples\nAdditional Resources\nFAQs\nDSPy Cheatsheet\nDeep Dive\nTutorials[02] Multi-Hop Question Answering\n[02] Multi-Hop Question Answering\n\nA single search query is often not enough for complex QA tasks. For instance, an example within HotPotQA includes a question about the birth city of the writer of \"Right Back At It Again\". A search query often identifies the author correctly as \"Jeremy McKinnon\", but lacks the capability to compose the intended answer in determining when he was born.\n\nThe standard approach for this challenge in retrieval-augmented NLP literature is to build multi-hop search systems, like GoldEn (Qi et al., 2019) and Baleen (Khattab et al., 2021). These systems read the retrieved results and then generate additional queries to gather additional information when necessary before arriving to a final answer. Using DSPy, we can easily simulate such systems in a few lines of code.\n\nConfiguring LM and RM​\n\nWe'll start by setting up the language model (LM) and retrieval model (RM), which DSPy supports through multiple LM and RM APIs and local models hosting.\n\nIn this notebook, we'll work with GPT-3.5 (gpt-3.5-turbo) and the ColBERTv2 retriever (a free server hosting a Wikipedia 2017 \"abstracts\" search index containing the first paragraph of each article from this 2017 dump). We configure the LM and RM within DSPy, allowing DSPy to internally call the respective module when needed for generation or retrieval.\n\nimport dspy\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\ndspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)\n\nLoading the Dataset​\n\nFor this tutorial, we make use of the mentioned HotPotQA dataset, a collection of complex question-answer pairs typically answered in a multi-hop fashion. We can load this dataset provided by DSPy through the HotPotQA class:\n\nfrom dspy.datasets import HotPotQA\n\n# Load the dataset.\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n\nlen(trainset), len(devset)\n\n\nOutput:\n\n(20, 50)\n\nBuilding Signature​\n\nNow that we have the data loaded let's start defining the signatures for sub-tasks of out Baleen pipeline.\n\nWe'll start by creating the GenerateAnswer signature that'll take context and question as input and give answer as output.\n\nclass GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n\nUnlike usual QA pipelines, we have an intermediate question-generation step in Baleen for which we'll need to define a new Signature for the \"hop\" behavior: inputting some context and a question to generate a search query to find missing information.\n\nclass GenerateSearchQuery(dspy.Signature):\n    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    query = dspy.OutputField()\n\nINFO\n\nWe could have written context = GenerateAnswer.signature.context to avoid duplicating the description of the context field.\n\nNow that we have the necessary signatures in place, we can start building the Baleen pipeline!\n\nBuilding the Pipeline​\n\nSo, let's define the program itself SimplifiedBaleen. There are many possible ways to implement this, but we'll keep this version down to the key elements.\n\nfrom dsp.utils import deduplicate\n\nclass SimplifiedBaleen(dspy.Module):\n    def __init__(self, passages_per_hop=3, max_hops=2):\n        super().__init__()\n\n        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n        self.max_hops = max_hops\n    \n    def forward(self, question):\n        context = []\n        \n        for hop in range(self.max_hops):\n            query = self.generate_query[hop](context=context, question=question).query\n            passages = self.retrieve(query).passages\n            context = deduplicate(context + passages)\n\n        pred = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=pred.answer)\n\n\nAs we can see, the __init__ method defines a few key sub-modules:\n\ngenerate_query: For each hop, we will have one dspy.ChainOfThought predictor with the GenerateSearchQuery signature.\nretrieve: This module will conduct the search using the generated queries over our defined ColBERT RM search index via the dspy.Retrieve module.\ngenerate_answer: This dspy.Predict module will be used with the GenerateAnswer signature to produce the final answer.\n\nThe forward method uses these sub-modules in simple control flow.\n\nFirst, we'll loop up to self.max_hops times.\nIn each iteration, we'll generate a search query using the predictor at self.generate_query[hop].\nWe'll retrieve the top-k passages using that query.\nWe'll add the (deduplicated) passages to our context accumulator.\nAfter the loop, we'll use self.generate_answer to produce an answer.\nWe'll return a prediction with the retrieved context and predicted answer.\nExecuting the Pipeline​\n\nLet's execute this program in its zero-shot (uncompiled) setting.\n\nThis doesn't necessarily imply the performance will be bad but rather that we're bottlenecked directly by the reliability of the underlying LM to understand our sub-tasks from minimal instructions. Often, this is perfectly fine when using the most expensive/powerful models (e.g., GPT-4) on the easiest and most standard tasks (e.g., answering simple questions about popular entities).\n\n# Ask any question you like to this simple RAG program.\nmy_question = \"How many storeys are in the castle that David Gregory inherited?\"\n\n# Get the prediction. This contains `pred.context` and `pred.answer`.\nuncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\npred = uncompiled_baleen(my_question)\n\n# Print the contexts and the answer.\nprint(f\"Question: {my_question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\nprint(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")\n\n\nOutput:\n\nQuestion: How many storeys are in the castle that David Gregory inherited?\nPredicted Answer: five\nRetrieved Contexts (truncated): ['David Gregory (physician) | David Gregory (20 December 1625 – 1720) was a Scottish physician and inventor. His surname is sometimes spelt as Gregorie, the original Scottish spelling. He inherited Kinn...', 'The Boleyn Inheritance | The Boleyn Inheritance is a novel by British author Philippa Gregory which was first published in 2006. It is a direct sequel to her previous novel \"The Other Boleyn Girl,\" an...', 'Gregory of Gaeta | Gregory was the Duke of Gaeta from 963 until his death. He was the second son of Docibilis II of Gaeta and his wife Orania. He succeeded his brother John II, who had left only daugh...', 'Kinnairdy Castle | Kinnairdy Castle is a tower house, having five storeys and a garret, two miles south of Aberchirder, Aberdeenshire, Scotland. The alternative name is Old Kinnairdy....', 'Kinnaird Head | Kinnaird Head (Scottish Gaelic: \"An Ceann Àrd\" , \"high headland\") is a headland projecting into the North Sea, within the town of Fraserburgh, Aberdeenshire on the east coast of Scotla...', 'Kinnaird Castle, Brechin | Kinnaird Castle is a 15th-century castle in Angus, Scotland. The castle has been home to the Carnegie family, the Earl of Southesk, for more than 600 years....']\n\n\nWe can inspect the last three calls to the LM (i.e., generating the first hop's query, generating the second hop's query, and generating the answer) using:\n\nturbo.inspect_history(n=3)\n\nOptimizing the Pipeline​\n\nHowever, a zero-shot approach quickly falls short for more specialized tasks, novel domains/settings, and more efficient (or open) models.\n\nTo address this, DSPy offers compilation. Let's compile our multi-hop (SimplifiedBaleen) program.\n\nLet's first define our validation logic for compilation:\n\nThe predicted answer matches the gold answer.\nThe retrieved context contains the gold answer.\nNone of the generated queries is rambling (i.e., none exceeds 100 characters in length).\nNone of the generated queries is roughly repeated (i.e., none is within 0.8 or higher F1 score of earlier queries).\ndef validate_context_and_answer_and_hops(example, pred, trace=None):\n    if not dspy.evaluate.answer_exact_match(example, pred): return False\n    if not dspy.evaluate.answer_passage_match(example, pred): return False\n\n    hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]\n\n    if max([len(h) for h in hops]) > 100: return False\n    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False\n\n    return True\n\n\nWe'll use one of the most basic teleprompters in DSPy, namely, BootstrapFewShot to optimize the predictors in pipeline with few-shot examples.\n\nfrom dspy.teleprompt import BootstrapFewShot\n\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)\ncompiled_baleen = teleprompter.compile(SimplifiedBaleen(), teacher=SimplifiedBaleen(passages_per_hop=2), trainset=trainset)\n\nEvaluating the Pipeline​\n\nLet's now define our evaluation function and compare the performance of the uncompiled and compiled Baleen pipelines. While this devset does not serve as a completely reliable benchmark, it is instructive to use for this tutorial.\n\nfrom dspy.evaluate.evaluate import Evaluate\n\n# Define metric to check if we retrieved the correct documents\ndef gold_passages_retrieved(example, pred, trace=None):\n    gold_titles = set(map(dspy.evaluate.normalize_text, example[\"gold_titles\"]))\n    found_titles = set(\n        map(dspy.evaluate.normalize_text, [c.split(\" | \")[0] for c in pred.context])\n    )\n    return gold_titles.issubset(found_titles)\n\n# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\nevaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n\nuncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(uncompiled_baleen, metric=gold_passages_retrieved, display=False)\n\ncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(compiled_baleen, metric=gold_passages_retrieved)\n\nprint(f\"## Retrieval Score for uncompiled Baleen: {uncompiled_baleen_retrieval_score}\")\nprint(f\"## Retrieval Score for compiled Baleen: {compiled_baleen_retrieval_score}\")\n\n\nOutput:\n\n## Retrieval Score for uncompiled Baleen: 36.0\n## Retrieval Score for compiled Baleen: 60.0\n\n\nExcellent! There might be something to this compiled, multi-hop program then.\n\nEarlier, we said simple programs are not very effective at finding all evidence required for answering each question. Is this resolved by the adding some greater prompting techniques in the forward function of SimplifiedBaleen? Does compiling programs improve performance?\n\nWhile in our tutorial we demonstrate our findings, the answer for these questions will not always be obvious. However, DSPy makes it extremely easy to try out the many diverse approaches with minimal effort.\n\nNow that you've seen a example of how to build a simple yet powerful pipeline, it's time for you to build one yourself!\n\nPrevious\n[01] RAG: Retrieval-Augmented Generation\nNext\nCommunity Examples\nConfiguring LM and RM\nLoading the Dataset\nBuilding Signature\nBuilding the Pipeline\nExecuting the Pipeline\nOptimizing the Pipeline\nEvaluating the Pipeline\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "[01] RAG: Retrieval-Augmented Generation | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/tutorials/rag",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\n[01] RAG: Retrieval-Augmented Generation\n[02] Multi-Hop Question Answering\nCommunity Examples\nAdditional Resources\nFAQs\nDSPy Cheatsheet\nDeep Dive\nTutorials[01] RAG: Retrieval-Augmented Generation\n[01] RAG: Retrieval-Augmented Generation\n\nRetrieval-augmented generation (RAG) is an approach that allows LLMs to tap into a large corpus of knowledge from sources and query its knowledge store to find relevant passages/content and produce a well-refined response.\n\nRAG ensures LLMs can dynamically utilize real-time knowledge even if not originally trained on the subject and give thoughtful answers. However, with this nuance comes greater complexities in setting up refined RAG pipelines. To reduce these intricacies, we turn to DSPy, which offers a seamless approach to setting up prompting pipelines!\n\nConfiguring LM and RM​\n\nWe'll start by setting up the language model (LM) and retrieval model (RM), which DSPy supports through multiple LM and RM APIs and local models hosting.\n\nIn this notebook, we'll work with GPT-3.5 (gpt-3.5-turbo) and the ColBERTv2 retriever (a free server hosting a Wikipedia 2017 \"abstracts\" search index containing the first paragraph of each article from this 2017 dump). We configure the LM and RM within DSPy, allowing DSPy to internally call the respective module when needed for generation or retrieval.\n\nimport dspy\n\nturbo = dspy.OpenAI(model='gpt-3.5-turbo')\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n\ndspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)\n\nLoading the Dataset​\n\nFor this tutorial, we make use of the HotPotQA dataset, a collection of complex question-answer pairs typically answered in a multi-hop fashion. We can load this dataset provided by DSPy through the HotPotQA class:\n\nfrom dspy.datasets import HotPotQA\n\n# Load the dataset.\ndataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n\n# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\ntrainset = [x.with_inputs('question') for x in dataset.train]\ndevset = [x.with_inputs('question') for x in dataset.dev]\n\nlen(trainset), len(devset)\n\n\nOutput:\n\n(20, 50)\n\nBuilding Signatures​\n\nNow that we have the data loaded, let's start defining the signatures for the sub-tasks of our pipeline.\n\nWe can identify our simple input question and output answer, but since we are building out a RAG pipeline, we wish to utilize some contextual information from our ColBERT corpus. So let's define our signature: context, question --> answer.\n\nclass GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\n\nWe include small descriptions for the context and answer fields to define more robust guidelines on what the model will receive and should generate.\n\nBuilding the Pipeline​\n\nWe will build our RAG pipeline as a DSPy module which will require two methods:\n\nThe __init__ method will simply declare the sub-modules it needs: dspy.Retrieve and dspy.ChainOfThought. The latter is defined to implement our GenerateAnswer signature.\nThe forward method will describe the control flow of answering the question using the modules we have: Given a question, we'll search for the top-3 relevant passages and then feed them as context for answer generation.\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n    \n    def forward(self, question):\n        context = self.retrieve(question).passages\n        prediction = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=prediction.answer)\n\nOptimizing the Pipeline​\nCompiling the RAG program​\n\nHaving defined this program, let's now compile it. Compiling a program will update the parameters stored in each module. In our setting, this is primarily in the form of collecting and selecting good demonstrations for inclusion within the prompt(s).\n\nCompiling depends on three things:\n\nA training set. We'll just use our 20 question–answer examples from trainset above.\nA metric for validation. We'll define a simple validate_context_and_answer that checks that the predicted answer is correct and that the retrieved context actually contains the answer.\nA specific teleprompter. The DSPy compiler includes a number of teleprompters that can optimize your programs.\nfrom dspy.teleprompt import BootstrapFewShot\n\n# Validation logic: check that the predicted answer is correct.\n# Also check that the retrieved context does actually contain that answer.\ndef validate_context_and_answer(example, pred, trace=None):\n    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n    return answer_EM and answer_PM\n\n# Set up a basic teleprompter, which will compile our RAG program.\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n\n# Compile!\ncompiled_rag = teleprompter.compile(RAG(), trainset=trainset)\n\nINFO\n\nTeleprompters: Teleprompters are powerful optimizers that can take any program and learn to bootstrap and select effective prompts for its modules. Hence the name which means \"prompting at a distance\".\n\nDifferent teleprompters offer various tradeoffs in terms of how much they optimize cost versus quality, etc. We will used a simple default BootstrapFewShot in the example above.\n\nIf you're into analogies, you could think of this as your training data, your loss function, and your optimizer in a standard DNN supervised learning setup. Whereas SGD is a basic optimizer, there are more sophisticated (and more expensive!) ones like Adam or RMSProp.\n\nExecuting the Pipeline​\n\nNow that we've compiled our RAG program, let's try it out.\n\n# Ask any question you like to this simple RAG program.\nmy_question = \"What castle did David Gregory inherit?\"\n\n# Get the prediction. This contains `pred.context` and `pred.answer`.\npred = compiled_rag(my_question)\n\n# Print the contexts and the answer.\nprint(f\"Question: {my_question}\")\nprint(f\"Predicted Answer: {pred.answer}\")\nprint(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")\n\n\nExcellent. How about we inspect the last prompt for the LM?\n\nturbo.inspect_history(n=1)\n\n\nOutput:\n\nAnswer questions with short factoid answers.\n\n---\n\nQuestion: At My Window was released by which American singer-songwriter?\nAnswer: John Townes Van Zandt\n\nQuestion: \"Everything Has Changed\" is a song from an album released under which record label ?\nAnswer: Big Machine Records\n...(truncated)\n\n\nEven though we haven't written any of this detailed demonstrations, we see that DSPy was able to bootstrap this 3,000 token prompt for 3-shot retrieval-augmented generation with hard negative passages and uses Chain-of-Thought reasoning within an extremely simply-written program.\n\nThis illustrates the power of composition and learning. Of course, this was just generated by a particular teleprompter, which may or may not be perfect in each setting. As you'll see in DSPy, there is a large but systematic space of options you have to optimize and validate with respect to your program's quality and cost.\n\nYou can also easily inspect the learned objects themselves.\n\nfor name, parameter in compiled_rag.named_predictors():\n    print(name)\n    print(parameter.demos[0])\n    print()\n\nEvaluating the Pipeline​\n\nWe can now evaluate our compiled_rag program on the dev set. Of course, this tiny set is not meant to be a reliable benchmark, but it'll be instructive to use it for illustration.\n\nLet's evaluate the accuracy (exact match) of the predicted answer.\n\nfrom dspy.evaluate.evaluate import Evaluate\n\n# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\nevaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=False, display_table=5)\n\n# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\nmetric = dspy.evaluate.answer_exact_match\nevaluate_on_hotpotqa(compiled_rag, metric=metric)\n\n\nOutput:\n\nAverage Metric: 22 / 50  (44.0): 100%|██████████| 50/50 [00:00<00:00, 116.45it/s]\nAverage Metric: 22 / 50  (44.0%)\n\n44.0\n\nEvaluating the Retrieval​\n\nIt may also be instructive to look at the accuracy of retrieval. While there are multiple ways to do this, we can simply check whether the retrieved passages contain the answer.\n\nWe can make use of our dev set which includes the gold titles that should be retrieved.\n\ndef gold_passages_retrieved(example, pred, trace=None):\n    gold_titles = set(map(dspy.evaluate.normalize_text, example['gold_titles']))\n    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(' | ')[0] for c in pred.context]))\n\n    return gold_titles.issubset(found_titles)\n\ncompiled_rag_retrieval_score = evaluate_on_hotpotqa(compiled_rag, metric=gold_passages_retrieved)\n\n\nOutput:\n\nAverage Metric: 13 / 50  (26.0): 100%|██████████| 50/50 [00:00<00:00, 671.76it/s]Average Metric: 13 / 50  (26.0%)\n\n\nAlthough this simple compiled_rag program is able to answer a decent fraction of the questions correctly (on this tiny set, over 40%), the quality of retrieval is much lower.\n\nThis potentially suggests that the LM is often relying on the knowledge it memorized during training to answer questions. To address this weak retrieval, let's explore a second program that involves more advanced search behavior.\n\nPrevious\nTutorials\nNext\n[02] Multi-Hop Question Answering\nConfiguring LM and RM\nLoading the Dataset\nBuilding Signatures\nBuilding the Pipeline\nOptimizing the Pipeline\nExecuting the Pipeline\nEvaluating the Pipeline\nEvaluating the Retrieval\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Deep Dive | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/deep-dive",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nData Handling\nSignatures\nModules\nTyped Predictors\nLanguage Model Clients\nRetrieval Model Clients\nTeleprompters\nDeep Dive\nDeep Dive\n\nDSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and optimizers that can tune the prompts (or weights) of modules.\n\n🗃️ Data Handling\n\n3 items\n\n🗃️ Signatures\n\n2 items\n\n🗃️ Modules\n\n6 items\n\n🗃️ Typed Predictors\n\n2 items\n\n🗃️ Language Model Clients\n\n3 items\n\n🗃️ Retrieval Model Clients\n\n5 items\n\n🗃️ Teleprompters\n\n2 items\n\nPrevious\nDSPy Cheatsheet\nNext\nData Handling\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "FAQs | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/faqs",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nFAQs\nFAQs\nIs DSPy right for me? DSPy vs. other frameworks​\n\nThe DSPy philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when DSPy is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable yes. If you're a practitioner doing other things, please read on.\n\nDSPy vs. thin wrappers for prompts (OpenAI API, MiniChain, basic templating) In other words: Why can't I just write my prompts directly as string templates? Well, for extremely simple settings, this might work just fine. (If you're familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.) However, when you need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components. DSPy cleanly abstracts away (and powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the same program expressed in 10 or 20 lines of DSPy can easily be compiled into multi-stage instructions for GPT-4, detailed prompts for Llama2-13b, or finetunes for T5-base. Oh, and you wouldn't need to maintain long, brittle, model-specific strings at the core of your project anymore.\n\nDSPy vs. application development libraries like LangChain, LlamaIndex LangChain and LlamaIndex target high-level application development; they offer batteries-included, pre-built application modules that plug in with your data or configuration. If you'd be happy to use a generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL, you will find a rich ecosystem in these libraries. DSPy doesn't internally contain hand-crafted prompts that target specific applications. Instead, DSPy introduces a small set of much more powerful and general-purpose modules that can learn to prompt (or finetune) your LM within your pipeline on your data. when you change your data, make tweaks to your program's control flow, or change your target LM, the DSPy compiler can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that DSPy obtains the highest quality for your task, with the least effort, provided you're willing to implement (or extend) your own short program. In short, DSPy is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing DSPy) and HuggingFace Transformers (i.e., representing the higher-level libraries).\n\nDSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines These are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression. This is very useful in many settings, but it's generally focused on low-level, structured control of a single LM call. It doesn't help ensure the JSON (or structured output) you get is going to be correct or useful for your task. In contrast, DSPy automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured outputs. That said, we are considering allowing Signatures in DSPy to express regex-like constraints that are implemented by these libraries.\n\nBasic Usage​\n\nHow should I use DSPy for my task? We wrote a eight-step guide on this. In short, using DSPy is an iterative process. You first define your task and the metrics you want to maximize, and prepare a few example inputs — typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers (modules) to use, giving each layer a signature (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy optimizer to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.\n\nHow do I convert my complex prompt into a DSPy pipeline? See the same answer above.\n\nWhat do DSPy optimizers tune? Or, what does compiling actually do? Each optimizer is different, but they all seek to maximize a metric on your program by updating prompts or LM weights. Current DSPy optimizers can inspect your data, simulate traces through your program to generate good/bad examples of each step, propose or refine instructions for each step based on past results, finetune the weights of your LM on self-generated examples, or combine several of these to improve quality or cut cost. We'd love to merge new optimizers that explore a richer space: most manual steps you currently go through for prompt engineering, \"synthetic data\" generation, or self-improvement can probably generalized into a DSPy optimizer that acts on arbitrary LM programs.\n\nOther FAQs. We welcome PRs to add formal answers to each of these here. You will find the answer in existing issues, tutorials, or the papers for all or most of these.\n\nHow do I get multiple outputs?\n\nYou can specify multiple output fields. For the short-form signature, you can list multiple outputs as comma separated values, following the \"->\" indicator (e.g. \"inputs -> output1, output2\"). For the long-form signature, you can include multiple dspy.OutputFields.\n\nHow can I work with long responses?\n\nYou can specify the generation of long responses as a dspy.OutputField. To ensure comprehensive checks of the content within the long-form generations, you can indicate the inclusion of citations per referenced context. Such constraints such as response length or citation inclusion can be stated through Signature descriptions, or concretely enforced through DSPy Assertions. Check out the LongFormQA notebook to learn more about Generating long-form length responses to answer questions.\n\nHow can I ensure that DSPy doesn't strip new line characters from my inputs or outputs?\n\nDSPy uses Signatures to format prompts passed into LMs. In order to ensure that new line characters aren't stripped from longer inputs, you must specify format=str when creating a field.\n\nclass UnstrippedSignature(dspy.Signature):\n    \"\"\"Enter some information for the model here.\"\"\"\n\n    title = dspy.InputField()\n    object = dspy.InputField(format=str)\n    result = dspy.OutputField(format=str)\n\n\nobject can now be a multi-line string without issue.\n\nHow do I define my own metrics? Can metrics return a float?\n\nYou can define metrics as simply Python functions that process model generations and evaluate them based on user-defined requirements. Metrics can compare existent data (e.g. gold labels) to model predictions or they can be used to assess various components of an output using validation feedback from LMs (e.g. LLMs-as-Judges). Metrics can return bool, int, and float types scores. Check out the official Metrics docs to learn more about defining custom metrics and advanced evaluations using AI feedback and/or DSPy programs.\n\nHow expensive or slow is compiling??\n\nTo reflect compiling metrics, we highlight an experiment for reference, compiling the SimplifiedBaleen using the dspy.BootstrapFewShotWithRandomSearch optimizer on the gpt-3.5-turbo-1106 model over 7 candidate programs and 10 threads. We report that compiling this program takes around 6 minutes with 3200 API calls, 2.7 million input tokens and 156,000 output tokens, reporting a total cost of $3 USD (at the current pricing of the OpenAI model).\n\nCompiling DSPy optimizers naturally will incur additional LM calls, but we substantiate this overhead with minimalistic executions with the goal of maximizing performance. This invites avenues to enhance performance of smaller models by compiling DSPy programs with larger models to learn enhanced behavior during compile-time and propagate such behavior to the tested smaller model during inference-time.\n\nDeployment or Reproducibility Concerns​\nHow do I save a checkpoint of my compiled program?\n\nHere is an example of saving/loading a compiled module:\n\ncot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)\n\n#Saving\ncot_compiled.save('compiled_cot_gsm8k.json')\n\n#Loading:\ncot = CoT()\ncot.load('compiled_cot_gsm8k.json')\n\nHow do I export for deployment?\n\nExporting DSPy programs is simply saving them as highlighted above!\n\nHow do I search my own data?\n\nOpen source libraries such as RAGautouille enable you to search for your own data through advanced retrieval models like ColBERT with tools to embdeed and index documents. Feel free to integrate such libraries to create searchable datasets while developing your DSPy programs!\n\nHow do I turn off the cache? How do I export the cache?\n\nYou can turn off the cache by setting the DSP_CACHEBOOL environment variable to False, which disables the cache_turn_on flag.\n\nYour local cache will be saved to the global env directory os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] which you can usually set to os.path.join(repo_path, 'cache') and export this cache from here.\n\nAdvanced Usage​\n\nHow do I parallelize? You can parallelize DSPy programs during both compilation and evaluation by specifying multiple thread settings in the respective DSPy optimizers or within the dspy.Evaluate utility function.\n\nHow do freeze a module?\n\nModules can be frozen by setting their ._compiled attribute to be True, indicating the module has gone through optimizer compilation and should not have its parameters adjusted. This is handled internally in optimizers such as dspy.BootstrapFewShot where the student program is ensured to be frozen before the teacher propagates the gathered few-shot demonstrations in the bootstrapping process.\n\nHow do I get JSON output?\n\nYou can specify JSON-type descriptions in the desc field of the long-form signature dspy.OutputField (e.g. output = dspy.OutputField(desc='key-value pairs')).\n\nIf you notice outputs are still not conforming to JSON formatting, try Asserting this constraint! Check out Assertions (or the next question!)\n\nHow do I use DSPy assertions?\n\na) How to Add Assertions to Your Program:\n\nDefine Constraints: Use dspy.Assert and/or dspy.Suggest to define constraints within your DSPy program. These are based on boolean validation checks for the outcomes you want to enforce, which can simply be Python functions to validate the model outputs.\nIntegrating Assertions: Keep your Assertion statements following a model generations (hint: following a module layer)\n\nb) How to Activate the Assertions:\n\nUsing assert_transform_module:\nWrap your DSPy module with assertions using the assert_transform_module function, along with a backtrack_handler. This function transforms your program to include internal assertions backtracking and retry logic, which can be customized as well: program_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)\nActivate Assertions:\nDirectly call activate_assertions on your DSPy program with assertions: program_with_assertions = ProgramWithAssertions().activate_assertions()\n\nNote: To use Assertions properly, you must activate a DSPy program that includes dspy.Assert or dspy.Suggest statements from either of the methods above.\n\nErrors​\nHow do I deal with \"context too long\" errors?\n\nIf you're dealing with \"context too long\" errors in DSPy, you're likely using DSPy optimizers to include demonstrations within your prompt, and this is exceeding your current context window. Try reducing these parameters (e.g. max_bootstrapped_demos and max_labeled_demos). Additionally, you can also reduce the number of retrieved passages/docs/embeddings to ensure your prompt is fitting within your model context length.\n\nA more general fix is simply increasing the number of max_tokens specified to the LM request (e.g. lm = dspy.OpenAI(model = ..., max_tokens = ...).\n\nHow do I deal with timeouts or backoff errors?\n\nFirstly, please refer to your LM/RM provider to ensure stable status or sufficient rate limits for your use case!\n\nAdditionally, try reducing the number of threads you are testing on as the corresponding servers may get overloaded with requests and trigger a backoff + retry mechanism.\n\nIf all variables seem stable, you may be experiencing timeouts or backoff errors due to incorrect payload requests sent to the api providers. Please verify your arguments are compatible with the SDK you are interacting with. At times, DSPy may have hard-coded arguments that are not relevant for your compatible, in which case, please free to open a PR alerting this or comment out these default settings for your usage.\n\nContributing​\n\nWhat if I have a better idea for prompting or synthetic data generation? Perfect. We encourage you to think if it's best expressed as a module or an optimizer, and we'd love to merge it in DSPy so everyone can use it. DSPy is not a complete project; it's an ongoing effort to create structure (modules and optimizers) in place of hacky prompt and pipeline engineering tricks.\n\nHow can I add my favorite LM or vector store?\n\nCheck out these walkthroughs on setting up a Custom LM client and Custom RM client.\n\nPrevious\nAdditional Resources\nNext\nDSPy Cheatsheet\nIs DSPy right for me? DSPy vs. other frameworks\nBasic Usage\nDeployment or Reproducibility Concerns\nAdvanced Usage\nErrors\nContributing\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "DSPy Building Blocks | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/dspy-building-blocks",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nUsing DSPy in 8 Steps\nLanguage Models\nSignatures\nModules\nData\nMetrics\nOptimizers (formerly Teleprompters)\nDSPy Assertions\nTyped Predictors\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Building Blocks\nDSPy Building Blocks\n\nDSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and optimizers that can tune the prompts (or weights) of modules.\n\n📄️ Using DSPy in 8 Steps\n\nUsing DSPy well for solving a new task is just doing good machine learning with LMs.\n\n📄️ Language Models\n\nThe most powerful features in DSPy revolve around algorithmically optimizing the prompts (or weights) of LMs, especially when you're building programs that use the LMs within a pipeline.\n\n📄️ Signatures\n\nWhen we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.\n\n📄️ Modules\n\nA DSPy module is a building block for programs that use LMs.\n\n📄️ Data\n\nDSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n\n📄️ Metrics\n\nDSPy is a machine learning framework, so you must think about your automatic metrics for evaluation (to track your progress) and optimization (so DSPy can make your programs more effective).\n\n📄️ Optimizers (formerly Teleprompters)\n\nA DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.\n\n📄️ DSPy Assertions\n\nIntroduction\n\n📄️ Typed Predictors\n\nIn DSPy Signatures, we have InputField and OutputField that define the nature of inputs and outputs of the field. However, the inputs and output to these fields are always str-typed, which requires input and output string processing.\n\nPrevious\nMinimal Working Example\nNext\nUsing DSPy in 8 Steps\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Quick Start | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/quick-start",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nInstallation\nMinimal Working Example\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nQuick Start\nQuick Start\n\nGetting started with DSPy, for building and optimizing LM pipelines.\n\n📄️ Installation\n\nTo install DSPy run:\n\n📄️ Minimal Working Example\n\nIn this post, we walk you through a minimal working example using the DSPy library.\n\nPrevious\nAbout DSPy\nNext\nInstallation\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "DSPy Cheatsheet | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/cheatsheet",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nDSPy Cheatsheet\nDSPy Cheatsheet\n\nThis page will contain snippets for frequent usage patterns.\n\nDSPy DataLoaders​\n\nImport and initializing a DataLoader Object:\n\nimport dspy\nfrom dspy.datasets import DataLoader\n\ndl = DataLoader()\n\nLoading from HuggingFace Datasets​\ncode_alpaca = dl.from_huggingface(\"HuggingFaceH4/CodeAlpaca_20K\")\n\n\nYou can access the dataset of the splits by calling key of the corresponding split:\n\ntrain_dataset = code_alpaca['train']\ntest_dataset = code_alpaca['test']\n\nLoading specific splits from HuggingFace​\n\nYou can also manually specify splits you want to include as a parameters and it'll return a dictionary where keys are splits that you specified:\n\ncode_alpaca = dl.from_huggingface(\n    \"HuggingFaceH4/CodeAlpaca_20K\",\n    split = [\"train\", \"test\"],\n)\n\nprint(f\"Splits in dataset: {code_alpaca.keys()}\")\n\n\nIf you specify a single split then dataloader will return a List of dspy.Example instead of dictionary:\n\ncode_alpaca = dl.from_huggingface(\n    \"HuggingFaceH4/CodeAlpaca_20K\",\n    split = \"train\",\n)\n\nprint(f\"Number of examples in split: {len(code_alpaca)}\")\n\n\nYou can slice the split just like you do with HuggingFace Dataset too:\n\ncode_alpaca_80 = dl.from_huggingface(\n    \"HuggingFaceH4/CodeAlpaca_20K\",\n    split = \"train[:80%]\",\n)\n\nprint(f\"Number of examples in split: {len(code_alpaca_80)}\")\n\ncode_alpaca_20_80 = dl.from_huggingface(\n    \"HuggingFaceH4/CodeAlpaca_20K\",\n    split = \"train[20%:80%]\",\n)\n\nprint(f\"Number of examples in split: {len(code_alpaca_20_80)}\")\n\nLoading specific subset from HuggingFace​\n\nIf a dataset has a subset you can pass it as an arg like you do with load_dataset in HuggingFace:\n\ngms8k = dl.from_huggingface(\n    \"gsm8k\",\n    \"main\",\n    input_keys = (\"question\",),\n)\n\nprint(f\"Keys present in the returned dict: {list(gms8k.keys())}\")\n\nprint(f\"Number of examples in train set: {len(gms8k['train'])}\")\nprint(f\"Number of examples in test set: {len(gms8k['test'])}\")\n\nLoading from CSV​\ndolly_100_dataset = dl.from_csv(\"dolly_subset_100_rows.csv\",)\n\n\nYou can choose only selected columns from the csv by specifying them in the arguments:\n\ndolly_100_dataset = dl.from_csv(\n    \"dolly_subset_100_rows.csv\",\n    fields=(\"instruction\", \"context\", \"response\"),\n    input_keys=(\"instruction\", \"context\")\n)\n\nSplitting a List of dspy.Example​\nsplits = dl.train_test_split(dataset, train_size=0.8) # `dataset` is a List of dspy.Example\ntrain_dataset = splits['train']\ntest_dataset = splits['test']\n\nSampling from List of dspy.Example​\nsampled_example = dl.sample(dataset, n=5) # `dataset` is a List of dspy.Example\n\nDSPy Programs​\ndspy.Signature​\nclass BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\ndspy.ChainOfThought​\ngenerate_answer = dspy.ChainOfThought(BasicQA)\n\n# Call the predictor on a particular input alongside a hint.\nquestion='What is the color of the sky?'\npred = generate_answer(question=question)\n\ndspy.ChainOfThoughtwithHint​\ngenerate_answer = dspy.ChainOfThoughtWithHint(BasicQA)\n\n# Call the predictor on a particular input alongside a hint.\nquestion='What is the color of the sky?'\nhint = \"It's what you often see during a sunny day.\"\npred = generate_answer(question=question, hint=hint)\n\ndspy.ProgramOfThought​\npot = dspy.ProgramOfThought(BasicQA)\n\nquestion = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'\nresult = pot(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Final Predicted Answer (after ProgramOfThought process): {result.answer}\")\n\ndspy.ReACT​\nreact_module = dspy.ReAct(BasicQA)\n\nquestion = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'\nresult = react_module(question=question)\n\nprint(f\"Question: {question}\")\nprint(f\"Final Predicted Answer (after ReAct process): {result.answer}\")\n\ndspy.Retrieve​\ncolbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\ndspy.settings.configure(rm=colbertv2_wiki17_abstracts)\n\n#Define Retrieve Module\nretriever = dspy.Retrieve(k=3)\n\nquery='When was the first FIFA World Cup held?'\n\n# Call the retriever on a particular query.\ntopK_passages = retriever(query).passages\n\nfor idx, passage in enumerate(topK_passages):\n    print(f'{idx+1}]', passage, '\\n')\n\nDSPy Metrics​\nFunction as Metric​\n\nTo create a custom metric you can create a function that returns either a number or a boolean value:\n\ndef parse_integer_answer(answer, only_first_line=True):\n    try:\n        if only_first_line:\n            answer = answer.strip().split('\\n')[0]\n\n        # find the last token that has a number in it\n        answer = [token for token in answer.split() if any(c.isdigit() for c in token)][-1]\n        answer = answer.split('.')[0]\n        answer = ''.join([c for c in answer if c.isdigit()])\n        answer = int(answer)\n\n    except (ValueError, IndexError):\n        # print(answer)\n        answer = 0\n    \n    return answer\n\n# Metric Function\ndef gsm8k_metric(gold, pred, trace=None) -> int:\n    return int(parse_integer_answer(str(gold.answer))) == int(parse_integer_answer(str(pred.answer)))\n\nLLM as Judge​\nclass FactJudge(dspy.Signature):\n    \"\"\"Judge if the answer is factually correct based on the context.\"\"\"\n\n    context = dspy.InputField(desc=\"Context for the prediciton\")\n    question = dspy.InputField(desc=\"Question to be answered\")\n    answer = dspy.InputField(desc=\"Answer for the question\")\n    factually_correct = dspy.OutputField(desc=\"Is the answer factually correct based on the context?\", prefix=\"Factual[Yes/No]:\")\n\njudge = dspy.ChainOfThought(FactJudge)\n\ndef factuality_metric(example, pred):\n    factual = judge(context=example.context, question=example.question, answer=pred.answer)\n    return int(factual==\"Yes\")\n\nDSPy Evaluation​\nfrom dspy.evaluate import Evaluate\n\nevaluate_program = Evaluate(devset=devset, metric=your_defined_metric, num_threads=NUM_THREADS, display_progress=True, display_table=num_rows_to_display)\n\nevaluate_program(your_dspy_program)\n\nDSPy Optimizers​\nLabeledFewShot​\nfrom dspy.teleprompt import LabeledFewShot\n\nlabeled_fewshot_optimizer = LabeledFewShot(k=8)\nyour_dspy_program_compiled = labeled_fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)\n\nBootstrapFewShot​\nfrom dspy.teleprompt import BootstrapFewShot\n\nfewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=5)\n\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)\n\nUsing another LM for compilation, specifying in teacher_settings​\nfrom dspy.teleprompt import BootstrapFewShot\n\nfewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=5, teacher_settings=dict(lm=gpt4))\n\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)\n\nCompiling a compiled program - bootstrapping a bootstraped program​\nyour_dspy_program_compiledx2 = teleprompter.compile(\n    your_dspy_program,\n    teacher=your_dspy_program_compiled,\n    trainset=trainset,\n)\n\nSaving/loading a compiled program​\nsave_path = './v1.json'\nyour_dspy_program_compiledx2.save(save_path)\n\nloaded_program = YourProgramClass()\nloaded_program.load(path=save_path)\n\nBootstrapFewShotWithRandomSearch​\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\n\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset, valset=devset)\n\n\n\nOther custom configurations are similar to customizing the BootstrapFewShot optimizer.\n\nEnsemble​\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\nfrom dspy.teleprompt.ensemble import Ensemble\n\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\nyour_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset, valset=devset)\n\nensemble_optimizer = Ensemble(reduce_fn=dspy.majority)\nprograms = [x[-1] for x in your_dspy_program_compiled.candidate_programs]\nyour_dspy_program_compiled_ensemble = ensemble_optimizer.compile(programs[:3])\n\nBootstrapFinetune​\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune\n\n#Compile program on current dspy.settings.lm\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_threads=NUM_THREADS)\nyour_dspy_program_compiled = tp.compile(your_dspy_program, trainset=trainset[:some_num], valset=trainset[some_num:])\n\n#Configure model to finetune\nconfig = dict(target=model_to_finetune, epochs=2, bf16=True, bsize=6, accumsteps=2, lr=5e-5)\n\n#Compile program on BootstrapFinetune\nfinetune_optimizer = BootstrapFinetune(metric=your_defined_metric)\nfinetune_program = finetune_optimizer.compile(your_dspy_program, trainset=some_new_dataset_for_finetuning_model, **config)\n\nfinetune_program = your_dspy_program\n\n#Load program and activate model's parameters in program before evaluation\nckpt_path = \"saved_checkpoint_path_from_finetuning\"\nLM = dspy.HFModel(checkpoint=ckpt_path, model=model_to_finetune)\n\nfor p in finetune_program.predictors():\n    p.lm = LM\n    p.activated = False\n\nCOPRO​\nfrom dspy.teleprompt import COPRO\n\neval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)\n\ncopro_teleprompter = COPRO(prompt_model=model_to_generate_prompts, task_model=model_that_solves_task, metric=your_defined_metric, breadth=num_new_prompts_generated, depth=times_to_generate_prompts, init_temperature=prompt_generation_temperature, verbose=False, log_dir=logging_directory)\n\ncompiled_program_optimized_signature = copro_teleprompter.compile(your_dspy_program, trainset=trainset, eval_kwargs=eval_kwargs)\n\nMIPRO​\nfrom dspy.teleprompt import MIPRO\n\nteleprompter = MIPRO(prompt_model=model_to_generate_prompts, task_model=model_that_solves_task, metric=your_defined_metric, num_candidates=num_new_prompts_generated, init_temperature=prompt_generation_temperature)\n\nkwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)\n\ncompiled_program_optimized_bayesian_signature = teleprompter.compile(your_dspy_program, trainset=trainset, num_trials=100, max_bootstrapped_demos=3, max_labeled_demos=5, eval_kwargs=kwargs)\n\nSignature Optimizer with Types​\nfrom dspy.teleprompt.signature_opt_typed import optimize_signature\nfrom dspy.evaluate.metrics import answer_exact_match\nfrom dspy.functional import TypedChainOfThought\n\ncompiled_program = optimize_signature(\n    student=TypedChainOfThought(\"question -> answer\"),\n    evaluator=Evaluate(devset=devset, metric=answer_exact_match, num_threads=10, display_progress=True),\n    n_iterations=50,\n).program\n\nKNNFewShot​\nfrom dspy.predict import KNN\nfrom dspy.teleprompt import KNNFewShot\n\nknn_optimizer = KNNFewShot(KNN, k=3, trainset=trainset)\n\nyour_dspy_program_compiled = knn_optimizer.compile(student=your_dspy_program, trainset=trainset, valset=devset)\n\nBootstrapFewShotWithOptuna​\nfrom dspy.teleprompt import BootstrapFewShotWithOptuna\n\nfewshot_optuna_optimizer = BootstrapFewShotWithOptuna(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)\n\nyour_dspy_program_compiled = fewshot_optuna_optimizer.compile(student=your_dspy_program, trainset=trainset, valset=devset)\n\n\nOther custom configurations are similar to customizing the dspy.BootstrapFewShot optimizer.\n\nDSPy Assertions​\nIncluding dspy.Assert and dspy.Suggest statements​\ndspy.Assert(your_validation_fn(model_outputs), \"your feedback message\", target_module=\"YourDSPyModuleSignature\")\n\ndspy.Suggest(your_validation_fn(model_outputs), \"your feedback message\", target_module=\"YourDSPyModuleSignature\")\n\nActivating DSPy Program with Assertions​\n\nNote: To use Assertions properly, you must activate a DSPy program that includes dspy.Assert or dspy.Suggest statements from either of the methods above.\n\n#1. Using `assert_transform_module:\nfrom dspy.primitives.assertions import assert_transform_module, backtrack_handler\n\nprogram_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)\n\n#2. Using `activate_assertions()`\nprogram_with_assertions = ProgramWithAssertions().activate_assertions()\n\nCompiling with DSPy Programs with Assertions​\nprogram_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)\nfewshot_optimizer = BootstrapFewShotWithRandomSearch(metric = your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=6)\ncompiled_dspy_program_with_assertions = fewshot_optimizer.compile(student=program_with_assertions, teacher = program_with_assertions, trainset=trainset, valset=devset) #student can also be program_without_assertions\n\nPrevious\nFAQs\nNext\nDeep Dive\nDSPy DataLoaders\nLoading from HuggingFace Datasets\nLoading specific splits from HuggingFace\nLoading specific subset from HuggingFace\nLoading from CSV\nSplitting a List of dspy.Example\nSampling from List of dspy.Example\nDSPy Programs\ndspy.Signature\ndspy.ChainOfThought\ndspy.ChainOfThoughtwithHint\ndspy.ProgramOfThought\ndspy.ReACT\ndspy.Retrieve\nDSPy Metrics\nFunction as Metric\nLLM as Judge\nDSPy Evaluation\nDSPy Optimizers\nLabeledFewShot\nBootstrapFewShot\nBootstrapFewShotWithRandomSearch\nEnsemble\nBootstrapFinetune\nCOPRO\nMIPRO\nSignature Optimizer with Types\nKNNFewShot\nBootstrapFewShotWithOptuna\nDSPy Assertions\nIncluding dspy.Assert and dspy.Suggest statements\nActivating DSPy Program with Assertions\nCompiling with DSPy Programs with Assertions\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "Tutorials | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/category/tutorials",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\n[01] RAG: Retrieval-Augmented Generation\n[02] Multi-Hop Question Answering\nCommunity Examples\nAdditional Resources\nFAQs\nDSPy Cheatsheet\nDeep Dive\nTutorials\nTutorials\n\nStep-by-step illustrations of solving a task in DSPy.\n\n📄️ [01] RAG: Retrieval-Augmented Generation\n\nRetrieval-augmented generation (RAG) is an approach that allows LLMs to tap into a large corpus of knowledge from sources and query its knowledge store to find relevant passages/content and produce a well-refined response.\n\n📄️ [02] Multi-Hop Question Answering\n\nA single search query is often not enough for complex QA tasks. For instance, an example within HotPotQA includes a question about the birth city of the writer of \"Right Back At It Again\". A search query often identifies the author correctly as \"Jeremy McKinnon\", but lacks the capability to compose the intended answer in determining when he was born.\n\n📄️ Community Examples\n\nThe DSPy team believes complexity has to be justified. We take this seriously: we never release a complex tutorial (above) or example (below) unless we can demonstrate empirically that this complexity has generally led to improved quality or cost. This kind of rule is rarely enforced by other frameworks or docs, but you can count on it in DSPy examples.\n\n📄️ Additional Resources\n\nTutorials\n\nPrevious\nTyped Predictors\nNext\n[01] RAG: Retrieval-Augmented Generation\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  },
  {
    "title": "About DSPy | DSPy",
    "url": "https://dspy-docs.vercel.app/docs/intro",
    "html": "Skip to main content\nDSPy\nDocumentation\nTutorials\nAPI References\nDSPy Cheatsheet\nGitHub\nctrl\nK\nAbout DSPy\nQuick Start\nDSPy Building Blocks\nTutorials\nFAQs\nDSPy Cheatsheet\nDeep Dive\nAbout DSPy\nAbout DSPy\n\nDSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change.\n\nTo make this more systematic and much more powerful, DSPy does two things. First, it separates the flow of your program (modules) from the parameters (LM prompts and weights) of each step. Second, DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a metric you want to maximize.\n\nDSPy can routinely teach powerful models like GPT-3.5 or GPT-4 and local models like T5-base or Llama2-13b to be much more reliable at tasks, i.e. having higher quality and/or avoiding specific failure patterns. DSPy optimizers will \"compile\" the same program into different instructions, few-shot prompts, and/or weight updates (finetunes) for each LM. This is a new paradigm in which LMs and their prompts fade into the background as optimizable pieces of a larger system that can learn from data. tldr; less prompting, higher scores, and a more systematic approach to solving hard tasks with LMs.\n\nAnalogy to Neural Networks​\n\nWhen we build neural networks, we don't write manual for-loops over lists of hand-tuned floats. Instead, you might use a framework like PyTorch to compose layers (e.g., Convolution or Dropout) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network.\n\nDitto! DSPy gives you the right general-purpose modules (e.g., ChainOfThought, ReAct, etc.), which replace string-based prompting tricks. To replace prompt hacking and one-off synthetic data generators, DSPy also gives you general optimizers (BootstrapFewShotWithRandomSearch or MIPRO), which are algorithms that update parameters in your program. Whenever you modify your code, your data, your assertions, or your metric, you can compile your program again and DSPy will create new effective prompts that fit your changes.\n\nNext\nQuick Start\nAnalogy to Neural Networks\nDocs\nDocumentation\nAPI Reference\nCommunity\nSee all 130+ contributors on GitHub\nMore\nGitHub\nBuilt with ⌨️"
  }
]